{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade pyDOE\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFRy3Y5Rxkno",
        "outputId": "bc71f8d5-ce74-422e-af3e-9d1f71ede599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyDOE\n",
            "  Downloading pyDOE-0.3.8.zip (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pyDOE) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from pyDOE) (1.7.3)\n",
            "Building wheels for collected packages: pyDOE\n",
            "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyDOE: filename=pyDOE-0.3.8-py3-none-any.whl size=18184 sha256=9a9e04d7418afa79311af34e996466b0a1b95811fa29fb67df650606ca8e91f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/3e/fa/5705bf59c2053c17c4799c3ab66a2e356c32f40a3044fe2134\n",
            "Successfully built pyDOE\n",
            "Installing collected packages: pyDOE\n",
            "Successfully installed pyDOE-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Buidling the collocation points for the domain"
      ],
      "metadata": {
        "id": "27PvMg60fphC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "@author: #SAMAN\n",
        "'''\n",
        "# import os\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import scipy.io\n",
        "import math\n",
        "import matplotlib.gridspec as gridspec\n",
        "#from plotting import newfig\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras import layers, activations\n",
        "from scipy.interpolate import griddata\n",
        "#from eager_lbfgs import lbfgs, Struct\n",
        "from pyDOE import lhs\n",
        "N_f = 10000\n",
        "Nu1 = 200\n",
        "\n",
        "\n",
        "xmax=6.0\n",
        "x1 = (np.linspace(0, xmax, 32)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, 32)).flatten()[:, None]\n",
        "t1 = (np.linspace(0, 1, 20)).flatten()[:, None]\n",
        "\n",
        "ttt1, ttt0 = np.meshgrid(x1, y1)\n",
        "\n",
        "tt1 = np.concatenate(([ttt1.flatten()[:, None], ttt0.flatten()[:, None], np.zeros((x1.shape[0] * y1.shape[0], 1))]), axis=1)\n",
        "x_1t = np.array([tt1[:, 0]]).T\n",
        "y_1t = np.array([tt1[:, 1]]).T\n",
        "t_1t = np.array([tt1[:, 2]]).T\n",
        "ut1 = -np.sin(t_1t) * np.sin(np.pi * x_1t) * np.sin(np.pi * x_1t) * np.sin(np.pi * y_1t) * np.cos(np.pi * y_1t)\n",
        "vt1 = np.sin(t_1t) * np.sin(np.pi * x_1t) * np.cos(np.pi * x_1t) * np.sin(np.pi * y_1t) * np.sin(np.pi * y_1t)\n",
        "\n",
        "yyy1, yyy0 = np.meshgrid(x1, t1)\n",
        "\n",
        "yy1 = np.concatenate(\n",
        "    ([yyy1.flatten()[:, None], np.min(y1) * np.ones((x1.shape[0] * t1.shape[0], 1)), yyy0.flatten()[:, None]]), axis=1)\n",
        "x_1y = np.array([yy1[:, 0]]).T\n",
        "y_1y = np.array([yy1[:, 1]]).T\n",
        "t_1y = np.array([yy1[:, 2]]).T\n",
        "uy1 = -np.sin(t_1y) * np.sin(np.pi * x_1y) * np.sin(np.pi * x_1y) * np.sin(np.pi * y_1y) * np.cos(np.pi * y_1y)\n",
        "vy1 = np.sin(t_1y) * np.sin(np.pi * x_1y) * np.cos(np.pi * x_1y) * np.sin(np.pi * y_1y) * np.sin(np.pi * y_1y)\n",
        "\n",
        "yy2 = np.concatenate(\n",
        "    ([yyy1.flatten()[:, None], np.max(y1) * np.ones((x1.shape[0] * t1.shape[0], 1)), yyy0.flatten()[:, None]]), axis=1)\n",
        "x_2y = np.array([yy2[:, 0]]).T\n",
        "y_2y = np.array([yy2[:, 1]]).T\n",
        "t_2y = np.array([yy2[:, 2]]).T\n",
        "uy2 = -np.sin(t_2y) * np.sin(np.pi * x_2y) * np.sin(np.pi * x_2y) * np.sin(np.pi * y_2y) * np.cos(np.pi * y_2y)\n",
        "vy2 = np.sin(t_2y) * np.sin(np.pi * x_2y) * np.cos(np.pi * x_2y) * np.sin(np.pi * y_2y) * np.sin(np.pi * y_2y)\n",
        "\n",
        "\n",
        "xxx1, xxx0 = np.meshgrid(y1, t1)\n",
        "\n",
        "xx1 = np.concatenate(\n",
        "    ([np.min(x1) * np.ones((y1.shape[0] * t1.shape[0], 1)), xxx1.flatten()[:, None], xxx0.flatten()[:, None]]), axis=1)\n",
        "x_1x = np.array([xx1[:, 0]]).T\n",
        "y_1x = np.array([xx1[:, 1]]).T\n",
        "t_1x = np.array([xx1[:, 2]]).T\n",
        "ux1 = -np.sin(t_1x) * np.sin(np.pi * x_1x) * np.sin(np.pi * x_1x) * np.sin(np.pi * y_1x) * np.cos(np.pi * y_1x)\n",
        "vx1 = np.sin(t_1x) * np.sin(np.pi * x_1x) * np.cos(np.pi * x_1x) * np.sin(np.pi * y_1x) * np.sin(np.pi * y_1x)\n",
        "\n",
        "xx2 = np.concatenate(\n",
        "    ([np.max(x1) * np.ones((y1.shape[0] * t1.shape[0], 1)), xxx1.flatten()[:, None], xxx0.flatten()[:, None]]), axis=1)\n",
        "x_2x = np.array([xx2[:, 0]]).T\n",
        "y_2x = np.array([xx2[:, 1]]).T\n",
        "t_2x = np.array([xx2[:, 2]]).T\n",
        "ux2 = -np.sin(t_2x) * np.sin(np.pi * x_2x) * np.sin(np.pi * x_2x) * np.sin(np.pi * y_2x) * np.cos(np.pi * y_2x)\n",
        "vx2 = np.sin(t_2x) * np.sin(np.pi * x_2x) * np.cos(np.pi * x_2x) * np.sin(np.pi * y_2x) * np.sin(np.pi * y_2x)\n",
        "\n",
        "X_u1 = np.vstack([tt1, yy1, yy2, xx1, xx2])\n",
        "u1 = np.vstack([ut1, uy1, uy2, ux1, ux2])\n",
        "v1 = np.vstack([vt1, vy1, vy2, vx1, vx2])\n",
        "\n",
        "idx_1 = np.random.choice(X_u1.shape[0], Nu1, replace=False)\n",
        "X_u_train = X_u1[idx_1, :]\n",
        "u_train = u1[idx_1, :]\n",
        "v_train = v1[idx_1, :]\n",
        "\n",
        "X1, Y1, T1 = np.meshgrid(x1, y1, t1)\n",
        "#    Exact = np.sin(np.pi*X)*np.sin(np.pi*T)*np.sin(np.pi*Z)  #100*100*100\n",
        "U_exact1 = -np.sin(T1) * np.sin(np.pi * X1) * np.sin(np.pi * X1) * np.sin(np.pi * Y1) * np.cos(np.pi * Y1)\n",
        "V_exact1 = np.sin(T1) * np.sin(np.pi * X1) * np.cos(np.pi * X1) * np.sin(np.pi * Y1) * np.sin(np.pi * Y1)\n",
        "P_exact1 = np.sin(T1) * np.sin(np.pi * X1) * np.cos(np.pi * Y1)\n",
        "C_exact1 = np.sin(T1) * np.sin(np.pi * X1) * np.cos(np.pi * Y1)\n",
        "t_exact1 = -np.sin(T1) * np.sin(np.pi * X1) * np.sin(np.pi * X1) * np.sin(np.pi * Y1) * np.cos(np.pi * Y1)\n",
        "SAI_exact1 = np.sin(T1) * np.sin(np.pi * X1) * np.cos(np.pi * X1) * np.sin(np.pi * Y1) * np.sin(np.pi * Y1)\n",
        "FI_exact1 = np.sin(T1) * np.sin(np.pi * X1) * np.cos(np.pi * Y1)\n",
        "\n",
        "X_star1 = np.hstack((X1.flatten()[:, None], Y1.flatten()[:, None], T1.flatten()[:, None]))\n",
        "x_star1 = np.array([X_star1[:, 0]]).T\n",
        "y_star1 = np.array([X_star1[:, 1]]).T\n",
        "t_star1 = np.array([X_star1[:, 2]]).T\n",
        "\n",
        "u_exact1 = -np.sin(t_star1) * np.sin(np.pi * x_star1) * np.sin(np.pi * x_star1) * np.sin(np.pi * y_star1) * np.cos(np.pi * y_star1)\n",
        "v_exact1 = np.sin(t_star1) * np.sin(np.pi * x_star1) * np.cos(np.pi * x_star1) * np.sin(np.pi * y_star1) * np.sin(np.pi * y_star1)\n",
        "p_exact1 = np.sin(t_star1) * np.sin(np.pi * x_star1) * np.cos(np.pi * y_star1)\n",
        "c_exact1 = np.sin(t_star1) * np.sin(np.pi * x_star1) * np.cos(np.pi * y_star1)\n",
        "T_exact1 = -np.sin(t_star1) * np.sin(np.pi * x_star1) * np.sin(np.pi * x_star1) * np.sin(np.pi * y_star1) * np.cos(np.pi * y_star1)\n",
        "sai_exact1 = np.sin(t_star1) * np.sin(np.pi * x_star1) * np.cos(np.pi * x_star1) * np.sin(np.pi * y_star1) * np.sin(np.pi * y_star1)\n",
        "fi_exact1 = np.sin(t_star1) * np.sin(np.pi * x_star1) * np.cos(np.pi * y_star1)\n",
        "lb1 = X_star1.min(0)\n",
        "ub1 = X_star1.max(0)\n",
        "\n",
        "X_f_train11 = lb1 + (ub1 - lb1) * lhs(3, N_f)\n",
        "X_f = np.vstack((X_f_train11, X_u_train))\n",
        "\n",
        "xb = tf.cast(X_u_train[:, 0:1], dtype=tf.float32)\n",
        "yb = tf.cast(X_u_train[:, 1:2], dtype=tf.float32)\n",
        "tb = tf.cast(X_u_train[:, 2:3], dtype=tf.float32)\n",
        "ub = tf.cast(u_train[:, 0:1], dtype=tf.float32)\n",
        "vb = tf.cast(v_train[:, 0:1], dtype=tf.float32)\n",
        "\n",
        "\n",
        "lb = X_star1.min(0)\n",
        "rb = X_star1.max(0)\n",
        "\n",
        "x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=tf.float32)\n",
        "y_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=tf.float32)\n",
        "t_f = tf.convert_to_tensor(X_f[:, 2:3], dtype=tf.float32)\n",
        "\n",
        "'''\n",
        "start_time = time.time()\n",
        "MSE_b1, MSE_f1, weightu, weightf = fit(x_f, y_f, t_f, xb, yb, tb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star1, tf_iter=10000, tf_iter2=1000, newton_iter1=5000,newton_iter2=15000)\n",
        "\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "u_pred, v_pred, p_pred = predict(X_star1)\n",
        "\n",
        "U_pred = u_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
        "V_pred = v_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
        "P_pred = p_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
        "\n",
        "error_uu = np.abs(u_exact1 - u_pred)\n",
        "error_vv = np.abs(v_exact1 - v_pred)\n",
        "error_pp = np.abs(p_exact1 - p_pred)\n",
        "\n",
        "error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
        "print('Error u: %e' % (error_u))\n",
        "\n",
        "error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
        "print('Error v: %e' % (error_v))\n",
        "\n",
        "error_p = np.linalg.norm(p_exact1 - p_pred, 2) / np.linalg.norm(p_exact1, 2)\n",
        "print('Error p: %e' % (error_p))\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "h2o1KTolx3cL",
        "outputId": "2de854d4-951b-4601-f6a3-0f8a6ce31ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nstart_time = time.time()\\nMSE_b1, MSE_f1, weightu, weightf = fit(x_f, y_f, t_f, xb, yb, tb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star1, tf_iter=10000, tf_iter2=1000, newton_iter1=5000,newton_iter2=15000)\\n\\n\\nelapsed = time.time() - start_time\\nprint('Training time: %.4f' % (elapsed))\\n\\nu_pred, v_pred, p_pred = predict(X_star1)\\n\\nU_pred = u_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\\nV_pred = v_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\\nP_pred = p_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\\n\\nerror_uu = np.abs(u_exact1 - u_pred)\\nerror_vv = np.abs(v_exact1 - v_pred)\\nerror_pp = np.abs(p_exact1 - p_pred)\\n\\nerror_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\\nprint('Error u: %e' % (error_u))\\n\\nerror_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\\nprint('Error v: %e' % (error_v))\\n\\nerror_p = np.linalg.norm(p_exact1 - p_pred, 2) / np.linalg.norm(p_exact1, 2)\\nprint('Error p: %e' % (error_p))\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x_f,y_f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "AI2Nxk1Okcfy",
        "outputId": "d23dd88b-cfac-4197-a4b6-64bb802009e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa6e032df10>]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYgUlEQVR4nO3dfXRV9Z3v8fc3zxASnhIQCBiEoIKjQlOrBR9Qp0JrpWum7cDtw7TLKZ2Z0tppV7t0ptc6zNzV25l7nU6n1o611dZp9TI6TqniWOvjaMUaFFBAbESQ8BieIUBCku/9I4f0JCQ5h3D22ed3/LzWYrnPPjv79zmoHzb77L1/5u6IiEj4CuIOICIimaFCFxHJEyp0EZE8oUIXEckTKnQRkTxRFNfAVVVVXltbG9fwIiJBWrVq1R53r+7rvdgKvba2loaGhriGFxEJkplt6e89nXIREckTKnQRkTyhQhcRyRMqdBGRPKFCFxHJEykL3cx+bGa7zez1ft43M/uumTWa2Vozm5X5mCIikko6R+j3AvMGeH8+UJf4tRi488xjiYjI6Up5Hbq7P2dmtQNssgD4qXc9h3elmY0ws3HuviNDGXv4xept3PTA6ih2LSLSpzlTq5g1aUSf77W2d/L69oO80LiXCyZUcvW5YwbcV0lRAZ9439mMLC/JeM5M3Fg0Adia9Lopse6UQjezxXQdxTNp0qRBDaYyF5Fse75xD8837km53evbDvH6tkMDbmMGl54zmvryUZmK1y2rX4q6+13uXu/u9dXVfd65mtKKL13evfzLJXOoKIvtZtesGVtZykU1w/n8FecMuN1fXTstS4lEpD+j+znyHltZyhevnspzX5tLfW3myxwyU+jbgIlJr2sS6yJx/riK7uUjre2s+NLlTBgxJKrh0vLx+hpGDi2ObP+7DrWypukg//rcpgG3+6dfv8nkqvLIcohIantb2vpc/9zX5/LVD5zLxFFDIxs7E4W+HPh04mqXS4GDUZ0/BzAzrr9wHACLfriSsZVl/PufX8bldVVRDZnSsoYm9h89Edv4yd7e0xJ3BBHpw53PvBX5GCnPV5jZ/cBVQJWZNQHfBIoB3P0HwArgg0AjcBT4bFRhT7r2/LE8srbrz4xp33gs6uFERM7YT1/cwkub9jFqWAnfXTiTwgLL+BjpXOWyKMX7DnwhY4lScHfqa0dmazgRkYzY19LGi5v2MraylI5Oj6TQg7tT9PF1u7jiH57usa66ojSmNCIip2fXoVbm/p9nItl3cIV+eV0VH3vPxB7rmg+3xpRGROT0lRRFU73BFXp5aRHf/uiFfPnaurijiIiclsvOGQ1AWXFhJPsPrtBPunHO5LgjiIik7boZY/nG9edHOkawhX7H09FfAiQikikzJ0V/MUewhf6BGWPjjiAiktK5Y7tuhly1ZT/tHR7pWMHdN3/b8nXc+5vNcccQEUnLyZv9nli/iyfW7wJgw46Bn/cyWMEdobe2d8YdQUQkLZv/94dYt/Q6ACK47PwUwRX6t/7oD7pv/RcRyWW3/MdaigsLOO+sCurGVKT+gTMUXKEDfO9/zOLKaYN7WqOISLbc/9ut3PfiZirKiti463Dk4wV3Dh1g1ZZ9PPtmc9wxRERS+p+/WNe9PG3sMN7cdSSysYI8Qp9aXcG44WVxxxAROS1RljkEWOgtre08+toOdhw8HncUEZGcElyh/9frO/nrh1+LO4aISM4JrtAvmzI67ggiIjkpuEJv2n8s7ggiIjkpuEJ/b+1IrjpXlyyKSDhmT83OmYXgCt3M+OGn6+OOISKSthca92ZlnOAKHWDvkTb+eFZN3DFERHJKcDcWHW1r59JvPRl3DBGRnBPcEfrQkuD+DBIRyYrgCh1g/PAy5kytijuGiEhOCbLQK4cUU1pUwKRRQ+OOIiKSM4I8f/HGzsO8sTP6J5eJiIQkuCP0zs5op3ASEQlVcIVekI1pP0REAhRcoScbPqQ47ggiIjkjyEIvLykE4PNXnsNN19TFnEZEJLWRQ6M/AA2y0N+fuGTxnhc2889P/o5rzhsTcyIRkYHtP3oi8jHCLPTEI3SbD7cC8OQbu+OMIyKSE4Is9Ikjdf25iEhvaRW6mc0zs41m1mhmN/fx/iQze9rMXjWztWb2wcxH7dLW3slDrzRFtXsRkWClLHQzKwTuAOYD04FFZja912bfAJa5+0xgIfD9TAc96dHXtvPY6zuj2r2ISEb9+ZVTAJhSXR75WOkcoV8CNLr7JndvAx4AFvTaxoHKxPJwYHvmIvZ0/YXjWTJ3alS7FxHJqB88+xYAbzW3RD5WOoU+Adia9LopsS7ZbcAnzawJWAF8sa8dmdliM2sws4bm5uZBxIXiwgL+cu6UQf2siEg+y9SXoouAe929BvggcJ+ZnbJvd7/L3evdvb66evDTyA0pLqS4UHeMiogkS6fQtwETk17XJNYluxFYBuDuLwJlQGTPtzUzKst+f5H+H04fG9VQIiLBSKfQXwbqzGyymZXQ9aXn8l7bvANcA2Bm59NV6IM7p5KGZS9vZW9LW/frJ9bvimooEZFgpCx0d28HlgCPAxvoupplnZktNbMbEpt9Fficma0B7gc+4+6RPBbx5c37+PpDa6PYtYhI0NJ6Hrq7r6Dry87kdbcmLa8HZmc2Wt/GVJRSXlJIS1tHNoYTEQlGcHeKnj26nNduuy7uGCIiOSe4Qgd6nD8XEZEuQRb6w6/q1n8RCcul54yKfIzgCt3dWdagQheRsHzzwzMiHyO4Qn/lnQM07j7CxRNHAL9/ToKISK569mtX0ZGF+ZCDK/RVW/YBv38m+kdmjmd0eUmckUREBlRgxtEsXJkXXKGfvLr9+890PfBm2/5jXDBheIyJREQG9pl7fsuxEyr0fo2pKAXgxp80MG3ssJjTiIj0763mFo61tUc+TrCFfsNF47uXf/jfb8eYREQkNR2hD+DDSYUuIpLLxg0v0zn0gUzOwuwfIiKZ8LnLz+GYCr1/RQV6HrqIhOG9taNU6CIi+eD8cRU6h55K3Rhd3SIiuW3O1CqKCgt0Dj2V3+0+EncEEZEBTU0ceLa0Rn/ZYlrPQ89F2w8cjzuCiEhK9/5mMw+/uo2Dx05EPlawR+jX3v5s3BFERNKSjTKHAAt9TdOBuCOIiOSk4Ap9xWs7u5fnTK0C4FOXnh1XHBGRnBFcoZeXFHYvP9+4B4D7Vm6JK46IyICGFBem3ihDgiv0ssRvzvwLzoo5iYhIatm4/vyk4Ar9pL/7yAV8vL4m7hgiIjkj2EIH+Fj9xLgjiIjkjKALffiQ4rgjiIjkjKALvbJMhS4iclLQha4jdBGR3wu20P/vrzZyz2+6Ziq6/sJxMacREYlfsM9yuf+3W7uXH1m7I8YkIiK5IdgjdBER6UmFLiKSJ9IqdDObZ2YbzazRzG7uZ5uPm9l6M1tnZj/PbMxTff6Kc6IeQkQkKCkL3cwKgTuA+cB0YJGZTe+1TR1wCzDb3WcAX44gaw9DSrL3fAQRkTOxdMGMrIyTzhH6JUCju29y9zbgAWBBr20+B9zh7vsB3H13ZmOe6ucvvRP1ECIiGWGWnUnt0yn0CcDWpNdNiXXJpgHTzOwFM1tpZvP62pGZLTazBjNraG5uHlzihN2HW8/o50VEsmXlpr1ZGSdTX4oWAXXAVcAi4IdmNqL3Ru5+l7vXu3t9dXV1hoYWEcltj2bp0up0Cn0bkPwUrJrEumRNwHJ3P+HubwNv0lXwIiKSJekU+stAnZlNNrMSYCGwvNc2/0nX0TlmVkXXKZhNGczZbW9LGwAN37iW7y6aGcUQIiJBSlno7t4OLAEeBzYAy9x9nZktNbMbEps9Duw1s/XA08DX3D3Sk0b7W9qoGzMsyiFERIKS1q3/7r4CWNFr3a1Jyw58JfErK7712Bs89UbkF9OIiAQj2DtFVeYiIj0FW+giIiEZOTT6x30HW+h/NmcyWbpWX0TkjE0aNTTyMYJ9fO7dz78ddwQRkbSVFUf/uJJgj9BFREIyNAvPn1Khi4hkwfETnZGPoUIXEcmCF7PwPBcVuohInlChi4jkCRW6iEieUKGLiOQJFbqISJ5QoYuI5AkVuohInlChi4jkCRW6iEieCL7Q/3nhxXFHEBHJCcEX+j0vbI47gohITgiu0EeXl/R4vXrrgZiSiIjkluAKXURE+qZCFxHJEyp0EZE8EVyhe9wBREQG6cKa4ZHuP7hC39fSFncEEZFBWdt0MNL9B1foIiLSNxW6iEieUKGLiOQJFbqISJ5QoYuI5AkVuohInlChi4jkibQK3czmmdlGM2s0s5sH2O6PzczNrD5zEUVEJB0pC93MCoE7gPnAdGCRmU3vY7sK4CbgpUyHFBGR1NI5Qr8EaHT3Te7eBjwALOhju78Dvg0cz2A+ERFJUzqFPgHYmvS6KbGum5nNAia6+6MD7cjMFptZg5k1NDc3n3ZYERHp3xl/KWpmBcDtwFdTbevud7l7vbvXV1dXn+nQIiKSJJ1C3wZMTHpdk1h3UgVwAfCMmW0GLgWW64tREZHsSqfQXwbqzGyymZUAC4HlJ99094PuXuXute5eC6wEbnD3hkgSi4hIn1IWuru3A0uAx4ENwDJ3X2dmS83shqgDiohIeorS2cjdVwAreq27tZ9trzrzWCIicrp0p6iISJ4IvtDHVpbGHUFEJCcEX+i7DrXGHUFEJCcEW+jDhxTHHUFEJKcEW+hfmDsl7ggiIjkl2ELf13Ii7ggiIjkl2EJv2LwPgPdPGc2bfz8/5jQiIvELt9C37AfgO39yMfta2mJOIyISv2ALvZvBT17cHHcKEZHYpXWnaC675H89GXcEEZGcEP4RuoiIACp0EZG8oUIXEcmSogKLdP8qdBGRLGnv9Ej3r0IXEckTKnQRkTyhQhcRyRPBFvqKL10edwQRkZwSbKE37T8adwQRkZwSbKEvvm9V3BFERE5L1DOsBVvoIiKhiXqGtaAL/cpp1XFHEBHJGUEX+rNvNscdQUQkZwRX6KPLS+KOICKSk4Ir9JMqSoN/8q+IvMtMHTMs0v0HW+hTIv6NERHJtKph0Z5hCK7QT3R0ArB664GYk4iInJ7Ne6K9fya4Qj90vD3uCCIig7Lz0PFI9x9coYuISN9U6CIieSKtQjezeWa20cwazezmPt7/ipmtN7O1ZvakmZ2d+agiIjKQlIVuZoXAHcB8YDqwyMym99rsVaDe3S8EHgT+IdNBRURkYOkcoV8CNLr7JndvAx4AFiRv4O5Pu/vJr29XAjWZjXmq2z7c+88UEZF3t3QKfQKwNel1U2Jdf24EHuvrDTNbbGYNZtbQ3Hxmt+3f9sv1Z/TzIiL5JqNfiprZJ4F64B/7et/d73L3enevr67Wg7VERDIpnfvntwETk17XJNb1YGbXAn8DXOnu0T4jEjADj3YCbRGRoKRzhP4yUGdmk82sBFgILE/ewMxmAv8K3ODuuzMf81R3fuI92RhGRCQYKQvd3duBJcDjwAZgmbuvM7OlZnZDYrN/BIYB/25mq81seT+7y5jp4yq7ly+qGR71cCIiOS+tRxa6+wpgRa91tyYtX5vhXKflqnPHsKbpYJwRRERiF+ydor/esAvoehzlc79rpqjAYk4kIhKvYAt96SNdly3+0awJrNl6gFGa+EJE3uWCLfSTntywm06H3Ycjv7BGRCSnBVfovaegW7Vlf0xJRERyS3CFLiIifQu20CeNGhp3BBGRnBJsob+zL9qpnEREQhNsoYuISE8qdBGRPKFCFxHJEyp0EZE8EXShFxYYD/3F++OOISKSE9J6OFcuWr5kNuePq+TOZ96KO4qISE4I9gh9/IghdHQ6tz/xZtxRRERyQrBH6D945i3ufv7tuGOIiOSMYI/QN+46HHcEEXkXqxpWGneEUwRb6DPGa5YiEYnP7Kmj+eWSOXHH6CHYUy4lRcH+WSQieeAXq7ez69BxxlSU5szju4Mt9M5OjzuCiLzLrdy0j8Icmi0t2MPc7z3dGHcEERE6Oj1nzhjkRgoRkYC1tXfGHQEIsNCnj68E4KIafSkqIpIsuEKfM7UKgDVNB2NOIiJyej76nppI9x9coYuIhOqdvdFOzKNCFxHJks17WyLdf3CFrosVRSRUUV+vHlyhr9t+KO4IIiI5KbhC/+Wa7XFHEBHJScEV+peunhp3BBGRnBRcoZeXBvu0AhGRSAVX6OkqLymMO4KISA/XnDcm0v2nVehmNs/MNppZo5nd3Mf7pWb2/xLvv2RmtZkOeroWXzHllHW3zD+ve/lDF47j+5+YxdUR/waLiJz0fOOeSPef8vyFmRUCdwB/CDQBL5vZcndfn7TZjcB+d59qZguBbwN/EkXgI63tfa7/zPtr2dfSxvLEl6b/9OtTp6b71mNvdC8/unYHj67dEUVEkZxXXVHKWZVl7Dx0nOYcefTru0FrxM98SeeE9CVAo7tvAjCzB4AFQHKhLwBuSyw/CHzPzMzdM37Z+L881fdTFu/9zeZMD5UxZcUFlBUXcuDoibijiADQfLg1ZZHXjBxC0/5jp6yvGzOM5iOtPf57rhszrMc2+1ra2NvS1v36nKpyNu0Z3E01I4YW9xhrSHEht3/8IrYdOEZbRyfbDxxj2/5jNO0/xjv7jp52ac6/4Cw+fVktuw8fZ+fB4+w50src88Zw/EQH33uqkVfeOdC97UcuHs9/ru55pd0ltaP42wUzONrWwfETHbS0trPqnf08smYH2w50/f5NH1fJ+h3RX3KdTqFPALYmvW4C3tffNu7ebmYHgdFAj79fmNliYDHApEmTBhX4wxeN7750saKsiMqyYirKiqgoK6K1vZO1aTzjZfzwMiaMHMLwIcUUFxZgg3ic8ZqtB6koK6Kto5M9h1uZU1fF0bYOntnYTHVFKe+tHdnnz7W1d+IOpcUFPZb7suK1nacfTCQDrpsxlsIC48J+HoJXN7arwN3p9/+f9g7nV+t3AXDeuArOG1fR4/3WE52s33GIHQePA/CFuVP42nVdp0Xb2jtpPtLKsbYOplSXc7i1nQcbmlj6yHp+sWQ208b23Fe63B0zo6PTOXaig6Ot7VRXlGL9fIirzxvLqi37eXbjbv5y7lTKigv5zsKZdHY62w8e463mFs4/q4IxlWU9fu4DM87ilvnnA3C0rZ0CM8qKC/nZS1v4m4df57OzaweVPxVLdRBtZh8F5rn7nyVefwp4n7svSdrm9cQ2TYnXbyW26feEUX19vTc0NGTgI4iIvHuY2Sp3r+/rvXS+FN0GTEx6XZNY1+c2ZlYEDAf2nn5UEREZrHQK/WWgzswmm1kJsBBY3mub5cCfJpY/CjwVxflzERHpX8pz6Ilz4kuAx4FC4Mfuvs7MlgIN7r4c+BFwn5k1AvvoKn0REcmitG67dPcVwIpe625NWj4OfCyz0URE5HTk7Z2iIiLvNip0EZE8oUIXEckTKnQRkTyR8saiyAY2awa2DPLHq+h1F2rA9FlyT758DtBnyVVn8lnOdvfqvt6IrdDPhJk19HenVGj0WXJPvnwO0GfJVVF9Fp1yERHJEyp0EZE8EWqh3xV3gAzSZ8k9+fI5QJ8lV0XyWYI8hy4iIqcK9QhdRER6UaGLiOSJ4Ao91YTVoTCzH5vZ7sTkIMEys4lm9rSZrTezdWZ2U9yZBsvMyszst2a2JvFZ/jbuTGfKzArN7FUzeyTuLGfCzDab2WtmttrMgp0Zx8xGmNmDZvaGmW0ws8syuv+QzqEnJqx+k6QJq4FFvSasDoKZXQEcAX7q7hfEnWewzGwcMM7dXzGzCmAV8JFA/50YUO7uR8ysGHgeuMndV8YcbdDM7CtAPVDp7tfHnWewzGwzUD/QLGghMLOfAP/t7ncn5pcY6u4HUv1cukI7Qu+esNrd24CTE1YHx92fo+vZ8UFz9x3u/kpi+TCwga45ZoPjXY4kXhYnfoVzxNOLmdUAHwLujjuLgJkNB66ga/4I3L0tk2UO4RV6XxNWB1ke+cjMaoGZwEvxJhm8xCmK1cBu4Al3D/azAN8Bvg50xh0kAxz4lZmtSkw2H6LJQDNwT+I02N1mVp7JAUIrdMlRZjYMeAj4srsfijvPYLl7h7tfTNfcuZeYWZCnw8zsemC3u6+KO0uGzHH3WcB84AuJU5ahKQJmAXe6+0ygBcjo94ChFXo6E1ZLliXONz8E/Mzd/yPuPJmQ+Kvw08C8uLMM0mzghsS55weAq83s3+KNNHjuvi3xz93Aw3Sdfg1NE9CU9Le+B+kq+IwJrdDTmbBasijxReKPgA3ufnvcec6EmVWb2YjE8hC6vnx/I95Ug+Put7h7jbvX0vX/yVPu/smYYw2KmZUnvnAncYriA0BwV4e5+05gq5mdm1h1DZDRiwfSmlM0V/Q3YXXMsQbFzO4HrgKqzKwJ+Ka7/yjeVIMyG/gU8Fri3DPAXyfmoQ3NOOAniaupCoBl7h705X55YizwcNexA0XAz939v+KNNGhfBH6WOCDdBHw2kzsP6rJFERHpX2inXEREpB8qdBGRPKFCFxHJEyp0EZE8oUIXEckTKnQRkTyhQhcRyRP/H4P8mD54WxB/AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adding boundary collocation points to the bottom wall"
      ],
      "metadata": {
        "id": "w4j6yrJZf1U3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bottom\n",
        "lb1=np.zeros(3)\n",
        "ub1=np.ones(3)\n",
        "ub1[0]=xmax\n",
        "ub1[1]=0.02\n",
        "X_f_train11_new = lb1 + (ub1 - lb1) * lhs(3, int(N_f/5))\n",
        "\n",
        "x_f_new = tf.convert_to_tensor(X_f_train11_new[:, 0:1], dtype=tf.float32)\n",
        "y_f_new = tf.convert_to_tensor(X_f_train11_new[:, 1:2], dtype=tf.float32)\n",
        "\n",
        "x_f=np.array(x_f)\n",
        "y_f=np.array(y_f)\n",
        "\n",
        "x_f =np.vstack((x_f,X_f_train11_new[:, 0:1]))\n",
        "y_f =np.vstack((y_f,X_f_train11_new[:, 1:2]))\n",
        "\n",
        "x_f=tf.convert_to_tensor(x_f, dtype=tf.float32)\n",
        "y_f=tf.convert_to_tensor(y_f, dtype=tf.float32)\n",
        "#plt.ylim([0,0.1])"
      ],
      "metadata": {
        "id": "lxuAkMbP5dJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x_f,y_f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Y7Ri-qXDkpAs",
        "outputId": "b350be62-7670-43dc-b152-f38af659c4a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0593722040>]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adding boundary collocation points to the top wall"
      ],
      "metadata": {
        "id": "U1KjTflogD6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#top\n",
        "lb1=np.zeros(3)\n",
        "ub1=np.ones(3)\n",
        "ub1[0]=xmax\n",
        "lb1[1]=0.98\n",
        "ub1[1]=1.0\n",
        "X_f_train11_new = lb1 + (ub1 - lb1) * lhs(3, int(N_f/5))\n",
        "\n",
        "x_f_new = tf.convert_to_tensor(X_f_train11_new[:, 0:1], dtype=tf.float32)\n",
        "y_f_new = tf.convert_to_tensor(X_f_train11_new[:, 1:2], dtype=tf.float32)\n",
        "\n",
        "x_f=np.array(x_f)\n",
        "y_f=np.array(y_f)\n",
        "\n",
        "x_f =np.vstack((x_f,X_f_train11_new[:, 0:1]))\n",
        "y_f =np.vstack((y_f,X_f_train11_new[:, 1:2]))\n",
        "\n",
        "x_f=tf.convert_to_tensor(x_f, dtype=tf.float32)\n",
        "y_f=tf.convert_to_tensor(y_f, dtype=tf.float32)\n",
        "#plt.ylim([0,0.1])"
      ],
      "metadata": {
        "id": "8OHl-8vgvpsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x_f,y_f)\n",
        "#plt.ylim([0,0.2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "57UcEhBK5R6N",
        "outputId": "619e30bb-a5f0-4118-f1d6-9b22a3ba439f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f88b94dc1c0>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Defining the equations\n"
      ],
      "metadata": {
        "id": "81q_f_efgIR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "DTYPE='float32'\n",
        "tf.keras.backend.set_floatx(DTYPE)\n",
        "# Set number of data points\n",
        "#N_0 = 100\n",
        "#N_b = 100\n",
        "#N_r = 5000\n",
        "\n",
        "N_b = Nu1\n",
        "N_r = N_f\n",
        "\n",
        "\n",
        "# Set boundary\n",
        "xmin = 0.\n",
        "#xmax = 8.\n",
        "ymin = 0.\n",
        "ymax = 1.\n",
        "# Set constants\n",
        "pi = tf.constant(np.pi, dtype=DTYPE)\n",
        "viscosity = .01/pi\n",
        "\n",
        "# Define initial condition\n",
        "def fun_u_0(x):\n",
        "    return -tf.sin(pi * x)\n",
        "\n",
        "# Define boundary condition\n",
        "def fun_u_b(x, y):\n",
        "    n = x.shape[0]\n",
        "    return tf.zeros((n,1), dtype=DTYPE)\n",
        "\n",
        "# Define residual of the PDE\n",
        "def Nsx(x, y, u, v, u_x, u_y, v_x, v_y, u_xx, u_yy, v_xx, v_yy, p_x, p_y):\n",
        "    return -u*u_x-v*u_y-p_x+(1.0/1.0)*(u_xx+u_yy)\n",
        "#    return -p_x+(1.0/1.0)*(u_xx+u_yy)\n",
        "\n",
        "\n",
        "def Nsy(x, y, u, v, u_x, u_y, v_x, v_y, u_xx, u_yy, v_xx, v_yy, p_x, p_y):\n",
        "    return -u*v_x-v*v_y-p_y+(1.0/1.0)*(v_xx+v_yy)\n",
        "#    return -p_y+(1.0/1.0)*(v_xx+v_yy)\n",
        "\n",
        "def Cont(u, v, u_x, u_y, v_x, v_y):\n",
        "    return u_x+v_y\n",
        "\n",
        "def concentration(u,v,c_x,c_y,c_xx,c_yy):\n",
        "  return u*c_x+v*c_y-(1/(10*10))*(c_xx+c_yy)\n",
        "\n",
        "def temperature(u,v,T_x,T_y,T_xx,T_yy,c_xx,c_yy):\n",
        "  return ((u*T_x) +(v*T_y))-(1/(10*9.4)*(T_xx+T_yy))-((1/(10*10))*(((4200/3335)*(c_xx))+((4200/3335)*(c_yy))+((2470/3335)*(1-c_xx))+((2470/3335)*(1-c_yy))))\n",
        "\n",
        "def sai(sai,sai_x,sai_y,sai_xx,sai_yy):\n",
        "  return ((sai_xx)+(sai_yy))-6*sai\n",
        "\n",
        "def fi(fi,fi_x,fi_y,fi_xx,fi_yy):\n",
        "  return ((fi_xx)+(fi_yy))\n",
        "\n",
        "# Lower bounds\n",
        "lb = tf.constant([xmin, ymin], dtype=DTYPE)\n",
        "# Upper bounds\n",
        "ub = tf.constant([xmax, ymax], dtype=DTYPE)\n",
        "\n",
        "# Set random seed for reproducible results\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# Draw uniform sample points for initial boundary data\n",
        "x_0 = lb[0] + (ub[0] - lb[0]) * tf.keras.backend.random_bernoulli((N_b,1), 0.5, dtype=DTYPE)\n",
        "y_0 = tf.random.uniform((N_b,1), lb[1], ub[1], dtype=DTYPE)\n",
        "X_0 = tf.concat([x_0, y_0], axis=1)\n",
        "\n",
        "# Evaluate intitial condition at x_0\n",
        "u_0 = fun_u_0(x_0)\n",
        "\n",
        "# Boundary data\n",
        "x_b = tf.random.uniform((N_b,1), lb[0], ub[0], dtype=DTYPE)\n",
        "y_b = lb[1] + (ub[1] - lb[1]) * tf.keras.backend.random_bernoulli((N_b,1), 0.5, dtype=DTYPE)\n",
        "X_b = tf.concat([x_b, y_b], axis=1)\n",
        "\n",
        "# Evaluate boundary condition at (t_b,x_b)\n",
        "u_b = fun_u_b(x_b, y_b)\n",
        "\n",
        "\n",
        "# Draw uniformly sampled collocation points\n",
        "x_r = tf.random.uniform((N_r,1), lb[0], ub[0], dtype=DTYPE)\n",
        "y_r = tf.random.uniform((N_r,1), lb[1], ub[1], dtype=DTYPE)\n",
        "X_r = tf.concat([x_r, y_r], axis=1)\n",
        "\n",
        "# Collect boundary and inital data in lists\n",
        "X_data = [X_0, X_b]\n",
        "u_data = [u_0, u_b]"
      ],
      "metadata": {
        "id": "QOqqbZdIyYOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Defining collocation point on the boundary"
      ],
      "metadata": {
        "id": "okydVogtgXs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DTYPE='float32'\n",
        "tf.keras.backend.set_floatx(DTYPE)\n",
        "# Set number of data points\n",
        "#N_0 = 100\n",
        "#N_b = 100\n",
        "#N_r = 5000\n",
        "\n",
        "N_b = Nu1\n",
        "N_r = N_f\n",
        "\n",
        "\n",
        "LL=10/8\n",
        "\n",
        "lo=np.zeros((N_b,1))\n",
        "#top1\n",
        "Nb_2= 80\n",
        "xtt1= tf.random.uniform((Nb_2,1),0.0,LL, dtype=DTYPE)\n",
        "ytt1= tf.constant(1.0,shape=(Nb_2,1), dtype=DTYPE)\n",
        "utt1=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "vtt1=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "\n",
        "#top2\n",
        "xtt2= tf.random.uniform((Nb_2,1),LL,2*LL, dtype=DTYPE)\n",
        "ytt2= tf.constant(1.0,shape=(Nb_2,1), dtype=DTYPE)\n",
        "utt2=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "vtt2=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "\n",
        "#top3\n",
        "xtt3= tf.random.uniform((Nb_2,1),2*LL,3*LL, dtype=DTYPE)\n",
        "ytt3= tf.constant(1.0,shape=(Nb_2,1), dtype=DTYPE)\n",
        "utt3=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "vtt3=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "\n",
        "#top4\n",
        "xtt4= tf.random.uniform((Nb_2,1),3*LL,4*LL, dtype=DTYPE)\n",
        "ytt4= tf.constant(1.0,shape=(Nb_2,1), dtype=DTYPE)\n",
        "utt4=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "vtt4=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "\n",
        "#top5\n",
        "xtt5= tf.random.uniform((Nb_2,1),4*LL,4.5*LL, dtype=DTYPE)\n",
        "ytt5= tf.constant(1.0,shape=(Nb_2,1), dtype=DTYPE)\n",
        "utt5=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "vtt5=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "\n",
        "#top6\n",
        "xtt6= tf.random.uniform((Nb_2,1),4.5*LL,xmax, dtype=DTYPE)\n",
        "ytt6= tf.constant(1.0,shape=(Nb_2,1), dtype=DTYPE)\n",
        "utt6=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "vtt6=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "\n",
        "\n",
        "#bottom1\n",
        "Nb_2=80\n",
        "xbb1= tf.random.uniform((Nb_2,1),0.0,LL, dtype=DTYPE)\n",
        "ybb1= tf.constant(0.0,shape=(Nb_2,1), dtype=DTYPE)\n",
        "ubb1=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "vbb1=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "\n",
        "#bottom2\n",
        "Nb_2= 80\n",
        "xbb2= tf.random.uniform((Nb_2,1),LL,2*LL, dtype=DTYPE)\n",
        "ybb2= tf.constant(0.0,shape=(Nb_2,1), dtype=DTYPE)\n",
        "ubb2=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "vbb2=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "\n",
        "#bottom3\n",
        "Nb_2= 80\n",
        "xbb3= tf.random.uniform((Nb_2,1),2*LL,3*LL,dtype=DTYPE)\n",
        "ybb3= tf.constant(0.0,shape=(Nb_2,1), dtype=DTYPE)\n",
        "ubb3=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "vbb3=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "\n",
        "#bottom4\n",
        "Nb_2= 80\n",
        "xbb4= tf.random.uniform((Nb_2,1),3*LL,4*LL, dtype=DTYPE)\n",
        "ybb4= tf.constant(0.0,shape=(Nb_2,1), dtype=DTYPE)\n",
        "ubb4=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "vbb4=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "\n",
        "#bottom5\n",
        "Nb_2= 80\n",
        "xbb5= tf.random.uniform((Nb_2,1),4*LL,4.5*LL, dtype=DTYPE)\n",
        "ybb5= tf.constant(0.0,shape=(Nb_2,1), dtype=DTYPE)\n",
        "ubb5=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "vbb5=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "\n",
        "#bottom6\n",
        "Nb_2= 80\n",
        "xbb6= tf.random.uniform((Nb_2,1),4.5*LL,xmax, dtype=DTYPE)\n",
        "ybb6= tf.constant(0.0,shape=(Nb_2,1), dtype=DTYPE)\n",
        "ubb6=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "vbb6=tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "\n",
        "\n",
        "#left1\n",
        "xx=[]\n",
        "yy=[]\n",
        "Nb_1= 50\n",
        "xll1= tf.zeros((Nb_1,1), dtype=DTYPE)\n",
        "yll1 =tf.random.uniform((Nb_1,1),0, 0.5, dtype=DTYPE)\n",
        "#xll1=tf.add(xll1[:,0],xx)\n",
        "#yll1=tf.add(yll1[:,0],yy)\n",
        "ull1=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "vll1=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "#left2\n",
        "xx=[]\n",
        "yy=[]\n",
        "Nb_2= 50\n",
        "xll2= tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "yll2= tf.random.uniform((Nb_2,1),0.5, 1.0, dtype=DTYPE)\n",
        "#xll2=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "#yll2=tf.zeros((len(yy),1), dtype=DTYPE)\n",
        "#xll2=tf.add(xll2[:,0],xx)\n",
        "#yll2=tf.add(yll2[:,0],yy)\n",
        "ull2=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "vll2=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "#right\n",
        "\n",
        "xx=[]\n",
        "yy=[]\n",
        "for i in range(N_b):\n",
        "      if X_data[0][i,0].numpy() == xmax:\n",
        "        xx.append(X_data[0][i,0].numpy())\n",
        "        yy.append(X_data[0][i,1].numpy())\n",
        "xrr=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "yrr=tf.zeros((len(yy),1), dtype=DTYPE)\n",
        "xrr=tf.add(xrr[:,0],xx)\n",
        "yrr=tf.add(yrr[:,0],yy)\n",
        "urr=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "vrr=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "xtt1=tf.reshape(xtt1,shape=[tf.shape(xtt1).numpy()[0],1])\n",
        "ytt1=tf.reshape(ytt1,shape=[tf.shape(ytt1).numpy()[0],1])\n",
        "\n",
        "xtt2=tf.reshape(xtt2,shape=[tf.shape(xtt2).numpy()[0],1])\n",
        "ytt2=tf.reshape(ytt2,shape=[tf.shape(ytt2).numpy()[0],1])\n",
        "\n",
        "xtt3=tf.reshape(xtt3,shape=[tf.shape(xtt3).numpy()[0],1])\n",
        "ytt3=tf.reshape(ytt3,shape=[tf.shape(ytt3).numpy()[0],1])\n",
        "\n",
        "xtt4=tf.reshape(xtt4,shape=[tf.shape(xtt4).numpy()[0],1])\n",
        "ytt4=tf.reshape(ytt4,shape=[tf.shape(ytt4).numpy()[0],1])\n",
        "\n",
        "#xbb1=tf.reshape(xbb1,shape=[tf.shape(xbb1).numpy()[0],1])\n",
        "#ybb1=tf.reshape(ybb1,shape=[tf.shape(ybb1).numpy()[0],1])\n",
        "\n",
        "#xbb2=tf.reshape(xbb1,shape=[tf.shape(xbb2).numpy()[0],1])\n",
        "#ybb2=tf.reshape(ybb1,shape=[tf.shape(ybb2).numpy()[0],1])\n",
        "\n",
        "#xbb3=tf.reshape(xbb1,shape=[tf.shape(xbb3).numpy()[0],1])\n",
        "#ybb3=tf.reshape(ybb1,shape=[tf.shape(ybb3).numpy()[0],1])\n",
        "\n",
        "#xbb4=tf.reshape(xbb1,shape=[tf.shape(xbb4).numpy()[0],1])\n",
        "#ybb4=tf.reshape(ybb1,shape=[tf.shape(ybb4).numpy()[0],1])\n",
        "\n",
        "\n",
        "xrr=tf.reshape(xrr,shape=[tf.shape(xrr).numpy()[0],1])\n",
        "yrr=tf.reshape(yrr,shape=[tf.shape(yrr).numpy()[0],1])\n",
        "\n",
        "xll1=tf.reshape(xll1,shape=[tf.shape(xll1).numpy()[0],1])\n",
        "yll1=tf.reshape(yll1,shape=[tf.shape(yll1).numpy()[0],1])\n",
        "xll2=tf.reshape(xll2,shape=[tf.shape(xll2).numpy()[0],1])\n",
        "yll2=tf.reshape(yll2,shape=[tf.shape(yll2).numpy()[0],1])\n",
        "\n",
        "xbound=tf.concat([xtt1,xtt2,xtt3,xtt4,xtt5,xtt6,xbb1,xbb2,xbb3,xbb4,xbb5,xbb6,xrr,xll1,xll2],0)\n",
        "ybound=tf.concat([ytt1,ytt2,ytt3,ytt4,ytt5,ytt6,ybb1,ybb2,ybb3,ybb4,ybb5,ybb6,yrr,yll1,yll2],0)\n",
        "#xbound=tf.concat([xtt1,xtt2,xtt3,xtt4,xbb1,xbb2,xbb3,xbb4],0)\n",
        "#ybound=tf.concat([ytt1,ytt2,ytt3,ytt4,ybb1,ybb2,ybb3,ybb4],0)\n",
        "plt.scatter(xbound,ybound)\n",
        "\n",
        "ubound=tf.concat([utt1,utt2,utt3,utt4,utt5,utt6,ubb1,ubb2,ubb3,ubb4,ubb5,ubb6,urr,ull1,ull2],0)\n",
        "vbound=tf.concat([vtt1,vtt2,vtt3,vtt4,vtt5,vtt6,vbb1,vbb2,vbb3,vbb4,vbb5,vbb6,vrr,vll1,vll2],0)\n",
        "\n",
        "xb=xbound\n",
        "yb=ybound\n",
        "\n",
        "ub=ubound\n",
        "vb=vbound\n",
        "#plt.scatter(xbb2,ybb2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "hz7B-lWIx9rC",
        "outputId": "4a9c0c7a-dd29-4da4-fcd8-d9d890a0951a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Solving ELectric layer due charged suraface on the wall"
      ],
      "metadata": {
        "id": "_WYVQGL6gk1Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E6eLtWcBxPxz",
        "outputId": "5b3cd6ba-1b9c-4e0b-aeea-59db23c50a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xxxxxxxxxxxxxx\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 2)]               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 20)                60        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 7)                 147       \n",
            "                                                                 \n",
            " tf.__operators__.getitem (S  (None, 1)                0         \n",
            " licingOpLambda)                                                 \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,727\n",
            "Trainable params: 2,727\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "weight_ub: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1000.], dtype=float32)>  weight_fu: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>\n",
            "starting Adam training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It: 0, Time: 31.11\n",
            "mse_b  [764202.1]  mse_f: [6817469.5]   total loss: [7581671.5]\n",
            "It: 40, Time: 0.83\n",
            "mse_b  [263126.94]  mse_f: [583952.6]   total loss: [847079.56]\n",
            "It: 80, Time: 0.82\n",
            "mse_b  [297577.16]  mse_f: [156278.9]   total loss: [453856.06]\n",
            "It: 120, Time: 0.84\n",
            "mse_b  [288567.3]  mse_f: [116999.445]   total loss: [405566.75]\n",
            "It: 160, Time: 0.85\n",
            "mse_b  [317277.8]  mse_f: [907.13947]   total loss: [318184.94]\n",
            "It: 200, Time: 0.85\n",
            "mse_b  [338686.9]  mse_f: [28032.852]   total loss: [366719.75]\n",
            "It: 240, Time: 0.82\n",
            "mse_b  [290479.]  mse_f: [90599.27]   total loss: [381078.28]\n",
            "It: 280, Time: 0.85\n",
            "mse_b  [316114.38]  mse_f: [1446.1598]   total loss: [317560.53]\n",
            "It: 320, Time: 0.89\n",
            "mse_b  [332052.38]  mse_f: [12385.115]   total loss: [344437.5]\n",
            "It: 360, Time: 0.89\n",
            "mse_b  [307066.9]  mse_f: [15549.521]   total loss: [322616.44]\n",
            "It: 400, Time: 0.83\n",
            "mse_b  [306292.53]  mse_f: [17429.752]   total loss: [323722.28]\n",
            "It: 440, Time: 0.82\n",
            "mse_b  [323688.9]  mse_f: [1338.9744]   total loss: [325027.88]\n",
            "It: 480, Time: 0.81\n",
            "mse_b  [313502.47]  mse_f: [3646.1626]   total loss: [317148.62]\n",
            "It: 520, Time: 0.83\n",
            "mse_b  [309393.38]  mse_f: [9993.613]   total loss: [319387.]\n",
            "It: 560, Time: 0.84\n",
            "mse_b  [319843.44]  mse_f: [78.92685]   total loss: [319922.38]\n",
            "It: 600, Time: 0.80\n",
            "mse_b  [310644.62]  mse_f: [7518.066]   total loss: [318162.7]\n",
            "It: 640, Time: 0.81\n",
            "mse_b  [315759.28]  mse_f: [1376.2017]   total loss: [317135.47]\n",
            "It: 680, Time: 0.83\n",
            "mse_b  [313453.3]  mse_f: [3380.6409]   total loss: [316833.97]\n",
            "It: 720, Time: 0.85\n",
            "mse_b  [314323.72]  mse_f: [2386.2336]   total loss: [316709.94]\n",
            "It: 760, Time: 0.85\n",
            "mse_b  [313071.78]  mse_f: [3531.2402]   total loss: [316603.03]\n",
            "It: 800, Time: 0.84\n",
            "mse_b  [314797.84]  mse_f: [1712.718]   total loss: [316510.56]\n",
            "It: 840, Time: 0.83\n",
            "mse_b  [311745.44]  mse_f: [4544.4414]   total loss: [316289.88]\n",
            "It: 880, Time: 0.81\n",
            "mse_b  [313287.44]  mse_f: [2409.3596]   total loss: [315696.8]\n",
            "It: 920, Time: 0.83\n",
            "mse_b  [312785.7]  mse_f: [2322.2708]   total loss: [315107.97]\n",
            "It: 960, Time: 0.95\n",
            "mse_b  [310633.84]  mse_f: [3416.3772]   total loss: [314050.22]\n",
            "It: 1000, Time: 1.14\n",
            "mse_b  [308322.9]  mse_f: [4268.5005]   total loss: [312591.4]\n",
            "It: 1040, Time: 1.03\n",
            "mse_b  [305543.97]  mse_f: [4937.2705]   total loss: [310481.25]\n",
            "It: 1080, Time: 0.86\n",
            "mse_b  [300597.53]  mse_f: [7127.769]   total loss: [307725.3]\n",
            "It: 1120, Time: 0.86\n",
            "mse_b  [292978.94]  mse_f: [11569.941]   total loss: [304548.88]\n",
            "It: 1160, Time: 0.82\n",
            "mse_b  [288198.38]  mse_f: [11029.07]   total loss: [299227.44]\n",
            "It: 1200, Time: 0.83\n",
            "mse_b  [277880.7]  mse_f: [10155.924]   total loss: [288036.62]\n",
            "It: 1240, Time: 0.83\n",
            "mse_b  [261318.23]  mse_f: [9126.704]   total loss: [270444.94]\n",
            "It: 1280, Time: 0.84\n",
            "mse_b  [252289.58]  mse_f: [8032.168]   total loss: [260321.75]\n",
            "It: 1320, Time: 0.85\n",
            "mse_b  [244155.73]  mse_f: [7619.6987]   total loss: [251775.44]\n",
            "It: 1360, Time: 0.81\n",
            "mse_b  [240321.14]  mse_f: [5278.6406]   total loss: [245599.78]\n",
            "It: 1400, Time: 0.83\n",
            "mse_b  [236410.58]  mse_f: [6665.744]   total loss: [243076.33]\n",
            "It: 1440, Time: 0.82\n",
            "mse_b  [235674.44]  mse_f: [4067.0396]   total loss: [239741.48]\n",
            "It: 1480, Time: 0.83\n",
            "mse_b  [235663.55]  mse_f: [3430.6746]   total loss: [239094.22]\n",
            "It: 1520, Time: 0.85\n",
            "mse_b  [232390.9]  mse_f: [5042.9067]   total loss: [237433.81]\n",
            "It: 1560, Time: 0.83\n",
            "mse_b  [232078.83]  mse_f: [4219.8633]   total loss: [236298.69]\n",
            "It: 1600, Time: 0.80\n",
            "mse_b  [229848.]  mse_f: [5220.997]   total loss: [235069.]\n",
            "It: 1640, Time: 0.79\n",
            "mse_b  [228805.08]  mse_f: [5343.2305]   total loss: [234148.31]\n",
            "It: 1680, Time: 0.86\n",
            "mse_b  [228004.1]  mse_f: [5182.652]   total loss: [233186.75]\n",
            "It: 1720, Time: 0.89\n",
            "mse_b  [225739.98]  mse_f: [6569.1973]   total loss: [232309.19]\n",
            "It: 1760, Time: 0.86\n",
            "mse_b  [225561.61]  mse_f: [5736.886]   total loss: [231298.5]\n",
            "It: 1800, Time: 0.86\n",
            "mse_b  [224186.58]  mse_f: [6154.5806]   total loss: [230341.16]\n",
            "It: 1840, Time: 0.83\n",
            "mse_b  [222556.67]  mse_f: [6777.628]   total loss: [229334.3]\n",
            "It: 1880, Time: 0.84\n",
            "mse_b  [221549.44]  mse_f: [6664.8145]   total loss: [228214.25]\n",
            "It: 1920, Time: 0.82\n",
            "mse_b  [220024.8]  mse_f: [7017.584]   total loss: [227042.38]\n",
            "It: 1960, Time: 0.81\n",
            "mse_b  [218035.55]  mse_f: [7759.7393]   total loss: [225795.28]\n",
            "It: 2000, Time: 0.81\n",
            "mse_b  [216526.61]  mse_f: [7909.134]   total loss: [224435.75]\n",
            "It: 2040, Time: 0.81\n",
            "mse_b  [214567.47]  mse_f: [8336.154]   total loss: [222903.62]\n",
            "It: 2080, Time: 0.83\n",
            "mse_b  [211874.83]  mse_f: [9094.885]   total loss: [220969.72]\n",
            "It: 2120, Time: 0.81\n",
            "mse_b  [208015.27]  mse_f: [10180.115]   total loss: [218195.38]\n",
            "It: 2160, Time: 0.81\n",
            "mse_b  [201331.23]  mse_f: [11880.455]   total loss: [213211.69]\n",
            "It: 2200, Time: 0.81\n",
            "mse_b  [185427.23]  mse_f: [12695.098]   total loss: [198122.33]\n",
            "It: 2240, Time: 0.81\n",
            "mse_b  [153057.62]  mse_f: [10759.496]   total loss: [163817.12]\n",
            "It: 2280, Time: 1.33\n",
            "mse_b  [133675.3]  mse_f: [7226.383]   total loss: [140901.69]\n",
            "It: 2320, Time: 0.82\n",
            "mse_b  [122354.06]  mse_f: [6581.7236]   total loss: [128935.79]\n",
            "It: 2360, Time: 0.83\n",
            "mse_b  [129459.32]  mse_f: [128061.89]   total loss: [257521.22]\n",
            "It: 2400, Time: 0.82\n",
            "mse_b  [281197.06]  mse_f: [5957.5713]   total loss: [287154.62]\n",
            "It: 2440, Time: 0.83\n",
            "mse_b  [275875.5]  mse_f: [19264.035]   total loss: [295139.53]\n",
            "It: 2480, Time: 0.92\n",
            "mse_b  [276871.7]  mse_f: [8258.996]   total loss: [285130.7]\n",
            "It: 2520, Time: 0.83\n",
            "mse_b  [250956.4]  mse_f: [19154.137]   total loss: [270110.53]\n",
            "It: 2560, Time: 0.83\n",
            "mse_b  [254390.14]  mse_f: [20262.318]   total loss: [274652.47]\n",
            "It: 2600, Time: 0.82\n",
            "mse_b  [243734.69]  mse_f: [7455.248]   total loss: [251189.94]\n",
            "It: 2640, Time: 0.81\n",
            "mse_b  [229817.95]  mse_f: [14325.239]   total loss: [244143.19]\n",
            "It: 2680, Time: 0.84\n",
            "mse_b  [221442.7]  mse_f: [4816.0977]   total loss: [226258.8]\n",
            "It: 2720, Time: 0.82\n",
            "mse_b  [216530.23]  mse_f: [7784.246]   total loss: [224314.48]\n",
            "It: 2760, Time: 0.83\n",
            "mse_b  [211812.61]  mse_f: [5284.032]   total loss: [217096.64]\n",
            "It: 2800, Time: 0.83\n",
            "mse_b  [207299.34]  mse_f: [5828.507]   total loss: [213127.84]\n",
            "It: 2840, Time: 0.80\n",
            "mse_b  [200871.22]  mse_f: [7308.3076]   total loss: [208179.53]\n",
            "It: 2880, Time: 0.81\n",
            "mse_b  [195394.11]  mse_f: [7754.454]   total loss: [203148.56]\n",
            "It: 2920, Time: 0.81\n",
            "mse_b  [189786.47]  mse_f: [7802.7896]   total loss: [197589.27]\n",
            "It: 2960, Time: 0.80\n",
            "mse_b  [179080.77]  mse_f: [9169.468]   total loss: [188250.23]\n",
            "It: 3000, Time: 0.89\n",
            "mse_b  [162518.7]  mse_f: [11720.544]   total loss: [174239.25]\n",
            "It: 3040, Time: 0.83\n",
            "mse_b  [137894.97]  mse_f: [15759.238]   total loss: [153654.2]\n",
            "It: 3080, Time: 0.82\n",
            "mse_b  [116961.]  mse_f: [16100.102]   total loss: [133061.1]\n",
            "It: 3120, Time: 0.80\n",
            "mse_b  [113012.76]  mse_f: [12427.83]   total loss: [125440.586]\n",
            "It: 3160, Time: 0.84\n",
            "mse_b  [107713.5]  mse_f: [12324.714]   total loss: [120038.21]\n",
            "It: 3200, Time: 0.83\n",
            "mse_b  [104108.84]  mse_f: [11549.357]   total loss: [115658.2]\n",
            "It: 3240, Time: 0.84\n",
            "mse_b  [99172.5]  mse_f: [11780.86]   total loss: [110953.36]\n",
            "It: 3280, Time: 0.82\n",
            "mse_b  [96709.555]  mse_f: [10706.761]   total loss: [107416.31]\n",
            "It: 3320, Time: 0.81\n",
            "mse_b  [93725.22]  mse_f: [10361.185]   total loss: [104086.41]\n",
            "It: 3360, Time: 0.82\n",
            "mse_b  [90855.875]  mse_f: [10007.601]   total loss: [100863.48]\n",
            "It: 3400, Time: 0.82\n",
            "mse_b  [88606.3]  mse_f: [8624.445]   total loss: [97230.74]\n",
            "It: 3440, Time: 0.82\n",
            "mse_b  [86668.95]  mse_f: [7014.726]   total loss: [93683.68]\n",
            "It: 3480, Time: 0.83\n",
            "mse_b  [84207.28]  mse_f: [5847.7397]   total loss: [90055.02]\n",
            "It: 3520, Time: 0.80\n",
            "mse_b  [80719.375]  mse_f: [5282.791]   total loss: [86002.164]\n",
            "It: 3560, Time: 0.81\n",
            "mse_b  [75385.92]  mse_f: [5558.1777]   total loss: [80944.1]\n",
            "It: 3600, Time: 0.82\n",
            "mse_b  [68764.16]  mse_f: [6533.7793]   total loss: [75297.94]\n",
            "It: 3640, Time: 0.82\n",
            "mse_b  [61574.242]  mse_f: [5851.172]   total loss: [67425.414]\n",
            "It: 3680, Time: 0.83\n",
            "mse_b  [53122.754]  mse_f: [4128.573]   total loss: [57251.33]\n",
            "It: 3720, Time: 0.80\n",
            "mse_b  [46144.316]  mse_f: [4169.26]   total loss: [50313.58]\n",
            "It: 3760, Time: 0.83\n",
            "mse_b  [56552.75]  mse_f: [60214.117]   total loss: [116766.87]\n",
            "It: 3800, Time: 0.81\n",
            "mse_b  [69021.68]  mse_f: [14030.819]   total loss: [83052.5]\n",
            "It: 3840, Time: 0.83\n",
            "mse_b  [62259.605]  mse_f: [14199.884]   total loss: [76459.49]\n",
            "It: 3880, Time: 0.82\n",
            "mse_b  [56758.805]  mse_f: [14719.609]   total loss: [71478.414]\n",
            "It: 3920, Time: 0.81\n",
            "mse_b  [49347.082]  mse_f: [9476.2]   total loss: [58823.28]\n",
            "It: 3960, Time: 0.82\n",
            "mse_b  [44065.01]  mse_f: [8189.1846]   total loss: [52254.195]\n",
            "It: 4000, Time: 0.83\n",
            "mse_b  [43457.363]  mse_f: [7471.001]   total loss: [50928.363]\n",
            "It: 4040, Time: 0.80\n",
            "mse_b  [42412.637]  mse_f: [5753.672]   total loss: [48166.31]\n",
            "It: 4080, Time: 0.80\n",
            "mse_b  [42084.535]  mse_f: [3572.7996]   total loss: [45657.336]\n",
            "It: 4120, Time: 0.84\n",
            "mse_b  [42388.684]  mse_f: [2273.9644]   total loss: [44662.65]\n",
            "It: 4160, Time: 0.81\n",
            "mse_b  [41657.316]  mse_f: [2172.1973]   total loss: [43829.516]\n",
            "It: 4200, Time: 0.81\n",
            "mse_b  [41223.727]  mse_f: [2005.2848]   total loss: [43229.01]\n",
            "It: 4240, Time: 0.84\n",
            "mse_b  [41019.434]  mse_f: [1773.1533]   total loss: [42792.586]\n",
            "It: 4280, Time: 0.82\n",
            "mse_b  [40623.703]  mse_f: [1810.4392]   total loss: [42434.14]\n",
            "It: 4320, Time: 0.81\n",
            "mse_b  [40286.785]  mse_f: [1630.1716]   total loss: [41916.957]\n",
            "It: 4360, Time: 0.82\n",
            "mse_b  [40090.504]  mse_f: [1367.1184]   total loss: [41457.62]\n",
            "It: 4400, Time: 0.82\n",
            "mse_b  [39909.805]  mse_f: [1218.4822]   total loss: [41128.285]\n",
            "It: 4440, Time: 0.85\n",
            "mse_b  [39622.14]  mse_f: [1195.6243]   total loss: [40817.766]\n",
            "It: 4480, Time: 0.82\n",
            "mse_b  [39296.133]  mse_f: [1206.7032]   total loss: [40502.836]\n",
            "It: 4520, Time: 0.82\n",
            "mse_b  [39033.973]  mse_f: [1117.1409]   total loss: [40151.113]\n",
            "It: 4560, Time: 0.80\n",
            "mse_b  [38815.152]  mse_f: [997.69604]   total loss: [39812.848]\n",
            "It: 4600, Time: 0.86\n",
            "mse_b  [38511.17]  mse_f: [958.6518]   total loss: [39469.824]\n",
            "It: 4640, Time: 0.87\n",
            "mse_b  [38192.902]  mse_f: [924.9931]   total loss: [39117.895]\n",
            "It: 4680, Time: 0.85\n",
            "mse_b  [37837.223]  mse_f: [919.3047]   total loss: [38756.527]\n",
            "It: 4720, Time: 0.85\n",
            "mse_b  [37485.51]  mse_f: [900.4609]   total loss: [38385.973]\n",
            "It: 4760, Time: 0.83\n",
            "mse_b  [37112.39]  mse_f: [889.7144]   total loss: [38002.105]\n",
            "It: 4800, Time: 0.85\n",
            "mse_b  [36679.707]  mse_f: [927.11786]   total loss: [37606.824]\n",
            "It: 4840, Time: 0.81\n",
            "mse_b  [36242.09]  mse_f: [958.8942]   total loss: [37200.984]\n",
            "It: 4880, Time: 0.83\n",
            "mse_b  [35770.914]  mse_f: [1021.2678]   total loss: [36792.184]\n",
            "It: 4920, Time: 0.82\n",
            "mse_b  [35284.113]  mse_f: [1096.0237]   total loss: [36380.137]\n",
            "It: 4960, Time: 0.80\n",
            "mse_b  [34775.74]  mse_f: [1187.6643]   total loss: [35963.402]\n",
            "It: 5000, Time: 0.80\n",
            "mse_b  [34246.477]  mse_f: [1293.8724]   total loss: [35540.348]\n",
            "It: 5040, Time: 0.81\n",
            "mse_b  [33723.656]  mse_f: [1387.706]   total loss: [35111.363]\n",
            "It: 5080, Time: 0.88\n",
            "mse_b  [33201.79]  mse_f: [1477.4619]   total loss: [34679.25]\n",
            "It: 5120, Time: 0.82\n",
            "mse_b  [32677.262]  mse_f: [1564.6975]   total loss: [34241.96]\n",
            "It: 5160, Time: 0.81\n",
            "mse_b  [32161.08]  mse_f: [1630.1003]   total loss: [33791.18]\n",
            "It: 5200, Time: 0.83\n",
            "mse_b  [31655.346]  mse_f: [1671.6365]   total loss: [33326.98]\n",
            "It: 5240, Time: 0.82\n",
            "mse_b  [31165.936]  mse_f: [1688.3632]   total loss: [32854.297]\n",
            "It: 5280, Time: 0.82\n",
            "mse_b  [30669.885]  mse_f: [1703.9155]   total loss: [32373.8]\n",
            "It: 5320, Time: 0.85\n",
            "mse_b  [30148.953]  mse_f: [1737.8079]   total loss: [31886.762]\n",
            "It: 5360, Time: 0.83\n",
            "mse_b  [29593.3]  mse_f: [1799.3527]   total loss: [31392.654]\n",
            "It: 5400, Time: 0.86\n",
            "mse_b  [29023.766]  mse_f: [1875.1852]   total loss: [30898.951]\n",
            "It: 5440, Time: 0.83\n",
            "mse_b  [28452.627]  mse_f: [1964.355]   total loss: [30416.982]\n",
            "It: 5480, Time: 0.82\n",
            "mse_b  [27886.932]  mse_f: [2057.2466]   total loss: [29944.178]\n",
            "It: 5520, Time: 0.81\n",
            "mse_b  [27349.184]  mse_f: [2124.688]   total loss: [29473.871]\n",
            "It: 5560, Time: 0.84\n",
            "mse_b  [26826.758]  mse_f: [2186.3743]   total loss: [29013.133]\n",
            "It: 5600, Time: 0.83\n",
            "mse_b  [26290.262]  mse_f: [2260.629]   total loss: [28550.89]\n",
            "It: 5640, Time: 0.83\n",
            "mse_b  [25727.55]  mse_f: [2331.5947]   total loss: [28059.145]\n",
            "It: 5680, Time: 0.82\n",
            "mse_b  [25074.578]  mse_f: [2502.2432]   total loss: [27576.82]\n",
            "It: 5720, Time: 0.86\n",
            "mse_b  [24480.264]  mse_f: [3164.6243]   total loss: [27644.889]\n",
            "It: 5760, Time: 0.85\n",
            "mse_b  [148154.58]  mse_f: [22205.746]   total loss: [170360.33]\n",
            "It: 5800, Time: 0.89\n",
            "mse_b  [115327.97]  mse_f: [24384.824]   total loss: [139712.8]\n",
            "It: 5840, Time: 0.82\n",
            "mse_b  [116984.664]  mse_f: [9708.845]   total loss: [126693.51]\n",
            "It: 5880, Time: 0.87\n",
            "mse_b  [114955.05]  mse_f: [5552.3716]   total loss: [120507.42]\n",
            "It: 5920, Time: 0.83\n",
            "mse_b  [111569.92]  mse_f: [9453.513]   total loss: [121023.44]\n",
            "It: 5960, Time: 0.82\n",
            "mse_b  [109720.92]  mse_f: [3718.5947]   total loss: [113439.516]\n",
            "It: 6000, Time: 0.83\n",
            "mse_b  [106677.18]  mse_f: [4273.5547]   total loss: [110950.734]\n",
            "It: 6040, Time: 0.87\n",
            "mse_b  [105313.805]  mse_f: [2731.6284]   total loss: [108045.43]\n",
            "It: 6080, Time: 0.89\n",
            "mse_b  [104347.49]  mse_f: [3073.7312]   total loss: [107421.23]\n",
            "It: 6120, Time: 0.82\n",
            "mse_b  [102127.305]  mse_f: [3369.917]   total loss: [105497.22]\n",
            "It: 6160, Time: 0.82\n",
            "mse_b  [101221.72]  mse_f: [3134.6938]   total loss: [104356.414]\n",
            "It: 6200, Time: 0.84\n",
            "mse_b  [100271.28]  mse_f: [2944.4868]   total loss: [103215.766]\n",
            "It: 6240, Time: 0.82\n",
            "mse_b  [98785.12]  mse_f: [3342.7363]   total loss: [102127.85]\n",
            "It: 6280, Time: 0.82\n",
            "mse_b  [96302.9]  mse_f: [4665.8677]   total loss: [100968.766]\n",
            "It: 6320, Time: 0.85\n",
            "mse_b  [94963.82]  mse_f: [5115.8745]   total loss: [100079.695]\n",
            "It: 6360, Time: 0.90\n",
            "mse_b  [93980.54]  mse_f: [5084.329]   total loss: [99064.87]\n",
            "It: 6400, Time: 0.84\n",
            "mse_b  [91718.49]  mse_f: [6030.5547]   total loss: [97749.05]\n",
            "It: 6440, Time: 0.87\n",
            "mse_b  [89476.086]  mse_f: [6595.953]   total loss: [96072.04]\n",
            "It: 6480, Time: 0.84\n",
            "mse_b  [85843.75]  mse_f: [7309.7344]   total loss: [93153.484]\n",
            "It: 6520, Time: 0.85\n",
            "mse_b  [77796.15]  mse_f: [8253.014]   total loss: [86049.164]\n",
            "It: 6560, Time: 0.82\n",
            "mse_b  [50195.79]  mse_f: [10635.015]   total loss: [60830.805]\n",
            "It: 6600, Time: 0.86\n",
            "mse_b  [37776.332]  mse_f: [9199.99]   total loss: [46976.32]\n",
            "It: 6640, Time: 0.85\n",
            "mse_b  [35506.43]  mse_f: [5578.0874]   total loss: [41084.516]\n",
            "It: 6680, Time: 0.87\n",
            "mse_b  [34439.53]  mse_f: [3083.518]   total loss: [37523.05]\n",
            "It: 6720, Time: 0.83\n",
            "mse_b  [33151.61]  mse_f: [3573.1973]   total loss: [36724.805]\n",
            "It: 6760, Time: 0.85\n",
            "mse_b  [31520.584]  mse_f: [2862.3606]   total loss: [34382.945]\n",
            "It: 6800, Time: 0.83\n",
            "mse_b  [30228.889]  mse_f: [3057.9211]   total loss: [33286.81]\n",
            "It: 6840, Time: 0.85\n",
            "mse_b  [29319.244]  mse_f: [2719.0413]   total loss: [32038.285]\n",
            "It: 6880, Time: 0.84\n",
            "mse_b  [28742.965]  mse_f: [2414.578]   total loss: [31157.543]\n",
            "It: 6920, Time: 0.82\n",
            "mse_b  [28132.033]  mse_f: [2331.2134]   total loss: [30463.246]\n",
            "It: 6960, Time: 0.83\n",
            "mse_b  [27488.518]  mse_f: [2337.9016]   total loss: [29826.42]\n",
            "It: 7000, Time: 0.83\n",
            "mse_b  [26895.846]  mse_f: [2388.8462]   total loss: [29284.691]\n",
            "It: 7040, Time: 0.85\n",
            "mse_b  [26425.355]  mse_f: [2380.3845]   total loss: [28805.74]\n",
            "It: 7080, Time: 0.86\n",
            "mse_b  [26025.754]  mse_f: [2339.9453]   total loss: [28365.7]\n",
            "It: 7120, Time: 0.82\n",
            "mse_b  [25617.783]  mse_f: [2325.3909]   total loss: [27943.174]\n",
            "It: 7160, Time: 0.84\n",
            "mse_b  [25148.12]  mse_f: [2398.316]   total loss: [27546.436]\n",
            "It: 7200, Time: 0.83\n",
            "mse_b  [24802.404]  mse_f: [2321.7825]   total loss: [27124.188]\n",
            "It: 7240, Time: 0.83\n",
            "mse_b  [24607.896]  mse_f: [2128.182]   total loss: [26736.078]\n",
            "It: 7280, Time: 0.83\n",
            "mse_b  [24343.158]  mse_f: [2006.0045]   total loss: [26349.162]\n",
            "It: 7320, Time: 0.87\n",
            "mse_b  [24081.941]  mse_f: [1887.605]   total loss: [25969.547]\n",
            "It: 7360, Time: 1.33\n",
            "mse_b  [23783.316]  mse_f: [1808.1509]   total loss: [25591.467]\n",
            "It: 7400, Time: 0.83\n",
            "mse_b  [23448.674]  mse_f: [1752.9949]   total loss: [25201.668]\n",
            "It: 7440, Time: 0.83\n",
            "mse_b  [23064.469]  mse_f: [1738.5474]   total loss: [24803.016]\n",
            "It: 7480, Time: 0.88\n",
            "mse_b  [22679.168]  mse_f: [1718.2748]   total loss: [24397.443]\n",
            "It: 7520, Time: 0.83\n",
            "mse_b  [22289.352]  mse_f: [1701.7981]   total loss: [23991.15]\n",
            "It: 7560, Time: 0.82\n",
            "mse_b  [22758.87]  mse_f: [2773.1086]   total loss: [25531.979]\n",
            "It: 7600, Time: 0.83\n",
            "mse_b  [125756.8]  mse_f: [37858.883]   total loss: [163615.69]\n",
            "It: 7640, Time: 0.85\n",
            "mse_b  [105636.24]  mse_f: [28219.012]   total loss: [133855.25]\n",
            "It: 7680, Time: 0.83\n",
            "mse_b  [113841.414]  mse_f: [20096.86]   total loss: [133938.28]\n",
            "It: 7720, Time: 0.80\n",
            "mse_b  [101766.81]  mse_f: [14367.497]   total loss: [116134.31]\n",
            "It: 7760, Time: 0.82\n",
            "mse_b  [97971.84]  mse_f: [16059.951]   total loss: [114031.8]\n",
            "It: 7800, Time: 0.82\n",
            "mse_b  [96371.35]  mse_f: [8509.799]   total loss: [104881.15]\n",
            "It: 7840, Time: 0.82\n",
            "mse_b  [94691.445]  mse_f: [9292.773]   total loss: [103984.22]\n",
            "It: 7880, Time: 0.82\n",
            "mse_b  [94774.12]  mse_f: [6212.492]   total loss: [100986.61]\n",
            "It: 7920, Time: 0.82\n",
            "mse_b  [93804.35]  mse_f: [5112.1235]   total loss: [98916.48]\n",
            "It: 7960, Time: 0.84\n",
            "mse_b  [92396.984]  mse_f: [5189.815]   total loss: [97586.8]\n",
            "It: 8000, Time: 0.82\n",
            "mse_b  [90185.836]  mse_f: [6330.225]   total loss: [96516.06]\n",
            "It: 8040, Time: 0.82\n",
            "mse_b  [88485.73]  mse_f: [6321.5586]   total loss: [94807.28]\n",
            "It: 8080, Time: 0.84\n",
            "mse_b  [86479.3]  mse_f: [7078.911]   total loss: [93558.21]\n",
            "It: 8120, Time: 0.97\n",
            "mse_b  [83756.17]  mse_f: [7992.5205]   total loss: [91748.695]\n",
            "It: 8160, Time: 1.60\n",
            "mse_b  [79033.85]  mse_f: [9168.881]   total loss: [88202.734]\n",
            "It: 8200, Time: 1.50\n",
            "mse_b  [62580.832]  mse_f: [12342.734]   total loss: [74923.56]\n",
            "It: 8240, Time: 1.00\n",
            "mse_b  [41259.52]  mse_f: [12623.962]   total loss: [53883.48]\n",
            "It: 8280, Time: 0.81\n",
            "mse_b  [32166.436]  mse_f: [11803.34]   total loss: [43969.773]\n",
            "It: 8320, Time: 0.81\n",
            "mse_b  [31529.06]  mse_f: [6806.1997]   total loss: [38335.26]\n",
            "It: 8360, Time: 0.80\n",
            "mse_b  [30261.693]  mse_f: [4103.8105]   total loss: [34365.504]\n",
            "It: 8400, Time: 0.81\n",
            "mse_b  [29441.943]  mse_f: [4205.7725]   total loss: [33647.715]\n",
            "It: 8440, Time: 0.81\n",
            "mse_b  [28320.9]  mse_f: [4424.501]   total loss: [32745.402]\n",
            "It: 8480, Time: 0.83\n",
            "mse_b  [27775.555]  mse_f: [3312.9019]   total loss: [31088.457]\n",
            "It: 8520, Time: 0.80\n",
            "mse_b  [27101.658]  mse_f: [3169.3982]   total loss: [30271.057]\n",
            "It: 8560, Time: 0.83\n",
            "mse_b  [26562.344]  mse_f: [3221.8213]   total loss: [29784.164]\n",
            "It: 8600, Time: 0.86\n",
            "mse_b  [26431.125]  mse_f: [2607.273]   total loss: [29038.398]\n",
            "It: 8640, Time: 0.83\n",
            "mse_b  [26144.152]  mse_f: [2446.353]   total loss: [28590.506]\n",
            "It: 8680, Time: 0.84\n",
            "mse_b  [25770.41]  mse_f: [2396.632]   total loss: [28167.043]\n",
            "It: 8720, Time: 0.82\n",
            "mse_b  [25494.455]  mse_f: [2307.605]   total loss: [27802.06]\n",
            "It: 8760, Time: 0.81\n",
            "mse_b  [25092.678]  mse_f: [2337.7583]   total loss: [27430.436]\n",
            "It: 8800, Time: 0.82\n",
            "mse_b  [24808.605]  mse_f: [2339.5518]   total loss: [27148.156]\n",
            "It: 8840, Time: 0.85\n",
            "mse_b  [24522.145]  mse_f: [2330.773]   total loss: [26852.918]\n",
            "It: 8880, Time: 0.83\n",
            "mse_b  [24234.473]  mse_f: [2336.482]   total loss: [26570.955]\n",
            "It: 8920, Time: 0.85\n",
            "mse_b  [23992.879]  mse_f: [2295.046]   total loss: [26287.926]\n",
            "It: 8960, Time: 0.87\n",
            "mse_b  [23725.447]  mse_f: [2283.81]   total loss: [26009.258]\n",
            "It: 9000, Time: 0.85\n",
            "mse_b  [23463.168]  mse_f: [2263.6304]   total loss: [25726.799]\n",
            "It: 9040, Time: 0.85\n",
            "mse_b  [23157.672]  mse_f: [2285.3877]   total loss: [25443.059]\n",
            "It: 9080, Time: 0.83\n",
            "mse_b  [22859.262]  mse_f: [2304.0579]   total loss: [25163.32]\n",
            "It: 9120, Time: 0.82\n",
            "mse_b  [22555.31]  mse_f: [2335.3013]   total loss: [24890.611]\n",
            "It: 9160, Time: 0.82\n",
            "mse_b  [22263.584]  mse_f: [2360.5242]   total loss: [24624.107]\n",
            "It: 9200, Time: 0.82\n",
            "mse_b  [21973.861]  mse_f: [2393.6948]   total loss: [24367.557]\n",
            "It: 9240, Time: 0.83\n",
            "mse_b  [21651.467]  mse_f: [2593.4062]   total loss: [24244.873]\n",
            "It: 9280, Time: 0.80\n",
            "mse_b  [21597.166]  mse_f: [2495.3162]   total loss: [24092.482]\n",
            "It: 9320, Time: 0.81\n",
            "mse_b  [21181.521]  mse_f: [2451.626]   total loss: [23633.148]\n",
            "It: 9360, Time: 0.82\n",
            "mse_b  [21014.193]  mse_f: [2445.6055]   total loss: [23459.799]\n",
            "It: 9400, Time: 0.82\n",
            "mse_b  [20781.598]  mse_f: [2437.8918]   total loss: [23219.49]\n",
            "It: 9440, Time: 0.81\n",
            "mse_b  [20580.502]  mse_f: [2431.5098]   total loss: [23012.012]\n",
            "It: 9480, Time: 0.82\n",
            "mse_b  [20312.584]  mse_f: [2401.1982]   total loss: [22713.781]\n",
            "It: 9520, Time: 0.86\n",
            "mse_b  [20360.21]  mse_f: [2584.332]   total loss: [22944.543]\n",
            "It: 9560, Time: 0.83\n",
            "mse_b  [19864.803]  mse_f: [2780.3]   total loss: [22645.104]\n",
            "It: 9600, Time: 0.81\n",
            "mse_b  [19995.736]  mse_f: [2582.838]   total loss: [22578.574]\n",
            "It: 9640, Time: 0.84\n",
            "mse_b  [19471.947]  mse_f: [2669.9155]   total loss: [22141.863]\n",
            "It: 9680, Time: 0.81\n",
            "mse_b  [19296.357]  mse_f: [2343.3882]   total loss: [21639.746]\n",
            "It: 9720, Time: 0.86\n",
            "mse_b  [19185.639]  mse_f: [2259.206]   total loss: [21444.844]\n",
            "It: 9760, Time: 0.86\n",
            "mse_b  [19104.023]  mse_f: [2281.425]   total loss: [21385.45]\n",
            "It: 9800, Time: 0.84\n",
            "mse_b  [18761.21]  mse_f: [2548.0444]   total loss: [21309.256]\n",
            "It: 9840, Time: 0.81\n",
            "mse_b  [18567.33]  mse_f: [2560.0388]   total loss: [21127.37]\n",
            "It: 9880, Time: 0.82\n",
            "mse_b  [18381.111]  mse_f: [2183.4216]   total loss: [20564.533]\n",
            "It: 9920, Time: 0.84\n",
            "mse_b  [18379.596]  mse_f: [2237.6597]   total loss: [20617.256]\n",
            "It: 9960, Time: 0.81\n",
            "mse_b  [18252.121]  mse_f: [2403.3413]   total loss: [20655.463]\n",
            "It: 10000, Time: 0.83\n",
            "mse_b  [17788.346]  mse_f: [2084.6055]   total loss: [19872.951]\n",
            "It: 10040, Time: 0.80\n",
            "mse_b  [17483.582]  mse_f: [2234.5032]   total loss: [19718.086]\n",
            "It: 10080, Time: 0.85\n",
            "mse_b  [17286.65]  mse_f: [2447.6145]   total loss: [19734.266]\n",
            "It: 10120, Time: 0.86\n",
            "mse_b  [17082.492]  mse_f: [2574.8171]   total loss: [19657.309]\n",
            "It: 10160, Time: 0.82\n",
            "mse_b  [16857.988]  mse_f: [2439.3516]   total loss: [19297.34]\n",
            "It: 10200, Time: 0.79\n",
            "mse_b  [16631.59]  mse_f: [1967.8945]   total loss: [18599.484]\n",
            "It: 10240, Time: 0.82\n",
            "mse_b  [16506.164]  mse_f: [1850.8228]   total loss: [18356.986]\n",
            "It: 10280, Time: 0.81\n",
            "mse_b  [16358.078]  mse_f: [1994.6156]   total loss: [18352.693]\n",
            "It: 10320, Time: 0.83\n",
            "mse_b  [16074.339]  mse_f: [2007.589]   total loss: [18081.928]\n",
            "It: 10360, Time: 0.83\n",
            "mse_b  [15776.592]  mse_f: [1978.8801]   total loss: [17755.473]\n",
            "It: 10400, Time: 0.87\n",
            "mse_b  [15453.8955]  mse_f: [1910.7697]   total loss: [17364.666]\n",
            "It: 10440, Time: 0.85\n",
            "mse_b  [14968.002]  mse_f: [1702.0709]   total loss: [16670.072]\n",
            "It: 10480, Time: 0.82\n",
            "mse_b  [14592.6]  mse_f: [2283.2488]   total loss: [16875.848]\n",
            "It: 10520, Time: 0.81\n",
            "mse_b  [14634.237]  mse_f: [2286.4058]   total loss: [16920.643]\n",
            "It: 10560, Time: 0.83\n",
            "mse_b  [14115.155]  mse_f: [1811.85]   total loss: [15927.005]\n",
            "It: 10600, Time: 0.85\n",
            "mse_b  [13977.141]  mse_f: [1819.9022]   total loss: [15797.043]\n",
            "It: 10640, Time: 0.85\n",
            "mse_b  [13769.489]  mse_f: [2041.0435]   total loss: [15810.533]\n",
            "It: 10680, Time: 0.83\n",
            "mse_b  [13735.052]  mse_f: [2203.5776]   total loss: [15938.629]\n",
            "It: 10720, Time: 0.82\n",
            "mse_b  [13510.8]  mse_f: [2410.303]   total loss: [15921.103]\n",
            "It: 10760, Time: 0.83\n",
            "mse_b  [13404.325]  mse_f: [1947.4241]   total loss: [15351.749]\n",
            "It: 10800, Time: 0.83\n",
            "mse_b  [13289.993]  mse_f: [1837.5088]   total loss: [15127.502]\n",
            "It: 10840, Time: 0.82\n",
            "mse_b  [13150.697]  mse_f: [1782.9348]   total loss: [14933.632]\n",
            "It: 10880, Time: 0.84\n",
            "mse_b  [13091.484]  mse_f: [1908.7115]   total loss: [15000.196]\n",
            "It: 10920, Time: 0.83\n",
            "mse_b  [12974.312]  mse_f: [1835.0795]   total loss: [14809.391]\n",
            "It: 10960, Time: 0.82\n",
            "mse_b  [12781.856]  mse_f: [2004.0859]   total loss: [14785.942]\n",
            "It: 11000, Time: 0.89\n",
            "mse_b  [12832.673]  mse_f: [2984.552]   total loss: [15817.225]\n",
            "It: 11040, Time: 0.83\n",
            "mse_b  [19471.8]  mse_f: [21465.184]   total loss: [40936.984]\n",
            "It: 11080, Time: 0.85\n",
            "mse_b  [24748.096]  mse_f: [23641.629]   total loss: [48389.727]\n",
            "It: 11120, Time: 0.82\n",
            "mse_b  [23642.467]  mse_f: [12573.548]   total loss: [36216.016]\n",
            "It: 11160, Time: 0.81\n",
            "mse_b  [19201.54]  mse_f: [8931.449]   total loss: [28132.988]\n",
            "It: 11200, Time: 0.81\n",
            "mse_b  [22942.078]  mse_f: [4859.983]   total loss: [27802.06]\n",
            "It: 11240, Time: 0.83\n",
            "mse_b  [18809.018]  mse_f: [4570.527]   total loss: [23379.545]\n",
            "It: 11280, Time: 0.80\n",
            "mse_b  [16961.021]  mse_f: [4337.0015]   total loss: [21298.023]\n",
            "It: 11320, Time: 0.84\n",
            "mse_b  [16240.677]  mse_f: [4276.7144]   total loss: [20517.39]\n",
            "It: 11360, Time: 0.82\n",
            "mse_b  [17180.125]  mse_f: [2524.3638]   total loss: [19704.488]\n",
            "It: 11400, Time: 0.80\n",
            "mse_b  [15300.335]  mse_f: [3285.6033]   total loss: [18585.938]\n",
            "It: 11440, Time: 0.81\n",
            "mse_b  [15612.148]  mse_f: [2344.9673]   total loss: [17957.115]\n",
            "It: 11480, Time: 0.83\n",
            "mse_b  [14564.077]  mse_f: [2405.2876]   total loss: [16969.365]\n",
            "It: 11520, Time: 0.82\n",
            "mse_b  [14344.242]  mse_f: [2287.3135]   total loss: [16631.555]\n",
            "It: 11560, Time: 0.85\n",
            "mse_b  [13964.768]  mse_f: [2133.6694]   total loss: [16098.4375]\n",
            "It: 11600, Time: 0.82\n",
            "mse_b  [13711.99]  mse_f: [2026.4484]   total loss: [15738.438]\n",
            "It: 11640, Time: 0.81\n",
            "mse_b  [13419.582]  mse_f: [1917.5991]   total loss: [15337.182]\n",
            "It: 11680, Time: 0.81\n",
            "mse_b  [13162.156]  mse_f: [1844.0521]   total loss: [15006.208]\n",
            "It: 11720, Time: 0.83\n",
            "mse_b  [12952.776]  mse_f: [1762.023]   total loss: [14714.799]\n",
            "It: 11760, Time: 0.84\n",
            "mse_b  [12683.785]  mse_f: [1760.1837]   total loss: [14443.969]\n",
            "It: 11800, Time: 0.80\n",
            "mse_b  [12511.48]  mse_f: [1694.3423]   total loss: [14205.822]\n",
            "It: 11840, Time: 0.81\n",
            "mse_b  [12341.719]  mse_f: [1681.2925]   total loss: [14023.012]\n",
            "It: 11880, Time: 0.87\n",
            "mse_b  [12175.745]  mse_f: [1670.5085]   total loss: [13846.254]\n",
            "It: 11920, Time: 0.84\n",
            "mse_b  [12087.345]  mse_f: [1602.7607]   total loss: [13690.105]\n",
            "It: 11960, Time: 0.81\n",
            "mse_b  [11990.408]  mse_f: [1565.4143]   total loss: [13555.822]\n",
            "It: 12000, Time: 0.80\n",
            "mse_b  [11913.005]  mse_f: [1509.7535]   total loss: [13422.759]\n",
            "It: 12040, Time: 0.81\n",
            "mse_b  [11829.27]  mse_f: [1470.1191]   total loss: [13299.389]\n",
            "It: 12080, Time: 0.81\n",
            "mse_b  [11748.997]  mse_f: [1436.0126]   total loss: [13185.01]\n",
            "It: 12120, Time: 0.81\n",
            "mse_b  [11652.568]  mse_f: [1426.0391]   total loss: [13078.607]\n",
            "It: 12160, Time: 0.82\n",
            "mse_b  [11556.896]  mse_f: [1419.7173]   total loss: [12976.613]\n",
            "It: 12200, Time: 0.81\n",
            "mse_b  [11464.052]  mse_f: [1411.4482]   total loss: [12875.5]\n",
            "It: 12240, Time: 0.81\n",
            "mse_b  [11377.616]  mse_f: [1398.562]   total loss: [12776.178]\n",
            "It: 12280, Time: 0.81\n",
            "mse_b  [11526.073]  mse_f: [4035.209]   total loss: [15561.282]\n",
            "It: 12320, Time: 0.81\n",
            "mse_b  [11504.121]  mse_f: [2846.0725]   total loss: [14350.193]\n",
            "It: 12360, Time: 1.31\n",
            "mse_b  [32604.1]  mse_f: [33085.664]   total loss: [65689.766]\n",
            "It: 12400, Time: 0.82\n",
            "mse_b  [199272.67]  mse_f: [25956.875]   total loss: [225229.55]\n",
            "It: 12440, Time: 0.81\n",
            "mse_b  [190257.06]  mse_f: [17893.492]   total loss: [208150.56]\n",
            "It: 12480, Time: 0.80\n",
            "mse_b  [175793.23]  mse_f: [15612.779]   total loss: [191406.02]\n",
            "It: 12520, Time: 0.80\n",
            "mse_b  [170726.08]  mse_f: [17783.951]   total loss: [188510.03]\n",
            "It: 12560, Time: 0.83\n",
            "mse_b  [173595.05]  mse_f: [6808.953]   total loss: [180404.]\n",
            "It: 12600, Time: 0.82\n",
            "mse_b  [171997.69]  mse_f: [7260.701]   total loss: [179258.39]\n",
            "It: 12640, Time: 0.80\n",
            "mse_b  [170764.1]  mse_f: [5625.453]   total loss: [176389.55]\n",
            "It: 12680, Time: 0.81\n",
            "mse_b  [170822.81]  mse_f: [2740.0518]   total loss: [173562.86]\n",
            "It: 12720, Time: 0.83\n",
            "mse_b  [168302.53]  mse_f: [4719.0947]   total loss: [173021.62]\n",
            "It: 12760, Time: 0.80\n",
            "mse_b  [167975.02]  mse_f: [3890.3296]   total loss: [171865.34]\n",
            "It: 12800, Time: 0.82\n",
            "mse_b  [167974.72]  mse_f: [3210.5664]   total loss: [171185.28]\n",
            "It: 12840, Time: 0.82\n",
            "mse_b  [165566.16]  mse_f: [4331.833]   total loss: [169897.98]\n",
            "It: 12880, Time: 0.83\n",
            "mse_b  [163618.83]  mse_f: [4755.143]   total loss: [168373.97]\n",
            "It: 12920, Time: 0.83\n",
            "mse_b  [159341.34]  mse_f: [6338.074]   total loss: [165679.42]\n",
            "It: 12960, Time: 0.82\n",
            "mse_b  [150555.33]  mse_f: [6467.0547]   total loss: [157022.38]\n",
            "It: 13000, Time: 0.84\n",
            "mse_b  [125887.36]  mse_f: [21978.803]   total loss: [147866.16]\n",
            "It: 13040, Time: 0.80\n",
            "mse_b  [172127.75]  mse_f: [9004.44]   total loss: [181132.19]\n",
            "It: 13080, Time: 0.81\n",
            "mse_b  [172585.78]  mse_f: [10448.857]   total loss: [183034.64]\n",
            "It: 13120, Time: 0.83\n",
            "mse_b  [166125.22]  mse_f: [8742.664]   total loss: [174867.88]\n",
            "It: 13160, Time: 0.80\n",
            "mse_b  [165278.12]  mse_f: [5383.829]   total loss: [170661.95]\n",
            "It: 13200, Time: 0.89\n",
            "mse_b  [164225.7]  mse_f: [6098.0557]   total loss: [170323.77]\n",
            "It: 13240, Time: 0.88\n",
            "mse_b  [164177.7]  mse_f: [4931.1846]   total loss: [169108.89]\n",
            "It: 13280, Time: 0.83\n",
            "mse_b  [161623.66]  mse_f: [6090.579]   total loss: [167714.23]\n",
            "It: 13320, Time: 0.89\n",
            "mse_b  [160181.12]  mse_f: [7110.6714]   total loss: [167291.8]\n",
            "It: 13360, Time: 0.85\n",
            "mse_b  [158175.14]  mse_f: [8220.449]   total loss: [166395.6]\n",
            "It: 13400, Time: 0.80\n",
            "mse_b  [157083.5]  mse_f: [8496.85]   total loss: [165580.34]\n",
            "It: 13440, Time: 0.80\n",
            "mse_b  [157045.28]  mse_f: [7624.3623]   total loss: [164669.64]\n",
            "It: 13480, Time: 0.82\n",
            "mse_b  [154076.25]  mse_f: [9385.231]   total loss: [163461.48]\n",
            "It: 13520, Time: 0.80\n",
            "mse_b  [152325.08]  mse_f: [9591.05]   total loss: [161916.12]\n",
            "It: 13560, Time: 0.82\n",
            "mse_b  [147681.33]  mse_f: [11483.471]   total loss: [159164.8]\n",
            "It: 13600, Time: 0.83\n",
            "mse_b  [134726.]  mse_f: [16267.351]   total loss: [150993.34]\n",
            "It: 13640, Time: 0.80\n",
            "mse_b  [91613.68]  mse_f: [28555.793]   total loss: [120169.47]\n",
            "It: 13680, Time: 0.80\n",
            "mse_b  [65604.52]  mse_f: [27089.186]   total loss: [92693.71]\n",
            "It: 13720, Time: 0.81\n",
            "mse_b  [39955.973]  mse_f: [17550.354]   total loss: [57506.33]\n",
            "It: 13760, Time: 0.81\n",
            "mse_b  [36153.688]  mse_f: [20729.508]   total loss: [56883.195]\n",
            "It: 13800, Time: 0.82\n",
            "mse_b  [199670.72]  mse_f: [7315.226]   total loss: [206985.94]\n",
            "It: 13840, Time: 0.82\n",
            "mse_b  [164763.8]  mse_f: [15822.652]   total loss: [180586.45]\n",
            "It: 13880, Time: 0.84\n",
            "mse_b  [61236.656]  mse_f: [71959.93]   total loss: [133196.6]\n",
            "It: 13920, Time: 0.82\n",
            "mse_b  [51739.74]  mse_f: [24500.523]   total loss: [76240.266]\n",
            "It: 13960, Time: 0.83\n",
            "mse_b  [43598.875]  mse_f: [22079.734]   total loss: [65678.61]\n",
            "It: 14000, Time: 0.86\n",
            "mse_b  [32830.28]  mse_f: [20990.262]   total loss: [53820.543]\n",
            "It: 14040, Time: 0.80\n",
            "mse_b  [30273.162]  mse_f: [21305.922]   total loss: [51579.086]\n",
            "It: 14080, Time: 0.80\n",
            "mse_b  [30221.414]  mse_f: [15021.799]   total loss: [45243.21]\n",
            "It: 14120, Time: 0.81\n",
            "mse_b  [30646.854]  mse_f: [8936.472]   total loss: [39583.324]\n",
            "It: 14160, Time: 0.80\n",
            "mse_b  [29501.75]  mse_f: [6412.968]   total loss: [35914.72]\n",
            "It: 14200, Time: 0.81\n",
            "mse_b  [26679.355]  mse_f: [7106.753]   total loss: [33786.11]\n",
            "It: 14240, Time: 0.81\n",
            "mse_b  [26489.281]  mse_f: [5913.68]   total loss: [32402.96]\n",
            "It: 14280, Time: 0.82\n",
            "mse_b  [25483.965]  mse_f: [6160.651]   total loss: [31644.615]\n",
            "It: 14320, Time: 0.81\n",
            "mse_b  [25016.268]  mse_f: [5876.427]   total loss: [30892.695]\n",
            "It: 14360, Time: 0.85\n",
            "mse_b  [24654.744]  mse_f: [5241.7695]   total loss: [29896.514]\n",
            "It: 14400, Time: 0.81\n",
            "mse_b  [24141.762]  mse_f: [5291.8096]   total loss: [29433.57]\n",
            "It: 14440, Time: 0.85\n",
            "mse_b  [23506.885]  mse_f: [5276.277]   total loss: [28783.162]\n",
            "It: 14480, Time: 0.81\n",
            "mse_b  [23225.092]  mse_f: [5049.2285]   total loss: [28274.32]\n",
            "It: 14520, Time: 0.82\n",
            "mse_b  [22821.273]  mse_f: [4867.998]   total loss: [27689.271]\n",
            "It: 14560, Time: 0.80\n",
            "mse_b  [22527.812]  mse_f: [4669.569]   total loss: [27197.38]\n",
            "It: 14600, Time: 0.82\n",
            "mse_b  [22245.678]  mse_f: [4529.293]   total loss: [26774.97]\n",
            "It: 14640, Time: 0.83\n",
            "mse_b  [21975.807]  mse_f: [4463.4644]   total loss: [26439.271]\n",
            "It: 14680, Time: 0.82\n",
            "mse_b  [21683.742]  mse_f: [4426.343]   total loss: [26110.086]\n",
            "It: 14720, Time: 0.83\n",
            "mse_b  [21440.143]  mse_f: [4351.0244]   total loss: [25791.168]\n",
            "It: 14760, Time: 0.81\n",
            "mse_b  [21164.326]  mse_f: [4300.4805]   total loss: [25464.807]\n",
            "It: 14800, Time: 0.85\n",
            "mse_b  [20924.717]  mse_f: [4220.916]   total loss: [25145.633]\n",
            "It: 14840, Time: 0.83\n",
            "mse_b  [20681.988]  mse_f: [4135.9785]   total loss: [24817.967]\n",
            "It: 14880, Time: 0.81\n",
            "mse_b  [20434.297]  mse_f: [4051.565]   total loss: [24485.861]\n",
            "It: 14920, Time: 0.82\n",
            "mse_b  [20168.137]  mse_f: [3958.5007]   total loss: [24126.637]\n",
            "It: 14960, Time: 0.81\n",
            "mse_b  [19898.285]  mse_f: [3820.5552]   total loss: [23718.84]\n",
            "It: 15000, Time: 0.83\n",
            "mse_b  [19606.115]  mse_f: [3694.2695]   total loss: [23300.385]\n",
            "It: 15040, Time: 0.81\n",
            "mse_b  [19189.357]  mse_f: [3728.3564]   total loss: [22917.715]\n",
            "It: 15080, Time: 0.82\n",
            "mse_b  [18726.248]  mse_f: [3808.9375]   total loss: [22535.186]\n",
            "It: 15120, Time: 0.82\n",
            "mse_b  [18256.21]  mse_f: [3904.4143]   total loss: [22160.625]\n",
            "It: 15160, Time: 0.82\n",
            "mse_b  [17817.33]  mse_f: [3958.054]   total loss: [21775.385]\n",
            "It: 15200, Time: 0.82\n",
            "mse_b  [17397.717]  mse_f: [4117.99]   total loss: [21515.707]\n",
            "It: 15240, Time: 0.81\n",
            "mse_b  [17111.016]  mse_f: [4097.6357]   total loss: [21208.652]\n",
            "It: 15280, Time: 0.82\n",
            "mse_b  [16840.678]  mse_f: [4145.6416]   total loss: [20986.32]\n",
            "It: 15320, Time: 0.80\n",
            "mse_b  [16550.408]  mse_f: [4369.736]   total loss: [20920.145]\n",
            "It: 15360, Time: 0.81\n",
            "mse_b  [16460.332]  mse_f: [4105.5312]   total loss: [20565.863]\n",
            "It: 15400, Time: 0.82\n",
            "mse_b  [16513.512]  mse_f: [4262.0483]   total loss: [20775.56]\n",
            "It: 15440, Time: 0.82\n",
            "mse_b  [16258.773]  mse_f: [3963.5876]   total loss: [20222.361]\n",
            "It: 15480, Time: 0.83\n",
            "mse_b  [15989.934]  mse_f: [4036.817]   total loss: [20026.75]\n",
            "It: 15520, Time: 0.81\n",
            "mse_b  [15855.663]  mse_f: [4396.5444]   total loss: [20252.207]\n",
            "It: 15560, Time: 0.80\n",
            "mse_b  [15759.919]  mse_f: [4085.574]   total loss: [19845.492]\n",
            "It: 15600, Time: 0.83\n",
            "mse_b  [15657.927]  mse_f: [3902.2754]   total loss: [19560.203]\n",
            "It: 15640, Time: 0.83\n",
            "mse_b  [15559.537]  mse_f: [3732.4292]   total loss: [19291.967]\n",
            "It: 15680, Time: 0.82\n",
            "mse_b  [15513.07]  mse_f: [3590.5173]   total loss: [19103.588]\n",
            "It: 15720, Time: 0.81\n",
            "mse_b  [15576.222]  mse_f: [3849.496]   total loss: [19425.719]\n",
            "It: 15760, Time: 0.81\n",
            "mse_b  [15370.815]  mse_f: [3775.4739]   total loss: [19146.29]\n",
            "It: 15800, Time: 0.81\n",
            "mse_b  [15068.329]  mse_f: [3585.9365]   total loss: [18654.266]\n",
            "It: 15840, Time: 0.81\n",
            "mse_b  [14983.744]  mse_f: [3578.6702]   total loss: [18562.414]\n",
            "It: 15880, Time: 0.83\n",
            "mse_b  [14996.52]  mse_f: [3748.3267]   total loss: [18744.846]\n",
            "It: 15920, Time: 0.83\n",
            "mse_b  [14904.253]  mse_f: [3760.23]   total loss: [18664.482]\n",
            "It: 15960, Time: 0.81\n",
            "mse_b  [14681.801]  mse_f: [3545.6143]   total loss: [18227.414]\n",
            "It: 16000, Time: 0.81\n",
            "mse_b  [14428.486]  mse_f: [4040.2625]   total loss: [18468.748]\n",
            "It: 16040, Time: 0.81\n",
            "mse_b  [14363.378]  mse_f: [3465.1887]   total loss: [17828.566]\n",
            "It: 16080, Time: 0.83\n",
            "mse_b  [14494.362]  mse_f: [3864.713]   total loss: [18359.074]\n",
            "It: 16120, Time: 0.82\n",
            "mse_b  [14375.471]  mse_f: [3919.92]   total loss: [18295.39]\n",
            "It: 16160, Time: 0.81\n",
            "mse_b  [14222.67]  mse_f: [3804.7283]   total loss: [18027.398]\n",
            "It: 16200, Time: 0.85\n",
            "mse_b  [14065.069]  mse_f: [3576.858]   total loss: [17641.928]\n",
            "It: 16240, Time: 0.82\n",
            "mse_b  [13991.837]  mse_f: [3621.2969]   total loss: [17613.133]\n",
            "It: 16280, Time: 0.87\n",
            "mse_b  [13884.806]  mse_f: [3530.2495]   total loss: [17415.055]\n",
            "It: 16320, Time: 0.83\n",
            "mse_b  [13662.657]  mse_f: [3255.5615]   total loss: [16918.219]\n",
            "It: 16360, Time: 0.83\n",
            "mse_b  [13597.087]  mse_f: [4199.981]   total loss: [17797.068]\n",
            "It: 16400, Time: 0.81\n",
            "mse_b  [13545.064]  mse_f: [3204.3809]   total loss: [16749.445]\n",
            "It: 16440, Time: 0.81\n",
            "mse_b  [13647.214]  mse_f: [3666.3535]   total loss: [17313.566]\n",
            "It: 16480, Time: 0.79\n",
            "mse_b  [13339.851]  mse_f: [3451.3755]   total loss: [16791.227]\n",
            "It: 16520, Time: 0.80\n",
            "mse_b  [13289.567]  mse_f: [3931.517]   total loss: [17221.084]\n",
            "It: 16560, Time: 0.80\n",
            "mse_b  [13449.45]  mse_f: [3693.3228]   total loss: [17142.773]\n",
            "It: 16600, Time: 0.80\n",
            "mse_b  [13154.029]  mse_f: [3531.5415]   total loss: [16685.57]\n",
            "It: 16640, Time: 0.81\n",
            "mse_b  [13096.903]  mse_f: [3264.408]   total loss: [16361.312]\n",
            "It: 16680, Time: 0.85\n",
            "mse_b  [13061.525]  mse_f: [3838.65]   total loss: [16900.176]\n",
            "It: 16720, Time: 0.81\n",
            "mse_b  [12979.992]  mse_f: [3220.9673]   total loss: [16200.959]\n",
            "It: 16760, Time: 0.82\n",
            "mse_b  [12908.913]  mse_f: [3113.338]   total loss: [16022.251]\n",
            "It: 16800, Time: 0.82\n",
            "mse_b  [12891.684]  mse_f: [3963.109]   total loss: [16854.793]\n",
            "It: 16840, Time: 0.80\n",
            "mse_b  [12818.666]  mse_f: [2840.273]   total loss: [15658.939]\n",
            "It: 16880, Time: 0.82\n",
            "mse_b  [16171.415]  mse_f: [9139.449]   total loss: [25310.863]\n",
            "It: 16920, Time: 0.83\n",
            "mse_b  [19050.32]  mse_f: [13445.807]   total loss: [32496.127]\n",
            "It: 16960, Time: 0.83\n",
            "mse_b  [19913.355]  mse_f: [22656.744]   total loss: [42570.1]\n",
            "It: 17000, Time: 0.81\n",
            "mse_b  [25314.598]  mse_f: [12031.348]   total loss: [37345.945]\n",
            "It: 17040, Time: 0.81\n",
            "mse_b  [15990.746]  mse_f: [7594.5015]   total loss: [23585.248]\n",
            "It: 17080, Time: 0.83\n",
            "mse_b  [14910.475]  mse_f: [5230.224]   total loss: [20140.7]\n",
            "It: 17120, Time: 0.82\n",
            "mse_b  [15077.908]  mse_f: [5549.0776]   total loss: [20626.986]\n",
            "It: 17160, Time: 0.82\n",
            "mse_b  [14473.242]  mse_f: [3924.359]   total loss: [18397.602]\n",
            "It: 17200, Time: 0.81\n",
            "mse_b  [14472.773]  mse_f: [3826.1724]   total loss: [18298.945]\n",
            "It: 17240, Time: 0.81\n",
            "mse_b  [13895.774]  mse_f: [3776.298]   total loss: [17672.072]\n",
            "It: 17280, Time: 0.86\n",
            "mse_b  [13773.076]  mse_f: [3226.5938]   total loss: [16999.67]\n",
            "It: 17320, Time: 0.86\n",
            "mse_b  [13867.069]  mse_f: [3034.1208]   total loss: [16901.19]\n",
            "It: 17360, Time: 0.83\n",
            "mse_b  [13447.563]  mse_f: [3007.9187]   total loss: [16455.482]\n",
            "It: 17400, Time: 0.82\n",
            "mse_b  [13377.628]  mse_f: [2781.2024]   total loss: [16158.83]\n",
            "It: 17440, Time: 0.81\n",
            "mse_b  [13385.036]  mse_f: [2684.7617]   total loss: [16069.798]\n",
            "It: 17480, Time: 0.84\n",
            "mse_b  [13225.958]  mse_f: [2697.1448]   total loss: [15923.103]\n",
            "It: 17520, Time: 0.81\n",
            "mse_b  [13194.52]  mse_f: [2634.7231]   total loss: [15829.242]\n",
            "It: 17560, Time: 0.81\n",
            "mse_b  [13104.705]  mse_f: [2595.7002]   total loss: [15700.405]\n",
            "It: 17600, Time: 0.82\n",
            "mse_b  [12980.072]  mse_f: [2624.5405]   total loss: [15604.613]\n",
            "It: 17640, Time: 0.82\n",
            "mse_b  [12897.555]  mse_f: [2594.2551]   total loss: [15491.81]\n",
            "It: 17680, Time: 0.83\n",
            "mse_b  [12835.]  mse_f: [2562.7207]   total loss: [15397.721]\n",
            "It: 17720, Time: 0.83\n",
            "mse_b  [12778.002]  mse_f: [2524.016]   total loss: [15302.018]\n",
            "It: 17760, Time: 0.83\n",
            "mse_b  [12713.467]  mse_f: [2500.1523]   total loss: [15213.619]\n",
            "It: 17800, Time: 0.83\n",
            "mse_b  [12634.036]  mse_f: [2483.4534]   total loss: [15117.489]\n",
            "It: 17840, Time: 0.80\n",
            "mse_b  [12563.225]  mse_f: [2461.297]   total loss: [15024.521]\n",
            "It: 17880, Time: 0.84\n",
            "mse_b  [12489.307]  mse_f: [2438.8384]   total loss: [14928.145]\n",
            "It: 17920, Time: 0.85\n",
            "mse_b  [12414.555]  mse_f: [2417.6294]   total loss: [14832.184]\n",
            "It: 17960, Time: 0.80\n",
            "mse_b  [12351.599]  mse_f: [2390.453]   total loss: [14742.052]\n",
            "It: 18000, Time: 0.81\n",
            "mse_b  [12284.075]  mse_f: [2373.8008]   total loss: [14657.876]\n",
            "It: 18040, Time: 0.81\n",
            "mse_b  [12225.637]  mse_f: [2348.527]   total loss: [14574.164]\n",
            "It: 18080, Time: 0.81\n",
            "mse_b  [12158.913]  mse_f: [2327.1306]   total loss: [14486.044]\n",
            "It: 18120, Time: 0.81\n",
            "mse_b  [12084.101]  mse_f: [2311.942]   total loss: [14396.043]\n",
            "It: 18160, Time: 0.81\n",
            "mse_b  [12005.74]  mse_f: [2437.721]   total loss: [14443.461]\n",
            "It: 18200, Time: 0.81\n",
            "mse_b  [11976.806]  mse_f: [2306.772]   total loss: [14283.578]\n",
            "It: 18240, Time: 0.80\n",
            "mse_b  [11954.864]  mse_f: [2404.3801]   total loss: [14359.244]\n",
            "It: 18280, Time: 0.79\n",
            "mse_b  [11823.542]  mse_f: [2225.3486]   total loss: [14048.891]\n",
            "It: 18320, Time: 0.82\n",
            "mse_b  [11753.411]  mse_f: [2590.3706]   total loss: [14343.781]\n",
            "It: 18360, Time: 0.80\n",
            "mse_b  [11714.02]  mse_f: [2184.5747]   total loss: [13898.594]\n",
            "It: 18400, Time: 0.83\n",
            "mse_b  [11821.512]  mse_f: [2594.9207]   total loss: [14416.433]\n",
            "It: 18440, Time: 0.84\n",
            "mse_b  [11618.23]  mse_f: [2902.6777]   total loss: [14520.908]\n",
            "It: 18480, Time: 0.82\n",
            "mse_b  [11557.396]  mse_f: [2896.646]   total loss: [14454.043]\n",
            "It: 18520, Time: 0.83\n",
            "mse_b  [11574.247]  mse_f: [2152.4443]   total loss: [13726.691]\n",
            "It: 18560, Time: 0.82\n",
            "mse_b  [11437.407]  mse_f: [2613.4268]   total loss: [14050.834]\n",
            "It: 18600, Time: 0.82\n",
            "mse_b  [11371.086]  mse_f: [2113.8008]   total loss: [13484.887]\n",
            "It: 18640, Time: 0.81\n",
            "mse_b  [11317.641]  mse_f: [2313.19]   total loss: [13630.83]\n",
            "It: 18680, Time: 0.82\n",
            "mse_b  [11371.814]  mse_f: [2340.264]   total loss: [13712.078]\n",
            "It: 18720, Time: 0.81\n",
            "mse_b  [11230.812]  mse_f: [2067.5576]   total loss: [13298.369]\n",
            "It: 18760, Time: 0.81\n",
            "mse_b  [11233.992]  mse_f: [3261.9695]   total loss: [14495.962]\n",
            "It: 18800, Time: 0.84\n",
            "mse_b  [11457.408]  mse_f: [3535.6301]   total loss: [14993.038]\n",
            "It: 18840, Time: 0.83\n",
            "mse_b  [11150.22]  mse_f: [3174.6343]   total loss: [14324.854]\n",
            "It: 18880, Time: 0.82\n",
            "mse_b  [11162.161]  mse_f: [3173.332]   total loss: [14335.493]\n",
            "It: 18920, Time: 0.85\n",
            "mse_b  [11455.658]  mse_f: [6389.039]   total loss: [17844.697]\n",
            "It: 18960, Time: 0.81\n",
            "mse_b  [12601.638]  mse_f: [14065.441]   total loss: [26667.078]\n",
            "It: 19000, Time: 0.84\n",
            "mse_b  [12444.053]  mse_f: [9589.457]   total loss: [22033.51]\n",
            "It: 19040, Time: 0.83\n",
            "mse_b  [14516.804]  mse_f: [6748.3286]   total loss: [21265.133]\n",
            "It: 19080, Time: 0.81\n",
            "mse_b  [14082.571]  mse_f: [3323.633]   total loss: [17406.205]\n",
            "It: 19120, Time: 0.81\n",
            "mse_b  [12446.28]  mse_f: [4059.817]   total loss: [16506.098]\n",
            "It: 19160, Time: 0.81\n",
            "mse_b  [12863.417]  mse_f: [3354.9214]   total loss: [16218.338]\n",
            "It: 19200, Time: 0.87\n",
            "mse_b  [11930.36]  mse_f: [2570.1606]   total loss: [14500.521]\n",
            "It: 19240, Time: 0.82\n",
            "mse_b  [12114.73]  mse_f: [2008.6833]   total loss: [14123.414]\n",
            "It: 19280, Time: 0.79\n",
            "mse_b  [11576.2295]  mse_f: [2496.8894]   total loss: [14073.119]\n",
            "It: 19320, Time: 0.79\n",
            "mse_b  [11406.031]  mse_f: [2172.151]   total loss: [13578.182]\n",
            "It: 19360, Time: 0.84\n",
            "mse_b  [11293.953]  mse_f: [2095.7632]   total loss: [13389.717]\n",
            "It: 19400, Time: 0.82\n",
            "mse_b  [11186.102]  mse_f: [2052.929]   total loss: [13239.03]\n",
            "It: 19440, Time: 0.79\n",
            "mse_b  [10985.571]  mse_f: [2108.3633]   total loss: [13093.935]\n",
            "It: 19480, Time: 0.80\n",
            "mse_b  [10920.165]  mse_f: [2003.3298]   total loss: [12923.495]\n",
            "It: 19520, Time: 0.81\n",
            "mse_b  [10868.5205]  mse_f: [1992.9867]   total loss: [12861.507]\n",
            "It: 19560, Time: 0.81\n",
            "mse_b  [10842.787]  mse_f: [1933.5187]   total loss: [12776.306]\n",
            "It: 19600, Time: 0.81\n",
            "mse_b  [10818.406]  mse_f: [1884.9314]   total loss: [12703.338]\n",
            "It: 19640, Time: 0.80\n",
            "mse_b  [10758.47]  mse_f: [1878.2162]   total loss: [12636.686]\n",
            "It: 19680, Time: 0.83\n",
            "mse_b  [10667.737]  mse_f: [1910.0026]   total loss: [12577.74]\n",
            "It: 19720, Time: 0.83\n",
            "mse_b  [10608.3545]  mse_f: [1903.5773]   total loss: [12511.932]\n",
            "It: 19760, Time: 0.81\n",
            "mse_b  [10548.502]  mse_f: [1901.2197]   total loss: [12449.722]\n",
            "It: 19800, Time: 0.80\n",
            "mse_b  [10492.97]  mse_f: [1892.804]   total loss: [12385.773]\n",
            "It: 19840, Time: 0.81\n",
            "mse_b  [10446.31]  mse_f: [1869.8684]   total loss: [12316.178]\n",
            "It: 19880, Time: 0.80\n",
            "mse_b  [10410.585]  mse_f: [1836.265]   total loss: [12246.85]\n",
            "It: 19920, Time: 0.82\n",
            "mse_b  [10317.564]  mse_f: [1881.4689]   total loss: [12199.033]\n",
            "It: 19960, Time: 0.83\n",
            "mse_b  [10285.777]  mse_f: [1967.648]   total loss: [12253.426]\n",
            "It: 20000, Time: 0.82\n",
            "mse_b  [10237.203]  mse_f: [1820.264]   total loss: [12057.467]\n",
            "It: 20040, Time: 0.81\n",
            "mse_b  [10244.363]  mse_f: [2156.2715]   total loss: [12400.635]\n",
            "It: 20080, Time: 0.81\n",
            "mse_b  [10100.603]  mse_f: [2238.0947]   total loss: [12338.697]\n",
            "It: 20120, Time: 0.82\n",
            "mse_b  [10040.383]  mse_f: [2069.1543]   total loss: [12109.537]\n",
            "It: 20160, Time: 0.83\n",
            "mse_b  [9998.806]  mse_f: [2574.4788]   total loss: [12573.284]\n",
            "It: 20200, Time: 0.82\n",
            "mse_b  [9914.948]  mse_f: [1829.4092]   total loss: [11744.357]\n",
            "It: 20240, Time: 0.84\n",
            "mse_b  [9892.182]  mse_f: [2981.2769]   total loss: [12873.459]\n",
            "It: 20280, Time: 0.85\n",
            "mse_b  [9803.337]  mse_f: [1772.7733]   total loss: [11576.11]\n",
            "It: 20320, Time: 0.84\n",
            "mse_b  [9783.108]  mse_f: [1940.2878]   total loss: [11723.396]\n",
            "It: 20360, Time: 0.82\n",
            "mse_b  [9693.688]  mse_f: [1950.3561]   total loss: [11644.045]\n",
            "It: 20400, Time: 0.81\n",
            "mse_b  [9553.884]  mse_f: [1743.0385]   total loss: [11296.922]\n",
            "It: 20440, Time: 0.84\n",
            "mse_b  [9469.656]  mse_f: [1780.2028]   total loss: [11249.859]\n",
            "It: 20480, Time: 0.82\n",
            "mse_b  [9552.819]  mse_f: [2488.7961]   total loss: [12041.615]\n",
            "It: 20520, Time: 0.85\n",
            "mse_b  [9341.86]  mse_f: [2460.7002]   total loss: [11802.561]\n",
            "It: 20560, Time: 0.86\n",
            "mse_b  [9270.532]  mse_f: [1725.6019]   total loss: [10996.134]\n",
            "It: 20600, Time: 0.87\n",
            "mse_b  [9301.866]  mse_f: [2075.5166]   total loss: [11377.383]\n",
            "It: 20640, Time: 0.82\n",
            "mse_b  [9297.601]  mse_f: [2359.9546]   total loss: [11657.555]\n",
            "It: 20680, Time: 0.90\n",
            "mse_b  [9091.299]  mse_f: [1657.9604]   total loss: [10749.26]\n",
            "It: 20720, Time: 0.85\n",
            "mse_b  [9026.943]  mse_f: [2116.122]   total loss: [11143.065]\n",
            "It: 20760, Time: 0.85\n",
            "mse_b  [9108.788]  mse_f: [2244.7134]   total loss: [11353.502]\n",
            "It: 20800, Time: 0.81\n",
            "mse_b  [8932.495]  mse_f: [2443.7593]   total loss: [11376.254]\n",
            "It: 20840, Time: 0.83\n",
            "mse_b  [8973.781]  mse_f: [1923.2858]   total loss: [10897.067]\n",
            "It: 20880, Time: 0.83\n",
            "mse_b  [8846.085]  mse_f: [1611.0078]   total loss: [10457.093]\n",
            "It: 20920, Time: 0.84\n",
            "mse_b  [8810.537]  mse_f: [1583.6016]   total loss: [10394.139]\n",
            "It: 20960, Time: 0.83\n",
            "mse_b  [8811.628]  mse_f: [1639.694]   total loss: [10451.322]\n",
            "It: 21000, Time: 0.82\n",
            "mse_b  [8723.387]  mse_f: [2383.3936]   total loss: [11106.78]\n",
            "It: 21040, Time: 0.82\n",
            "mse_b  [8672.965]  mse_f: [1610.6803]   total loss: [10283.6455]\n",
            "It: 21080, Time: 0.81\n",
            "mse_b  [8718.204]  mse_f: [1987.7535]   total loss: [10705.958]\n",
            "It: 21120, Time: 0.81\n",
            "mse_b  [8579.342]  mse_f: [1587.1729]   total loss: [10166.515]\n",
            "It: 21160, Time: 0.81\n",
            "mse_b  [8561.905]  mse_f: [2378.285]   total loss: [10940.19]\n",
            "It: 21200, Time: 0.82\n",
            "mse_b  [8648.122]  mse_f: [2542.377]   total loss: [11190.499]\n",
            "It: 21240, Time: 0.81\n",
            "mse_b  [8753.136]  mse_f: [2262.206]   total loss: [11015.342]\n",
            "It: 21280, Time: 0.83\n",
            "mse_b  [8485.359]  mse_f: [1862.079]   total loss: [10347.438]\n",
            "It: 21320, Time: 0.82\n",
            "mse_b  [8387.119]  mse_f: [1484.2686]   total loss: [9871.388]\n",
            "It: 21360, Time: 0.85\n",
            "mse_b  [8332.999]  mse_f: [1878.2573]   total loss: [10211.256]\n",
            "It: 21400, Time: 0.82\n",
            "mse_b  [8296.115]  mse_f: [1488.0374]   total loss: [9784.152]\n",
            "It: 21440, Time: 0.81\n",
            "mse_b  [8258.493]  mse_f: [1414.1467]   total loss: [9672.64]\n",
            "It: 21480, Time: 0.82\n",
            "mse_b  [8196.513]  mse_f: [1468.1326]   total loss: [9664.6455]\n",
            "It: 21520, Time: 0.81\n",
            "mse_b  [8188.2803]  mse_f: [2432.247]   total loss: [10620.527]\n",
            "It: 21560, Time: 0.81\n",
            "mse_b  [8250.876]  mse_f: [1881.4297]   total loss: [10132.306]\n",
            "It: 21600, Time: 0.82\n",
            "mse_b  [10424.407]  mse_f: [32808.523]   total loss: [43232.93]\n",
            "It: 21640, Time: 0.82\n",
            "mse_b  [22459.117]  mse_f: [24442.727]   total loss: [46901.844]\n",
            "It: 21680, Time: 0.80\n",
            "mse_b  [20403.8]  mse_f: [14024.153]   total loss: [34427.953]\n",
            "It: 21720, Time: 0.81\n",
            "mse_b  [15008.32]  mse_f: [11877.506]   total loss: [26885.826]\n",
            "It: 21760, Time: 0.81\n",
            "mse_b  [12638.357]  mse_f: [5197.1016]   total loss: [17835.459]\n",
            "It: 21800, Time: 0.82\n",
            "mse_b  [13687.044]  mse_f: [4058.4326]   total loss: [17745.477]\n",
            "It: 21840, Time: 0.82\n",
            "mse_b  [12928.276]  mse_f: [3243.7397]   total loss: [16172.016]\n",
            "It: 21880, Time: 0.81\n",
            "mse_b  [11546.704]  mse_f: [2941.3875]   total loss: [14488.092]\n",
            "It: 21920, Time: 0.81\n",
            "mse_b  [10682.766]  mse_f: [2573.231]   total loss: [13255.996]\n",
            "It: 21960, Time: 0.80\n",
            "mse_b  [10667.62]  mse_f: [2006.6183]   total loss: [12674.238]\n",
            "It: 22000, Time: 0.82\n",
            "mse_b  [10093.764]  mse_f: [1855.5912]   total loss: [11949.3545]\n",
            "It: 22040, Time: 0.82\n",
            "mse_b  [9714.315]  mse_f: [2091.839]   total loss: [11806.154]\n",
            "It: 22080, Time: 0.82\n",
            "mse_b  [9564.644]  mse_f: [1739.9365]   total loss: [11304.58]\n",
            "It: 22120, Time: 0.84\n",
            "mse_b  [9326.923]  mse_f: [1756.1973]   total loss: [11083.12]\n",
            "It: 22160, Time: 0.87\n",
            "mse_b  [9269.129]  mse_f: [1532.9438]   total loss: [10802.072]\n",
            "It: 22200, Time: 0.83\n",
            "mse_b  [9165.626]  mse_f: [1514.7966]   total loss: [10680.423]\n",
            "It: 22240, Time: 0.81\n",
            "mse_b  [9005.844]  mse_f: [1456.8046]   total loss: [10462.648]\n",
            "It: 22280, Time: 0.83\n",
            "mse_b  [8963.325]  mse_f: [1395.2917]   total loss: [10358.617]\n",
            "It: 22320, Time: 0.83\n",
            "mse_b  [8846.441]  mse_f: [1371.5883]   total loss: [10218.029]\n",
            "It: 22360, Time: 0.82\n",
            "mse_b  [8767.072]  mse_f: [1349.2861]   total loss: [10116.358]\n",
            "It: 22400, Time: 1.14\n",
            "mse_b  [8687.006]  mse_f: [1342.5532]   total loss: [10029.559]\n",
            "It: 22440, Time: 1.30\n",
            "mse_b  [8603.536]  mse_f: [1345.7189]   total loss: [9949.255]\n",
            "It: 22480, Time: 1.63\n",
            "mse_b  [8522.831]  mse_f: [1349.4019]   total loss: [9872.232]\n",
            "It: 22520, Time: 0.92\n",
            "mse_b  [8453.748]  mse_f: [1344.1904]   total loss: [9797.938]\n",
            "It: 22560, Time: 0.81\n",
            "mse_b  [8392.424]  mse_f: [1334.6257]   total loss: [9727.05]\n",
            "It: 22600, Time: 0.82\n",
            "mse_b  [8341.282]  mse_f: [1322.5801]   total loss: [9663.862]\n",
            "It: 22640, Time: 0.83\n",
            "mse_b  [8281.341]  mse_f: [1321.2996]   total loss: [9602.641]\n",
            "It: 22680, Time: 0.81\n",
            "mse_b  [8238.021]  mse_f: [1304.1521]   total loss: [9542.174]\n",
            "It: 22720, Time: 0.83\n",
            "mse_b  [8184.532]  mse_f: [1297.7283]   total loss: [9482.261]\n",
            "It: 22760, Time: 0.79\n",
            "mse_b  [8134.2285]  mse_f: [1287.9294]   total loss: [9422.158]\n",
            "It: 22800, Time: 0.82\n",
            "mse_b  [8085.4834]  mse_f: [1274.3821]   total loss: [9359.865]\n",
            "It: 22840, Time: 0.80\n",
            "mse_b  [8030.6196]  mse_f: [1262.572]   total loss: [9293.191]\n",
            "It: 22880, Time: 0.81\n",
            "mse_b  [7976.6895]  mse_f: [1246.8444]   total loss: [9223.534]\n",
            "It: 22920, Time: 0.81\n",
            "mse_b  [7918.6343]  mse_f: [1240.4629]   total loss: [9159.098]\n",
            "It: 22960, Time: 0.83\n",
            "mse_b  [7903.1045]  mse_f: [1627.0199]   total loss: [9530.124]\n",
            "It: 23000, Time: 0.82\n",
            "mse_b  [7981.4097]  mse_f: [4777.456]   total loss: [12758.865]\n",
            "It: 23040, Time: 0.82\n",
            "mse_b  [8117.8486]  mse_f: [3367.7437]   total loss: [11485.592]\n",
            "It: 23080, Time: 0.80\n",
            "mse_b  [7819.6406]  mse_f: [1774.3523]   total loss: [9593.993]\n",
            "It: 23120, Time: 0.80\n",
            "mse_b  [7751.0596]  mse_f: [1815.0675]   total loss: [9566.127]\n",
            "It: 23160, Time: 0.83\n",
            "mse_b  [7795.1914]  mse_f: [1997.921]   total loss: [9793.112]\n",
            "It: 23200, Time: 0.80\n",
            "mse_b  [7861.2046]  mse_f: [1547.6112]   total loss: [9408.815]\n",
            "It: 23240, Time: 0.80\n",
            "mse_b  [7637.128]  mse_f: [1630.5535]   total loss: [9267.682]\n",
            "It: 23280, Time: 0.78\n",
            "mse_b  [7620.057]  mse_f: [1557.2849]   total loss: [9177.342]\n",
            "It: 23320, Time: 0.82\n",
            "mse_b  [7576.3545]  mse_f: [1373.8276]   total loss: [8950.182]\n",
            "It: 23360, Time: 0.79\n",
            "mse_b  [7557.8784]  mse_f: [1203.6848]   total loss: [8761.563]\n",
            "It: 23400, Time: 0.81\n",
            "mse_b  [7503.356]  mse_f: [1248.8235]   total loss: [8752.18]\n",
            "It: 23440, Time: 0.81\n",
            "mse_b  [7474.146]  mse_f: [1238.4912]   total loss: [8712.637]\n",
            "It: 23480, Time: 0.79\n",
            "mse_b  [7451.8003]  mse_f: [1175.0596]   total loss: [8626.859]\n",
            "It: 23520, Time: 0.84\n",
            "mse_b  [7430.934]  mse_f: [1170.0684]   total loss: [8601.002]\n",
            "It: 23560, Time: 0.86\n",
            "mse_b  [7371.8906]  mse_f: [1183.2507]   total loss: [8555.142]\n",
            "It: 23600, Time: 0.81\n",
            "mse_b  [7340.196]  mse_f: [1142.5803]   total loss: [8482.776]\n",
            "It: 23640, Time: 0.83\n",
            "mse_b  [7281.846]  mse_f: [1169.9478]   total loss: [8451.794]\n",
            "It: 23680, Time: 0.80\n",
            "mse_b  [7253.2417]  mse_f: [1135.6017]   total loss: [8388.844]\n",
            "It: 23720, Time: 0.82\n",
            "mse_b  [7224.5093]  mse_f: [1115.8458]   total loss: [8340.355]\n",
            "It: 23760, Time: 0.81\n",
            "mse_b  [7185.237]  mse_f: [1103.8735]   total loss: [8289.11]\n",
            "It: 23800, Time: 0.83\n",
            "mse_b  [7165.6772]  mse_f: [1144.8188]   total loss: [8310.496]\n",
            "It: 23840, Time: 0.83\n",
            "mse_b  [7091.933]  mse_f: [1308.3845]   total loss: [8400.317]\n",
            "It: 23880, Time: 0.86\n",
            "mse_b  [7109.185]  mse_f: [4252.4556]   total loss: [11361.641]\n",
            "It: 23920, Time: 0.90\n",
            "mse_b  [7684.9604]  mse_f: [6567.3594]   total loss: [14252.32]\n",
            "It: 23960, Time: 0.91\n",
            "mse_b  [7321.524]  mse_f: [2739.0996]   total loss: [10060.623]\n",
            "It: 24000, Time: 0.91\n",
            "mse_b  [7275.75]  mse_f: [1674.4432]   total loss: [8950.193]\n",
            "It: 24040, Time: 0.90\n",
            "mse_b  [7302.6733]  mse_f: [2046.4847]   total loss: [9349.158]\n",
            "It: 24080, Time: 0.93\n",
            "mse_b  [7171.889]  mse_f: [1368.7627]   total loss: [8540.652]\n",
            "It: 24120, Time: 0.88\n",
            "mse_b  [7103.7837]  mse_f: [1639.2949]   total loss: [8743.078]\n",
            "It: 24160, Time: 0.90\n",
            "mse_b  [7056.6157]  mse_f: [1798.0894]   total loss: [8854.705]\n",
            "It: 24200, Time: 0.89\n",
            "mse_b  [7007.3965]  mse_f: [1293.8931]   total loss: [8301.289]\n",
            "It: 24240, Time: 0.93\n",
            "mse_b  [7091.0444]  mse_f: [1180.9805]   total loss: [8272.025]\n",
            "It: 24280, Time: 0.90\n",
            "mse_b  [7001.935]  mse_f: [1106.3591]   total loss: [8108.294]\n",
            "It: 24320, Time: 0.90\n",
            "mse_b  [6952.7485]  mse_f: [1086.9775]   total loss: [8039.726]\n",
            "It: 24360, Time: 0.90\n",
            "mse_b  [6905.2085]  mse_f: [1045.4027]   total loss: [7950.6113]\n",
            "It: 24400, Time: 0.88\n",
            "mse_b  [6885.189]  mse_f: [1029.2758]   total loss: [7914.465]\n",
            "It: 24440, Time: 0.91\n",
            "mse_b  [6849.8994]  mse_f: [1007.99963]   total loss: [7857.899]\n",
            "It: 24480, Time: 0.90\n",
            "mse_b  [6804.9097]  mse_f: [1034.3778]   total loss: [7839.2876]\n",
            "It: 24520, Time: 0.89\n",
            "mse_b  [6813.51]  mse_f: [967.3231]   total loss: [7780.833]\n",
            "It: 24560, Time: 0.88\n",
            "mse_b  [6756.877]  mse_f: [975.6736]   total loss: [7732.551]\n",
            "It: 24600, Time: 0.88\n",
            "mse_b  [6729.413]  mse_f: [972.54504]   total loss: [7701.958]\n",
            "It: 24640, Time: 0.87\n",
            "mse_b  [6687.0723]  mse_f: [968.0797]   total loss: [7655.152]\n",
            "It: 24680, Time: 0.90\n",
            "mse_b  [6658.604]  mse_f: [952.4413]   total loss: [7611.0454]\n",
            "It: 24720, Time: 0.92\n",
            "mse_b  [6621.237]  mse_f: [951.8733]   total loss: [7573.1104]\n",
            "It: 24760, Time: 0.91\n",
            "mse_b  [6599.8677]  mse_f: [990.02747]   total loss: [7589.895]\n",
            "It: 24800, Time: 0.87\n",
            "mse_b  [6568.3516]  mse_f: [933.4541]   total loss: [7501.8057]\n",
            "It: 24840, Time: 0.81\n",
            "mse_b  [6565.7285]  mse_f: [968.9758]   total loss: [7534.704]\n",
            "It: 24880, Time: 0.80\n",
            "mse_b  [6542.2715]  mse_f: [959.12134]   total loss: [7501.3926]\n",
            "It: 24920, Time: 0.85\n",
            "mse_b  [6456.426]  mse_f: [1051.2051]   total loss: [7507.631]\n",
            "It: 24960, Time: 0.88\n",
            "mse_b  [6432.4854]  mse_f: [1239.7031]   total loss: [7672.1885]\n",
            "It: 25000, Time: 0.84\n",
            "mse_b  [6432.377]  mse_f: [1062.0968]   total loss: [7494.4736]\n",
            "It: 25040, Time: 0.80\n",
            "mse_b  [6471.3965]  mse_f: [1271.363]   total loss: [7742.76]\n",
            "It: 25080, Time: 0.81\n",
            "mse_b  [6506.3154]  mse_f: [1148.365]   total loss: [7654.6807]\n",
            "It: 25120, Time: 0.80\n",
            "mse_b  [6425.269]  mse_f: [987.94257]   total loss: [7413.2114]\n",
            "It: 25160, Time: 0.80\n",
            "mse_b  [6372.2354]  mse_f: [1076.4349]   total loss: [7448.6704]\n",
            "It: 25200, Time: 0.80\n",
            "mse_b  [6340.048]  mse_f: [1110.095]   total loss: [7450.1426]\n",
            "It: 25240, Time: 0.80\n",
            "mse_b  [6317.875]  mse_f: [1249.7983]   total loss: [7567.6733]\n",
            "It: 25280, Time: 0.81\n",
            "mse_b  [6277.4116]  mse_f: [961.1553]   total loss: [7238.567]\n",
            "It: 25320, Time: 0.83\n",
            "mse_b  [6252.5854]  mse_f: [997.473]   total loss: [7250.0586]\n",
            "It: 25360, Time: 0.81\n",
            "mse_b  [6305.6255]  mse_f: [902.3524]   total loss: [7207.978]\n",
            "It: 25400, Time: 0.81\n",
            "mse_b  [6205.1826]  mse_f: [1047.363]   total loss: [7252.546]\n",
            "It: 25440, Time: 0.83\n",
            "mse_b  [6349.157]  mse_f: [1193.6868]   total loss: [7542.8438]\n",
            "It: 25480, Time: 0.83\n",
            "mse_b  [6176.7207]  mse_f: [1054.084]   total loss: [7230.8047]\n",
            "It: 25520, Time: 0.81\n",
            "mse_b  [6115.9746]  mse_f: [1018.3888]   total loss: [7134.3633]\n",
            "It: 25560, Time: 0.88\n",
            "mse_b  [6200.4683]  mse_f: [1118.3953]   total loss: [7318.8633]\n",
            "It: 25600, Time: 0.88\n",
            "mse_b  [6143.819]  mse_f: [923.97473]   total loss: [7067.7935]\n",
            "It: 25640, Time: 0.81\n",
            "mse_b  [6064.8833]  mse_f: [875.35925]   total loss: [6940.2427]\n",
            "It: 25680, Time: 0.83\n",
            "mse_b  [6041.741]  mse_f: [1047.3278]   total loss: [7089.069]\n",
            "It: 25720, Time: 0.82\n",
            "mse_b  [6025.0483]  mse_f: [947.3176]   total loss: [6972.366]\n",
            "It: 25760, Time: 0.83\n",
            "mse_b  [6033.2407]  mse_f: [980.068]   total loss: [7013.3086]\n",
            "It: 25800, Time: 0.82\n",
            "mse_b  [6104.348]  mse_f: [1165.3689]   total loss: [7269.717]\n",
            "It: 25840, Time: 0.83\n",
            "mse_b  [5991.1255]  mse_f: [990.62866]   total loss: [6981.754]\n",
            "It: 25880, Time: 0.81\n",
            "mse_b  [6060.694]  mse_f: [1256.476]   total loss: [7317.17]\n",
            "It: 25920, Time: 0.82\n",
            "mse_b  [5921.5337]  mse_f: [969.9557]   total loss: [6891.4893]\n",
            "It: 25960, Time: 0.81\n",
            "mse_b  [5950.524]  mse_f: [823.6863]   total loss: [6774.21]\n",
            "It: 26000, Time: 0.83\n",
            "mse_b  [5859.7363]  mse_f: [882.23755]   total loss: [6741.9736]\n",
            "It: 26040, Time: 0.80\n",
            "mse_b  [5895.2646]  mse_f: [852.2798]   total loss: [6747.5444]\n",
            "It: 26080, Time: 0.84\n",
            "mse_b  [5857.316]  mse_f: [795.58215]   total loss: [6652.898]\n",
            "It: 26120, Time: 0.83\n",
            "mse_b  [5831.661]  mse_f: [861.61694]   total loss: [6693.2783]\n",
            "It: 26160, Time: 0.84\n",
            "mse_b  [5784.2363]  mse_f: [905.31067]   total loss: [6689.547]\n",
            "It: 26200, Time: 0.82\n",
            "mse_b  [6019.673]  mse_f: [1328.3572]   total loss: [7348.0303]\n",
            "It: 26240, Time: 0.80\n",
            "mse_b  [5809.796]  mse_f: [2285.7515]   total loss: [8095.5474]\n",
            "It: 26280, Time: 0.85\n",
            "mse_b  [5828.5737]  mse_f: [1401.1095]   total loss: [7229.683]\n",
            "It: 26320, Time: 0.81\n",
            "mse_b  [5896.9824]  mse_f: [1786.9409]   total loss: [7683.9233]\n",
            "It: 26360, Time: 0.83\n",
            "mse_b  [5955.044]  mse_f: [1668.9933]   total loss: [7624.037]\n",
            "It: 26400, Time: 0.83\n",
            "mse_b  [5840.821]  mse_f: [1733.8973]   total loss: [7574.7183]\n",
            "It: 26440, Time: 0.80\n",
            "mse_b  [5897.554]  mse_f: [1252.9299]   total loss: [7150.4844]\n",
            "It: 26480, Time: 0.80\n",
            "mse_b  [5723.043]  mse_f: [1246.4692]   total loss: [6969.512]\n",
            "It: 26520, Time: 0.83\n",
            "mse_b  [5688.1943]  mse_f: [812.8082]   total loss: [6501.0024]\n",
            "It: 26560, Time: 0.85\n",
            "mse_b  [5674.6973]  mse_f: [855.70544]   total loss: [6530.403]\n",
            "It: 26600, Time: 0.82\n",
            "mse_b  [5610.057]  mse_f: [798.53]   total loss: [6408.587]\n",
            "It: 26640, Time: 0.82\n",
            "mse_b  [5591.505]  mse_f: [746.9824]   total loss: [6338.4873]\n",
            "It: 26680, Time: 0.81\n",
            "mse_b  [5550.7617]  mse_f: [1117.4221]   total loss: [6668.1836]\n",
            "It: 26720, Time: 0.80\n",
            "mse_b  [5665.8887]  mse_f: [1233.0635]   total loss: [6898.952]\n",
            "It: 26760, Time: 0.83\n",
            "mse_b  [5608.3145]  mse_f: [853.1577]   total loss: [6461.472]\n",
            "It: 26800, Time: 0.83\n",
            "mse_b  [5552.067]  mse_f: [768.0619]   total loss: [6320.129]\n",
            "It: 26840, Time: 0.81\n",
            "mse_b  [5579.5024]  mse_f: [776.92163]   total loss: [6356.424]\n",
            "It: 26880, Time: 0.81\n",
            "mse_b  [5493.284]  mse_f: [755.2013]   total loss: [6248.4854]\n",
            "It: 26920, Time: 0.81\n",
            "mse_b  [5438.6885]  mse_f: [778.90125]   total loss: [6217.59]\n",
            "It: 26960, Time: 0.83\n",
            "mse_b  [5455.3833]  mse_f: [747.38525]   total loss: [6202.7686]\n",
            "It: 27000, Time: 0.81\n",
            "mse_b  [5418.2837]  mse_f: [1206.9619]   total loss: [6625.2456]\n",
            "It: 27040, Time: 0.81\n",
            "mse_b  [5499.609]  mse_f: [1475.2008]   total loss: [6974.8096]\n",
            "It: 27080, Time: 0.84\n",
            "mse_b  [5467.5234]  mse_f: [869.5211]   total loss: [6337.0444]\n",
            "It: 27120, Time: 0.82\n",
            "mse_b  [5417.1826]  mse_f: [1308.5121]   total loss: [6725.695]\n",
            "It: 27160, Time: 0.79\n",
            "mse_b  [5403.267]  mse_f: [919.8721]   total loss: [6323.139]\n",
            "It: 27200, Time: 0.81\n",
            "mse_b  [5352.1465]  mse_f: [788.08264]   total loss: [6140.229]\n",
            "It: 27240, Time: 0.81\n",
            "mse_b  [5312.444]  mse_f: [743.66974]   total loss: [6056.114]\n",
            "It: 27280, Time: 0.81\n",
            "mse_b  [5265.852]  mse_f: [667.3369]   total loss: [5933.189]\n",
            "It: 27320, Time: 0.82\n",
            "mse_b  [5216.0947]  mse_f: [849.4596]   total loss: [6065.554]\n",
            "It: 27360, Time: 0.82\n",
            "mse_b  [5210.924]  mse_f: [857.63837]   total loss: [6068.562]\n",
            "It: 27400, Time: 0.80\n",
            "mse_b  [5223.0205]  mse_f: [860.6329]   total loss: [6083.6533]\n",
            "It: 27440, Time: 0.81\n",
            "mse_b  [5240.8447]  mse_f: [968.98486]   total loss: [6209.8296]\n",
            "It: 27480, Time: 0.84\n",
            "mse_b  [5275.91]  mse_f: [743.7033]   total loss: [6019.6133]\n",
            "It: 27520, Time: 0.87\n",
            "mse_b  [5194.0806]  mse_f: [735.1564]   total loss: [5929.237]\n",
            "It: 27560, Time: 0.87\n",
            "mse_b  [5207.1685]  mse_f: [970.94604]   total loss: [6178.1143]\n",
            "It: 27600, Time: 0.83\n",
            "mse_b  [5682.246]  mse_f: [1442.8318]   total loss: [7125.078]\n",
            "It: 27640, Time: 0.83\n",
            "mse_b  [5547.003]  mse_f: [1773.8938]   total loss: [7320.8965]\n",
            "It: 27680, Time: 0.81\n",
            "mse_b  [6615.649]  mse_f: [3258.1357]   total loss: [9873.785]\n",
            "It: 27720, Time: 0.82\n",
            "mse_b  [5558.997]  mse_f: [1772.3375]   total loss: [7331.3345]\n",
            "It: 27760, Time: 0.80\n",
            "mse_b  [5383.3457]  mse_f: [1363.1096]   total loss: [6746.455]\n",
            "It: 27800, Time: 0.81\n",
            "mse_b  [5255.511]  mse_f: [1182.5691]   total loss: [6438.08]\n",
            "It: 27840, Time: 0.81\n",
            "mse_b  [5342.3457]  mse_f: [1013.4385]   total loss: [6355.784]\n",
            "It: 27880, Time: 0.88\n",
            "mse_b  [5197.1035]  mse_f: [944.74445]   total loss: [6141.848]\n",
            "It: 27920, Time: 0.81\n",
            "mse_b  [5186.245]  mse_f: [697.7849]   total loss: [5884.0303]\n",
            "It: 27960, Time: 0.82\n",
            "mse_b  [5101.915]  mse_f: [729.9283]   total loss: [5831.8433]\n",
            "It: 28000, Time: 0.80\n",
            "mse_b  [5053.195]  mse_f: [657.8629]   total loss: [5711.0576]\n",
            "It: 28040, Time: 0.81\n",
            "mse_b  [5020.958]  mse_f: [647.1552]   total loss: [5668.1133]\n",
            "It: 28080, Time: 0.81\n",
            "mse_b  [4992.1616]  mse_f: [659.0035]   total loss: [5651.165]\n",
            "It: 28120, Time: 0.79\n",
            "mse_b  [5062.909]  mse_f: [644.89636]   total loss: [5707.8057]\n",
            "It: 28160, Time: 0.80\n",
            "mse_b  [4987.933]  mse_f: [1164.348]   total loss: [6152.2812]\n",
            "It: 28200, Time: 0.81\n",
            "mse_b  [5044.19]  mse_f: [905.3059]   total loss: [5949.496]\n",
            "It: 28240, Time: 0.83\n",
            "mse_b  [5011.3257]  mse_f: [743.6699]   total loss: [5754.9956]\n",
            "It: 28280, Time: 0.84\n",
            "mse_b  [5020.2856]  mse_f: [669.68774]   total loss: [5689.9736]\n",
            "It: 28320, Time: 0.81\n",
            "mse_b  [5051.938]  mse_f: [793.7711]   total loss: [5845.709]\n",
            "It: 28360, Time: 0.84\n",
            "mse_b  [4918.05]  mse_f: [679.5811]   total loss: [5597.631]\n",
            "It: 28400, Time: 0.81\n",
            "mse_b  [4936.1523]  mse_f: [726.6605]   total loss: [5662.813]\n",
            "It: 28440, Time: 0.80\n",
            "mse_b  [4880.7583]  mse_f: [724.47034]   total loss: [5605.2285]\n",
            "It: 28480, Time: 0.81\n",
            "mse_b  [4897.7153]  mse_f: [1069.0586]   total loss: [5966.774]\n",
            "It: 28520, Time: 0.80\n",
            "mse_b  [4895.806]  mse_f: [997.02234]   total loss: [5892.8286]\n",
            "It: 28560, Time: 0.83\n",
            "mse_b  [4873.704]  mse_f: [748.1357]   total loss: [5621.84]\n",
            "It: 28600, Time: 0.83\n",
            "mse_b  [4925.26]  mse_f: [875.7]   total loss: [5800.96]\n",
            "It: 28640, Time: 0.83\n",
            "mse_b  [4849.5264]  mse_f: [673.64594]   total loss: [5523.1724]\n",
            "It: 28680, Time: 0.85\n",
            "mse_b  [4881.3105]  mse_f: [722.3772]   total loss: [5603.6875]\n",
            "It: 28720, Time: 0.86\n",
            "mse_b  [4759.905]  mse_f: [756.9547]   total loss: [5516.8594]\n",
            "It: 28760, Time: 0.81\n",
            "mse_b  [4870.8555]  mse_f: [729.3832]   total loss: [5600.239]\n",
            "It: 28800, Time: 0.83\n",
            "mse_b  [4785.9517]  mse_f: [787.8068]   total loss: [5573.7583]\n",
            "It: 28840, Time: 0.81\n",
            "mse_b  [4749.155]  mse_f: [775.8926]   total loss: [5525.0474]\n",
            "It: 28880, Time: 0.81\n",
            "mse_b  [4823.2256]  mse_f: [1109.3127]   total loss: [5932.538]\n",
            "It: 28920, Time: 0.85\n",
            "mse_b  [4838.227]  mse_f: [1042.6631]   total loss: [5880.89]\n",
            "It: 28960, Time: 0.84\n",
            "mse_b  [4865.2314]  mse_f: [2377.828]   total loss: [7243.0596]\n",
            "It: 29000, Time: 0.84\n",
            "mse_b  [5102.9453]  mse_f: [1883.727]   total loss: [6986.6724]\n",
            "It: 29040, Time: 0.84\n",
            "mse_b  [4809.328]  mse_f: [2140.7422]   total loss: [6950.0703]\n",
            "It: 29080, Time: 0.83\n",
            "mse_b  [4784.771]  mse_f: [932.9783]   total loss: [5717.749]\n",
            "It: 29120, Time: 0.82\n",
            "mse_b  [4711.3745]  mse_f: [1096.4551]   total loss: [5807.8296]\n",
            "It: 29160, Time: 0.81\n",
            "mse_b  [4775.0654]  mse_f: [853.27625]   total loss: [5628.342]\n",
            "It: 29200, Time: 0.80\n",
            "mse_b  [4623.209]  mse_f: [793.1344]   total loss: [5416.3433]\n",
            "It: 29240, Time: 0.83\n",
            "mse_b  [4619.589]  mse_f: [661.407]   total loss: [5280.996]\n",
            "It: 29280, Time: 0.83\n",
            "mse_b  [4615.6934]  mse_f: [627.5768]   total loss: [5243.27]\n",
            "It: 29320, Time: 0.89\n",
            "mse_b  [4555.0205]  mse_f: [581.79114]   total loss: [5136.8115]\n",
            "It: 29360, Time: 0.83\n",
            "mse_b  [4524.1797]  mse_f: [665.5277]   total loss: [5189.7075]\n",
            "It: 29400, Time: 0.83\n",
            "mse_b  [4468.525]  mse_f: [704.13983]   total loss: [5172.6646]\n",
            "It: 29440, Time: 0.85\n",
            "mse_b  [4479.496]  mse_f: [624.4022]   total loss: [5103.8984]\n",
            "It: 29480, Time: 0.88\n",
            "mse_b  [5343.671]  mse_f: [1861.3868]   total loss: [7205.0576]\n",
            "It: 29520, Time: 0.83\n",
            "mse_b  [5172.107]  mse_f: [1830.492]   total loss: [7002.5986]\n",
            "It: 29560, Time: 0.81\n",
            "mse_b  [4816.242]  mse_f: [2454.9587]   total loss: [7271.201]\n",
            "It: 29600, Time: 0.81\n",
            "mse_b  [5144.7915]  mse_f: [1741.5647]   total loss: [6886.3564]\n",
            "It: 29640, Time: 0.82\n",
            "mse_b  [4841.832]  mse_f: [1460.0479]   total loss: [6301.88]\n",
            "It: 29680, Time: 0.81\n",
            "mse_b  [4634.905]  mse_f: [1274.1536]   total loss: [5909.0586]\n",
            "It: 29720, Time: 0.81\n",
            "mse_b  [4539.3164]  mse_f: [916.7779]   total loss: [5456.094]\n",
            "It: 29760, Time: 0.80\n",
            "mse_b  [4565.7446]  mse_f: [758.89355]   total loss: [5324.638]\n",
            "It: 29800, Time: 0.83\n",
            "mse_b  [4494.606]  mse_f: [668.3547]   total loss: [5162.9604]\n",
            "It: 29840, Time: 0.81\n",
            "mse_b  [4423.6763]  mse_f: [631.3253]   total loss: [5055.0015]\n",
            "It: 29880, Time: 0.82\n",
            "mse_b  [4384.6494]  mse_f: [620.9482]   total loss: [5005.5977]\n",
            "It: 29920, Time: 0.84\n",
            "mse_b  [4387.545]  mse_f: [842.7447]   total loss: [5230.2896]\n",
            "It: 29960, Time: 0.85\n",
            "mse_b  [4471.588]  mse_f: [684.74207]   total loss: [5156.33]\n",
            "It: 30000, Time: 0.83\n",
            "mse_b  [4329.8643]  mse_f: [783.13416]   total loss: [5112.9985]\n",
            "It: 30040, Time: 0.83\n",
            "mse_b  [4362.9106]  mse_f: [679.11914]   total loss: [5042.03]\n",
            "It: 30080, Time: 0.83\n",
            "mse_b  [4341.607]  mse_f: [784.0709]   total loss: [5125.6777]\n",
            "It: 30120, Time: 0.83\n",
            "mse_b  [4360.361]  mse_f: [686.38586]   total loss: [5046.7466]\n",
            "It: 30160, Time: 0.82\n",
            "mse_b  [4274.758]  mse_f: [748.50757]   total loss: [5023.2656]\n",
            "It: 30200, Time: 0.83\n",
            "mse_b  [4332.267]  mse_f: [676.6533]   total loss: [5008.9204]\n",
            "It: 30240, Time: 0.83\n",
            "mse_b  [4314.9395]  mse_f: [1228.158]   total loss: [5543.0977]\n",
            "It: 30280, Time: 0.84\n",
            "mse_b  [4345.1123]  mse_f: [1054.343]   total loss: [5399.455]\n",
            "It: 30320, Time: 0.82\n",
            "mse_b  [4296.6157]  mse_f: [1033.3489]   total loss: [5329.965]\n",
            "It: 30360, Time: 0.83\n",
            "mse_b  [4289.873]  mse_f: [874.6763]   total loss: [5164.5493]\n",
            "It: 30400, Time: 0.84\n",
            "mse_b  [4244.6797]  mse_f: [776.6384]   total loss: [5021.3184]\n",
            "It: 30440, Time: 0.86\n",
            "mse_b  [4238.9336]  mse_f: [766.182]   total loss: [5005.1157]\n",
            "It: 30480, Time: 0.83\n",
            "mse_b  [4191.1987]  mse_f: [799.12836]   total loss: [4990.327]\n",
            "It: 30520, Time: 0.82\n",
            "mse_b  [4292.448]  mse_f: [720.3271]   total loss: [5012.7754]\n",
            "It: 30560, Time: 0.81\n",
            "mse_b  [4166.3843]  mse_f: [781.9766]   total loss: [4948.361]\n",
            "It: 30600, Time: 0.81\n",
            "mse_b  [4821.0874]  mse_f: [2494.1902]   total loss: [7315.2773]\n",
            "It: 30640, Time: 0.84\n",
            "mse_b  [4923.2915]  mse_f: [2705.7354]   total loss: [7629.027]\n",
            "It: 30680, Time: 0.84\n",
            "mse_b  [4570.43]  mse_f: [2247.1572]   total loss: [6817.5874]\n",
            "It: 30720, Time: 0.84\n",
            "mse_b  [4640.0376]  mse_f: [1214.6702]   total loss: [5854.708]\n",
            "It: 30760, Time: 0.84\n",
            "mse_b  [4479.846]  mse_f: [981.2324]   total loss: [5461.0786]\n",
            "It: 30800, Time: 0.86\n",
            "mse_b  [4434.1763]  mse_f: [902.9292]   total loss: [5337.1055]\n",
            "It: 30840, Time: 0.83\n",
            "mse_b  [4290.542]  mse_f: [755.4031]   total loss: [5045.9453]\n",
            "It: 30880, Time: 0.83\n",
            "mse_b  [4191.608]  mse_f: [840.8351]   total loss: [5032.443]\n",
            "It: 30920, Time: 0.82\n",
            "mse_b  [4169.8237]  mse_f: [665.5325]   total loss: [4835.3564]\n",
            "It: 30960, Time: 0.83\n",
            "mse_b  [4158.3984]  mse_f: [570.68933]   total loss: [4729.088]\n",
            "It: 31000, Time: 0.82\n",
            "mse_b  [4070.97]  mse_f: [636.49207]   total loss: [4707.462]\n",
            "It: 31040, Time: 0.83\n",
            "mse_b  [4058.0396]  mse_f: [691.16534]   total loss: [4749.205]\n",
            "It: 31080, Time: 0.80\n",
            "mse_b  [4104.9297]  mse_f: [801.48486]   total loss: [4906.4146]\n",
            "It: 31120, Time: 0.85\n",
            "mse_b  [4195.248]  mse_f: [780.03186]   total loss: [4975.28]\n",
            "It: 31160, Time: 0.81\n",
            "mse_b  [4097.3647]  mse_f: [910.9334]   total loss: [5008.2983]\n",
            "It: 31200, Time: 0.81\n",
            "mse_b  [4288.8584]  mse_f: [1508.131]   total loss: [5796.9893]\n",
            "It: 31240, Time: 0.83\n",
            "mse_b  [4148.7524]  mse_f: [1151.2927]   total loss: [5300.045]\n",
            "It: 31280, Time: 0.81\n",
            "mse_b  [4124.4106]  mse_f: [1171.7449]   total loss: [5296.1553]\n",
            "It: 31320, Time: 0.80\n",
            "mse_b  [4137.072]  mse_f: [1020.2838]   total loss: [5157.3555]\n",
            "It: 31360, Time: 0.80\n",
            "mse_b  [4072.6594]  mse_f: [820.5904]   total loss: [4893.25]\n",
            "It: 31400, Time: 0.79\n",
            "mse_b  [4149.204]  mse_f: [802.7251]   total loss: [4951.929]\n",
            "It: 31440, Time: 0.81\n",
            "mse_b  [4092.084]  mse_f: [654.8266]   total loss: [4746.9106]\n",
            "It: 31480, Time: 0.80\n",
            "mse_b  [4071.7817]  mse_f: [692.48035]   total loss: [4764.262]\n",
            "It: 31520, Time: 0.80\n",
            "mse_b  [3991.458]  mse_f: [825.1666]   total loss: [4816.6245]\n",
            "It: 31560, Time: 0.83\n",
            "mse_b  [4045.116]  mse_f: [757.28516]   total loss: [4802.4014]\n",
            "It: 31600, Time: 0.80\n",
            "mse_b  [4315.1675]  mse_f: [2406.3906]   total loss: [6721.558]\n",
            "It: 31640, Time: 0.81\n",
            "mse_b  [4304.726]  mse_f: [1179.8972]   total loss: [5484.623]\n",
            "It: 31680, Time: 0.83\n",
            "mse_b  [4151.079]  mse_f: [1442.729]   total loss: [5593.808]\n",
            "It: 31720, Time: 0.81\n",
            "mse_b  [4193.423]  mse_f: [895.9419]   total loss: [5089.3647]\n",
            "It: 31760, Time: 0.82\n",
            "mse_b  [4297.67]  mse_f: [1232.7268]   total loss: [5530.3965]\n",
            "It: 31800, Time: 0.84\n",
            "mse_b  [4089.0713]  mse_f: [1127.7355]   total loss: [5216.8066]\n",
            "It: 31840, Time: 0.83\n",
            "mse_b  [4193.2383]  mse_f: [1119.5048]   total loss: [5312.743]\n",
            "It: 31880, Time: 0.83\n",
            "mse_b  [4558.1606]  mse_f: [2426.2488]   total loss: [6984.409]\n",
            "It: 31920, Time: 0.81\n",
            "mse_b  [4898.963]  mse_f: [2751.7668]   total loss: [7650.7295]\n",
            "It: 31960, Time: 0.82\n",
            "mse_b  [4597.418]  mse_f: [1553.284]   total loss: [6150.702]\n",
            "It: 32000, Time: 0.83\n",
            "mse_b  [4485.3228]  mse_f: [1183.9297]   total loss: [5669.2524]\n",
            "It: 32040, Time: 0.81\n",
            "mse_b  [4309.36]  mse_f: [975.79724]   total loss: [5285.157]\n",
            "It: 32080, Time: 0.82\n",
            "mse_b  [4221.653]  mse_f: [805.1349]   total loss: [5026.7876]\n",
            "It: 32120, Time: 0.82\n",
            "mse_b  [4114.0083]  mse_f: [987.1743]   total loss: [5101.1826]\n",
            "It: 32160, Time: 0.84\n",
            "mse_b  [4029.1624]  mse_f: [904.4373]   total loss: [4933.5996]\n",
            "It: 32200, Time: 0.83\n",
            "mse_b  [4863.4785]  mse_f: [1946.146]   total loss: [6809.6245]\n",
            "It: 32240, Time: 0.90\n",
            "mse_b  [4415.3174]  mse_f: [1414.1448]   total loss: [5829.462]\n",
            "It: 32280, Time: 0.84\n",
            "mse_b  [4346.4727]  mse_f: [1003.36316]   total loss: [5349.836]\n",
            "It: 32320, Time: 0.83\n",
            "mse_b  [4196.969]  mse_f: [909.6852]   total loss: [5106.6543]\n",
            "It: 32360, Time: 0.82\n",
            "mse_b  [4092.97]  mse_f: [851.7106]   total loss: [4944.6807]\n",
            "It: 32400, Time: 0.82\n",
            "mse_b  [4073.131]  mse_f: [1489.9426]   total loss: [5563.0737]\n",
            "It: 32440, Time: 0.81\n",
            "mse_b  [4039.173]  mse_f: [1039.3352]   total loss: [5078.5083]\n",
            "It: 32480, Time: 0.83\n",
            "mse_b  [4014.6265]  mse_f: [1094.3491]   total loss: [5108.9756]\n",
            "It: 32520, Time: 0.81\n",
            "mse_b  [3957.286]  mse_f: [917.9049]   total loss: [4875.191]\n",
            "It: 32560, Time: 0.81\n",
            "mse_b  [3917.8315]  mse_f: [767.6232]   total loss: [4685.4546]\n",
            "It: 32600, Time: 0.81\n",
            "mse_b  [3880.953]  mse_f: [638.84326]   total loss: [4519.796]\n",
            "It: 32640, Time: 0.80\n",
            "mse_b  [3848.1145]  mse_f: [601.9568]   total loss: [4450.0713]\n",
            "It: 32680, Time: 0.81\n",
            "mse_b  [3830.1191]  mse_f: [563.14795]   total loss: [4393.267]\n",
            "It: 32720, Time: 0.82\n",
            "mse_b  [3774.3196]  mse_f: [859.0345]   total loss: [4633.354]\n",
            "It: 32760, Time: 0.82\n",
            "mse_b  [3777.063]  mse_f: [652.463]   total loss: [4429.526]\n",
            "It: 32800, Time: 0.84\n",
            "mse_b  [3784.9507]  mse_f: [814.58093]   total loss: [4599.5317]\n",
            "It: 32840, Time: 0.84\n",
            "mse_b  [3773.4922]  mse_f: [928.0353]   total loss: [4701.5273]\n",
            "It: 32880, Time: 0.81\n",
            "mse_b  [3842.1226]  mse_f: [777.0526]   total loss: [4619.1753]\n",
            "It: 32920, Time: 0.81\n",
            "mse_b  [3859.9893]  mse_f: [1204.593]   total loss: [5064.582]\n",
            "It: 32960, Time: 0.83\n",
            "mse_b  [3789.3564]  mse_f: [1139.4766]   total loss: [4928.833]\n",
            "It: 33000, Time: 0.83\n",
            "mse_b  [3804.4294]  mse_f: [847.5582]   total loss: [4651.988]\n",
            "It: 33040, Time: 0.83\n",
            "mse_b  [3766.0952]  mse_f: [872.546]   total loss: [4638.641]\n",
            "It: 33080, Time: 0.83\n",
            "mse_b  [3704.0554]  mse_f: [639.4351]   total loss: [4343.4907]\n",
            "It: 33120, Time: 0.83\n",
            "mse_b  [3775.6648]  mse_f: [744.4641]   total loss: [4520.129]\n",
            "It: 33160, Time: 0.81\n",
            "mse_b  [3687.7341]  mse_f: [664.3502]   total loss: [4352.0845]\n",
            "It: 33200, Time: 0.82\n",
            "mse_b  [3710.9795]  mse_f: [786.05347]   total loss: [4497.033]\n",
            "It: 33240, Time: 0.82\n",
            "mse_b  [3692.7217]  mse_f: [1052.0011]   total loss: [4744.7227]\n",
            "It: 33280, Time: 0.84\n",
            "mse_b  [3760.5212]  mse_f: [976.01917]   total loss: [4736.5405]\n",
            "It: 33320, Time: 0.82\n",
            "mse_b  [3764.7952]  mse_f: [868.3386]   total loss: [4633.134]\n",
            "It: 33360, Time: 0.86\n",
            "mse_b  [3694.1875]  mse_f: [912.78217]   total loss: [4606.9697]\n",
            "It: 33400, Time: 0.82\n",
            "mse_b  [3648.5698]  mse_f: [1145.4976]   total loss: [4794.0674]\n",
            "It: 33440, Time: 0.82\n",
            "mse_b  [3737.918]  mse_f: [601.692]   total loss: [4339.61]\n",
            "It: 33480, Time: 0.81\n",
            "mse_b  [3657.491]  mse_f: [1001.3127]   total loss: [4658.8037]\n",
            "It: 33520, Time: 0.80\n",
            "mse_b  [3650.9448]  mse_f: [548.1128]   total loss: [4199.0576]\n",
            "It: 33560, Time: 0.80\n",
            "mse_b  [3671.7305]  mse_f: [1400.9382]   total loss: [5072.669]\n",
            "It: 33600, Time: 0.81\n",
            "mse_b  [4090.8809]  mse_f: [1697.3279]   total loss: [5788.209]\n",
            "It: 33640, Time: 0.80\n",
            "mse_b  [3845.4387]  mse_f: [1456.9766]   total loss: [5302.415]\n",
            "It: 33680, Time: 0.86\n",
            "mse_b  [3901.7795]  mse_f: [941.34814]   total loss: [4843.128]\n",
            "It: 33720, Time: 0.88\n",
            "mse_b  [3797.3472]  mse_f: [927.8613]   total loss: [4725.2085]\n",
            "It: 33760, Time: 0.84\n",
            "mse_b  [3685.721]  mse_f: [820.9219]   total loss: [4506.6426]\n",
            "It: 33800, Time: 0.84\n",
            "mse_b  [3710.8796]  mse_f: [554.69653]   total loss: [4265.576]\n",
            "It: 33840, Time: 0.84\n",
            "mse_b  [3804.865]  mse_f: [1145.3767]   total loss: [4950.2417]\n",
            "It: 33880, Time: 0.82\n",
            "mse_b  [3855.801]  mse_f: [1514.2136]   total loss: [5370.0146]\n",
            "It: 33920, Time: 0.82\n",
            "mse_b  [3850.09]  mse_f: [922.71576]   total loss: [4772.8057]\n",
            "It: 33960, Time: 0.82\n",
            "mse_b  [4101.7476]  mse_f: [1491.4082]   total loss: [5593.156]\n",
            "It: 34000, Time: 0.85\n",
            "mse_b  [3776.6003]  mse_f: [1105.2832]   total loss: [4881.884]\n",
            "It: 34040, Time: 0.82\n",
            "mse_b  [3802.5955]  mse_f: [959.8382]   total loss: [4762.4336]\n",
            "It: 34080, Time: 0.80\n",
            "mse_b  [3698.629]  mse_f: [645.0071]   total loss: [4343.6357]\n",
            "It: 34120, Time: 0.83\n",
            "mse_b  [3660.2944]  mse_f: [584.28796]   total loss: [4244.5825]\n",
            "It: 34160, Time: 0.82\n",
            "mse_b  [3632.004]  mse_f: [608.254]   total loss: [4240.258]\n",
            "It: 34200, Time: 0.84\n",
            "mse_b  [3641.4568]  mse_f: [643.68933]   total loss: [4285.146]\n",
            "It: 34240, Time: 0.83\n",
            "mse_b  [3581.9802]  mse_f: [639.709]   total loss: [4221.6895]\n",
            "It: 34280, Time: 0.83\n",
            "mse_b  [3515.595]  mse_f: [586.3065]   total loss: [4101.9014]\n",
            "It: 34320, Time: 0.84\n",
            "mse_b  [3549.3296]  mse_f: [889.18835]   total loss: [4438.518]\n",
            "It: 34360, Time: 0.84\n",
            "mse_b  [3559.6567]  mse_f: [952.17645]   total loss: [4511.833]\n",
            "It: 34400, Time: 0.82\n",
            "mse_b  [3513.6475]  mse_f: [713.1945]   total loss: [4226.842]\n",
            "It: 34440, Time: 0.85\n",
            "mse_b  [3556.6887]  mse_f: [710.7905]   total loss: [4267.4795]\n",
            "It: 34480, Time: 0.81\n",
            "mse_b  [3478.2283]  mse_f: [959.6545]   total loss: [4437.883]\n",
            "It: 34520, Time: 0.84\n",
            "mse_b  [3546.7334]  mse_f: [967.6974]   total loss: [4514.4307]\n",
            "It: 34560, Time: 0.82\n",
            "mse_b  [3550.158]  mse_f: [699.0211]   total loss: [4249.179]\n",
            "It: 34600, Time: 0.82\n",
            "mse_b  [3535.8086]  mse_f: [797.0165]   total loss: [4332.825]\n",
            "It: 34640, Time: 0.82\n",
            "mse_b  [3471.714]  mse_f: [729.2776]   total loss: [4200.9917]\n",
            "It: 34680, Time: 0.84\n",
            "mse_b  [3510.9365]  mse_f: [977.441]   total loss: [4488.3774]\n",
            "It: 34720, Time: 0.85\n",
            "mse_b  [3559.6243]  mse_f: [879.5575]   total loss: [4439.1816]\n",
            "It: 34760, Time: 0.83\n",
            "mse_b  [3608.462]  mse_f: [902.41266]   total loss: [4510.8745]\n",
            "It: 34800, Time: 0.83\n",
            "mse_b  [3596.0938]  mse_f: [1037.2832]   total loss: [4633.377]\n",
            "It: 34840, Time: 0.83\n",
            "mse_b  [3543.192]  mse_f: [836.44257]   total loss: [4379.6343]\n",
            "It: 34880, Time: 0.86\n",
            "mse_b  [3445.415]  mse_f: [734.9985]   total loss: [4180.4136]\n",
            "It: 34920, Time: 0.84\n",
            "mse_b  [3442.813]  mse_f: [1404.0723]   total loss: [4846.8853]\n",
            "It: 34960, Time: 0.86\n",
            "mse_b  [6665.787]  mse_f: [4000.569]   total loss: [10666.356]\n",
            "It: 35000, Time: 0.84\n",
            "mse_b  [5753.0527]  mse_f: [2878.5388]   total loss: [8631.592]\n",
            "It: 35040, Time: 0.85\n",
            "mse_b  [5119.1475]  mse_f: [1917.5156]   total loss: [7036.663]\n",
            "It: 35080, Time: 0.81\n",
            "mse_b  [4717.6636]  mse_f: [1766.7245]   total loss: [6484.388]\n",
            "It: 35120, Time: 0.82\n",
            "mse_b  [4281.592]  mse_f: [1532.2991]   total loss: [5813.8906]\n",
            "It: 35160, Time: 0.89\n",
            "mse_b  [4082.621]  mse_f: [1260.3439]   total loss: [5342.965]\n",
            "It: 35200, Time: 0.83\n",
            "mse_b  [4007.7]  mse_f: [1078.0986]   total loss: [5085.799]\n",
            "It: 35240, Time: 0.82\n",
            "mse_b  [3881.688]  mse_f: [820.858]   total loss: [4702.546]\n",
            "It: 35280, Time: 0.82\n",
            "mse_b  [3723.0535]  mse_f: [895.0266]   total loss: [4618.08]\n",
            "It: 35320, Time: 0.82\n",
            "mse_b  [5590.824]  mse_f: [2682.8096]   total loss: [8273.634]\n",
            "It: 35360, Time: 0.84\n",
            "mse_b  [5006.8467]  mse_f: [2360.7754]   total loss: [7367.622]\n",
            "It: 35400, Time: 0.82\n",
            "mse_b  [4933.879]  mse_f: [1746.0686]   total loss: [6679.9473]\n",
            "It: 35440, Time: 0.82\n",
            "mse_b  [4559.3853]  mse_f: [1470.4309]   total loss: [6029.8164]\n",
            "It: 35480, Time: 0.83\n",
            "mse_b  [4173.94]  mse_f: [1130.2216]   total loss: [5304.1616]\n",
            "It: 35520, Time: 0.82\n",
            "mse_b  [3981.0425]  mse_f: [1002.2291]   total loss: [4983.2715]\n",
            "It: 35560, Time: 0.81\n",
            "mse_b  [3847.9976]  mse_f: [678.12506]   total loss: [4526.1226]\n",
            "It: 35600, Time: 0.84\n",
            "mse_b  [3759.8496]  mse_f: [707.0706]   total loss: [4466.9204]\n",
            "It: 35640, Time: 0.81\n",
            "mse_b  [3701.426]  mse_f: [552.8429]   total loss: [4254.269]\n",
            "It: 35680, Time: 0.82\n",
            "mse_b  [3603.5093]  mse_f: [537.0958]   total loss: [4140.605]\n",
            "It: 35720, Time: 0.82\n",
            "mse_b  [3564.0334]  mse_f: [703.02997]   total loss: [4267.0635]\n",
            "It: 35760, Time: 0.83\n",
            "mse_b  [3515.863]  mse_f: [895.5715]   total loss: [4411.4346]\n",
            "It: 35800, Time: 0.81\n",
            "mse_b  [3582.465]  mse_f: [925.4181]   total loss: [4507.8833]\n",
            "It: 35840, Time: 0.81\n",
            "mse_b  [3497.0667]  mse_f: [623.1156]   total loss: [4120.182]\n",
            "It: 35880, Time: 0.81\n",
            "mse_b  [3668.9675]  mse_f: [762.3519]   total loss: [4431.3193]\n",
            "It: 35920, Time: 0.81\n",
            "mse_b  [3544.03]  mse_f: [805.9636]   total loss: [4349.9937]\n",
            "It: 35960, Time: 0.82\n",
            "mse_b  [3513.6372]  mse_f: [557.9971]   total loss: [4071.6343]\n",
            "It: 36000, Time: 0.79\n",
            "mse_b  [3534.2356]  mse_f: [688.0558]   total loss: [4222.2915]\n",
            "It: 36040, Time: 0.82\n",
            "mse_b  [3427.28]  mse_f: [778.73444]   total loss: [4206.0146]\n",
            "It: 36080, Time: 0.82\n",
            "mse_b  [4168.5337]  mse_f: [1874.1882]   total loss: [6042.7217]\n",
            "It: 36120, Time: 0.83\n",
            "mse_b  [3818.6516]  mse_f: [1513.5138]   total loss: [5332.1655]\n",
            "It: 36160, Time: 0.84\n",
            "mse_b  [3802.0505]  mse_f: [1271.1287]   total loss: [5073.179]\n",
            "It: 36200, Time: 0.82\n",
            "mse_b  [3660.1396]  mse_f: [856.85126]   total loss: [4516.9907]\n",
            "It: 36240, Time: 0.82\n",
            "mse_b  [3534.9717]  mse_f: [828.0287]   total loss: [4363.0005]\n",
            "It: 36280, Time: 0.83\n",
            "mse_b  [3567.524]  mse_f: [1008.0114]   total loss: [4575.535]\n",
            "It: 36320, Time: 0.81\n",
            "mse_b  [4883.4995]  mse_f: [2806.7876]   total loss: [7690.287]\n",
            "It: 36360, Time: 0.84\n",
            "mse_b  [4493.2725]  mse_f: [2935.8115]   total loss: [7429.084]\n",
            "It: 36400, Time: 0.83\n",
            "mse_b  [4254.9297]  mse_f: [1951.8965]   total loss: [6206.826]\n",
            "It: 36440, Time: 1.29\n",
            "mse_b  [4200.417]  mse_f: [1329.1428]   total loss: [5529.5596]\n",
            "It: 36480, Time: 1.62\n",
            "mse_b  [3870.482]  mse_f: [1097.2087]   total loss: [4967.6904]\n",
            "It: 36520, Time: 1.27\n",
            "mse_b  [3778.985]  mse_f: [911.2709]   total loss: [4690.256]\n",
            "It: 36560, Time: 0.91\n",
            "mse_b  [3627.2727]  mse_f: [773.20013]   total loss: [4400.4727]\n",
            "It: 36600, Time: 0.85\n",
            "mse_b  [3532.2856]  mse_f: [711.311]   total loss: [4243.5967]\n",
            "It: 36640, Time: 0.81\n",
            "mse_b  [3880.7124]  mse_f: [1917.8357]   total loss: [5798.548]\n",
            "It: 36680, Time: 0.83\n",
            "mse_b  [3694.0146]  mse_f: [1226.905]   total loss: [4920.92]\n",
            "It: 36720, Time: 0.83\n",
            "mse_b  [3638.3037]  mse_f: [934.78467]   total loss: [4573.0884]\n",
            "It: 36760, Time: 0.81\n",
            "mse_b  [4320.9214]  mse_f: [2345.1602]   total loss: [6666.0815]\n",
            "It: 36800, Time: 0.84\n",
            "mse_b  [4371.801]  mse_f: [1723.8761]   total loss: [6095.677]\n",
            "It: 36840, Time: 0.83\n",
            "mse_b  [3807.7383]  mse_f: [1318.9893]   total loss: [5126.7275]\n",
            "It: 36880, Time: 0.81\n",
            "mse_b  [3591.8667]  mse_f: [972.17773]   total loss: [4564.0444]\n",
            "It: 36920, Time: 0.81\n",
            "mse_b  [3525.834]  mse_f: [676.9637]   total loss: [4202.798]\n",
            "It: 36960, Time: 0.80\n",
            "mse_b  [4024.119]  mse_f: [1868.4166]   total loss: [5892.5356]\n",
            "It: 37000, Time: 0.80\n",
            "mse_b  [3859.6401]  mse_f: [1988.7698]   total loss: [5848.41]\n",
            "It: 37040, Time: 0.80\n",
            "mse_b  [3675.0264]  mse_f: [1372.6218]   total loss: [5047.6484]\n",
            "It: 37080, Time: 0.81\n",
            "mse_b  [5455.561]  mse_f: [2152.454]   total loss: [7608.015]\n",
            "It: 37120, Time: 0.81\n",
            "mse_b  [4740.0723]  mse_f: [1863.9977]   total loss: [6604.07]\n",
            "It: 37160, Time: 0.91\n",
            "mse_b  [4123.656]  mse_f: [1668.4575]   total loss: [5792.1133]\n",
            "It: 37200, Time: 0.80\n",
            "mse_b  [4118.4404]  mse_f: [1190.5723]   total loss: [5309.0127]\n",
            "It: 37240, Time: 0.84\n",
            "mse_b  [3891.8792]  mse_f: [1036.1414]   total loss: [4928.0205]\n",
            "It: 37280, Time: 0.82\n",
            "mse_b  [3661.7913]  mse_f: [809.5531]   total loss: [4471.344]\n",
            "It: 37320, Time: 0.84\n",
            "mse_b  [3531.3076]  mse_f: [652.1787]   total loss: [4183.4863]\n",
            "It: 37360, Time: 0.81\n",
            "mse_b  [3494.9285]  mse_f: [1158.8754]   total loss: [4653.8037]\n",
            "It: 37400, Time: 0.81\n",
            "mse_b  [3576.0542]  mse_f: [857.3992]   total loss: [4433.453]\n",
            "It: 37440, Time: 0.82\n",
            "mse_b  [3501.1426]  mse_f: [756.3843]   total loss: [4257.527]\n",
            "It: 37480, Time: 0.82\n",
            "mse_b  [3377.2593]  mse_f: [608.7437]   total loss: [3986.003]\n",
            "It: 37520, Time: 0.81\n",
            "mse_b  [3323.7314]  mse_f: [507.22964]   total loss: [3830.9612]\n",
            "It: 37560, Time: 0.81\n",
            "mse_b  [3303.754]  mse_f: [722.59326]   total loss: [4026.3472]\n",
            "It: 37600, Time: 0.82\n",
            "mse_b  [3242.0688]  mse_f: [652.7628]   total loss: [3894.8315]\n",
            "It: 37640, Time: 0.82\n",
            "mse_b  [3268.7234]  mse_f: [591.50964]   total loss: [3860.233]\n",
            "It: 37680, Time: 0.84\n",
            "mse_b  [3203.8264]  mse_f: [716.6439]   total loss: [3920.4702]\n",
            "It: 37720, Time: 0.83\n",
            "mse_b  [3160.7568]  mse_f: [709.7411]   total loss: [3870.498]\n",
            "It: 37760, Time: 0.82\n",
            "mse_b  [3228.5496]  mse_f: [744.8345]   total loss: [3973.384]\n",
            "It: 37800, Time: 0.81\n",
            "mse_b  [3826.674]  mse_f: [1269.2795]   total loss: [5095.9536]\n",
            "It: 37840, Time: 0.84\n",
            "mse_b  [3553.9246]  mse_f: [1075.3939]   total loss: [4629.3184]\n",
            "It: 37880, Time: 0.81\n",
            "mse_b  [3477.592]  mse_f: [972.8856]   total loss: [4450.4775]\n",
            "It: 37920, Time: 0.82\n",
            "mse_b  [3339.2974]  mse_f: [733.89746]   total loss: [4073.1948]\n",
            "It: 37960, Time: 0.82\n",
            "mse_b  [3246.664]  mse_f: [930.77563]   total loss: [4177.4395]\n",
            "It: 38000, Time: 0.84\n",
            "mse_b  [5034.045]  mse_f: [2764.67]   total loss: [7798.715]\n",
            "It: 38040, Time: 0.82\n",
            "mse_b  [4874.8433]  mse_f: [1572.7651]   total loss: [6447.6084]\n",
            "It: 38080, Time: 0.81\n",
            "mse_b  [4002.529]  mse_f: [1730.1838]   total loss: [5732.713]\n",
            "It: 38120, Time: 0.85\n",
            "mse_b  [3900.8457]  mse_f: [1224.2812]   total loss: [5125.127]\n",
            "It: 38160, Time: 0.82\n",
            "mse_b  [3818.3213]  mse_f: [925.4958]   total loss: [4743.817]\n",
            "It: 38200, Time: 0.83\n",
            "mse_b  [3616.7139]  mse_f: [1193.3158]   total loss: [4810.03]\n",
            "It: 38240, Time: 0.81\n",
            "mse_b  [3430.011]  mse_f: [2996.4565]   total loss: [6426.468]\n",
            "It: 38280, Time: 0.80\n",
            "mse_b  [3409.432]  mse_f: [1116.5371]   total loss: [4525.9688]\n",
            "It: 38320, Time: 0.82\n",
            "mse_b  [3391.3015]  mse_f: [748.13684]   total loss: [4139.4385]\n",
            "It: 38360, Time: 0.83\n",
            "mse_b  [3290.954]  mse_f: [485.5255]   total loss: [3776.4795]\n",
            "It: 38400, Time: 0.82\n",
            "mse_b  [3194.1523]  mse_f: [494.22632]   total loss: [3688.3787]\n",
            "It: 38440, Time: 0.81\n",
            "mse_b  [3170.6682]  mse_f: [526.9329]   total loss: [3697.601]\n",
            "It: 38480, Time: 0.80\n",
            "mse_b  [3173.2524]  mse_f: [598.44867]   total loss: [3771.7012]\n",
            "It: 38520, Time: 0.82\n",
            "mse_b  [3156.335]  mse_f: [489.26428]   total loss: [3645.599]\n",
            "It: 38560, Time: 0.82\n",
            "mse_b  [3369.2146]  mse_f: [1415.4508]   total loss: [4784.6655]\n",
            "It: 38600, Time: 0.82\n",
            "mse_b  [3295.3252]  mse_f: [779.7649]   total loss: [4075.09]\n",
            "It: 38640, Time: 0.81\n",
            "mse_b  [3251.6885]  mse_f: [1167.8702]   total loss: [4419.5586]\n",
            "It: 38680, Time: 0.80\n",
            "mse_b  [3208.2036]  mse_f: [960.4916]   total loss: [4168.6953]\n",
            "It: 38720, Time: 0.80\n",
            "mse_b  [3099.001]  mse_f: [687.9468]   total loss: [3786.9478]\n",
            "It: 38760, Time: 0.80\n",
            "mse_b  [3279.9148]  mse_f: [1382.5803]   total loss: [4662.495]\n",
            "It: 38800, Time: 0.84\n",
            "mse_b  [3149.9653]  mse_f: [717.78723]   total loss: [3867.7524]\n",
            "It: 38840, Time: 0.82\n",
            "mse_b  [3334.858]  mse_f: [1261.9231]   total loss: [4596.7812]\n",
            "It: 38880, Time: 0.82\n",
            "mse_b  [3440.0583]  mse_f: [1067.9327]   total loss: [4507.991]\n",
            "It: 38920, Time: 0.81\n",
            "mse_b  [3298.5935]  mse_f: [892.88477]   total loss: [4191.4785]\n",
            "It: 38960, Time: 0.83\n",
            "mse_b  [3198.938]  mse_f: [936.5125]   total loss: [4135.4507]\n",
            "It: 39000, Time: 0.84\n",
            "mse_b  [4482.864]  mse_f: [1604.0652]   total loss: [6086.9287]\n",
            "It: 39040, Time: 0.82\n",
            "mse_b  [3673.1382]  mse_f: [2175.0762]   total loss: [5848.2144]\n",
            "It: 39080, Time: 0.83\n",
            "mse_b  [3308.506]  mse_f: [1342.4563]   total loss: [4650.9624]\n",
            "It: 39120, Time: 0.81\n",
            "mse_b  [3345.1992]  mse_f: [1121.7505]   total loss: [4466.9497]\n",
            "It: 39160, Time: 0.81\n",
            "mse_b  [3164.7114]  mse_f: [994.4649]   total loss: [4159.1763]\n",
            "It: 39200, Time: 0.80\n",
            "mse_b  [3096.7163]  mse_f: [568.78235]   total loss: [3665.4985]\n",
            "It: 39240, Time: 0.80\n",
            "mse_b  [3141.8748]  mse_f: [1010.8879]   total loss: [4152.7627]\n",
            "It: 39280, Time: 0.84\n",
            "mse_b  [3060.1978]  mse_f: [690.8356]   total loss: [3751.0332]\n",
            "It: 39320, Time: 0.80\n",
            "mse_b  [3160.1526]  mse_f: [740.47327]   total loss: [3900.626]\n",
            "It: 39360, Time: 0.81\n",
            "mse_b  [3001.2039]  mse_f: [557.87476]   total loss: [3559.0786]\n",
            "It: 39400, Time: 0.82\n",
            "mse_b  [3023.5525]  mse_f: [591.5729]   total loss: [3615.1255]\n",
            "It: 39440, Time: 0.82\n",
            "mse_b  [3021.081]  mse_f: [811.5096]   total loss: [3832.5906]\n",
            "It: 39480, Time: 0.85\n",
            "mse_b  [3080.1406]  mse_f: [945.29095]   total loss: [4025.4316]\n",
            "It: 39520, Time: 0.83\n",
            "mse_b  [3087.338]  mse_f: [846.1909]   total loss: [3933.5288]\n",
            "It: 39560, Time: 0.82\n",
            "mse_b  [2961.4243]  mse_f: [720.2938]   total loss: [3681.7183]\n",
            "It: 39600, Time: 0.84\n",
            "mse_b  [3168.687]  mse_f: [1658.0835]   total loss: [4826.7705]\n",
            "It: 39640, Time: 0.86\n",
            "mse_b  [3157.4612]  mse_f: [855.7869]   total loss: [4013.248]\n",
            "It: 39680, Time: 0.85\n",
            "mse_b  [2991.6382]  mse_f: [995.5309]   total loss: [3987.169]\n",
            "It: 39720, Time: 0.84\n",
            "mse_b  [2954.155]  mse_f: [566.6386]   total loss: [3520.7937]\n",
            "It: 39760, Time: 0.81\n",
            "mse_b  [3517.0828]  mse_f: [1522.9558]   total loss: [5040.0386]\n",
            "It: 39800, Time: 0.82\n",
            "mse_b  [3248.443]  mse_f: [1341.675]   total loss: [4590.118]\n",
            "It: 39840, Time: 0.81\n",
            "mse_b  [3427.2598]  mse_f: [1836.405]   total loss: [5263.665]\n",
            "It: 39880, Time: 0.80\n",
            "mse_b  [3570.5376]  mse_f: [1627.9714]   total loss: [5198.509]\n",
            "It: 39920, Time: 0.83\n",
            "mse_b  [3311.4114]  mse_f: [1095.77]   total loss: [4407.1816]\n",
            "It: 39960, Time: 0.82\n",
            "mse_b  [3172.6191]  mse_f: [863.8596]   total loss: [4036.4788]\n",
            "It: 40000, Time: 0.81\n",
            "mse_b  [3286.4827]  mse_f: [2401.406]   total loss: [5687.8887]\n",
            "It: 40040, Time: 0.81\n",
            "mse_b  [3426.6367]  mse_f: [1587.4496]   total loss: [5014.0864]\n",
            "It: 40080, Time: 0.80\n",
            "mse_b  [3183.802]  mse_f: [791.31494]   total loss: [3975.117]\n",
            "It: 40120, Time: 0.83\n",
            "mse_b  [3143.7483]  mse_f: [722.4032]   total loss: [3866.1514]\n",
            "It: 40160, Time: 0.81\n",
            "mse_b  [2979.4978]  mse_f: [611.8954]   total loss: [3591.393]\n",
            "It: 40200, Time: 0.79\n",
            "mse_b  [2961.2935]  mse_f: [738.6931]   total loss: [3699.9866]\n",
            "It: 40240, Time: 0.85\n",
            "mse_b  [2907.4197]  mse_f: [639.249]   total loss: [3546.6687]\n",
            "It: 40280, Time: 0.82\n",
            "mse_b  [2916.6746]  mse_f: [525.14343]   total loss: [3441.8179]\n",
            "It: 40320, Time: 0.82\n",
            "mse_b  [2991.7595]  mse_f: [608.1255]   total loss: [3599.885]\n",
            "It: 40360, Time: 0.81\n",
            "mse_b  [3197.8936]  mse_f: [888.6337]   total loss: [4086.5273]\n",
            "It: 40400, Time: 0.83\n",
            "mse_b  [3026.6885]  mse_f: [779.0874]   total loss: [3805.776]\n",
            "It: 40440, Time: 0.81\n",
            "mse_b  [2904.4844]  mse_f: [1787.7579]   total loss: [4692.242]\n",
            "It: 40480, Time: 0.83\n",
            "mse_b  [3086.0515]  mse_f: [1684.4857]   total loss: [4770.537]\n",
            "It: 40520, Time: 0.84\n",
            "mse_b  [3250.6233]  mse_f: [1638.1438]   total loss: [4888.767]\n",
            "It: 40560, Time: 0.81\n",
            "mse_b  [3530.7769]  mse_f: [1474.3008]   total loss: [5005.0776]\n",
            "It: 40600, Time: 0.80\n",
            "mse_b  [3346.9155]  mse_f: [1049.2223]   total loss: [4396.1377]\n",
            "It: 40640, Time: 0.81\n",
            "mse_b  [3133.9607]  mse_f: [1244.1329]   total loss: [4378.0938]\n",
            "It: 40680, Time: 0.82\n",
            "mse_b  [3062.8406]  mse_f: [767.6892]   total loss: [3830.5298]\n",
            "It: 40720, Time: 0.84\n",
            "mse_b  [2972.8242]  mse_f: [690.7174]   total loss: [3663.5415]\n",
            "It: 40760, Time: 0.83\n",
            "mse_b  [3014.4824]  mse_f: [880.25385]   total loss: [3894.7363]\n",
            "It: 40800, Time: 0.82\n",
            "mse_b  [2910.9377]  mse_f: [731.72626]   total loss: [3642.664]\n",
            "It: 40840, Time: 0.81\n",
            "mse_b  [3277.735]  mse_f: [1770.2505]   total loss: [5047.9854]\n",
            "It: 40880, Time: 0.81\n",
            "mse_b  [3260.8936]  mse_f: [1730.6848]   total loss: [4991.578]\n",
            "It: 40920, Time: 0.82\n",
            "mse_b  [3375.007]  mse_f: [1251.5308]   total loss: [4626.538]\n",
            "It: 40960, Time: 0.82\n",
            "mse_b  [3188.872]  mse_f: [1229.6223]   total loss: [4418.494]\n",
            "It: 41000, Time: 0.81\n",
            "mse_b  [3032.7612]  mse_f: [708.4779]   total loss: [3741.2393]\n",
            "It: 41040, Time: 0.82\n",
            "mse_b  [2950.358]  mse_f: [758.4287]   total loss: [3708.7866]\n",
            "It: 41080, Time: 0.89\n",
            "mse_b  [2922.4792]  mse_f: [1182.654]   total loss: [4105.1333]\n",
            "It: 41120, Time: 0.83\n",
            "mse_b  [3014.0852]  mse_f: [1034.3777]   total loss: [4048.463]\n",
            "It: 41160, Time: 0.82\n",
            "mse_b  [3059.2917]  mse_f: [1131.5161]   total loss: [4190.8076]\n",
            "It: 41200, Time: 0.82\n",
            "mse_b  [2892.77]  mse_f: [902.7682]   total loss: [3795.538]\n",
            "It: 41240, Time: 0.82\n",
            "mse_b  [2919.8494]  mse_f: [602.84875]   total loss: [3522.6982]\n",
            "It: 41280, Time: 0.84\n",
            "mse_b  [3146.8206]  mse_f: [1011.28845]   total loss: [4158.109]\n",
            "It: 41320, Time: 0.83\n",
            "mse_b  [3092.7334]  mse_f: [1036.1494]   total loss: [4128.883]\n",
            "It: 41360, Time: 0.84\n",
            "mse_b  [3122.0522]  mse_f: [821.52454]   total loss: [3943.5767]\n",
            "It: 41400, Time: 0.82\n",
            "mse_b  [2955.699]  mse_f: [865.6441]   total loss: [3821.343]\n",
            "It: 41440, Time: 0.83\n",
            "mse_b  [2845.1296]  mse_f: [955.36005]   total loss: [3800.4897]\n",
            "It: 41480, Time: 0.85\n",
            "mse_b  [2845.0566]  mse_f: [578.8162]   total loss: [3423.8728]\n",
            "It: 41520, Time: 0.81\n",
            "mse_b  [2757.5835]  mse_f: [979.1176]   total loss: [3736.7012]\n",
            "It: 41560, Time: 0.84\n",
            "mse_b  [4255.123]  mse_f: [1973.033]   total loss: [6228.1562]\n",
            "It: 41600, Time: 0.84\n",
            "mse_b  [3485.106]  mse_f: [1366.8402]   total loss: [4851.9463]\n",
            "It: 41640, Time: 0.82\n",
            "mse_b  [3068.8984]  mse_f: [1375.6787]   total loss: [4444.577]\n",
            "It: 41680, Time: 0.85\n",
            "mse_b  [3320.4102]  mse_f: [1296.4178]   total loss: [4616.828]\n",
            "It: 41720, Time: 0.84\n",
            "mse_b  [3258.3142]  mse_f: [1255.6548]   total loss: [4513.9688]\n",
            "It: 41760, Time: 0.84\n",
            "mse_b  [3117.879]  mse_f: [976.0322]   total loss: [4093.9111]\n",
            "It: 41800, Time: 0.84\n",
            "mse_b  [2956.0315]  mse_f: [670.10956]   total loss: [3626.141]\n",
            "It: 41840, Time: 0.85\n",
            "mse_b  [3101.2358]  mse_f: [1164.5726]   total loss: [4265.8086]\n",
            "It: 41880, Time: 0.88\n",
            "mse_b  [2947.914]  mse_f: [844.9172]   total loss: [3792.8313]\n",
            "It: 41920, Time: 0.84\n",
            "mse_b  [2847.4001]  mse_f: [664.19385]   total loss: [3511.594]\n",
            "It: 41960, Time: 0.84\n",
            "mse_b  [2828.506]  mse_f: [725.8345]   total loss: [3554.3406]\n",
            "It: 42000, Time: 0.82\n",
            "mse_b  [2825.1472]  mse_f: [516.8593]   total loss: [3342.0066]\n",
            "It: 42040, Time: 0.83\n",
            "mse_b  [2871.1753]  mse_f: [924.4025]   total loss: [3795.578]\n",
            "It: 42080, Time: 0.84\n",
            "mse_b  [2908.6965]  mse_f: [797.7478]   total loss: [3706.4443]\n",
            "It: 42120, Time: 0.82\n",
            "mse_b  [2781.312]  mse_f: [900.9456]   total loss: [3682.2576]\n",
            "It: 42160, Time: 0.83\n",
            "mse_b  [2785.8025]  mse_f: [572.7731]   total loss: [3358.5757]\n",
            "It: 42200, Time: 0.82\n",
            "mse_b  [2932.7598]  mse_f: [960.8061]   total loss: [3893.566]\n",
            "It: 42240, Time: 0.83\n",
            "mse_b  [3286.0842]  mse_f: [1685.5051]   total loss: [4971.5894]\n",
            "It: 42280, Time: 0.83\n",
            "mse_b  [3146.9985]  mse_f: [1429.9829]   total loss: [4576.9814]\n",
            "It: 42320, Time: 0.84\n",
            "mse_b  [6130.551]  mse_f: [3450.515]   total loss: [9581.065]\n",
            "It: 42360, Time: 0.82\n",
            "mse_b  [5280.594]  mse_f: [1893.3024]   total loss: [7173.8965]\n",
            "It: 42400, Time: 0.83\n",
            "mse_b  [4605.0957]  mse_f: [1840.7771]   total loss: [6445.873]\n",
            "It: 42440, Time: 0.80\n",
            "mse_b  [3963.7449]  mse_f: [1394.9612]   total loss: [5358.706]\n",
            "It: 42480, Time: 0.81\n",
            "mse_b  [3565.6414]  mse_f: [1381.4829]   total loss: [4947.124]\n",
            "It: 42520, Time: 0.86\n",
            "mse_b  [3349.175]  mse_f: [835.5847]   total loss: [4184.76]\n",
            "It: 42560, Time: 0.88\n",
            "mse_b  [3300.5994]  mse_f: [731.4053]   total loss: [4032.0046]\n",
            "It: 42600, Time: 0.86\n",
            "mse_b  [3280.3904]  mse_f: [2001.673]   total loss: [5282.0635]\n",
            "It: 42640, Time: 0.82\n",
            "mse_b  [3301.277]  mse_f: [1777.6173]   total loss: [5078.8945]\n",
            "It: 42680, Time: 0.83\n",
            "mse_b  [3400.8591]  mse_f: [1758.7207]   total loss: [5159.58]\n",
            "It: 42720, Time: 0.82\n",
            "mse_b  [3276.3079]  mse_f: [1192.045]   total loss: [4468.353]\n",
            "It: 42760, Time: 0.82\n",
            "mse_b  [3060.6208]  mse_f: [899.20276]   total loss: [3959.8237]\n",
            "It: 42800, Time: 0.83\n",
            "mse_b  [2935.096]  mse_f: [663.0532]   total loss: [3598.1492]\n",
            "It: 42840, Time: 0.81\n",
            "mse_b  [2897.4114]  mse_f: [721.5864]   total loss: [3618.9978]\n",
            "It: 42880, Time: 0.84\n",
            "mse_b  [2853.2964]  mse_f: [422.52203]   total loss: [3275.8184]\n",
            "It: 42920, Time: 0.82\n",
            "mse_b  [2815.054]  mse_f: [465.81052]   total loss: [3280.8645]\n",
            "It: 42960, Time: 0.81\n",
            "mse_b  [2742.4207]  mse_f: [601.60785]   total loss: [3344.0286]\n",
            "It: 43000, Time: 0.82\n",
            "mse_b  [2897.9875]  mse_f: [1243.7936]   total loss: [4141.7812]\n",
            "It: 43040, Time: 0.86\n",
            "mse_b  [2930.428]  mse_f: [958.83026]   total loss: [3889.2583]\n",
            "It: 43080, Time: 0.84\n",
            "mse_b  [2895.9253]  mse_f: [875.2715]   total loss: [3771.1968]\n",
            "It: 43120, Time: 0.83\n",
            "mse_b  [2827.6338]  mse_f: [605.99097]   total loss: [3433.6248]\n",
            "It: 43160, Time: 0.83\n",
            "mse_b  [2792.2124]  mse_f: [919.8934]   total loss: [3712.1057]\n",
            "It: 43200, Time: 0.82\n",
            "mse_b  [2761.9084]  mse_f: [536.2237]   total loss: [3298.132]\n",
            "It: 43240, Time: 0.84\n",
            "mse_b  [2783.2441]  mse_f: [687.5592]   total loss: [3470.8032]\n",
            "It: 43280, Time: 0.85\n",
            "mse_b  [2741.9807]  mse_f: [643.87213]   total loss: [3385.8528]\n",
            "It: 43320, Time: 0.82\n",
            "mse_b  [2923.1758]  mse_f: [991.75793]   total loss: [3914.9336]\n",
            "It: 43360, Time: 0.84\n",
            "mse_b  [3298.7847]  mse_f: [2092.2612]   total loss: [5391.046]\n",
            "It: 43400, Time: 0.84\n",
            "mse_b  [5087.0464]  mse_f: [2818.5957]   total loss: [7905.642]\n",
            "It: 43440, Time: 0.83\n",
            "mse_b  [4515.6567]  mse_f: [2199.9062]   total loss: [6715.563]\n",
            "It: 43480, Time: 0.83\n",
            "mse_b  [4007.5693]  mse_f: [1468.595]   total loss: [5476.164]\n",
            "It: 43520, Time: 0.84\n",
            "mse_b  [3675.7563]  mse_f: [1344.8044]   total loss: [5020.5605]\n",
            "It: 43560, Time: 0.82\n",
            "mse_b  [3497.5625]  mse_f: [1028.4362]   total loss: [4525.9985]\n",
            "It: 43600, Time: 0.84\n",
            "mse_b  [3265.475]  mse_f: [759.3231]   total loss: [4024.7983]\n",
            "It: 43640, Time: 0.83\n",
            "mse_b  [3096.9294]  mse_f: [611.89197]   total loss: [3708.8213]\n",
            "It: 43680, Time: 0.82\n",
            "mse_b  [3018.212]  mse_f: [7947.7837]   total loss: [10965.996]\n",
            "It: 43720, Time: 0.82\n",
            "mse_b  [3485.9895]  mse_f: [2391.871]   total loss: [5877.8604]\n",
            "It: 43760, Time: 0.84\n",
            "mse_b  [3701.202]  mse_f: [1422.7593]   total loss: [5123.961]\n",
            "It: 43800, Time: 0.84\n",
            "mse_b  [3195.9788]  mse_f: [1019.9491]   total loss: [4215.9277]\n",
            "It: 43840, Time: 0.83\n",
            "mse_b  [3162.3015]  mse_f: [850.58044]   total loss: [4012.8818]\n",
            "It: 43880, Time: 0.81\n",
            "mse_b  [2914.2307]  mse_f: [500.3636]   total loss: [3414.5942]\n",
            "It: 43920, Time: 0.81\n",
            "mse_b  [2836.529]  mse_f: [513.3054]   total loss: [3349.8345]\n",
            "It: 43960, Time: 0.86\n",
            "mse_b  [2759.6523]  mse_f: [535.4272]   total loss: [3295.0796]\n",
            "It: 44000, Time: 0.87\n",
            "mse_b  [2718.8247]  mse_f: [419.4636]   total loss: [3138.2883]\n",
            "It: 44040, Time: 0.83\n",
            "mse_b  [2719.557]  mse_f: [354.6712]   total loss: [3074.228]\n",
            "It: 44080, Time: 0.82\n",
            "mse_b  [2837.1592]  mse_f: [483.19043]   total loss: [3320.3496]\n",
            "It: 44120, Time: 0.82\n",
            "mse_b  [2945.2944]  mse_f: [1003.3884]   total loss: [3948.6829]\n",
            "It: 44160, Time: 0.82\n",
            "mse_b  [2705.8386]  mse_f: [618.11005]   total loss: [3323.9487]\n",
            "It: 44200, Time: 0.83\n",
            "mse_b  [2816.1436]  mse_f: [727.42395]   total loss: [3543.5674]\n",
            "It: 44240, Time: 0.82\n",
            "mse_b  [3121.095]  mse_f: [2021.0251]   total loss: [5142.12]\n",
            "It: 44280, Time: 0.85\n",
            "mse_b  [3301.14]  mse_f: [1258.2329]   total loss: [4559.373]\n",
            "It: 44320, Time: 0.84\n",
            "mse_b  [3006.1118]  mse_f: [848.6034]   total loss: [3854.7153]\n",
            "It: 44360, Time: 0.83\n",
            "mse_b  [2966.195]  mse_f: [963.3313]   total loss: [3929.5264]\n",
            "It: 44400, Time: 0.83\n",
            "mse_b  [2832.0398]  mse_f: [881.8322]   total loss: [3713.872]\n",
            "It: 44440, Time: 0.83\n",
            "mse_b  [2688.6636]  mse_f: [823.58093]   total loss: [3512.2446]\n",
            "It: 44480, Time: 0.83\n",
            "mse_b  [2791.2275]  mse_f: [707.85986]   total loss: [3499.0874]\n",
            "It: 44520, Time: 0.85\n",
            "mse_b  [2976.4224]  mse_f: [709.2882]   total loss: [3685.7104]\n",
            "It: 44560, Time: 0.83\n",
            "mse_b  [2712.657]  mse_f: [751.42664]   total loss: [3464.0835]\n",
            "It: 44600, Time: 0.82\n",
            "mse_b  [3294.3774]  mse_f: [1137.0077]   total loss: [4431.3853]\n",
            "It: 44640, Time: 0.83\n",
            "mse_b  [3017.2158]  mse_f: [989.1886]   total loss: [4006.4043]\n",
            "It: 44680, Time: 0.84\n",
            "mse_b  [3249.8494]  mse_f: [2564.1152]   total loss: [5813.965]\n",
            "It: 44720, Time: 0.83\n",
            "mse_b  [3186.73]  mse_f: [1440.6702]   total loss: [4627.4004]\n",
            "It: 44760, Time: 0.84\n",
            "mse_b  [2994.1584]  mse_f: [1059.3342]   total loss: [4053.4927]\n",
            "It: 44800, Time: 0.83\n",
            "mse_b  [2924.666]  mse_f: [761.0962]   total loss: [3685.7622]\n",
            "It: 44840, Time: 0.81\n",
            "mse_b  [2760.5]  mse_f: [1449.9003]   total loss: [4210.4004]\n",
            "It: 44880, Time: 0.83\n",
            "mse_b  [3036.8948]  mse_f: [1430.2338]   total loss: [4467.1284]\n",
            "It: 44920, Time: 0.92\n",
            "mse_b  [2873.28]  mse_f: [1051.7405]   total loss: [3925.0205]\n",
            "It: 44960, Time: 0.81\n",
            "mse_b  [2799.1072]  mse_f: [753.17615]   total loss: [3552.2832]\n",
            "It: 45000, Time: 0.81\n",
            "mse_b  [2708.602]  mse_f: [737.8474]   total loss: [3446.4495]\n",
            "It: 45040, Time: 0.82\n",
            "mse_b  [2607.7651]  mse_f: [673.39465]   total loss: [3281.1597]\n",
            "It: 45080, Time: 0.82\n",
            "mse_b  [2649.9358]  mse_f: [552.4015]   total loss: [3202.3374]\n",
            "It: 45120, Time: 0.80\n",
            "mse_b  [2630.8489]  mse_f: [503.33917]   total loss: [3134.188]\n",
            "It: 45160, Time: 0.82\n",
            "mse_b  [2577.1394]  mse_f: [587.35583]   total loss: [3164.495]\n",
            "It: 45200, Time: 0.82\n",
            "mse_b  [2557.3857]  mse_f: [556.97754]   total loss: [3114.3633]\n",
            "It: 45240, Time: 0.84\n",
            "mse_b  [3792.9292]  mse_f: [2363.108]   total loss: [6156.037]\n",
            "It: 45280, Time: 0.83\n",
            "mse_b  [3218.4207]  mse_f: [1166.7161]   total loss: [4385.1367]\n",
            "It: 45320, Time: 0.84\n",
            "mse_b  [2991.9783]  mse_f: [1228.1028]   total loss: [4220.081]\n",
            "It: 45360, Time: 0.81\n",
            "mse_b  [2772.6882]  mse_f: [1293.2905]   total loss: [4065.9788]\n",
            "It: 45400, Time: 0.84\n",
            "mse_b  [2671.3962]  mse_f: [1333.0051]   total loss: [4004.4014]\n",
            "It: 45440, Time: 0.89\n",
            "mse_b  [2705.443]  mse_f: [773.2121]   total loss: [3478.6553]\n",
            "It: 45480, Time: 0.83\n",
            "mse_b  [2652.698]  mse_f: [972.50586]   total loss: [3625.2039]\n",
            "It: 45520, Time: 0.82\n",
            "mse_b  [2752.8652]  mse_f: [671.99005]   total loss: [3424.8552]\n",
            "It: 45560, Time: 0.85\n",
            "mse_b  [2723.6013]  mse_f: [748.63556]   total loss: [3472.2368]\n",
            "It: 45600, Time: 0.82\n",
            "mse_b  [3355.8215]  mse_f: [1400.4136]   total loss: [4756.2354]\n",
            "It: 45640, Time: 0.83\n",
            "mse_b  [3400.1094]  mse_f: [1246.4697]   total loss: [4646.579]\n",
            "It: 45680, Time: 0.85\n",
            "mse_b  [2867.2856]  mse_f: [2386.4954]   total loss: [5253.7812]\n",
            "It: 45720, Time: 0.83\n",
            "mse_b  [3270.3123]  mse_f: [1835.7761]   total loss: [5106.0884]\n",
            "It: 45760, Time: 0.83\n",
            "mse_b  [2887.3984]  mse_f: [1113.2563]   total loss: [4000.6548]\n",
            "It: 45800, Time: 0.84\n",
            "mse_b  [2871.7932]  mse_f: [1153.2284]   total loss: [4025.0215]\n",
            "It: 45840, Time: 0.82\n",
            "mse_b  [2760.403]  mse_f: [766.4467]   total loss: [3526.8499]\n",
            "It: 45880, Time: 0.89\n",
            "mse_b  [3209.3936]  mse_f: [2054.4075]   total loss: [5263.801]\n",
            "It: 45920, Time: 0.81\n",
            "mse_b  [3118.617]  mse_f: [1402.5208]   total loss: [4521.1377]\n",
            "It: 45960, Time: 0.82\n",
            "mse_b  [2894.4763]  mse_f: [1511.2843]   total loss: [4405.7607]\n",
            "It: 46000, Time: 0.84\n",
            "mse_b  [2821.7747]  mse_f: [854.82794]   total loss: [3676.6025]\n",
            "It: 46040, Time: 0.83\n",
            "mse_b  [3129.9004]  mse_f: [1790.2085]   total loss: [4920.109]\n",
            "It: 46080, Time: 0.81\n",
            "mse_b  [3353.0767]  mse_f: [1645.8271]   total loss: [4998.904]\n",
            "It: 46120, Time: 0.82\n",
            "mse_b  [2963.3547]  mse_f: [872.8431]   total loss: [3836.1978]\n",
            "It: 46160, Time: 0.83\n",
            "mse_b  [2820.0293]  mse_f: [908.4866]   total loss: [3728.5159]\n",
            "It: 46200, Time: 0.85\n",
            "mse_b  [2761.3667]  mse_f: [889.2572]   total loss: [3650.624]\n",
            "It: 46240, Time: 0.85\n",
            "mse_b  [2769.6348]  mse_f: [1047.5814]   total loss: [3817.2163]\n",
            "It: 46280, Time: 0.83\n",
            "mse_b  [2707.0002]  mse_f: [770.54865]   total loss: [3477.5488]\n",
            "It: 46320, Time: 0.82\n",
            "mse_b  [2607.6323]  mse_f: [710.81934]   total loss: [3318.4517]\n",
            "It: 46360, Time: 0.83\n",
            "mse_b  [2563.343]  mse_f: [572.56757]   total loss: [3135.9106]\n",
            "It: 46400, Time: 0.81\n",
            "mse_b  [2693.5986]  mse_f: [1000.6824]   total loss: [3694.281]\n",
            "It: 46440, Time: 0.84\n",
            "mse_b  [2710.7878]  mse_f: [1672.3333]   total loss: [4383.121]\n",
            "It: 46480, Time: 0.82\n",
            "mse_b  [2613.8496]  mse_f: [800.5933]   total loss: [3414.4429]\n",
            "It: 46520, Time: 0.83\n",
            "mse_b  [2581.035]  mse_f: [506.46832]   total loss: [3087.5032]\n",
            "It: 46560, Time: 0.82\n",
            "mse_b  [2535.3674]  mse_f: [395.898]   total loss: [2931.2654]\n",
            "It: 46600, Time: 0.85\n",
            "mse_b  [2592.457]  mse_f: [727.7274]   total loss: [3320.1846]\n",
            "It: 46640, Time: 0.83\n",
            "mse_b  [3081.9507]  mse_f: [1608.279]   total loss: [4690.2295]\n",
            "It: 46680, Time: 0.86\n",
            "mse_b  [3090.8274]  mse_f: [1652.04]   total loss: [4742.867]\n",
            "It: 46720, Time: 0.83\n",
            "mse_b  [2756.0425]  mse_f: [1370.1226]   total loss: [4126.165]\n",
            "It: 46760, Time: 0.80\n",
            "mse_b  [2733.9082]  mse_f: [855.2615]   total loss: [3589.1697]\n",
            "It: 46800, Time: 0.82\n",
            "mse_b  [2575.5286]  mse_f: [830.59485]   total loss: [3406.1235]\n",
            "It: 46840, Time: 0.81\n",
            "mse_b  [2939.5503]  mse_f: [1110.0647]   total loss: [4049.615]\n",
            "It: 46880, Time: 1.41\n",
            "mse_b  [2784.5278]  mse_f: [1208.2014]   total loss: [3992.7292]\n",
            "It: 46920, Time: 0.82\n",
            "mse_b  [2741.2197]  mse_f: [973.7744]   total loss: [3714.9941]\n",
            "It: 46960, Time: 0.83\n",
            "mse_b  [2806.276]  mse_f: [920.90216]   total loss: [3727.178]\n",
            "It: 47000, Time: 0.83\n",
            "mse_b  [2727.5652]  mse_f: [1179.265]   total loss: [3906.83]\n",
            "It: 47040, Time: 0.83\n",
            "mse_b  [2826.5286]  mse_f: [1017.6723]   total loss: [3844.201]\n",
            "It: 47080, Time: 0.83\n",
            "mse_b  [2645.898]  mse_f: [632.0459]   total loss: [3277.9438]\n",
            "It: 47120, Time: 0.83\n",
            "mse_b  [2556.6282]  mse_f: [662.37646]   total loss: [3219.0046]\n",
            "It: 47160, Time: 0.83\n",
            "mse_b  [2525.8357]  mse_f: [472.9282]   total loss: [2998.764]\n",
            "It: 47200, Time: 0.81\n",
            "mse_b  [2691.7903]  mse_f: [1257.6267]   total loss: [3949.417]\n",
            "It: 47240, Time: 0.82\n",
            "mse_b  [2664.0686]  mse_f: [1219.6304]   total loss: [3883.699]\n",
            "It: 47280, Time: 0.83\n",
            "mse_b  [2620.3442]  mse_f: [1047.4214]   total loss: [3667.7656]\n",
            "It: 47320, Time: 0.83\n",
            "mse_b  [2555.5889]  mse_f: [896.59045]   total loss: [3452.1792]\n",
            "It: 47360, Time: 0.84\n",
            "mse_b  [2917.202]  mse_f: [1602.8025]   total loss: [4520.0044]\n",
            "It: 47400, Time: 0.82\n",
            "mse_b  [3058.2952]  mse_f: [1414.7769]   total loss: [4473.0723]\n",
            "It: 47440, Time: 0.81\n",
            "mse_b  [2808.5117]  mse_f: [1163.4338]   total loss: [3971.9456]\n",
            "It: 47480, Time: 0.81\n",
            "mse_b  [2795.7488]  mse_f: [882.61633]   total loss: [3678.3652]\n",
            "It: 47520, Time: 0.81\n",
            "mse_b  [2536.0886]  mse_f: [917.775]   total loss: [3453.8638]\n",
            "It: 47560, Time: 0.84\n",
            "mse_b  [2641.531]  mse_f: [687.8139]   total loss: [3329.345]\n",
            "It: 47600, Time: 0.84\n",
            "mse_b  [2693.73]  mse_f: [1585.8364]   total loss: [4279.5664]\n",
            "It: 47640, Time: 0.84\n",
            "mse_b  [2637.969]  mse_f: [620.3873]   total loss: [3258.3564]\n",
            "It: 47680, Time: 0.83\n",
            "mse_b  [2481.5874]  mse_f: [1218.208]   total loss: [3699.7954]\n",
            "It: 47720, Time: 0.84\n",
            "mse_b  [2616.1384]  mse_f: [1030.4966]   total loss: [3646.635]\n",
            "It: 47760, Time: 0.82\n",
            "mse_b  [2539.4202]  mse_f: [857.7812]   total loss: [3397.2014]\n",
            "It: 47800, Time: 0.82\n",
            "mse_b  [2503.8887]  mse_f: [535.8359]   total loss: [3039.7246]\n",
            "It: 47840, Time: 0.83\n",
            "mse_b  [2605.2656]  mse_f: [633.09216]   total loss: [3238.358]\n",
            "It: 47880, Time: 0.86\n",
            "mse_b  [2554.4678]  mse_f: [1227.5276]   total loss: [3781.9954]\n",
            "It: 47920, Time: 0.86\n",
            "mse_b  [2624.7473]  mse_f: [1445.2798]   total loss: [4070.027]\n",
            "It: 47960, Time: 0.81\n",
            "mse_b  [2650.8516]  mse_f: [736.76276]   total loss: [3387.6143]\n",
            "It: 48000, Time: 0.86\n",
            "mse_b  [2490.5789]  mse_f: [643.19666]   total loss: [3133.7754]\n",
            "It: 48040, Time: 0.85\n",
            "mse_b  [2515.0576]  mse_f: [712.90247]   total loss: [3227.96]\n",
            "It: 48080, Time: 0.82\n",
            "mse_b  [2444.9895]  mse_f: [681.16174]   total loss: [3126.1514]\n",
            "It: 48120, Time: 0.82\n",
            "mse_b  [2428.3147]  mse_f: [621.6429]   total loss: [3049.9575]\n",
            "It: 48160, Time: 0.81\n",
            "mse_b  [2487.4333]  mse_f: [673.3575]   total loss: [3160.7908]\n",
            "It: 48200, Time: 0.81\n",
            "mse_b  [2532.3]  mse_f: [2115.42]   total loss: [4647.7197]\n",
            "It: 48240, Time: 0.84\n",
            "mse_b  [2919.978]  mse_f: [1302.8201]   total loss: [4222.798]\n",
            "It: 48280, Time: 0.87\n",
            "mse_b  [2908.5508]  mse_f: [1354.1262]   total loss: [4262.677]\n",
            "It: 48320, Time: 0.88\n",
            "mse_b  [2542.0042]  mse_f: [819.5687]   total loss: [3361.5728]\n",
            "It: 48360, Time: 0.87\n",
            "mse_b  [2477.2188]  mse_f: [854.2916]   total loss: [3331.5103]\n",
            "It: 48400, Time: 0.84\n",
            "mse_b  [2914.6658]  mse_f: [5553.878]   total loss: [8468.544]\n",
            "It: 48440, Time: 0.84\n",
            "mse_b  [3456.9258]  mse_f: [1905.9186]   total loss: [5362.844]\n",
            "It: 48480, Time: 0.83\n",
            "mse_b  [3322.648]  mse_f: [1297.6453]   total loss: [4620.293]\n",
            "It: 48520, Time: 0.84\n",
            "mse_b  [2988.2957]  mse_f: [1387.9294]   total loss: [4376.225]\n",
            "It: 48560, Time: 0.85\n",
            "mse_b  [2669.6353]  mse_f: [970.82275]   total loss: [3640.458]\n",
            "It: 48600, Time: 0.84\n",
            "mse_b  [2536.1577]  mse_f: [763.4821]   total loss: [3299.64]\n",
            "It: 48640, Time: 0.85\n",
            "mse_b  [2509.234]  mse_f: [2598.1172]   total loss: [5107.351]\n",
            "It: 48680, Time: 0.85\n",
            "mse_b  [2747.4045]  mse_f: [1246.8479]   total loss: [3994.2524]\n",
            "It: 48720, Time: 0.87\n",
            "mse_b  [2849.916]  mse_f: [732.91785]   total loss: [3582.834]\n",
            "It: 48760, Time: 0.88\n",
            "mse_b  [2654.558]  mse_f: [796.0742]   total loss: [3450.6323]\n",
            "It: 48800, Time: 0.85\n",
            "mse_b  [2548.5586]  mse_f: [550.3237]   total loss: [3098.8823]\n",
            "It: 48840, Time: 0.82\n",
            "mse_b  [2441.6445]  mse_f: [649.0127]   total loss: [3090.6572]\n",
            "It: 48880, Time: 0.83\n",
            "mse_b  [2397.7083]  mse_f: [429.245]   total loss: [2826.9531]\n",
            "It: 48920, Time: 0.82\n",
            "mse_b  [2380.5051]  mse_f: [391.31488]   total loss: [2771.82]\n",
            "It: 48960, Time: 0.83\n",
            "mse_b  [2396.5215]  mse_f: [701.6904]   total loss: [3098.212]\n",
            "It: 49000, Time: 0.83\n",
            "mse_b  [2389.2012]  mse_f: [783.41693]   total loss: [3172.6182]\n",
            "It: 49040, Time: 0.82\n",
            "mse_b  [2593.7239]  mse_f: [662.40247]   total loss: [3256.1265]\n",
            "It: 49080, Time: 0.84\n",
            "mse_b  [2648.2898]  mse_f: [740.5801]   total loss: [3388.8699]\n",
            "It: 49120, Time: 0.83\n",
            "mse_b  [2659.882]  mse_f: [877.9258]   total loss: [3537.8079]\n",
            "It: 49160, Time: 0.82\n",
            "mse_b  [2495.557]  mse_f: [708.4852]   total loss: [3204.042]\n",
            "It: 49200, Time: 0.83\n",
            "mse_b  [2407.3691]  mse_f: [604.56616]   total loss: [3011.9353]\n",
            "It: 49240, Time: 0.84\n",
            "mse_b  [2340.3586]  mse_f: [738.47974]   total loss: [3078.8384]\n",
            "It: 49280, Time: 0.83\n",
            "mse_b  [2430.02]  mse_f: [505.96085]   total loss: [2935.981]\n",
            "It: 49320, Time: 0.84\n",
            "mse_b  [2386.5964]  mse_f: [608.1915]   total loss: [2994.788]\n",
            "It: 49360, Time: 0.83\n",
            "mse_b  [2523.0383]  mse_f: [1132.0708]   total loss: [3655.1091]\n",
            "It: 49400, Time: 0.81\n",
            "mse_b  [2473.532]  mse_f: [681.75916]   total loss: [3155.291]\n",
            "It: 49440, Time: 0.83\n",
            "mse_b  [2466.3982]  mse_f: [730.1861]   total loss: [3196.5842]\n",
            "It: 49480, Time: 0.83\n",
            "mse_b  [2654.625]  mse_f: [656.48755]   total loss: [3311.1125]\n",
            "It: 49520, Time: 0.84\n",
            "mse_b  [2614.2322]  mse_f: [941.61554]   total loss: [3555.8477]\n",
            "It: 49560, Time: 0.82\n",
            "mse_b  [2659.956]  mse_f: [738.8132]   total loss: [3398.7693]\n",
            "It: 49600, Time: 0.84\n",
            "mse_b  [2526.8625]  mse_f: [541.5117]   total loss: [3068.3743]\n",
            "It: 49640, Time: 0.85\n",
            "mse_b  [2393.835]  mse_f: [473.5436]   total loss: [2867.3787]\n",
            "It: 49680, Time: 0.82\n",
            "mse_b  [2347.9763]  mse_f: [548.1949]   total loss: [2896.1711]\n",
            "It: 49720, Time: 0.91\n",
            "mse_b  [2305.3064]  mse_f: [466.1167]   total loss: [2771.423]\n",
            "It: 49760, Time: 0.85\n",
            "mse_b  [2352.6582]  mse_f: [813.2593]   total loss: [3165.9175]\n",
            "It: 49800, Time: 0.85\n",
            "mse_b  [2355.8025]  mse_f: [629.45703]   total loss: [2985.2595]\n",
            "It: 49840, Time: 0.83\n",
            "mse_b  [2362.2986]  mse_f: [779.27466]   total loss: [3141.5732]\n",
            "It: 49880, Time: 0.82\n",
            "mse_b  [2389.5066]  mse_f: [772.1473]   total loss: [3161.6538]\n",
            "It: 49920, Time: 0.83\n",
            "mse_b  [2562.9607]  mse_f: [2003.4202]   total loss: [4566.381]\n",
            "It: 49960, Time: 0.85\n",
            "mse_b  [3325.0005]  mse_f: [1365.6611]   total loss: [4690.6616]\n",
            "It: 50000, Time: 0.86\n",
            "mse_b  [2913.807]  mse_f: [1108.954]   total loss: [4022.7607]\n",
            "It: 50040, Time: 1.18\n",
            "mse_b  [2590.378]  mse_f: [939.86096]   total loss: [3530.2388]\n",
            "It: 50080, Time: 1.41\n",
            "mse_b  [2683.3242]  mse_f: [1040.1921]   total loss: [3723.5164]\n",
            "It: 50120, Time: 1.56\n",
            "mse_b  [2531.336]  mse_f: [1263.5188]   total loss: [3794.8547]\n",
            "It: 50160, Time: 0.91\n",
            "mse_b  [2429.2358]  mse_f: [973.73895]   total loss: [3402.9749]\n",
            "It: 50200, Time: 0.83\n",
            "mse_b  [2434.1997]  mse_f: [746.0988]   total loss: [3180.2986]\n",
            "It: 50240, Time: 0.84\n",
            "mse_b  [2399.0364]  mse_f: [578.5934]   total loss: [2977.63]\n",
            "It: 50280, Time: 0.83\n",
            "mse_b  [2341.7314]  mse_f: [671.81696]   total loss: [3013.5483]\n",
            "It: 50320, Time: 0.84\n",
            "mse_b  [2382.552]  mse_f: [600.8532]   total loss: [2983.4053]\n",
            "It: 50360, Time: 0.82\n",
            "mse_b  [2346.842]  mse_f: [463.5796]   total loss: [2810.4216]\n",
            "It: 50400, Time: 0.82\n",
            "mse_b  [2379.248]  mse_f: [957.3447]   total loss: [3336.5928]\n",
            "It: 50440, Time: 0.83\n",
            "mse_b  [2585.9136]  mse_f: [645.4672]   total loss: [3231.3809]\n",
            "It: 50480, Time: 0.80\n",
            "mse_b  [2568.931]  mse_f: [799.6233]   total loss: [3368.5542]\n",
            "It: 50520, Time: 0.84\n",
            "mse_b  [2406.4111]  mse_f: [1093.5264]   total loss: [3499.9375]\n",
            "It: 50560, Time: 0.87\n",
            "mse_b  [2456.994]  mse_f: [689.00476]   total loss: [3145.9985]\n",
            "It: 50600, Time: 0.81\n",
            "mse_b  [2364.8142]  mse_f: [678.7975]   total loss: [3043.6118]\n",
            "It: 50640, Time: 0.83\n",
            "mse_b  [2342.3162]  mse_f: [388.89062]   total loss: [2731.2068]\n",
            "It: 50680, Time: 0.83\n",
            "mse_b  [2352.8755]  mse_f: [515.2786]   total loss: [2868.154]\n",
            "It: 50720, Time: 0.83\n",
            "mse_b  [2404.8342]  mse_f: [846.9663]   total loss: [3251.8005]\n",
            "It: 50760, Time: 0.81\n",
            "mse_b  [2418.0164]  mse_f: [1136.9104]   total loss: [3554.9268]\n",
            "It: 50800, Time: 0.83\n",
            "mse_b  [3542.2974]  mse_f: [2626.1323]   total loss: [6168.4297]\n",
            "It: 50840, Time: 0.84\n",
            "mse_b  [2935.4465]  mse_f: [1945.849]   total loss: [4881.2954]\n",
            "It: 50880, Time: 0.85\n",
            "mse_b  [2973.3628]  mse_f: [1711.2646]   total loss: [4684.6274]\n",
            "It: 50920, Time: 0.81\n",
            "mse_b  [2671.5967]  mse_f: [1196.4232]   total loss: [3868.02]\n",
            "It: 50960, Time: 0.82\n",
            "mse_b  [2534.674]  mse_f: [815.32495]   total loss: [3349.999]\n",
            "It: 51000, Time: 0.81\n",
            "mse_b  [2486.9575]  mse_f: [809.3696]   total loss: [3296.3271]\n",
            "It: 51040, Time: 0.84\n",
            "mse_b  [2674.5667]  mse_f: [1372.6757]   total loss: [4047.2422]\n",
            "It: 51080, Time: 0.82\n",
            "mse_b  [2547.5881]  mse_f: [789.24536]   total loss: [3336.8335]\n",
            "It: 51120, Time: 0.91\n",
            "mse_b  [2747.5034]  mse_f: [1063.1138]   total loss: [3810.6172]\n",
            "It: 51160, Time: 0.85\n",
            "mse_b  [2472.599]  mse_f: [734.4763]   total loss: [3207.0754]\n",
            "It: 51200, Time: 0.86\n",
            "mse_b  [2377.2239]  mse_f: [847.15497]   total loss: [3224.379]\n",
            "It: 51240, Time: 0.88\n",
            "mse_b  [2325.5334]  mse_f: [773.75433]   total loss: [3099.2878]\n",
            "It: 51280, Time: 0.83\n",
            "mse_b  [2306.427]  mse_f: [552.02637]   total loss: [2858.4534]\n",
            "It: 51320, Time: 0.82\n",
            "mse_b  [2318.1638]  mse_f: [668.2455]   total loss: [2986.4092]\n",
            "It: 51360, Time: 0.83\n",
            "mse_b  [2296.8926]  mse_f: [709.06177]   total loss: [3005.9543]\n",
            "It: 51400, Time: 0.83\n",
            "mse_b  [2274.5957]  mse_f: [523.6375]   total loss: [2798.2332]\n",
            "It: 51440, Time: 0.84\n",
            "mse_b  [2248.2192]  mse_f: [579.7236]   total loss: [2827.9429]\n",
            "It: 51480, Time: 0.84\n",
            "mse_b  [2252.1853]  mse_f: [416.81143]   total loss: [2668.9968]\n",
            "It: 51520, Time: 0.81\n",
            "mse_b  [2197.9446]  mse_f: [311.0471]   total loss: [2508.9917]\n",
            "It: 51560, Time: 0.83\n",
            "mse_b  [2557.0107]  mse_f: [1476.304]   total loss: [4033.3147]\n",
            "It: 51600, Time: 0.81\n",
            "mse_b  [2583.55]  mse_f: [1145.2833]   total loss: [3728.8335]\n",
            "It: 51640, Time: 0.86\n",
            "mse_b  [2727.3245]  mse_f: [1266.8932]   total loss: [3994.2178]\n",
            "It: 51680, Time: 0.83\n",
            "mse_b  [2779.1282]  mse_f: [1047.4376]   total loss: [3826.566]\n",
            "It: 51720, Time: 0.86\n",
            "mse_b  [2547.811]  mse_f: [813.4147]   total loss: [3361.2256]\n",
            "It: 51760, Time: 0.83\n",
            "mse_b  [2288.6062]  mse_f: [728.07263]   total loss: [3016.6787]\n",
            "It: 51800, Time: 0.84\n",
            "mse_b  [2649.1345]  mse_f: [1110.9458]   total loss: [3760.0803]\n",
            "It: 51840, Time: 0.85\n",
            "mse_b  [2665.703]  mse_f: [792.8247]   total loss: [3458.5276]\n",
            "It: 51880, Time: 0.82\n",
            "mse_b  [2393.8677]  mse_f: [942.6478]   total loss: [3336.5156]\n",
            "It: 51920, Time: 0.83\n",
            "mse_b  [2622.1228]  mse_f: [1237.0269]   total loss: [3859.1497]\n",
            "It: 51960, Time: 0.83\n",
            "mse_b  [2800.7732]  mse_f: [1573.8721]   total loss: [4374.6455]\n",
            "It: 52000, Time: 0.83\n",
            "mse_b  [2533.4297]  mse_f: [1057.2451]   total loss: [3590.6748]\n",
            "It: 52040, Time: 0.83\n",
            "mse_b  [2634.1758]  mse_f: [1801.4843]   total loss: [4435.66]\n",
            "It: 52080, Time: 0.82\n",
            "mse_b  [2622.1567]  mse_f: [1136.5049]   total loss: [3758.6616]\n",
            "It: 52120, Time: 0.81\n",
            "mse_b  [2400.5933]  mse_f: [937.82837]   total loss: [3338.4216]\n",
            "It: 52160, Time: 0.84\n",
            "mse_b  [2308.4365]  mse_f: [717.23956]   total loss: [3025.676]\n",
            "It: 52200, Time: 0.86\n",
            "mse_b  [2654.0432]  mse_f: [1216.7166]   total loss: [3870.7598]\n",
            "It: 52240, Time: 0.83\n",
            "mse_b  [2540.0024]  mse_f: [807.1844]   total loss: [3347.1868]\n",
            "It: 52280, Time: 0.82\n",
            "mse_b  [2538.6843]  mse_f: [785.2086]   total loss: [3323.893]\n",
            "It: 52320, Time: 0.81\n",
            "mse_b  [2336.4124]  mse_f: [621.7604]   total loss: [2958.1729]\n",
            "It: 52360, Time: 0.83\n",
            "mse_b  [2497.8423]  mse_f: [479.10007]   total loss: [2976.9424]\n",
            "It: 52400, Time: 0.84\n",
            "mse_b  [2280.6309]  mse_f: [629.8547]   total loss: [2910.4856]\n",
            "It: 52440, Time: 0.82\n",
            "mse_b  [2342.609]  mse_f: [667.01636]   total loss: [3009.6252]\n",
            "It: 52480, Time: 0.84\n",
            "mse_b  [2330.9236]  mse_f: [659.3623]   total loss: [2990.286]\n",
            "It: 52520, Time: 0.83\n",
            "mse_b  [2470.8147]  mse_f: [907.1975]   total loss: [3378.0122]\n",
            "It: 52560, Time: 0.90\n",
            "mse_b  [2742.754]  mse_f: [1532.7476]   total loss: [4275.5015]\n",
            "It: 52600, Time: 0.83\n",
            "mse_b  [3259.0383]  mse_f: [1789.5751]   total loss: [5048.6133]\n",
            "It: 52640, Time: 0.81\n",
            "mse_b  [3323.677]  mse_f: [2204.2878]   total loss: [5527.965]\n",
            "It: 52680, Time: 0.82\n",
            "mse_b  [3241.485]  mse_f: [1580.6255]   total loss: [4822.1104]\n",
            "It: 52720, Time: 0.85\n",
            "mse_b  [2884.3755]  mse_f: [1233.01]   total loss: [4117.3857]\n",
            "It: 52760, Time: 0.82\n",
            "mse_b  [2597.1455]  mse_f: [1087.0012]   total loss: [3684.1467]\n",
            "It: 52800, Time: 0.83\n",
            "mse_b  [2425.0989]  mse_f: [893.528]   total loss: [3318.627]\n",
            "It: 52840, Time: 0.82\n",
            "mse_b  [4168.2563]  mse_f: [1887.9604]   total loss: [6056.217]\n",
            "It: 52880, Time: 0.84\n",
            "mse_b  [3959.985]  mse_f: [2892.058]   total loss: [6852.043]\n",
            "It: 52920, Time: 0.83\n",
            "mse_b  [3682.9163]  mse_f: [1761.066]   total loss: [5443.9824]\n",
            "It: 52960, Time: 0.84\n",
            "mse_b  [3213.2935]  mse_f: [1735.3911]   total loss: [4948.6846]\n",
            "It: 53000, Time: 0.85\n",
            "mse_b  [2860.3108]  mse_f: [1737.3845]   total loss: [4597.6953]\n",
            "It: 53040, Time: 0.84\n",
            "mse_b  [2749.0874]  mse_f: [915.3739]   total loss: [3664.4614]\n",
            "It: 53080, Time: 0.82\n",
            "mse_b  [2574.1152]  mse_f: [798.0106]   total loss: [3372.126]\n",
            "It: 53120, Time: 0.83\n",
            "mse_b  [2377.0408]  mse_f: [749.7704]   total loss: [3126.811]\n",
            "It: 53160, Time: 0.84\n",
            "mse_b  [2388.2703]  mse_f: [428.0293]   total loss: [2816.2996]\n",
            "It: 53200, Time: 0.82\n",
            "mse_b  [2382.3152]  mse_f: [659.63293]   total loss: [3041.9482]\n",
            "It: 53240, Time: 0.85\n",
            "mse_b  [2286.7666]  mse_f: [861.8266]   total loss: [3148.5933]\n",
            "It: 53280, Time: 0.83\n",
            "mse_b  [2350.5737]  mse_f: [851.1097]   total loss: [3201.6833]\n",
            "It: 53320, Time: 0.84\n",
            "mse_b  [2322.3445]  mse_f: [586.5704]   total loss: [2908.9148]\n",
            "It: 53360, Time: 0.83\n",
            "mse_b  [2313.2578]  mse_f: [495.70862]   total loss: [2808.9663]\n",
            "It: 53400, Time: 0.85\n",
            "mse_b  [2240.56]  mse_f: [495.6641]   total loss: [2736.224]\n",
            "It: 53440, Time: 0.83\n",
            "mse_b  [2301.8748]  mse_f: [579.85034]   total loss: [2881.725]\n",
            "It: 53480, Time: 0.83\n",
            "mse_b  [2311.7932]  mse_f: [927.1982]   total loss: [3238.9915]\n",
            "It: 53520, Time: 0.81\n",
            "mse_b  [2230.3645]  mse_f: [855.409]   total loss: [3085.7734]\n",
            "It: 53560, Time: 0.82\n",
            "mse_b  [2288.3682]  mse_f: [442.59348]   total loss: [2730.9617]\n",
            "It: 53600, Time: 0.82\n",
            "mse_b  [2755.6267]  mse_f: [758.2964]   total loss: [3513.923]\n",
            "It: 53640, Time: 0.82\n",
            "mse_b  [3018.7366]  mse_f: [1156.8081]   total loss: [4175.545]\n",
            "It: 53680, Time: 0.82\n",
            "mse_b  [2745.9363]  mse_f: [1304.4299]   total loss: [4050.3662]\n",
            "It: 53720, Time: 0.83\n",
            "mse_b  [2559.0776]  mse_f: [1299.3542]   total loss: [3858.432]\n",
            "It: 53760, Time: 0.84\n",
            "mse_b  [2468.1125]  mse_f: [719.1109]   total loss: [3187.2234]\n",
            "It: 53800, Time: 0.83\n",
            "mse_b  [2302.9624]  mse_f: [559.12036]   total loss: [2862.0828]\n",
            "It: 53840, Time: 0.82\n",
            "mse_b  [2646.3796]  mse_f: [901.42285]   total loss: [3547.8025]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bed1b30eb2fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m MSE_b1, MSE_f1, weightu, weightf, loss_saman, list_model_sai = fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu,sai_exact1,\n\u001b[0m\u001b[1;32m    367\u001b[0m                                                    \u001b[0mfi_exact1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_exact1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_exact1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp_exact1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_exact1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT_exact1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_star1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                                                    \u001b[0mxtt2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtt3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtt4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtt5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtt6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytt2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytt3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytt4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytt5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytt6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxbb1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxbb2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxbb3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxbb4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxbb5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxbb6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mybb1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mybb2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mybb3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mybb4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-bed1b30eb2fb>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, sai_exact1, fi_exact1, u_exact1, v_exact1, p_exact1, c_exact1, T_exact1, X_star, xtt1, xtt2, xtt3, xtt4, xtt5, xtt6, ytt1, ytt2, ytt3, ytt4, ytt5, ytt6, xbb1, xbb2, xbb3, xbb4, xbb5, xbb6, ybb1, ybb2, ybb3, ybb4, ybb5, ybb6, xrr, yrr, xll1, yll1, xll2, yll2, tf_iter, tf_iter2, newton_iter1, newton_iter2)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             loss_value, mse_b, mse_f, grads, grads_ub, grads_fu = grad(u_model, x_f_batch, y_f_batch,\n\u001b[0m\u001b[1;32m    280\u001b[0m                                                                        \u001b[0mxb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                                                                        \u001b[0mub_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_ub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#solving sai\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import scipy.io\n",
        "import math\n",
        "import matplotlib.gridspec as gridspec\n",
        "#from plotting import newfig\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras import layers, activations\n",
        "from scipy.interpolate import griddata\n",
        "#from eager_lbfgs import lbfgs, Struct\n",
        "from pyDOE import lhs\n",
        "\n",
        "weight_ub = tf.Variable([1000.0], dtype=tf.float32)\n",
        "weight_fu = tf.Variable([1.0], dtype=tf.float32)\n",
        "layer_sizes = [2, 20, 20, 20, 20, 20, 20, 20, 7]\n",
        "sizes_w = []\n",
        "sizes_b = []\n",
        "loss_saman=[]\n",
        "list_model_sai=[]\n",
        "for i, width in enumerate(layer_sizes):\n",
        "    if i != 1:\n",
        "        sizes_w.append(int(width * layer_sizes[1]))\n",
        "        sizes_b.append(int(width if i != 0 else layer_sizes[1]))\n",
        "\n",
        "\n",
        "# L-BFGS weight getting and setting from https://github.com/pierremtb/PINNs-TF2.0\n",
        "\n",
        "def set_weights(model, w, sizes_w, sizes_b):  # 重新设置参数\n",
        "\n",
        "    for i, layer in enumerate(model.layers[1:len(sizes_w) + 1]):\n",
        "        start_weights = sum(sizes_w[:i]) + sum(sizes_b[:i])\n",
        "        end_weights = sum(sizes_w[:i + 1]) + sum(sizes_b[:i])\n",
        "        weights = w[start_weights:end_weights]\n",
        "        w_div = int(sizes_w[i] / sizes_b[i])\n",
        "        weights = tf.reshape(weights, [w_div, sizes_b[i]])\n",
        "        biases = w[end_weights:end_weights + sizes_b[i]]\n",
        "        weights_biases = [weights, biases]\n",
        "        layer.set_weights(weights_biases)\n",
        "\n",
        "def get_weights(model):\n",
        "    w = []\n",
        "    for layer in model.layers[1:len(sizes_w) + 1]:\n",
        "        weights_biases = layer.get_weights()\n",
        "        weights = weights_biases[0].flatten()\n",
        "        biases = weights_biases[1]\n",
        "        w.extend(weights)\n",
        "        w.extend(biases)\n",
        "    w = tf.convert_to_tensor(w)\n",
        "    return w\n",
        "\n",
        "def xavier_init(layer_sizes):\n",
        "    in_dim = layer_sizes[0]\n",
        "    out_dim = layer_sizes[1]\n",
        "    xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "    return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "\n",
        "def neural_net(layer_sizes):\n",
        "\n",
        "    input_tensor = keras.Input(shape=(layer_sizes[0],))\n",
        "\n",
        "    hide_layer_list = []\n",
        "    flag = True\n",
        "    for width in layer_sizes[1:-1]:\n",
        "        if flag:\n",
        "            x = layers.Dense(\n",
        "                width, activation=tf.nn.tanh,\n",
        "                kernel_initializer=\"glorot_normal\")(input_tensor)\n",
        "            flag = False\n",
        "        else:\n",
        "            x = layers.Dense(\n",
        "                width, activation=tf.nn.tanh,\n",
        "                kernel_initializer=\"glorot_normal\")(x)\n",
        "    output_tensor = layers.Dense(layer_sizes[-1], activation=None,kernel_initializer=\"glorot_normal\")(x)\n",
        "    print(\"xxxxxxxxxxxxxx\")\n",
        "    output0 = output_tensor[:, 0:1]\n",
        "\n",
        "\n",
        "    model_output = keras.models.Model(input_tensor, [output0])\n",
        "\n",
        "    return model_output\n",
        "\n",
        "# initialize the NN\n",
        "u_model = neural_net(layer_sizes)\n",
        "# view the NN\n",
        "u_model.summary()\n",
        "\n",
        "\n",
        "# define the loss\n",
        "def loss(x_f_batch, y_f_batch, xb, yb, ub, vb, weight_ub,weight_fu,xtop1,xtop2,xtop3,xtop4,xtop5,xtop6,ytop1,ytop2,ytop3,ytop4,ytop5,ytop6,xbottom1,xbottom2,xbottom3,xbottom4,xbottom5,xbottom6,ybottom1,ybottom2,ybottom3,ybottom4,ybottom5,ybottom6,xright,yright,xleft1,yleft1,xleft2,yleft2):\n",
        "\n",
        "    f_fi_pred= f_model(x_f_batch, y_f_batch)\n",
        "\n",
        "\n",
        "    #sai_pred,fi_pred,u_pred,v_pred,p_pred,c_pred,T_pred = u_model(tf.concat([xb, yb], 1))\n",
        "\n",
        "    #mse_b = 100*weight_ub*(tf.reduce_sum(tf.square(u_pred - ub)) + tf.reduce_sum(tf.square(v_pred - vb)))\n",
        "    #mse_b = 1*weight_ub*(tf.reduce_sum(tf.square(u_pred - ub)) + tf.reduce_sum(tf.square(v_pred - vb)))\n",
        "    loss_2 = loss_bd(xtop1,xtop2,xtop3,xtop4,xtop5,xtop6,ytop1,ytop2,ytop3,ytop4,ytop5,ytop6,xbottom1,xbottom2,xbottom3,xbottom4,xbottom5,xbottom6,ybottom1,ybottom2,ybottom3,ybottom4,ybottom5,ybottom6,xright,yright,xleft1,yleft1,xleft2,yleft2)\n",
        "    mse_b = loss_2*weight_ub#+ mse_b\n",
        "\n",
        "    mse_f = weight_fu*( tf.reduce_sum(tf.square(f_fi_pred)))\n",
        "    #tf.print('reduce_max',tf.reduce_max(f_u_pred))\n",
        "    #tf.print('min or max',tf.math.minimum(f_u_pred))\n",
        "    return mse_b + mse_f, mse_b, mse_f\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def loss_bd(xtop1,xtop2,xtop3,xtop4,xtop5,xtop6,ytop1,ytop2,ytop3,ytop4,ytop5,ytop6,xbottom1,xbottom2,xbottom3,xbottom4,xbottom5,xbottom6,ybottom1,ybottom2,ybottom3,ybottom4,ybottom5,ybottom6,xright,yright,xleft1,yleft1,xleft2,yleft2):\n",
        "  saitop1 = u_model(tf.concat([xtop1, ytop1],1))\n",
        "  saitop2 = u_model(tf.concat([xtop2, ytop2],1))\n",
        "  saitop3 = u_model(tf.concat([xtop3, ytop3],1))\n",
        "  saitop4 = u_model(tf.concat([xtop4, ytop4],1))\n",
        "  saitop5 = u_model(tf.concat([xtop5, ytop5],1))\n",
        "  saitop6 = u_model(tf.concat([xtop6, ytop6],1))\n",
        "  saibottom1 =u_model(tf.concat([xbottom1, ybottom1],1))\n",
        "  saibottom2=u_model(tf.concat([xbottom2, ybottom2],1))\n",
        "  saibottom3=u_model(tf.concat([xbottom3, ybottom3],1))\n",
        "  saibottom4=u_model(tf.concat([xbottom4, ybottom4],1))\n",
        "  saibottom5=u_model(tf.concat([xbottom5, ybottom5],1))\n",
        "  saibottom6=u_model(tf.concat([xbottom6, ybottom6],1))\n",
        "  sairight= u_model(tf.concat([xright, yright],1))\n",
        "  saileft1= u_model(tf.concat([xleft1, yleft1],1))\n",
        "  saileft2= u_model(tf.concat([xleft2, yleft2],1))\n",
        "\n",
        "  #tf.print('ybb3*******',ybottom3)\n",
        "  #tf.print('=======================pbottom',tf.shape(pbottom),'ybottom',tf.shape(ybottom)) reduce_sum\n",
        "\n",
        "  loss_bd = tf.reduce_sum(tf.square(saitop1+1.0)) + tf.reduce_sum(tf.square(saitop2+0.0)) +tf.reduce_sum(tf.square(saitop3+1.0))\\\n",
        "  +tf.reduce_sum(tf.square(saitop4+0))+tf.reduce_sum(tf.square(saitop5+0))+tf.reduce_sum(tf.square(saitop6-0))\\\n",
        "  +tf.reduce_sum(tf.square(saibottom1-0.0)) + tf.reduce_sum(tf.square(saibottom2+1.0)) +tf.reduce_sum(tf.square(saibottom3-0.0))\\\n",
        "  +tf.reduce_sum(tf.square(saibottom4+1.0)) + tf.reduce_sum(tf.square(saibottom5+0.0)) +tf.reduce_sum(tf.square(saibottom6+0.0))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(saileft1,xleft1)[0]))+tf.reduce_sum(tf.square(tf.gradients(saileft2,xleft2)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(sairight,xright)[0]))\\\n",
        "\n",
        "\n",
        "  #loss_bd = 0\n",
        "\n",
        "  return loss_bd\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def f_model(x, y):\n",
        "\n",
        "    sai= u_model(tf.concat([x, y],1))\n",
        "\n",
        "    #fi_t = tf.gradients(fi, t)[0]\n",
        "    sai_x= tf.gradients(sai, x)[0]\n",
        "    sai_y= tf.gradients(sai, y)[0]\n",
        "    sai_xx= tf.gradients(sai_x, x)[0]\n",
        "    sai_yy= tf.gradients(sai_y, y)[0]\n",
        "\n",
        "    cc=tf.constant(60.0, dtype=tf.float32)\n",
        "    f_sai=((sai_xx)+(sai_yy))-cc*sai\n",
        "\n",
        "    #wh^2=6\n",
        "\n",
        "    return f_sai\n",
        "\n",
        "@tf.function\n",
        "def u_x_model(x, y):\n",
        "    sai= u_model(tf.concat([x, y], 1))\n",
        "    return sai\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def grad(u_model, x_f_batch, y_f_batch, xb_batch, yb_batch, ub_batch, vb_batch, weight_ub,\n",
        "         weight_fu,x_top1,x_top2,x_top3,x_top4,x_top5,x_top6,y_top1,y_top2,y_top3,y_top4,y_top5,y_top6,\n",
        "         x_bottom1,x_bottom2,x_bottom3,x_bottom4,x_bottom5,x_bottom6,y_bottom1,y_bottom2,y_bottom3,y_bottom4,y_bottom5,y_bottom6,x_right,y_right,x_left1,y_left1,x_left2,y_left2):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "        loss_value, mse_b, mse_f = loss(x_f_batch, y_f_batch, xb_batch, yb_batch, ub_batch,vb_batch, weight_ub, weight_fu,x_top1,x_top2,x_top3,x_top4,x_top5,x_top6,y_top1,y_top2,y_top3,y_top4,y_top5,y_top6,\n",
        "                                        x_bottom1,x_bottom2,x_bottom3,x_bottom4,x_bottom5,x_bottom6,y_bottom1,y_bottom2,y_bottom3,y_bottom4,y_bottom5,y_bottom6,\n",
        "                                        x_right,y_right,x_left1,y_left1,x_left2,y_left2)\n",
        "        grads = tape.gradient(loss_value, u_model.trainable_variables)\n",
        "\n",
        "        grads_ub = tape.gradient(loss_value, weight_ub)\n",
        "\n",
        "        grads_fu = tape.gradient(loss_value, weight_fu)\n",
        "\n",
        "    return loss_value, mse_b, mse_f, grads, grads_ub, grads_fu\n",
        "\n",
        "\n",
        "def fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, sai_exact1, fi_exact1,u_exact1,v_exact1,p_exact1,c_exact1,T_exact1, X_star,xtt1,xtt2,xtt3,xtt4,xtt5,xtt6,ytt1,ytt2,ytt3,ytt4,ytt5,ytt6,xbb1,xbb2,xbb3,xbb4,xbb5,xbb6,ybb1,ybb2,ybb3,ybb4,ybb5,ybb6,xrr,yrr,xll1,yll1,xll2,yll2, tf_iter, tf_iter2,newton_iter1, newton_iter2):\n",
        "\n",
        "    batch_sz = N_f\n",
        "    n_batches = N_f // batch_sz\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    tf_optimizer = tf.keras.optimizers.Adam(lr=0.01, beta_1=.99)\n",
        "    tf_optimizer_weights = tf.keras.optimizers.Adam(lr=0.003, beta_1=.99)\n",
        "    tf_optimizer_u = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
        "\n",
        "    tf.print(f\"weight_ub: {weight_ub}  weight_fu: {weight_fu}\")\n",
        "    print(\"starting Adam training\")\n",
        "\n",
        "    a = np.random.rand(1000)\n",
        "    loss_history = list(a)\n",
        "    MSE_b0 = list(a)\n",
        "    MSE_f0 = list(a)\n",
        "\n",
        "\n",
        "    MSE_b1 = []\n",
        "    MSE_f1 = []\n",
        "\n",
        "    weightu = []\n",
        "    weightf = []\n",
        "    # For mini-batch (if used)\n",
        "    for epoch in range(tf_iter):\n",
        "        for i in range(n_batches):\n",
        "            xb_batch = xb\n",
        "            yb_batch = yb\n",
        "\n",
        "            ub_batch = ub\n",
        "            vb_batch = vb\n",
        "\n",
        "            x_top1=xtt1\n",
        "            y_top1=ytt1\n",
        "\n",
        "            x_top2=xtt2\n",
        "            y_top2=ytt2\n",
        "\n",
        "            x_top3=xtt3\n",
        "            y_top3=ytt3\n",
        "\n",
        "            x_top4=xtt4\n",
        "            y_top4=ytt4\n",
        "\n",
        "            x_top5=xtt5\n",
        "            y_top5=ytt5\n",
        "\n",
        "            x_top6=xtt6\n",
        "            y_top6=ytt6\n",
        "\n",
        "            x_bottom1=xbb1\n",
        "            y_bottom1=ybb1\n",
        "\n",
        "            x_bottom2=xbb2\n",
        "            y_bottom2=ybb2\n",
        "\n",
        "            x_bottom3=xbb3\n",
        "            y_bottom3=ybb3\n",
        "\n",
        "            x_bottom4=xbb4\n",
        "            y_bottom4=ybb4\n",
        "\n",
        "            x_bottom5=xbb5\n",
        "            y_bottom5=ybb5\n",
        "\n",
        "            x_bottom6=xbb6\n",
        "            y_bottom6=ybb6\n",
        "\n",
        "            x_right=xrr\n",
        "            y_right=yrr\n",
        "\n",
        "            x_left1=xll1\n",
        "            y_left1=yll1\n",
        "\n",
        "            x_left2=xll2\n",
        "            y_left2=yll2\n",
        "\n",
        "            x_f_batch = x_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "            y_f_batch = y_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "            t_f_batch = t_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "\n",
        "\n",
        "            loss_value, mse_b, mse_f, grads, grads_ub, grads_fu = grad(u_model, x_f_batch, y_f_batch,\n",
        "                                                                       xb_batch, yb_batch,\n",
        "                                                                       ub_batch, vb_batch, weight_ub,\n",
        "                                                                       weight_fu,x_top1,x_top2,x_top3,x_top4,x_top5,x_top6,y_top1,y_top2,y_top3,y_top4,y_top5,y_top6,\n",
        "                                                                       x_bottom1,x_bottom2,x_bottom3,x_bottom4,x_bottom5,x_bottom6,y_bottom1,\n",
        "                                                                       y_bottom2,y_bottom3,y_bottom4,y_bottom5,y_bottom6,x_right,y_right,x_left1,y_left1,x_left2,y_left2)\n",
        "\n",
        "            tf_optimizer.apply_gradients(zip(grads, u_model.trainable_variables))\n",
        "            MSE_b0.append(mse_b)\n",
        "            MSE_f0.append(mse_f)\n",
        "\n",
        "            loss_history.append(loss_value)\n",
        "            loss_saman.append(loss_value)\n",
        "            list_model_sai.append(u_model)\n",
        "            #if loss_history[-1] < loss_history[-2] and loss_history[-2] < loss_history[-3] and loss_history[-1] < \\\n",
        "            #        loss_history[-10]:\n",
        "            #    tf_optimizer_weights.apply_gradients(zip([-grads_fu], [weight_fu]))\n",
        "            #    tf_optimizer_u.apply_gradients(zip([-grads_ub], [weight_ub]))\n",
        "\n",
        "        if epoch % 40 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('It: %d, Time: %.2f' % (epoch, elapsed))\n",
        "            tf.print(f\"mse_b  {mse_b}  mse_f: {mse_f}   total loss: {loss_value}\")\n",
        "\n",
        "            wu = weight_ub.numpy()\n",
        "            wf = weight_fu.numpy()\n",
        "\n",
        "            MSE_b1.append(mse_b)\n",
        "            MSE_f1.append(mse_f)\n",
        "\n",
        "            weightu.append(wu)\n",
        "            weightf.append(wf)\n",
        "\n",
        "            start_time = time.time()\n",
        "    tf.print(f\"weight_ub: {weight_ub}  weight_fu: {weight_fu}\")\n",
        "    fi_pred = predict(X_star)\n",
        "    tf.print('epoch',epoch,'loss',loss_value)\n",
        "    #error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
        "    #print('Error u: %e' % (error_u))\n",
        "    #error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
        "    #print('Error v: %e' % (error_v))\n",
        "    #print(\"Starting L-BFGS training\")\n",
        "\n",
        "    '''\n",
        "    loss_and_flat_grad = get_loss_and_flat_grad(x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch,\n",
        "                                                vb_batch, weight_ub, weight_fu)\n",
        "\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "          get_weights(u_model),\n",
        "          Struct(), maxIter=newton_iter1, learningRate=0.8)\n",
        "\n",
        "    u_pred, v_pred, p_pred = predict(X_star)\n",
        "    error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
        "    print('Error u: %e' % (error_u))\n",
        "    error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
        "    print('Error v: %e' % (error_v))\n",
        "\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "          get_weights(u_model),\n",
        "          Struct(), maxIter=newton_iter2, learningRate=0.8)\n",
        "    '''\n",
        "    return MSE_b1, MSE_f1,  weightu, weightf,loss_saman,list_model_sai\n",
        "\n",
        "# L-BFGS implementation from https://github.com/pierremtb/PINNs-TF2.0\n",
        "def get_loss_and_flat_grad(x_f_batch, y_f_batch , xb_batch, yb_batch,ub_batch, vb_batch,weight_ub, weight_fu):\n",
        "    def loss_and_flat_grad(w):\n",
        "        with tf.GradientTape() as tape:\n",
        "            set_weights(u_model, w, sizes_w, sizes_b)\n",
        "            loss_value, _, _ = loss(x_f_batch, y_f_batch, xb_batch, yb_batch, ub_batch, vb_batch, weight_ub, weight_fu)\n",
        "        grad = tape.gradient(loss_value, u_model.trainable_variables)\n",
        "        grad_flat = []\n",
        "        for g in grad:\n",
        "            grad_flat.append(tf.reshape(g, [-1]))\n",
        "        grad_flat = tf.concat(grad_flat, 0)\n",
        "        # print(loss_value, grad_flat)\n",
        "        return loss_value, grad_flat\n",
        "\n",
        "    return loss_and_flat_grad\n",
        "\n",
        "\n",
        "def predict(X_star):\n",
        "    X_star = tf.convert_to_tensor(X_star, dtype=tf.float32)\n",
        "    sai_star= u_x_model(X_star[:, 0:1], X_star[:, 1:2])\n",
        "    return sai_star.numpy()\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "MSE_b1, MSE_f1, weightu, weightf, loss_saman, list_model_sai = fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu,sai_exact1,\n",
        "                                                   fi_exact1,u_exact1,v_exact1,p_exact1,c_exact1,T_exact1,X_star1,xtt1,\n",
        "                                                   xtt2,xtt3,xtt4,xtt5,xtt6,ytt1,ytt2,ytt3,ytt4,ytt5,ytt6,xbb1,xbb2,xbb3,xbb4,xbb5,xbb6,ybb1,ybb2,ybb3,ybb4,\n",
        "                                                   ybb5,ybb6,xrr,yrr,xll1,yll1,xll2,yll2,tf_iter=340000, tf_iter2=1000, newton_iter1=5000,newton_iter2=15000)\n",
        "\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_saman[14000:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "cDrdc2qMsVxg",
        "outputId": "aa4e35de-0c8c-4aa1-92eb-386487ecba5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f582ed20410>]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "loss2=[]\n",
        "for i in range(len(loss_saman)):\n",
        "  loss2.append(loss_saman[i][0].numpy())\n",
        "print(\"Min: \", pd.Series(loss2).idxmin())\n",
        "print(\"Max: \", pd.Series(loss2).idxmax())\n",
        "print('loss_min',loss_saman[pd.Series(loss2).idxmin()])\n",
        "\n",
        "model_sai=list_model_sai[pd.Series(loss2).idxmin()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gd5S1tdbyqOU",
        "outputId": "64eda1d3-8500-4447-ea15-e3985116807e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min:  51520\n",
            "Max:  1\n",
            "loss_min tf.Tensor([2508.9917], shape=(1,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_sai.save('my_sai_model_Lx6_Asym_2top_2bott.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w1_jGnFI7ME",
        "outputId": "bf35ddc6-2875-4ce1-f406-67fbef7cc2e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model('my_sai_model.h5')\n",
        "model_sai=new_model"
      ],
      "metadata": {
        "id": "CMtFTKvEI_MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualization of predicted electric layer using PINNs\n"
      ],
      "metadata": {
        "id": "nxIWPB-1gwZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Set up meshgrid\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "N=100\n",
        "x1 = (np.linspace(0, xmax, N+1)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, N+1)).flatten()[:, None]\n",
        "X1, Y1 = np.meshgrid(x1, y1)\n",
        "\n",
        "X_star1 = np.hstack((X1.flatten()[:, None], Y1.flatten()[:, None]))\n",
        "\n",
        "X_star = tf.convert_to_tensor(X_star1, dtype=tf.float32)\n",
        "#up, vp, pp = u_x_model(X_star[:, 0:1], X_star[:, 1:2])\n",
        "\n",
        "UU=model_sai(tf.concat([X_star[:, 0:1], X_star[:, 1:2]],1))\n",
        "uuu2=tf.reshape(UU,shape=[tf.shape(UU).numpy()[0]])\n",
        "U = uuu2.numpy().reshape(N+1,N+1)\n",
        "plt. contourf(X1, Y1, U,16\n",
        "              , cmap='rainbow');\n",
        "plt.colorbar();\n",
        "\n",
        "\n",
        "x1 = (np.linspace(0, xmax, N+1)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, N+1)).flatten()[:, None]\n",
        "X1, Y1 = np.meshgrid(x1, y1)\n",
        "fig, ax = plt.subplots(figsize=(4, 1))\n",
        "ax.contourf(X1,Y1,U,60,cmap='rainbow')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "v2mpe7e_0m0V",
        "outputId": "8e73829a-dbcc-4e7b-ba85-bd7980d52ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.contour.QuadContourSet at 0x7fa6697db190>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x72 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running PINNs for soliving electric field distribution in the domain due to external electric field"
      ],
      "metadata": {
        "id": "pVrwF96vg60w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FI\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import scipy.io\n",
        "import math\n",
        "import matplotlib.gridspec as gridspec\n",
        "#from plotting import newfig\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras import layers, activations\n",
        "from scipy.interpolate import griddata\n",
        "#from eager_lbfgs import lbfgs, Struct\n",
        "from pyDOE import lhs\n",
        "\n",
        "weight_ub = tf.Variable([10000.0], dtype=tf.float32)\n",
        "weight_fu = tf.Variable([1.0], dtype=tf.float32)\n",
        "layer_sizes = [2, 20, 20, 20, 20, 20, 20, 20, 7]\n",
        "sizes_w = []\n",
        "sizes_b = []\n",
        "loss_saman=[]\n",
        "list_model_fi=[]\n",
        "for i, width in enumerate(layer_sizes):\n",
        "    if i != 1:\n",
        "        sizes_w.append(int(width * layer_sizes[1]))\n",
        "        sizes_b.append(int(width if i != 0 else layer_sizes[1]))\n",
        "\n",
        "\n",
        "# L-BFGS weight getting and setting from https://github.com/pierremtb/PINNs-TF2.0\n",
        "\n",
        "def set_weights(model, w, sizes_w, sizes_b):  # 重新设置参数\n",
        "\n",
        "    for i, layer in enumerate(model.layers[1:len(sizes_w) + 1]):\n",
        "        start_weights = sum(sizes_w[:i]) + sum(sizes_b[:i])\n",
        "        end_weights = sum(sizes_w[:i + 1]) + sum(sizes_b[:i])\n",
        "        weights = w[start_weights:end_weights]\n",
        "        w_div = int(sizes_w[i] / sizes_b[i])\n",
        "        weights = tf.reshape(weights, [w_div, sizes_b[i]])\n",
        "        biases = w[end_weights:end_weights + sizes_b[i]]\n",
        "        weights_biases = [weights, biases]\n",
        "        layer.set_weights(weights_biases)\n",
        "\n",
        "\n",
        "def get_weights(model):\n",
        "    w = []\n",
        "    for layer in model.layers[1:len(sizes_w) + 1]:\n",
        "        weights_biases = layer.get_weights()\n",
        "        weights = weights_biases[0].flatten()\n",
        "        biases = weights_biases[1]\n",
        "        w.extend(weights)\n",
        "        w.extend(biases)\n",
        "    w = tf.convert_to_tensor(w)\n",
        "    return w\n",
        "\n",
        "def xavier_init(layer_sizes):\n",
        "    in_dim = layer_sizes[0]\n",
        "    out_dim = layer_sizes[1]\n",
        "    xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "    return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "\n",
        "def neural_net(layer_sizes):\n",
        "\n",
        "    input_tensor = keras.Input(shape=(layer_sizes[0],))\n",
        "\n",
        "    hide_layer_list = []\n",
        "    flag = True\n",
        "    for width in layer_sizes[1:-1]:\n",
        "        if flag:\n",
        "            x = layers.Dense(\n",
        "                width, activation=tf.nn.tanh,\n",
        "                kernel_initializer=\"glorot_normal\")(input_tensor)\n",
        "            flag = False\n",
        "        else:\n",
        "            x = layers.Dense(\n",
        "                width, activation=tf.nn.tanh,\n",
        "                kernel_initializer=\"glorot_normal\")(x)\n",
        "    output_tensor = layers.Dense(layer_sizes[-1], activation=None,kernel_initializer=\"glorot_normal\")(x)\n",
        "    print(\"xxxxxxxxxxxxxx\")\n",
        "    output0 = output_tensor[:, 0:1]\n",
        "\n",
        "\n",
        "    model_output = keras.models.Model(input_tensor, [output0])\n",
        "\n",
        "    return model_output\n",
        "\n",
        "# initialize the NN\n",
        "u_model = neural_net(layer_sizes)\n",
        "# view the NN\n",
        "u_model.summary()\n",
        "\n",
        "\n",
        "# define the loss\n",
        "def loss(x_f_batch, y_f_batch, xb, yb, ub, vb, weight_ub,weight_fu,xtop1,xtop2,xtop3,xtop4,ytop1,ytop2,ytop3,ytop4,xbottom1,xbottom2,xbottom3,xbottom4,ybottom1,ybottom2,ybottom3,ybottom4,xright,yright,xleft1,yleft1,xleft2,yleft2):\n",
        "\n",
        "    f_fi_pred= f_model(x_f_batch, y_f_batch)\n",
        "\n",
        "\n",
        "    #sai_pred,fi_pred,u_pred,v_pred,p_pred,c_pred,T_pred = u_model(tf.concat([xb, yb], 1))\n",
        "\n",
        "    #mse_b = 100*weight_ub*(tf.reduce_sum(tf.square(u_pred - ub)) + tf.reduce_sum(tf.square(v_pred - vb)))\n",
        "    #mse_b = 1*weight_ub*(tf.reduce_sum(tf.square(u_pred - ub)) + tf.reduce_sum(tf.square(v_pred - vb)))\n",
        "    loss_2 = loss_bd(xtop1,xtop2,xtop3,xtop4,ytop1,ytop2,ytop3,ytop4,xbottom1,xbottom2,xbottom3,xbottom4,ybottom1,ybottom2,ybottom3,ybottom4,xright,yright,xleft1,yleft1,xleft2,yleft2)\n",
        "    mse_b = loss_2*weight_ub#+ mse_b\n",
        "\n",
        "    mse_f = weight_fu*( tf.reduce_sum(tf.square(f_fi_pred)))\n",
        "    #tf.print('reduce_max',tf.reduce_max(f_u_pred))\n",
        "    #tf.print('min or max',tf.math.minimum(f_u_pred))\n",
        "    return mse_b + mse_f, mse_b, mse_f\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def loss_bd(xtop1,xtop2,xtop3,xtop4,ytop1,ytop2,ytop3,ytop4,xbottom1,xbottom2,xbottom3,xbottom4,ybottom1,ybottom2,ybottom3,ybottom4,xright,yright,xleft1,yleft1,xleft2,yleft2):\n",
        "  ftop1 = u_model(tf.concat([xtop1, ytop1],1))\n",
        "  ftop2 = u_model(tf.concat([xtop2, ytop2],1))\n",
        "  ftop3 = u_model(tf.concat([xtop3, ytop3],1))\n",
        "  ftop4 = u_model(tf.concat([xtop4, ytop4],1))\n",
        "  fbottom1 =u_model(tf.concat([xbottom1, ybottom1],1))\n",
        "  fbottom2=u_model(tf.concat([xbottom2, ybottom2],1))\n",
        "  fbottom3=u_model(tf.concat([xbottom3, ybottom3],1))\n",
        "  fbottom4=u_model(tf.concat([xbottom4, ybottom4],1))\n",
        "  fright= u_model(tf.concat([xright, yright],1))\n",
        "  fleft1= u_model(tf.concat([xleft1, yleft1],1))\n",
        "  fleft2= u_model(tf.concat([xleft2, yleft2],1))\n",
        "\n",
        "  #tf.print('ybb3*******',ybottom3)\n",
        "  #tf.print('=======================pbottom',tf.shape(pbottom),'ybottom',tf.shape(ybottom)) reduce_sum\n",
        "\n",
        "  loss_bd = tf.reduce_sum(tf.square(fright-0))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(ftop1,ytop1)[0]))+tf.reduce_sum(tf.square(tf.gradients(ftop2,ytop2)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(ftop3,ytop3)[0]))+tf.reduce_sum(tf.square(tf.gradients(ftop4,ytop4)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(fbottom1,ybottom1)[0]))+tf.reduce_sum(tf.square(tf.gradients(fbottom2,ybottom2)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(fbottom3,ybottom3)[0]))+tf.reduce_sum(tf.square(tf.gradients(fbottom4,ybottom4)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(fleft1-1))+tf.reduce_sum(tf.square(fleft2-1))\n",
        "\n",
        "  #loss_bd = 0\n",
        "\n",
        "  return loss_bd\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def f_model(x, y):\n",
        "\n",
        "    fi= u_model(tf.concat([x, y],1))\n",
        "\n",
        "    #fi_t = tf.gradients(fi, t)[0]\n",
        "    fi_x= tf.gradients(fi, x)[0]\n",
        "    fi_y= tf.gradients(fi, y)[0]\n",
        "    fi_xx= tf.gradients(fi_x, x)[0]\n",
        "    fi_yy= tf.gradients(fi_y, y)[0]\n",
        "\n",
        "    f_fi=((fi_xx)+(fi_yy))\n",
        "\n",
        "    #wh^2=6\n",
        "\n",
        "    return f_fi\n",
        "\n",
        "@tf.function\n",
        "def u_x_model(x, y):\n",
        "    fi= u_model(tf.concat([x, y], 1))\n",
        "    return fi\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def grad(u_model, x_f_batch, y_f_batch, xb_batch, yb_batch, ub_batch, vb_batch, weight_ub,\n",
        "         weight_fu,x_top1,x_top2,x_top3,x_top4,y_top1,y_top2,y_top3,y_top4,\n",
        "         x_bottom1,x_bottom2,x_bottom3,x_bottom4,y_bottom1,y_bottom2,y_bottom3,y_bottom4,x_right,y_right,x_left1,y_left1,x_left2,y_left2):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "        loss_value, mse_b, mse_f = loss(x_f_batch, y_f_batch, xb_batch, yb_batch, ub_batch,vb_batch, weight_ub, weight_fu,x_top1,x_top2,x_top3,x_top4,y_top1,y_top2,y_top3,y_top4,\n",
        "                                        x_bottom1,x_bottom2,x_bottom3,x_bottom4,y_bottom1,y_bottom2,y_bottom3,y_bottom4,\n",
        "                                        x_right,y_right,x_left1,y_left1,x_left2,y_left2)\n",
        "        grads = tape.gradient(loss_value, u_model.trainable_variables)\n",
        "\n",
        "        grads_ub = tape.gradient(loss_value, weight_ub)\n",
        "\n",
        "        grads_fu = tape.gradient(loss_value, weight_fu)\n",
        "\n",
        "    return loss_value, mse_b, mse_f, grads, grads_ub, grads_fu\n",
        "\n",
        "\n",
        "def fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, sai_exact1, fi_exact1,u_exact1,v_exact1,p_exact1,c_exact1,T_exact1, X_star,xtt1,xtt2,xtt3,xtt4,ytt1,ytt2,ytt3,ytt4,xbb1,xbb2,xbb3,xbb4,ybb1,ybb2,ybb3,ybb4,xrr,yrr,xll1,yll1,xll2,yll2, tf_iter, tf_iter2,newton_iter1, newton_iter2):\n",
        "\n",
        "    batch_sz = N_f\n",
        "    n_batches = N_f // batch_sz\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    tf_optimizer = tf.keras.optimizers.Adam(lr=0.005, beta_1=.99)\n",
        "    tf_optimizer_weights = tf.keras.optimizers.Adam(lr=0.003, beta_1=.99)\n",
        "    tf_optimizer_u = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
        "\n",
        "    tf.print(f\"weight_ub: {weight_ub}  weight_fu: {weight_fu}\")\n",
        "    print(\"starting Adam training\")\n",
        "\n",
        "    a = np.random.rand(1000)\n",
        "    loss_history = list(a)\n",
        "    MSE_b0 = list(a)\n",
        "    MSE_f0 = list(a)\n",
        "\n",
        "\n",
        "    MSE_b1 = []\n",
        "    MSE_f1 = []\n",
        "\n",
        "    weightu = []\n",
        "    weightf = []\n",
        "    # For mini-batch (if used)\n",
        "    for epoch in range(tf_iter):\n",
        "        for i in range(n_batches):\n",
        "            xb_batch = xb\n",
        "            yb_batch = yb\n",
        "\n",
        "            ub_batch = ub\n",
        "            vb_batch = vb\n",
        "\n",
        "            x_top1=xtt1\n",
        "            y_top1=ytt1\n",
        "\n",
        "            x_top2=xtt2\n",
        "            y_top2=ytt2\n",
        "\n",
        "            x_top3=xtt3\n",
        "            y_top3=ytt3\n",
        "\n",
        "            x_top4=xtt4\n",
        "            y_top4=ytt4\n",
        "\n",
        "            x_bottom1=xbb1\n",
        "            y_bottom1=ybb1\n",
        "\n",
        "            x_bottom2=xbb2\n",
        "            y_bottom2=ybb2\n",
        "\n",
        "            x_bottom3=xbb3\n",
        "            y_bottom3=ybb3\n",
        "\n",
        "            x_bottom4=xbb4\n",
        "            y_bottom4=ybb4\n",
        "\n",
        "            x_right=xrr\n",
        "            y_right=yrr\n",
        "\n",
        "            x_left1=xll1\n",
        "            y_left1=yll1\n",
        "\n",
        "            x_left2=xll2\n",
        "            y_left2=yll2\n",
        "\n",
        "            x_f_batch = x_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "            y_f_batch = y_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "            t_f_batch = t_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "\n",
        "\n",
        "            loss_value, mse_b, mse_f, grads, grads_ub, grads_fu = grad(u_model, x_f_batch, y_f_batch,\n",
        "                                                                       xb_batch, yb_batch,\n",
        "                                                                       ub_batch, vb_batch, weight_ub,\n",
        "                                                                       weight_fu,x_top1,x_top2,x_top3,x_top4,y_top1,y_top2,y_top3,y_top4,\n",
        "                                                                       x_bottom1,x_bottom2,x_bottom3,x_bottom4,y_bottom1,\n",
        "                                                                       y_bottom2,y_bottom3,y_bottom4,x_right,y_right,x_left1,y_left1,x_left2,y_left2)\n",
        "\n",
        "            tf_optimizer.apply_gradients(zip(grads, u_model.trainable_variables))\n",
        "            MSE_b0.append(mse_b)\n",
        "            MSE_f0.append(mse_f)\n",
        "\n",
        "            loss_history.append(loss_value)\n",
        "            loss_saman.append(loss_value)\n",
        "            list_model_fi.append(u_model)\n",
        "            #if loss_history[-1] < loss_history[-2] and loss_history[-2] < loss_history[-3] and loss_history[-1] < \\\n",
        "            #        loss_history[-10]:\n",
        "            #    tf_optimizer_weights.apply_gradients(zip([-grads_fu], [weight_fu]))\n",
        "            #    tf_optimizer_u.apply_gradients(zip([-grads_ub], [weight_ub]))\n",
        "\n",
        "        if epoch % 40 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('It: %d, Time: %.2f' % (epoch, elapsed))\n",
        "            tf.print(f\"mse_b  {mse_b}  mse_f: {mse_f}   total loss: {loss_value}\")\n",
        "\n",
        "            wu = weight_ub.numpy()\n",
        "            wf = weight_fu.numpy()\n",
        "\n",
        "            MSE_b1.append(mse_b)\n",
        "            MSE_f1.append(mse_f)\n",
        "\n",
        "            weightu.append(wu)\n",
        "            weightf.append(wf)\n",
        "\n",
        "            start_time = time.time()\n",
        "    tf.print(f\"weight_ub: {weight_ub}  weight_fu: {weight_fu}\")\n",
        "    fi_pred = predict(X_star)\n",
        "    tf.print('epoch',epoch,'loss',loss_value)\n",
        "    #error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
        "    #print('Error u: %e' % (error_u))\n",
        "    #error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
        "    #print('Error v: %e' % (error_v))\n",
        "    #print(\"Starting L-BFGS training\")\n",
        "\n",
        "    '''\n",
        "    loss_and_flat_grad = get_loss_and_flat_grad(x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch,\n",
        "                                                vb_batch, weight_ub, weight_fu)\n",
        "\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "          get_weights(u_model),\n",
        "          Struct(), maxIter=newton_iter1, learningRate=0.8)\n",
        "\n",
        "    u_pred, v_pred, p_pred = predict(X_star)\n",
        "    error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
        "    print('Error u: %e' % (error_u))\n",
        "    error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
        "    print('Error v: %e' % (error_v))\n",
        "\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "          get_weights(u_model),\n",
        "          Struct(), maxIter=newton_iter2, learningRate=0.8)\n",
        "    '''\n",
        "    return MSE_b1, MSE_f1,  weightu, weightf,loss_saman, list_model\n",
        "\n",
        "# L-BFGS implementation from https://github.com/pierremtb/PINNs-TF2.0\n",
        "def get_loss_and_flat_grad(x_f_batch, y_f_batch , xb_batch, yb_batch,ub_batch, vb_batch,weight_ub, weight_fu):\n",
        "    def loss_and_flat_grad(w):\n",
        "        with tf.GradientTape() as tape:\n",
        "            set_weights(u_model, w, sizes_w, sizes_b)\n",
        "            loss_value, _, _ = loss(x_f_batch, y_f_batch, xb_batch, yb_batch, ub_batch, vb_batch, weight_ub, weight_fu)\n",
        "        grad = tape.gradient(loss_value, u_model.trainable_variables)\n",
        "        grad_flat = []\n",
        "        for g in grad:\n",
        "            grad_flat.append(tf.reshape(g, [-1]))\n",
        "        grad_flat = tf.concat(grad_flat, 0)\n",
        "        # print(loss_value, grad_flat)\n",
        "        return loss_value, grad_flat\n",
        "\n",
        "    return loss_and_flat_grad\n",
        "\n",
        "\n",
        "def predict(X_star):\n",
        "    X_star = tf.convert_to_tensor(X_star, dtype=tf.float32)\n",
        "    fi_star= u_x_model(X_star[:, 0:1], X_star[:, 1:2])\n",
        "    return fi_star.numpy()\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "MSE_b1, MSE_f1, weightu, weightf, loss_saman, list_model_fi= fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu,sai_exact1,\n",
        "                                                   fi_exact1,u_exact1,v_exact1,p_exact1,c_exact1,T_exact1,X_star1,xtt1,\n",
        "                                                   xtt2,xtt3,xtt4,ytt1,ytt2,ytt3,ytt4,xbb1,xbb2,xbb3,xbb4,ybb1,ybb2,ybb3,ybb4,\n",
        "                                                   xrr,yrr,xll1,yll1,xll2,yll2,tf_iter=140000, tf_iter2=1000, newton_iter1=5000,newton_iter2=15000)\n",
        "\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "\n",
        "'''\n",
        "N_f = 10000\n",
        "Nu1 = 200\n",
        "\n",
        "weight_ub = tf.Variable([1.0], dtype=tf.float32)\n",
        "weight_fu = tf.Variable([1.0], dtype=tf.float32)\n",
        "\n",
        "x1 = (np.linspace(0, 1, 32)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, 32)).flatten()[:, None]\n",
        "t1 = (np.linspace(0, 1, 20)).flatten()[:, None]\n",
        "\n",
        "ttt1, ttt0 = np.meshgrid(x1, y1)\n",
        "\n",
        "tt1 = np.concatenate(([ttt1.flatten()[:, None], ttt0.flatten()[:, None], np.zeros((x1.shape[0] * y1.shape[0], 1))]), axis=1)\n",
        "x_1t = np.array([tt1[:, 0]]).T\n",
        "y_1t = np.array([tt1[:, 1]]).T\n",
        "t_1t = np.array([tt1[:, 2]]).T\n",
        "ut1 = -np.sin(t_1t) * np.sin(np.pi * x_1t) * np.sin(np.pi * x_1t) * np.sin(np.pi * y_1t) * np.cos(np.pi * y_1t)\n",
        "vt1 = np.sin(t_1t) * np.sin(np.pi * x_1t) * np.cos(np.pi * x_1t) * np.sin(np.pi * y_1t) * np.sin(np.pi * y_1t)\n",
        "\n",
        "yyy1, yyy0 = np.meshgrid(x1, t1)\n",
        "\n",
        "yy1 = np.concatenate(\n",
        "    ([yyy1.flatten()[:, None], np.min(y1) * np.ones((x1.shape[0] * t1.shape[0], 1)), yyy0.flatten()[:, None]]), axis=1)\n",
        "x_1y = np.array([yy1[:, 0]]).T\n",
        "y_1y = np.array([yy1[:, 1]]).T\n",
        "t_1y = np.array([yy1[:, 2]]).T\n",
        "uy1 = -np.sin(t_1y) * np.sin(np.pi * x_1y) * np.sin(np.pi * x_1y) * np.sin(np.pi * y_1y) * np.cos(np.pi * y_1y)\n",
        "vy1 = np.sin(t_1y) * np.sin(np.pi * x_1y) * np.cos(np.pi * x_1y) * np.sin(np.pi * y_1y) * np.sin(np.pi * y_1y)\n",
        "\n",
        "yy2 = np.concatenate(\n",
        "    ([yyy1.flatten()[:, None], np.max(y1) * np.ones((x1.shape[0] * t1.shape[0], 1)), yyy0.flatten()[:, None]]), axis=1)\n",
        "x_2y = np.array([yy2[:, 0]]).T\n",
        "y_2y = np.array([yy2[:, 1]]).T\n",
        "t_2y = np.array([yy2[:, 2]]).T\n",
        "uy2 = -np.sin(t_2y) * np.sin(np.pi * x_2y) * np.sin(np.pi * x_2y) * np.sin(np.pi * y_2y) * np.cos(np.pi * y_2y)\n",
        "vy2 = np.sin(t_2y) * np.sin(np.pi * x_2y) * np.cos(np.pi * x_2y) * np.sin(np.pi * y_2y) * np.sin(np.pi * y_2y)\n",
        "\n",
        "\n",
        "xxx1, xxx0 = np.meshgrid(y1, t1)\n",
        "\n",
        "xx1 = np.concatenate(\n",
        "    ([np.min(x1) * np.ones((y1.shape[0] * t1.shape[0], 1)), xxx1.flatten()[:, None], xxx0.flatten()[:, None]]), axis=1)\n",
        "x_1x = np.array([xx1[:, 0]]).T\n",
        "y_1x = np.array([xx1[:, 1]]).T\n",
        "t_1x = np.array([xx1[:, 2]]).T\n",
        "ux1 = -np.sin(t_1x) * np.sin(np.pi * x_1x) * np.sin(np.pi * x_1x) * np.sin(np.pi * y_1x) * np.cos(np.pi * y_1x)\n",
        "vx1 = np.sin(t_1x) * np.sin(np.pi * x_1x) * np.cos(np.pi * x_1x) * np.sin(np.pi * y_1x) * np.sin(np.pi * y_1x)\n",
        "\n",
        "xx2 = np.concatenate(\n",
        "    ([np.max(x1) * np.ones((y1.shape[0] * t1.shape[0], 1)), xxx1.flatten()[:, None], xxx0.flatten()[:, None]]), axis=1)\n",
        "x_2x = np.array([xx2[:, 0]]).T\n",
        "y_2x = np.array([xx2[:, 1]]).T\n",
        "t_2x = np.array([xx2[:, 2]]).T\n",
        "ux2 = -np.sin(t_2x) * np.sin(np.pi * x_2x) * np.sin(np.pi * x_2x) * np.sin(np.pi * y_2x) * np.cos(np.pi * y_2x)\n",
        "vx2 = np.sin(t_2x) * np.sin(np.pi * x_2x) * np.cos(np.pi * x_2x) * np.sin(np.pi * y_2x) * np.sin(np.pi * y_2x)\n",
        "\n",
        "X_u1 = np.vstack([tt1, yy1, yy2, xx1, xx2])\n",
        "u1 = np.vstack([ut1, uy1, uy2, ux1, ux2])\n",
        "v1 = np.vstack([vt1, vy1, vy2, vx1, vx2])\n",
        "\n",
        "idx_1 = np.random.choice(X_u1.shape[0], Nu1, replace=False)\n",
        "X_u_train = X_u1[idx_1, :]\n",
        "u_train = u1[idx_1, :]\n",
        "v_train = v1[idx_1, :]\n",
        "\n",
        "X1, Y1, T1 = np.meshgrid(x1, y1, t1)\n",
        "#    Exact = np.sin(np.pi*X)*np.sin(np.pi*T)*np.sin(np.pi*Z)  #100*100*100\n",
        "U_exact1 = -np.sin(T1) * np.sin(np.pi * X1) * np.sin(np.pi * X1) * np.sin(np.pi * Y1) * np.cos(np.pi * Y1)\n",
        "V_exact1 = np.sin(T1) * np.sin(np.pi * X1) * np.cos(np.pi * X1) * np.sin(np.pi * Y1) * np.sin(np.pi * Y1)\n",
        "P_exact1 = np.sin(T1) * np.sin(np.pi * X1) * np.cos(np.pi * Y1)\n",
        "\n",
        "X_star1 = np.hstack((X1.flatten()[:, None], Y1.flatten()[:, None], T1.flatten()[:, None]))\n",
        "x_star1 = np.array([X_star1[:, 0]]).T\n",
        "y_star1 = np.array([X_star1[:, 1]]).T\n",
        "t_star1 = np.array([X_star1[:, 2]]).T\n",
        "\n",
        "u_exact1 = -np.sin(t_star1) * np.sin(np.pi * x_star1) * np.sin(np.pi * x_star1) * np.sin(np.pi * y_star1) * np.cos(np.pi * y_star1)\n",
        "v_exact1 = np.sin(t_star1) * np.sin(np.pi * x_star1) * np.cos(np.pi * x_star1) * np.sin(np.pi * y_star1) * np.sin(np.pi * y_star1)\n",
        "p_exact1 = np.sin(t_star1) * np.sin(np.pi * x_star1) * np.cos(np.pi * y_star1)\n",
        "\n",
        "lb1 = X_star1.min(0)\n",
        "ub1 = X_star1.max(0)\n",
        "\n",
        "X_f_train11 = lb1 + (ub1 - lb1) * lhs(3, N_f)\n",
        "X_f = np.vstack((X_f_train11, X_u_train))\n",
        "\n",
        "xb = tf.cast(X_u_train[:, 0:1], dtype=tf.float32)\n",
        "yb = tf.cast(X_u_train[:, 1:2], dtype=tf.float32)\n",
        "tb = tf.cast(X_u_train[:, 2:3], dtype=tf.float32)\n",
        "ub = tf.cast(u_train[:, 0:1], dtype=tf.float32)\n",
        "vb = tf.cast(v_train[:, 0:1], dtype=tf.float32)\n",
        "\n",
        "\n",
        "lb = X_star1.min(0)\n",
        "rb = X_star1.max(0)\n",
        "\n",
        "x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=tf.float32)\n",
        "y_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=tf.float32)\n",
        "t_f = tf.convert_to_tensor(X_f[:, 2:3], dtype=tf.float32)\n",
        "\n",
        "start_time = time.time()\n",
        "MSE_b1, MSE_f1, weightu, weightf = fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star1, tf_iter=10000, tf_iter2=1000, newton_iter1=5000,newton_iter2=15000)\n",
        "\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "u_pred, v_pred, p_pred = predict(X_star1)\n",
        "\n",
        "U_pred = u_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
        "V_pred = v_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
        "P_pred = p_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
        "\n",
        "error_uu = np.abs(u_exact1 - u_pred)\n",
        "error_vv = np.abs(v_exact1 - v_pred)\n",
        "error_pp = np.abs(p_exact1 - p_pred)\n",
        "\n",
        "error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
        "print('Error u: %e' % (error_u))\n",
        "\n",
        "error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
        "print('Error v: %e' % (error_v))\n",
        "\n",
        "error_p = np.linalg.norm(p_exact1 - p_pred, 2) / np.linalg.norm(p_exact1, 2)\n",
        "print('Error p: %e' % (error_p))\n",
        "\n",
        "dataNewNS = 'D://NS_hisyory.mat'\n",
        "scipy.io.savemat(dataNewNS, {'w_MSE_b': MSE_b1, 'w_MSE_f': MSE_f1, 'weight_u': weightu,\n",
        "                  'weight_f': weightf, 'U_pred': U_pred, 'V_pred': V_pred, 'P_pred': P_pred})\n",
        "'''"
      ],
      "metadata": {
        "id": "KBEILCSZ1jSd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f04ed8d6-3d34-45a9-9e6b-8149ad9488a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xxxxxxxxxxxxxx\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 2)]               0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 20)                60        \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 7)                 147       \n",
            "                                                                 \n",
            " tf.__operators__.getitem_6   (None, 1)                0         \n",
            " (SlicingOpLambda)                                               \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,727\n",
            "Trainable params: 2,727\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "weight_ub: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([10000.], dtype=float32)>  weight_fu: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>\n",
            "starting Adam training\n",
            "It: 0, Time: 34.36\n",
            "mse_b  [906030.75]  mse_f: [165.94037]   total loss: [906196.7]\n",
            "It: 40, Time: 2.74\n",
            "mse_b  [9882.293]  mse_f: [4853.1377]   total loss: [14735.431]\n",
            "It: 80, Time: 2.76\n",
            "mse_b  [1446.9404]  mse_f: [1055.6455]   total loss: [2502.586]\n",
            "It: 120, Time: 2.77\n",
            "mse_b  [2450.2078]  mse_f: [269.32068]   total loss: [2719.5283]\n",
            "It: 160, Time: 2.74\n",
            "mse_b  [1960.897]  mse_f: [307.9548]   total loss: [2268.8518]\n",
            "It: 200, Time: 2.77\n",
            "mse_b  [220.37666]  mse_f: [293.102]   total loss: [513.47864]\n",
            "It: 240, Time: 2.72\n",
            "mse_b  [987.7238]  mse_f: [249.62642]   total loss: [1237.3502]\n",
            "It: 280, Time: 4.04\n",
            "mse_b  [994.28015]  mse_f: [260.29846]   total loss: [1254.5786]\n",
            "It: 320, Time: 2.70\n",
            "mse_b  [416.22012]  mse_f: [248.64816]   total loss: [664.8683]\n",
            "It: 360, Time: 2.72\n",
            "mse_b  [229.04446]  mse_f: [201.52182]   total loss: [430.56628]\n",
            "It: 400, Time: 2.69\n",
            "mse_b  [318.35144]  mse_f: [173.16794]   total loss: [491.51938]\n",
            "It: 440, Time: 2.70\n",
            "mse_b  [176.40417]  mse_f: [133.33174]   total loss: [309.7359]\n",
            "It: 480, Time: 2.72\n",
            "mse_b  [81.49682]  mse_f: [110.90544]   total loss: [192.40225]\n",
            "It: 520, Time: 2.77\n",
            "mse_b  [84.33139]  mse_f: [91.169586]   total loss: [175.50098]\n",
            "It: 560, Time: 2.73\n",
            "mse_b  [82.82838]  mse_f: [69.487885]   total loss: [152.31625]\n",
            "It: 600, Time: 2.69\n",
            "mse_b  [57.15394]  mse_f: [59.55631]   total loss: [116.71025]\n",
            "It: 640, Time: 2.69\n",
            "mse_b  [33.223633]  mse_f: [48.805893]   total loss: [82.029526]\n",
            "It: 680, Time: 2.69\n",
            "mse_b  [17.008667]  mse_f: [42.150784]   total loss: [59.15945]\n",
            "It: 720, Time: 2.65\n",
            "mse_b  [10.46777]  mse_f: [35.95762]   total loss: [46.42539]\n",
            "It: 760, Time: 2.62\n",
            "mse_b  [11.468477]  mse_f: [31.034061]   total loss: [42.50254]\n",
            "It: 800, Time: 2.59\n",
            "mse_b  [7.4265733]  mse_f: [26.95985]   total loss: [34.386425]\n",
            "It: 840, Time: 2.58\n",
            "mse_b  [5.559876]  mse_f: [23.933496]   total loss: [29.493372]\n",
            "It: 880, Time: 2.55\n",
            "mse_b  [5.9789014]  mse_f: [21.379189]   total loss: [27.35809]\n",
            "It: 920, Time: 2.60\n",
            "mse_b  [5.445931]  mse_f: [19.22629]   total loss: [24.67222]\n",
            "It: 960, Time: 2.57\n",
            "mse_b  [5.14592]  mse_f: [17.4501]   total loss: [22.59602]\n",
            "It: 1000, Time: 2.61\n",
            "mse_b  [4.5823607]  mse_f: [15.99916]   total loss: [20.58152]\n",
            "It: 1040, Time: 2.98\n",
            "mse_b  [3.648331]  mse_f: [14.825954]   total loss: [18.474285]\n",
            "It: 1080, Time: 2.62\n",
            "mse_b  [2.7596557]  mse_f: [13.855872]   total loss: [16.615528]\n",
            "It: 1120, Time: 2.51\n",
            "mse_b  [2.5019972]  mse_f: [12.990021]   total loss: [15.492018]\n",
            "It: 1160, Time: 2.49\n",
            "mse_b  [2.320578]  mse_f: [12.187822]   total loss: [14.5084]\n",
            "It: 1200, Time: 2.46\n",
            "mse_b  [1.8998535]  mse_f: [11.459993]   total loss: [13.359847]\n",
            "It: 1240, Time: 2.43\n",
            "mse_b  [1.6997622]  mse_f: [10.795216]   total loss: [12.494978]\n",
            "It: 1280, Time: 2.48\n",
            "mse_b  [1.5442653]  mse_f: [10.196266]   total loss: [11.740532]\n",
            "It: 1320, Time: 2.43\n",
            "mse_b  [1.3572663]  mse_f: [9.622398]   total loss: [10.979665]\n",
            "It: 1360, Time: 2.42\n",
            "mse_b  [1.2832179]  mse_f: [9.089825]   total loss: [10.373043]\n",
            "It: 1400, Time: 2.42\n",
            "mse_b  [1.1726382]  mse_f: [8.595683]   total loss: [9.768321]\n",
            "It: 1440, Time: 2.44\n",
            "mse_b  [1.114129]  mse_f: [8.144164]   total loss: [9.258293]\n",
            "It: 1480, Time: 2.43\n",
            "mse_b  [1.0398362]  mse_f: [7.719426]   total loss: [8.759262]\n",
            "It: 1520, Time: 2.36\n",
            "mse_b  [0.9804851]  mse_f: [7.330918]   total loss: [8.311403]\n",
            "It: 1560, Time: 2.36\n",
            "mse_b  [0.9289434]  mse_f: [6.962039]   total loss: [7.8909826]\n",
            "It: 1600, Time: 2.35\n",
            "mse_b  [0.8826244]  mse_f: [6.615438]   total loss: [7.498062]\n",
            "It: 1640, Time: 2.36\n",
            "mse_b  [0.847918]  mse_f: [6.2868376]   total loss: [7.1347556]\n",
            "It: 1680, Time: 2.33\n",
            "mse_b  [0.81509876]  mse_f: [5.975252]   total loss: [6.790351]\n",
            "It: 1720, Time: 2.34\n",
            "mse_b  [0.7870852]  mse_f: [5.6795807]   total loss: [6.4666657]\n",
            "It: 1760, Time: 2.32\n",
            "mse_b  [0.762629]  mse_f: [5.398628]   total loss: [6.1612573]\n",
            "It: 1800, Time: 2.31\n",
            "mse_b  [0.7403975]  mse_f: [5.132208]   total loss: [5.8726053]\n",
            "It: 1840, Time: 2.29\n",
            "mse_b  [0.7178494]  mse_f: [4.880599]   total loss: [5.5984483]\n",
            "It: 1880, Time: 2.29\n",
            "mse_b  [0.69533867]  mse_f: [4.6424527]   total loss: [5.3377914]\n",
            "It: 1920, Time: 2.78\n",
            "mse_b  [0.6727534]  mse_f: [4.4171677]   total loss: [5.089921]\n",
            "It: 1960, Time: 2.29\n",
            "mse_b  [0.65057904]  mse_f: [4.203106]   total loss: [4.853685]\n",
            "It: 2000, Time: 2.41\n",
            "mse_b  [0.6290525]  mse_f: [3.9992785]   total loss: [4.628331]\n",
            "It: 2040, Time: 2.36\n",
            "mse_b  [0.60798126]  mse_f: [3.80514]   total loss: [4.413121]\n",
            "It: 2080, Time: 2.34\n",
            "mse_b  [0.5874813]  mse_f: [3.619967]   total loss: [4.2074485]\n",
            "It: 2120, Time: 2.32\n",
            "mse_b  [0.5674866]  mse_f: [3.4433064]   total loss: [4.010793]\n",
            "It: 2160, Time: 2.34\n",
            "mse_b  [0.547518]  mse_f: [3.2751856]   total loss: [3.8227036]\n",
            "It: 2200, Time: 2.30\n",
            "mse_b  [0.5279186]  mse_f: [3.1148314]   total loss: [3.64275]\n",
            "It: 2240, Time: 2.28\n",
            "mse_b  [0.5083455]  mse_f: [2.9622953]   total loss: [3.4706407]\n",
            "It: 2280, Time: 2.31\n",
            "mse_b  [0.4889785]  mse_f: [2.8170772]   total loss: [3.3060555]\n",
            "It: 2320, Time: 2.32\n",
            "mse_b  [0.46982947]  mse_f: [2.6788626]   total loss: [3.1486921]\n",
            "It: 2360, Time: 2.29\n",
            "mse_b  [0.45120776]  mse_f: [2.5471158]   total loss: [2.9983234]\n",
            "It: 2400, Time: 2.28\n",
            "mse_b  [0.43300173]  mse_f: [2.4217317]   total loss: [2.8547335]\n",
            "It: 2440, Time: 2.39\n",
            "mse_b  [0.4152133]  mse_f: [2.3024497]   total loss: [2.717663]\n",
            "It: 2480, Time: 2.30\n",
            "mse_b  [0.3979358]  mse_f: [2.1890376]   total loss: [2.5869734]\n",
            "It: 2520, Time: 2.40\n",
            "mse_b  [0.38121042]  mse_f: [2.0812206]   total loss: [2.462431]\n",
            "It: 2560, Time: 2.29\n",
            "mse_b  [0.36504814]  mse_f: [1.9788146]   total loss: [2.3438628]\n",
            "It: 2600, Time: 2.27\n",
            "mse_b  [0.3494293]  mse_f: [1.8816539]   total loss: [2.2310832]\n",
            "It: 2640, Time: 2.31\n",
            "mse_b  [0.33447987]  mse_f: [1.7894464]   total loss: [2.1239262]\n",
            "It: 2680, Time: 2.35\n",
            "mse_b  [0.3201534]  mse_f: [1.7020512]   total loss: [2.0222046]\n",
            "It: 2720, Time: 2.38\n",
            "mse_b  [0.30648324]  mse_f: [1.6192353]   total loss: [1.9257185]\n",
            "It: 2760, Time: 2.32\n",
            "mse_b  [0.29345527]  mse_f: [1.5408611]   total loss: [1.8343164]\n",
            "It: 2800, Time: 2.34\n",
            "mse_b  [0.28108913]  mse_f: [1.4667103]   total loss: [1.7477994]\n",
            "It: 2840, Time: 2.33\n",
            "mse_b  [0.26932678]  mse_f: [1.3966497]   total loss: [1.6659765]\n",
            "It: 2880, Time: 2.35\n",
            "mse_b  [0.25821903]  mse_f: [1.330445]   total loss: [1.588664]\n",
            "It: 2920, Time: 2.32\n",
            "mse_b  [0.24766734]  mse_f: [1.2679923]   total loss: [1.5156596]\n",
            "It: 2960, Time: 2.28\n",
            "mse_b  [0.23775868]  mse_f: [1.2090373]   total loss: [1.446796]\n",
            "It: 3000, Time: 2.36\n",
            "mse_b  [0.2284265]  mse_f: [1.1534526]   total loss: [1.3818791]\n",
            "It: 3040, Time: 2.41\n",
            "mse_b  [0.21963973]  mse_f: [1.1010706]   total loss: [1.3207104]\n",
            "It: 3080, Time: 2.36\n",
            "mse_b  [0.51711154]  mse_f: [1.0468645]   total loss: [1.563976]\n",
            "It: 3120, Time: 2.36\n",
            "mse_b  [0.6272917]  mse_f: [1.0068309]   total loss: [1.6341226]\n",
            "It: 3160, Time: 2.33\n",
            "mse_b  [1.4292388]  mse_f: [0.97433543]   total loss: [2.4035742]\n",
            "It: 3200, Time: 2.43\n",
            "mse_b  [0.19161627]  mse_f: [0.958032]   total loss: [1.1496483]\n",
            "It: 3240, Time: 2.38\n",
            "mse_b  [0.5250488]  mse_f: [0.935532]   total loss: [1.4605808]\n",
            "It: 3280, Time: 2.37\n",
            "mse_b  [0.17668937]  mse_f: [0.9032263]   total loss: [1.0799156]\n",
            "It: 3320, Time: 2.35\n",
            "mse_b  [0.78852326]  mse_f: [0.86792415]   total loss: [1.6564474]\n",
            "It: 3360, Time: 2.38\n",
            "mse_b  [0.2789548]  mse_f: [0.84530044]   total loss: [1.1242552]\n",
            "It: 3400, Time: 2.37\n",
            "mse_b  [0.7531881]  mse_f: [0.80841994]   total loss: [1.5616081]\n",
            "It: 3440, Time: 2.33\n",
            "mse_b  [0.1582023]  mse_f: [0.78386414]   total loss: [0.94206643]\n",
            "It: 3480, Time: 2.33\n",
            "mse_b  [0.15260361]  mse_f: [0.75599015]   total loss: [0.9085938]\n",
            "It: 3520, Time: 2.34\n",
            "mse_b  [1.4990349]  mse_f: [0.7368932]   total loss: [2.235928]\n",
            "It: 3560, Time: 2.39\n",
            "mse_b  [0.15561958]  mse_f: [0.70618224]   total loss: [0.8618018]\n",
            "It: 3600, Time: 2.35\n",
            "mse_b  [1.1328455]  mse_f: [0.71082056]   total loss: [1.8436661]\n",
            "It: 3640, Time: 2.36\n",
            "mse_b  [3.9862742]  mse_f: [0.7130003]   total loss: [4.6992745]\n",
            "It: 3680, Time: 2.34\n",
            "mse_b  [2.9282064]  mse_f: [0.7261413]   total loss: [3.6543477]\n",
            "It: 3720, Time: 2.28\n",
            "mse_b  [0.1363446]  mse_f: [0.72704226]   total loss: [0.86338687]\n",
            "It: 3760, Time: 2.38\n",
            "mse_b  [2.3531437]  mse_f: [0.7134659]   total loss: [3.0666096]\n",
            "It: 3800, Time: 2.75\n",
            "mse_b  [1.6340408]  mse_f: [0.6740094]   total loss: [2.3080502]\n",
            "It: 3840, Time: 2.31\n",
            "mse_b  [0.12869708]  mse_f: [0.6509235]   total loss: [0.7796206]\n",
            "It: 3880, Time: 2.33\n",
            "mse_b  [1.0622383]  mse_f: [0.61868525]   total loss: [1.6809236]\n",
            "It: 3920, Time: 2.39\n",
            "mse_b  [0.14308633]  mse_f: [0.5987657]   total loss: [0.741852]\n",
            "It: 3960, Time: 2.32\n",
            "mse_b  [0.6263922]  mse_f: [0.57258034]   total loss: [1.1989725]\n",
            "It: 4000, Time: 2.36\n",
            "mse_b  [0.15019205]  mse_f: [0.5567971]   total loss: [0.70698917]\n",
            "It: 4040, Time: 2.45\n",
            "mse_b  [0.7396248]  mse_f: [0.54075605]   total loss: [1.2803808]\n",
            "It: 4080, Time: 2.42\n",
            "mse_b  [7.478301]  mse_f: [0.52783495]   total loss: [8.006136]\n",
            "It: 4120, Time: 2.37\n",
            "mse_b  [2.4280777]  mse_f: [0.6218128]   total loss: [3.0498905]\n",
            "It: 4160, Time: 2.38\n",
            "mse_b  [0.7980327]  mse_f: [0.7115397]   total loss: [1.5095724]\n",
            "It: 4200, Time: 2.35\n",
            "mse_b  [0.18227977]  mse_f: [0.742084]   total loss: [0.9243638]\n",
            "It: 4240, Time: 2.34\n",
            "mse_b  [0.12655462]  mse_f: [0.7236481]   total loss: [0.8502027]\n",
            "It: 4280, Time: 2.38\n",
            "mse_b  [1.1153733]  mse_f: [0.67637444]   total loss: [1.7917477]\n",
            "It: 4320, Time: 2.39\n",
            "mse_b  [0.60028315]  mse_f: [0.6242361]   total loss: [1.2245193]\n",
            "It: 4360, Time: 2.39\n",
            "mse_b  [0.24254909]  mse_f: [0.57196856]   total loss: [0.8145176]\n",
            "It: 4400, Time: 2.38\n",
            "mse_b  [0.18160826]  mse_f: [0.5284377]   total loss: [0.71004593]\n",
            "It: 4440, Time: 2.36\n",
            "mse_b  [0.19054776]  mse_f: [0.49663335]   total loss: [0.6871811]\n",
            "It: 4480, Time: 2.36\n",
            "mse_b  [0.62072796]  mse_f: [0.47109896]   total loss: [1.0918269]\n",
            "It: 4520, Time: 2.31\n",
            "mse_b  [0.56733656]  mse_f: [0.4566293]   total loss: [1.0239658]\n",
            "It: 4560, Time: 2.43\n",
            "mse_b  [0.45118183]  mse_f: [0.43708858]   total loss: [0.8882704]\n",
            "It: 4600, Time: 2.37\n",
            "mse_b  [0.35182232]  mse_f: [0.4147706]   total loss: [0.7665929]\n",
            "It: 4640, Time: 2.30\n",
            "mse_b  [0.10553227]  mse_f: [0.3998498]   total loss: [0.50538206]\n",
            "It: 4680, Time: 2.27\n",
            "mse_b  [1.4603467]  mse_f: [0.38316464]   total loss: [1.8435113]\n",
            "It: 4720, Time: 2.29\n",
            "mse_b  [6.8722734]  mse_f: [0.3907897]   total loss: [7.263063]\n",
            "It: 4760, Time: 2.27\n",
            "mse_b  [0.2622646]  mse_f: [0.42566627]   total loss: [0.6879309]\n",
            "It: 4800, Time: 2.31\n",
            "mse_b  [0.90069807]  mse_f: [0.4292475]   total loss: [1.3299456]\n",
            "It: 4840, Time: 2.27\n",
            "mse_b  [0.44791216]  mse_f: [0.4229137]   total loss: [0.8708259]\n",
            "It: 4880, Time: 2.28\n",
            "mse_b  [2.1131272]  mse_f: [0.40551746]   total loss: [2.5186448]\n",
            "It: 4920, Time: 2.28\n",
            "mse_b  [0.11957747]  mse_f: [0.3781129]   total loss: [0.49769038]\n",
            "It: 4960, Time: 2.28\n",
            "mse_b  [1.1247299]  mse_f: [0.35788038]   total loss: [1.4826102]\n",
            "It: 5000, Time: 2.24\n",
            "mse_b  [2.8185265]  mse_f: [0.34178758]   total loss: [3.160314]\n",
            "It: 5040, Time: 2.31\n",
            "mse_b  [19.5437]  mse_f: [0.34223875]   total loss: [19.885939]\n",
            "It: 5080, Time: 2.32\n",
            "mse_b  [5.9511037]  mse_f: [0.36560008]   total loss: [6.316704]\n",
            "It: 5120, Time: 2.42\n",
            "mse_b  [4.348337]  mse_f: [0.39194858]   total loss: [4.740286]\n",
            "It: 5160, Time: 2.35\n",
            "mse_b  [1.9684051]  mse_f: [0.40719157]   total loss: [2.3755968]\n",
            "It: 5200, Time: 2.37\n",
            "mse_b  [0.6349684]  mse_f: [0.39764923]   total loss: [1.0326176]\n",
            "It: 5240, Time: 2.34\n",
            "mse_b  [2.4181075]  mse_f: [0.37179095]   total loss: [2.7898984]\n",
            "It: 5280, Time: 2.27\n",
            "mse_b  [1.9273992]  mse_f: [0.34913498]   total loss: [2.276534]\n",
            "It: 5320, Time: 2.31\n",
            "mse_b  [1.6256237]  mse_f: [0.31903023]   total loss: [1.944654]\n",
            "It: 5360, Time: 2.32\n",
            "mse_b  [0.4943018]  mse_f: [0.2909568]   total loss: [0.7852586]\n",
            "It: 5400, Time: 2.30\n",
            "mse_b  [0.13058026]  mse_f: [0.2770604]   total loss: [0.40764064]\n",
            "It: 5440, Time: 2.31\n",
            "mse_b  [4.4895763]  mse_f: [0.2610193]   total loss: [4.7505956]\n",
            "It: 5480, Time: 2.31\n",
            "mse_b  [0.37748933]  mse_f: [0.267636]   total loss: [0.6451253]\n",
            "It: 5520, Time: 2.34\n",
            "mse_b  [2.5333757]  mse_f: [0.27335393]   total loss: [2.8067298]\n",
            "It: 5560, Time: 2.28\n",
            "mse_b  [3.299736]  mse_f: [0.2729001]   total loss: [3.5726361]\n",
            "It: 5600, Time: 2.26\n",
            "mse_b  [1.6564218]  mse_f: [0.259637]   total loss: [1.9160588]\n",
            "It: 5640, Time: 2.36\n",
            "mse_b  [0.37235314]  mse_f: [0.25533727]   total loss: [0.62769043]\n",
            "It: 5680, Time: 2.30\n",
            "mse_b  [0.15407774]  mse_f: [0.2471003]   total loss: [0.40117803]\n",
            "It: 5720, Time: 2.30\n",
            "mse_b  [5.576074]  mse_f: [0.25324744]   total loss: [5.8293214]\n",
            "It: 5760, Time: 2.31\n",
            "mse_b  [4.6886315]  mse_f: [0.24188027]   total loss: [4.930512]\n",
            "It: 5800, Time: 2.29\n",
            "mse_b  [3.8995867]  mse_f: [0.24717465]   total loss: [4.1467614]\n",
            "It: 5840, Time: 2.28\n",
            "mse_b  [0.11954763]  mse_f: [0.2360499]   total loss: [0.35559753]\n",
            "It: 5880, Time: 2.29\n",
            "mse_b  [3.1219885]  mse_f: [0.22572875]   total loss: [3.3477173]\n",
            "It: 5920, Time: 2.26\n",
            "mse_b  [4.1505685]  mse_f: [0.22186016]   total loss: [4.3724284]\n",
            "It: 5960, Time: 2.30\n",
            "mse_b  [3.0655313]  mse_f: [0.22851768]   total loss: [3.294049]\n",
            "It: 6000, Time: 2.38\n",
            "mse_b  [0.17781645]  mse_f: [0.22170201]   total loss: [0.39951846]\n",
            "It: 6040, Time: 2.31\n",
            "mse_b  [5.0226345]  mse_f: [0.21077922]   total loss: [5.2334137]\n",
            "It: 6080, Time: 2.29\n",
            "mse_b  [0.15612899]  mse_f: [0.21038517]   total loss: [0.36651415]\n",
            "It: 6120, Time: 2.34\n",
            "mse_b  [4.856504]  mse_f: [0.21217977]   total loss: [5.0686836]\n",
            "It: 6160, Time: 2.38\n",
            "mse_b  [0.1433435]  mse_f: [0.20510024]   total loss: [0.34844375]\n",
            "It: 6200, Time: 2.38\n",
            "mse_b  [4.9956183]  mse_f: [0.1960648]   total loss: [5.1916833]\n",
            "It: 6240, Time: 2.42\n",
            "mse_b  [0.2470146]  mse_f: [0.19782045]   total loss: [0.44483507]\n",
            "It: 6280, Time: 2.43\n",
            "mse_b  [5.003288]  mse_f: [0.19752169]   total loss: [5.2008095]\n",
            "It: 6320, Time: 2.34\n",
            "mse_b  [0.21814573]  mse_f: [0.18868653]   total loss: [0.40683228]\n",
            "It: 6360, Time: 2.35\n",
            "mse_b  [4.694442]  mse_f: [0.1818676]   total loss: [4.8763094]\n",
            "It: 6400, Time: 2.36\n",
            "mse_b  [0.73442394]  mse_f: [0.18479808]   total loss: [0.919222]\n",
            "It: 6440, Time: 2.38\n",
            "mse_b  [4.3848453]  mse_f: [0.18368597]   total loss: [4.568531]\n",
            "It: 6480, Time: 2.37\n",
            "mse_b  [0.6911093]  mse_f: [0.17459698]   total loss: [0.86570626]\n",
            "It: 6520, Time: 2.35\n",
            "mse_b  [4.2908273]  mse_f: [0.16909131]   total loss: [4.4599185]\n",
            "It: 6560, Time: 2.46\n",
            "mse_b  [1.2485183]  mse_f: [0.17257118]   total loss: [1.4210895]\n",
            "It: 6600, Time: 2.53\n",
            "mse_b  [3.3699605]  mse_f: [0.17090505]   total loss: [3.5408657]\n",
            "It: 6640, Time: 2.73\n",
            "mse_b  [1.8564223]  mse_f: [0.16168074]   total loss: [2.0181031]\n",
            "It: 6680, Time: 2.38\n",
            "mse_b  [3.119718]  mse_f: [0.15797187]   total loss: [3.27769]\n",
            "It: 6720, Time: 2.38\n",
            "mse_b  [2.0967088]  mse_f: [0.16132173]   total loss: [2.2580304]\n",
            "It: 6760, Time: 2.38\n",
            "mse_b  [2.5344796]  mse_f: [0.15893677]   total loss: [2.6934164]\n",
            "It: 6800, Time: 2.37\n",
            "mse_b  [2.6542149]  mse_f: [0.15031539]   total loss: [2.8045301]\n",
            "It: 6840, Time: 2.36\n",
            "mse_b  [2.1878097]  mse_f: [0.14799294]   total loss: [2.3358026]\n",
            "It: 6880, Time: 2.37\n",
            "mse_b  [2.7918158]  mse_f: [0.15117139]   total loss: [2.9429872]\n",
            "It: 6920, Time: 2.38\n",
            "mse_b  [1.8806099]  mse_f: [0.14831631]   total loss: [2.0289261]\n",
            "It: 6960, Time: 2.35\n",
            "mse_b  [3.2203522]  mse_f: [0.14038491]   total loss: [3.360737]\n",
            "It: 7000, Time: 2.34\n",
            "mse_b  [1.3975785]  mse_f: [0.1392695]   total loss: [1.536848]\n",
            "It: 7040, Time: 2.36\n",
            "mse_b  [3.5565193]  mse_f: [0.14229546]   total loss: [3.6988146]\n",
            "It: 7080, Time: 2.35\n",
            "mse_b  [1.0699544]  mse_f: [0.13874424]   total loss: [1.2086986]\n",
            "It: 7120, Time: 2.32\n",
            "mse_b  [3.7828681]  mse_f: [0.13157207]   total loss: [3.9144402]\n",
            "It: 7160, Time: 2.39\n",
            "mse_b  [0.9385946]  mse_f: [0.13132185]   total loss: [1.0699165]\n",
            "It: 7200, Time: 2.29\n",
            "mse_b  [3.7747517]  mse_f: [0.13415453]   total loss: [3.9089062]\n",
            "It: 7240, Time: 2.31\n",
            "mse_b  [0.84878874]  mse_f: [0.13077092]   total loss: [0.97955966]\n",
            "It: 7280, Time: 2.42\n",
            "mse_b  [4.0364733]  mse_f: [0.12426324]   total loss: [4.1607366]\n",
            "It: 7320, Time: 2.35\n",
            "mse_b  [0.4823033]  mse_f: [0.12482196]   total loss: [0.6071253]\n",
            "It: 7360, Time: 2.34\n",
            "mse_b  [4.2395296]  mse_f: [0.12741038]   total loss: [4.36694]\n",
            "It: 7400, Time: 2.32\n",
            "mse_b  [0.33109593]  mse_f: [0.12357436]   total loss: [0.4546703]\n",
            "It: 7440, Time: 2.37\n",
            "mse_b  [4.3696313]  mse_f: [0.11784278]   total loss: [4.487474]\n",
            "It: 7480, Time: 2.34\n",
            "mse_b  [0.19694024]  mse_f: [0.11916196]   total loss: [0.3161022]\n",
            "It: 7520, Time: 2.35\n",
            "mse_b  [4.389179]  mse_f: [0.12149404]   total loss: [4.510673]\n",
            "It: 7560, Time: 2.31\n",
            "mse_b  [0.18856399]  mse_f: [0.11770502]   total loss: [0.306269]\n",
            "It: 7600, Time: 2.32\n",
            "mse_b  [4.37514]  mse_f: [0.11260968]   total loss: [4.48775]\n",
            "It: 7640, Time: 2.32\n",
            "mse_b  [0.16318722]  mse_f: [0.11423945]   total loss: [0.27742666]\n",
            "It: 7680, Time: 2.39\n",
            "mse_b  [4.4222383]  mse_f: [0.11675194]   total loss: [4.5389905]\n",
            "It: 7720, Time: 2.30\n",
            "mse_b  [0.05117698]  mse_f: [0.11296467]   total loss: [0.16414165]\n",
            "It: 7760, Time: 2.34\n",
            "mse_b  [4.468866]  mse_f: [0.10852401]   total loss: [4.5773897]\n",
            "It: 7800, Time: 2.36\n",
            "mse_b  [0.03593751]  mse_f: [0.11078812]   total loss: [0.14672562]\n",
            "It: 7840, Time: 2.29\n",
            "mse_b  [4.465015]  mse_f: [0.11266726]   total loss: [4.577682]\n",
            "It: 7880, Time: 2.33\n",
            "mse_b  [0.03584314]  mse_f: [0.10889072]   total loss: [0.14473386]\n",
            "It: 7920, Time: 2.28\n",
            "mse_b  [4.4022603]  mse_f: [0.1051477]   total loss: [4.507408]\n",
            "It: 7960, Time: 2.31\n",
            "mse_b  [0.07554151]  mse_f: [0.10781562]   total loss: [0.18335713]\n",
            "It: 8000, Time: 2.32\n",
            "mse_b  [4.359772]  mse_f: [0.10959119]   total loss: [4.469363]\n",
            "It: 8040, Time: 2.33\n",
            "mse_b  [0.0933749]  mse_f: [0.10582139]   total loss: [0.19919628]\n",
            "It: 8080, Time: 2.33\n",
            "mse_b  [4.2911425]  mse_f: [0.10258739]   total loss: [4.3937297]\n",
            "It: 8120, Time: 2.34\n",
            "mse_b  [0.15334953]  mse_f: [0.10554353]   total loss: [0.25889307]\n",
            "It: 8160, Time: 2.37\n",
            "mse_b  [4.177958]  mse_f: [0.10723974]   total loss: [4.2851977]\n",
            "It: 8200, Time: 2.35\n",
            "mse_b  [0.25013983]  mse_f: [0.10341695]   total loss: [0.35355678]\n",
            "It: 8240, Time: 2.37\n",
            "mse_b  [4.084909]  mse_f: [0.10072722]   total loss: [4.185636]\n",
            "It: 8280, Time: 2.39\n",
            "mse_b  [0.34209627]  mse_f: [0.1039827]   total loss: [0.44607896]\n",
            "It: 8320, Time: 2.41\n",
            "mse_b  [3.8723547]  mse_f: [0.10543291]   total loss: [3.9777877]\n",
            "It: 8360, Time: 2.37\n",
            "mse_b  [0.549965]  mse_f: [0.10152943]   total loss: [0.65149444]\n",
            "It: 8400, Time: 2.35\n",
            "mse_b  [3.7078192]  mse_f: [0.09946234]   total loss: [3.8072815]\n",
            "It: 8440, Time: 2.32\n",
            "mse_b  [0.6112444]  mse_f: [0.10286294]   total loss: [0.71410733]\n",
            "It: 8480, Time: 2.34\n",
            "mse_b  [3.5933442]  mse_f: [0.10404471]   total loss: [3.697389]\n",
            "It: 8520, Time: 2.31\n",
            "mse_b  [0.7850348]  mse_f: [0.10025296]   total loss: [0.88528776]\n",
            "It: 8560, Time: 2.32\n",
            "mse_b  [3.316108]  mse_f: [0.09872003]   total loss: [3.414828]\n",
            "It: 8600, Time: 2.32\n",
            "mse_b  [1.0385458]  mse_f: [0.10233501]   total loss: [1.1408808]\n",
            "It: 8640, Time: 2.31\n",
            "mse_b  [3.0179152]  mse_f: [0.10304158]   total loss: [3.120957]\n",
            "It: 8680, Time: 2.33\n",
            "mse_b  [1.2868071]  mse_f: [0.09915216]   total loss: [1.3859593]\n",
            "It: 8720, Time: 2.36\n",
            "mse_b  [2.7449803]  mse_f: [0.09828563]   total loss: [2.843266]\n",
            "It: 8760, Time: 2.33\n",
            "mse_b  [1.5412838]  mse_f: [0.10200138]   total loss: [1.6432853]\n",
            "It: 8800, Time: 2.29\n",
            "mse_b  [2.4539695]  mse_f: [0.1022574]   total loss: [2.556227]\n",
            "It: 8840, Time: 2.33\n",
            "mse_b  [1.7509986]  mse_f: [0.09840623]   total loss: [1.8494048]\n",
            "It: 8880, Time: 2.35\n",
            "mse_b  [2.1682448]  mse_f: [0.09815685]   total loss: [2.2664018]\n",
            "It: 8920, Time: 2.35\n",
            "mse_b  [2.0918925]  mse_f: [0.10190621]   total loss: [2.1937988]\n",
            "It: 8960, Time: 2.33\n",
            "mse_b  [1.7383819]  mse_f: [0.10166202]   total loss: [1.8400439]\n",
            "It: 9000, Time: 2.29\n",
            "mse_b  [2.4023452]  mse_f: [0.09781297]   total loss: [2.500158]\n",
            "It: 9040, Time: 2.31\n",
            "mse_b  [1.4014531]  mse_f: [0.09828848]   total loss: [1.4997416]\n",
            "It: 9080, Time: 2.27\n",
            "mse_b  [2.6997566]  mse_f: [0.10184357]   total loss: [2.8016002]\n",
            "It: 9120, Time: 2.30\n",
            "mse_b  [1.1033701]  mse_f: [0.10109001]   total loss: [1.2044601]\n",
            "It: 9160, Time: 2.30\n",
            "mse_b  [2.9948266]  mse_f: [0.0973891]   total loss: [3.0922155]\n",
            "It: 9200, Time: 2.37\n",
            "mse_b  [0.70145655]  mse_f: [0.09860811]   total loss: [0.8000647]\n",
            "It: 9240, Time: 2.40\n",
            "mse_b  [3.3069434]  mse_f: [0.10189555]   total loss: [3.408839]\n",
            "It: 9280, Time: 2.32\n",
            "mse_b  [0.42823288]  mse_f: [0.10041234]   total loss: [0.5286452]\n",
            "It: 9320, Time: 2.35\n",
            "mse_b  [3.547823]  mse_f: [0.09696509]   total loss: [3.644788]\n",
            "It: 9360, Time: 2.29\n",
            "mse_b  [0.15997829]  mse_f: [0.09907492]   total loss: [0.2590532]\n",
            "It: 9400, Time: 2.24\n",
            "mse_b  [3.7479608]  mse_f: [0.10191258]   total loss: [3.8498733]\n",
            "It: 9440, Time: 2.40\n",
            "mse_b  [0.02988861]  mse_f: [0.0994897]   total loss: [0.1293783]\n",
            "It: 9480, Time: 2.53\n",
            "mse_b  [3.7884738]  mse_f: [0.09666359]   total loss: [3.8851373]\n",
            "It: 9520, Time: 2.23\n",
            "mse_b  [0.08494135]  mse_f: [0.09961946]   total loss: [0.1845608]\n",
            "It: 9560, Time: 2.25\n",
            "mse_b  [3.6618936]  mse_f: [0.10152455]   total loss: [3.7634182]\n",
            "It: 9600, Time: 2.24\n",
            "mse_b  [0.16749455]  mse_f: [0.09848575]   total loss: [0.2659803]\n",
            "It: 9640, Time: 2.20\n",
            "mse_b  [3.5718746]  mse_f: [0.09634224]   total loss: [3.668217]\n",
            "It: 9680, Time: 2.25\n",
            "mse_b  [0.40715078]  mse_f: [0.09982193]   total loss: [0.5069727]\n",
            "It: 9720, Time: 2.19\n",
            "mse_b  [3.0853188]  mse_f: [0.10113317]   total loss: [3.186452]\n",
            "It: 9760, Time: 2.21\n",
            "mse_b  [0.7957692]  mse_f: [0.09743179]   total loss: [0.893201]\n",
            "It: 9800, Time: 2.25\n",
            "mse_b  [2.889708]  mse_f: [0.09609757]   total loss: [2.9858055]\n",
            "It: 9840, Time: 2.16\n",
            "mse_b  [1.0518116]  mse_f: [0.09976768]   total loss: [1.1515793]\n",
            "It: 9880, Time: 2.23\n",
            "mse_b  [2.2890952]  mse_f: [0.10033996]   total loss: [2.389435]\n",
            "It: 9920, Time: 2.18\n",
            "mse_b  [1.7274522]  mse_f: [0.09634528]   total loss: [1.8237975]\n",
            "It: 9960, Time: 2.17\n",
            "mse_b  [1.7293471]  mse_f: [0.09604454]   total loss: [1.8253917]\n",
            "It: 10000, Time: 2.14\n",
            "mse_b  [2.0851312]  mse_f: [0.09958642]   total loss: [2.1847177]\n",
            "It: 10040, Time: 2.14\n",
            "mse_b  [1.158308]  mse_f: [0.09905279]   total loss: [1.2573608]\n",
            "It: 10080, Time: 2.14\n",
            "mse_b  [2.7367349]  mse_f: [0.09504844]   total loss: [2.8317833]\n",
            "It: 10120, Time: 2.15\n",
            "mse_b  [0.6477191]  mse_f: [0.09592886]   total loss: [0.74364793]\n",
            "It: 10160, Time: 2.09\n",
            "mse_b  [3.0205178]  mse_f: [0.09906007]   total loss: [3.119578]\n",
            "It: 10200, Time: 2.10\n",
            "mse_b  [0.2765509]  mse_f: [0.09737876]   total loss: [0.37392965]\n",
            "It: 10240, Time: 2.16\n",
            "mse_b  [3.4203663]  mse_f: [0.09379749]   total loss: [3.5141637]\n",
            "It: 10280, Time: 2.16\n",
            "mse_b  [0.02300921]  mse_f: [0.09603738]   total loss: [0.11904659]\n",
            "It: 10320, Time: 2.11\n",
            "mse_b  [3.4873388]  mse_f: [0.09814684]   total loss: [3.5854857]\n",
            "It: 10360, Time: 2.25\n",
            "mse_b  [0.09331283]  mse_f: [0.09504025]   total loss: [0.18835308]\n",
            "It: 10400, Time: 2.07\n",
            "mse_b  [3.283405]  mse_f: [0.09249459]   total loss: [3.3758996]\n",
            "It: 10440, Time: 2.09\n",
            "mse_b  [0.35749844]  mse_f: [0.09552075]   total loss: [0.4530192]\n",
            "It: 10480, Time: 2.07\n",
            "mse_b  [2.995493]  mse_f: [0.09666932]   total loss: [3.0921624]\n",
            "It: 10520, Time: 2.07\n",
            "mse_b  [0.770197]  mse_f: [0.09284447]   total loss: [0.86304146]\n",
            "It: 10560, Time: 2.07\n",
            "mse_b  [2.3263023]  mse_f: [0.09153913]   total loss: [2.4178414]\n",
            "It: 10600, Time: 2.04\n",
            "mse_b  [1.4608316]  mse_f: [0.09491319]   total loss: [1.5557449]\n",
            "It: 10640, Time: 2.02\n",
            "mse_b  [1.6774406]  mse_f: [0.09456331]   total loss: [1.7720039]\n",
            "It: 10680, Time: 2.01\n",
            "mse_b  [2.044319]  mse_f: [0.09038001]   total loss: [2.1346989]\n",
            "It: 10720, Time: 2.06\n",
            "mse_b  [0.87148714]  mse_f: [0.09077771]   total loss: [0.96226484]\n",
            "It: 10760, Time: 2.02\n",
            "mse_b  [2.8609188]  mse_f: [0.09392737]   total loss: [2.9548461]\n",
            "It: 10800, Time: 1.99\n",
            "mse_b  [0.17077848]  mse_f: [0.09170054]   total loss: [0.262479]\n",
            "It: 10840, Time: 1.99\n",
            "mse_b  [3.2726858]  mse_f: [0.08796498]   total loss: [3.3606508]\n",
            "It: 10880, Time: 2.00\n",
            "mse_b  [0.02311689]  mse_f: [0.09023801]   total loss: [0.1133549]\n",
            "It: 10920, Time: 2.03\n",
            "mse_b  [3.203189]  mse_f: [0.0920352]   total loss: [3.2952242]\n",
            "It: 10960, Time: 2.09\n",
            "mse_b  [0.24971823]  mse_f: [0.08840087]   total loss: [0.3381191]\n",
            "It: 11000, Time: 1.98\n",
            "mse_b  [2.891485]  mse_f: [0.086096]   total loss: [2.977581]\n",
            "It: 11040, Time: 2.00\n",
            "mse_b  [0.7416259]  mse_f: [0.08933802]   total loss: [0.8309639]\n",
            "It: 11080, Time: 1.98\n",
            "mse_b  [2.0215275]  mse_f: [0.08962993]   total loss: [2.1111574]\n",
            "It: 11120, Time: 2.01\n",
            "mse_b  [1.6959774]  mse_f: [0.08518121]   total loss: [1.7811587]\n",
            "It: 11160, Time: 1.98\n",
            "mse_b  [1.1216617]  mse_f: [0.08495651]   total loss: [1.2066182]\n",
            "It: 11200, Time: 2.00\n",
            "mse_b  [2.484245]  mse_f: [0.08807635]   total loss: [2.5723214]\n",
            "It: 11240, Time: 1.97\n",
            "mse_b  [0.3215895]  mse_f: [0.08614823]   total loss: [0.40773773]\n",
            "It: 11280, Time: 2.02\n",
            "mse_b  [3.0189998]  mse_f: [0.08218122]   total loss: [3.101181]\n",
            "It: 11320, Time: 1.93\n",
            "mse_b  [0.03463726]  mse_f: [0.08397536]   total loss: [0.11861262]\n",
            "It: 11360, Time: 1.94\n",
            "mse_b  [3.1228676]  mse_f: [0.0860187]   total loss: [3.2088864]\n",
            "It: 11400, Time: 1.96\n",
            "mse_b  [0.20486937]  mse_f: [0.0824731]   total loss: [0.2873425]\n",
            "It: 11440, Time: 1.95\n",
            "mse_b  [2.6244113]  mse_f: [0.08010691]   total loss: [2.7045183]\n",
            "It: 11480, Time: 1.91\n",
            "mse_b  [0.8531944]  mse_f: [0.08301128]   total loss: [0.9362057]\n",
            "It: 11520, Time: 1.90\n",
            "mse_b  [1.9747679]  mse_f: [0.08298668]   total loss: [2.0577545]\n",
            "It: 11560, Time: 1.96\n",
            "mse_b  [1.4653573]  mse_f: [0.07877829]   total loss: [1.5441356]\n",
            "It: 11600, Time: 2.02\n",
            "mse_b  [1.2250581]  mse_f: [0.07837339]   total loss: [1.3034315]\n",
            "It: 11640, Time: 1.94\n",
            "mse_b  [2.2209973]  mse_f: [0.08135012]   total loss: [2.3023474]\n",
            "It: 11680, Time: 1.91\n",
            "mse_b  [0.42833707]  mse_f: [0.07962297]   total loss: [0.50796]\n",
            "It: 11720, Time: 1.89\n",
            "mse_b  [2.8202608]  mse_f: [0.07565022]   total loss: [2.895911]\n",
            "It: 11760, Time: 1.92\n",
            "mse_b  [0.040443]  mse_f: [0.07719945]   total loss: [0.11764245]\n",
            "It: 11800, Time: 1.88\n",
            "mse_b  [2.973477]  mse_f: [0.07907254]   total loss: [3.0525494]\n",
            "It: 11840, Time: 1.88\n",
            "mse_b  [0.12712802]  mse_f: [0.07568176]   total loss: [0.20280978]\n",
            "It: 11880, Time: 1.88\n",
            "mse_b  [2.6674585]  mse_f: [0.07317183]   total loss: [2.7406304]\n",
            "It: 11920, Time: 1.86\n",
            "mse_b  [0.5891814]  mse_f: [0.07586649]   total loss: [0.6650479]\n",
            "It: 11960, Time: 1.83\n",
            "mse_b  [2.0556867]  mse_f: [0.07613228]   total loss: [2.131819]\n",
            "It: 12000, Time: 1.84\n",
            "mse_b  [1.3691834]  mse_f: [0.07199991]   total loss: [1.4411833]\n",
            "It: 12040, Time: 1.83\n",
            "mse_b  [1.1433158]  mse_f: [0.07144413]   total loss: [1.21476]\n",
            "It: 12080, Time: 1.86\n",
            "mse_b  [2.060607]  mse_f: [0.07412471]   total loss: [2.1347318]\n",
            "It: 12120, Time: 1.85\n",
            "mse_b  [0.5074568]  mse_f: [0.07260121]   total loss: [0.580058]\n",
            "It: 12160, Time: 1.82\n",
            "mse_b  [2.601588]  mse_f: [0.06878748]   total loss: [2.6703756]\n",
            "It: 12200, Time: 1.82\n",
            "mse_b  [0.06999768]  mse_f: [0.070083]   total loss: [0.14008069]\n",
            "It: 12240, Time: 1.88\n",
            "mse_b  [2.8279598]  mse_f: [0.07185072]   total loss: [2.8998106]\n",
            "It: 12280, Time: 1.84\n",
            "mse_b  [0.02290344]  mse_f: [0.06894305]   total loss: [0.0918465]\n",
            "It: 12320, Time: 1.83\n",
            "mse_b  [2.7270381]  mse_f: [0.06624802]   total loss: [2.793286]\n",
            "It: 12360, Time: 1.81\n",
            "mse_b  [0.28606832]  mse_f: [0.06860445]   total loss: [0.3546728]\n",
            "It: 12400, Time: 1.79\n",
            "mse_b  [2.292071]  mse_f: [0.06922468]   total loss: [2.3612957]\n",
            "It: 12440, Time: 1.81\n",
            "mse_b  [0.8836283]  mse_f: [0.06543885]   total loss: [0.9490672]\n",
            "It: 12480, Time: 1.80\n",
            "mse_b  [1.4741409]  mse_f: [0.06447036]   total loss: [1.5386113]\n",
            "It: 12520, Time: 1.78\n",
            "mse_b  [1.7402552]  mse_f: [0.06714945]   total loss: [1.8074046]\n",
            "It: 12560, Time: 1.79\n",
            "mse_b  [0.6150513]  mse_f: [0.06584021]   total loss: [0.6808915]\n",
            "It: 12600, Time: 1.78\n",
            "mse_b  [2.3707678]  mse_f: [0.06216527]   total loss: [2.432933]\n",
            "It: 12640, Time: 1.75\n",
            "mse_b  [0.10443614]  mse_f: [0.06330502]   total loss: [0.16774115]\n",
            "It: 12680, Time: 1.80\n",
            "mse_b  [2.678871]  mse_f: [0.06514295]   total loss: [2.7440138]\n",
            "It: 12720, Time: 1.77\n",
            "mse_b  [0.06162157]  mse_f: [0.06220438]   total loss: [0.12382595]\n",
            "It: 12760, Time: 2.11\n",
            "mse_b  [2.5181718]  mse_f: [0.05982393]   total loss: [2.5779958]\n",
            "It: 12800, Time: 1.92\n",
            "mse_b  [0.44230005]  mse_f: [0.06224542]   total loss: [0.50454545]\n",
            "It: 12840, Time: 1.76\n",
            "mse_b  [1.8689723]  mse_f: [0.06254514]   total loss: [1.9315175]\n",
            "It: 12880, Time: 1.76\n",
            "mse_b  [1.1578751]  mse_f: [0.05889672]   total loss: [1.2167718]\n",
            "It: 12920, Time: 1.76\n",
            "mse_b  [1.0425967]  mse_f: [0.05852694]   total loss: [1.1011237]\n",
            "It: 12960, Time: 1.75\n",
            "mse_b  [2.0254796]  mse_f: [0.06099132]   total loss: [2.0864708]\n",
            "It: 13000, Time: 1.73\n",
            "mse_b  [0.28344265]  mse_f: [0.05928203]   total loss: [0.34272468]\n",
            "It: 13040, Time: 1.70\n",
            "mse_b  [2.4572248]  mse_f: [0.05614691]   total loss: [2.5133717]\n",
            "It: 13080, Time: 1.69\n",
            "mse_b  [0.01016838]  mse_f: [0.05768265]   total loss: [0.06785104]\n",
            "It: 13120, Time: 1.69\n",
            "mse_b  [2.5346155]  mse_f: [0.05901647]   total loss: [2.593632]\n",
            "It: 13160, Time: 1.69\n",
            "mse_b  [0.13846791]  mse_f: [0.05618539]   total loss: [0.1946533]\n",
            "It: 13200, Time: 1.71\n",
            "mse_b  [2.2771409]  mse_f: [0.05433511]   total loss: [2.331476]\n",
            "It: 13240, Time: 1.71\n",
            "mse_b  [0.5689988]  mse_f: [0.05665774]   total loss: [0.62565655]\n",
            "It: 13280, Time: 1.71\n",
            "mse_b  [1.6224145]  mse_f: [0.05678254]   total loss: [1.6791971]\n",
            "It: 13320, Time: 1.68\n",
            "mse_b  [1.3011439]  mse_f: [0.05341468]   total loss: [1.3545586]\n",
            "It: 13360, Time: 1.68\n",
            "mse_b  [0.8814082]  mse_f: [0.053193]   total loss: [0.9346012]\n",
            "It: 13400, Time: 1.66\n",
            "mse_b  [1.8948996]  mse_f: [0.05542371]   total loss: [1.9503233]\n",
            "It: 13440, Time: 1.65\n",
            "mse_b  [0.2821923]  mse_f: [0.05416021]   total loss: [0.3363525]\n",
            "It: 13480, Time: 1.64\n",
            "mse_b  [2.3460815]  mse_f: [0.05118755]   total loss: [2.397269]\n",
            "It: 13520, Time: 1.64\n",
            "mse_b  [0.0547656]  mse_f: [0.05216249]   total loss: [0.10692809]\n",
            "It: 13560, Time: 1.64\n",
            "mse_b  [2.4211547]  mse_f: [0.05365003]   total loss: [2.4748049]\n",
            "It: 13600, Time: 1.66\n",
            "mse_b  [0.03425026]  mse_f: [0.05138502]   total loss: [0.08563527]\n",
            "It: 13640, Time: 1.73\n",
            "mse_b  [2.3770766]  mse_f: [0.04957272]   total loss: [2.4266493]\n",
            "It: 13680, Time: 1.67\n",
            "mse_b  [0.17387934]  mse_f: [0.05110303]   total loss: [0.22498237]\n",
            "It: 13720, Time: 1.62\n",
            "mse_b  [2.0457163]  mse_f: [0.05213088]   total loss: [2.0978472]\n",
            "It: 13760, Time: 1.62\n",
            "mse_b  [0.7852659]  mse_f: [0.0493634]   total loss: [0.8346293]\n",
            "It: 13800, Time: 1.61\n",
            "mse_b  [1.2729269]  mse_f: [0.04856952]   total loss: [1.3214965]\n",
            "It: 13840, Time: 1.60\n",
            "mse_b  [1.4281571]  mse_f: [0.05060539]   total loss: [1.4787625]\n",
            "It: 13880, Time: 1.63\n",
            "mse_b  [0.5890883]  mse_f: [0.04985395]   total loss: [0.63894224]\n",
            "It: 13920, Time: 1.62\n",
            "mse_b  [2.1064734]  mse_f: [0.04704104]   total loss: [2.1535144]\n",
            "It: 13960, Time: 1.59\n",
            "mse_b  [0.02402998]  mse_f: [0.04836794]   total loss: [0.07239792]\n",
            "It: 14000, Time: 1.59\n",
            "mse_b  [2.305524]  mse_f: [0.04973027]   total loss: [2.3552544]\n",
            "It: 14040, Time: 1.64\n",
            "mse_b  [0.22231361]  mse_f: [0.0470967]   total loss: [0.2694103]\n",
            "It: 14080, Time: 1.62\n",
            "mse_b  [1.8910562]  mse_f: [0.04588288]   total loss: [1.936939]\n",
            "It: 14120, Time: 1.60\n",
            "mse_b  [0.8486047]  mse_f: [0.04804776]   total loss: [0.89665246]\n",
            "It: 14160, Time: 1.57\n",
            "mse_b  [0.8876319]  mse_f: [0.04787947]   total loss: [0.93551135]\n",
            "It: 14200, Time: 1.59\n",
            "mse_b  [1.808419]  mse_f: [0.04479804]   total loss: [1.853217]\n",
            "It: 14240, Time: 1.60\n",
            "mse_b  [0.23934337]  mse_f: [0.04552142]   total loss: [0.28486478]\n",
            "It: 14280, Time: 1.61\n",
            "mse_b  [2.222027]  mse_f: [0.04729908]   total loss: [2.2693262]\n",
            "It: 14320, Time: 1.63\n",
            "mse_b  [0.01701411]  mse_f: [0.04535074]   total loss: [0.06236485]\n",
            "It: 14360, Time: 1.63\n",
            "mse_b  [2.1300273]  mse_f: [0.04343248]   total loss: [2.1734598]\n",
            "It: 14400, Time: 1.65\n",
            "mse_b  [0.33302173]  mse_f: [0.04518537]   total loss: [0.3782071]\n",
            "It: 14440, Time: 1.57\n",
            "mse_b  [1.7025597]  mse_f: [0.04562371]   total loss: [1.7481834]\n",
            "It: 14480, Time: 1.55\n",
            "mse_b  [0.9146903]  mse_f: [0.04296543]   total loss: [0.9576557]\n",
            "It: 14520, Time: 1.56\n",
            "mse_b  [1.0825282]  mse_f: [0.04251568]   total loss: [1.1250439]\n",
            "It: 14560, Time: 1.54\n",
            "mse_b  [1.4154551]  mse_f: [0.04454005]   total loss: [1.4599952]\n",
            "It: 14600, Time: 1.53\n",
            "mse_b  [0.47325236]  mse_f: [0.04394019]   total loss: [0.51719254]\n",
            "It: 14640, Time: 1.55\n",
            "mse_b  [1.9462848]  mse_f: [0.04155334]   total loss: [1.9878381]\n",
            "It: 14680, Time: 1.54\n",
            "mse_b  [0.10478035]  mse_f: [0.04202992]   total loss: [0.14681026]\n",
            "It: 14720, Time: 1.55\n",
            "mse_b  [2.1382437]  mse_f: [0.04350124]   total loss: [2.1817448]\n",
            "It: 14760, Time: 1.52\n",
            "mse_b  [0.02093174]  mse_f: [0.04180442]   total loss: [0.06273615]\n",
            "It: 14800, Time: 1.52\n",
            "mse_b  [2.1431465]  mse_f: [0.03991777]   total loss: [2.1830642]\n",
            "It: 14840, Time: 1.52\n",
            "mse_b  [0.30023313]  mse_f: [0.04199708]   total loss: [0.3422302]\n",
            "It: 14880, Time: 1.52\n",
            "mse_b  [1.6095859]  mse_f: [0.04197681]   total loss: [1.6515627]\n",
            "It: 14920, Time: 1.52\n",
            "mse_b  [0.78613716]  mse_f: [0.03970787]   total loss: [0.825845]\n",
            "It: 14960, Time: 1.50\n",
            "mse_b  [0.8068273]  mse_f: [0.03967442]   total loss: [0.8465017]\n",
            "It: 15000, Time: 1.48\n",
            "mse_b  [1.7074513]  mse_f: [0.04106041]   total loss: [1.7485118]\n",
            "It: 15040, Time: 1.46\n",
            "mse_b  [0.3122225]  mse_f: [0.04003303]   total loss: [0.35225555]\n",
            "It: 15080, Time: 1.47\n",
            "mse_b  [2.0199168]  mse_f: [0.03800948]   total loss: [2.0579262]\n",
            "It: 15120, Time: 1.48\n",
            "mse_b  [0.04201666]  mse_f: [0.03941976]   total loss: [0.08143641]\n",
            "It: 15160, Time: 1.48\n",
            "mse_b  [2.0599754]  mse_f: [0.03968459]   total loss: [2.09966]\n",
            "It: 15200, Time: 1.69\n",
            "mse_b  [0.30231667]  mse_f: [0.03806063]   total loss: [0.3403773]\n",
            "It: 15240, Time: 1.48\n",
            "mse_b  [1.2671325]  mse_f: [0.03707538]   total loss: [1.3042079]\n",
            "It: 15280, Time: 1.46\n",
            "mse_b  [1.09809]  mse_f: [0.03841336]   total loss: [1.1365035]\n",
            "It: 15320, Time: 1.43\n",
            "mse_b  [0.60391444]  mse_f: [0.03822]   total loss: [0.6421344]\n",
            "It: 15360, Time: 1.45\n",
            "mse_b  [1.8543143]  mse_f: [0.0356776]   total loss: [1.8899919]\n",
            "It: 15400, Time: 1.43\n",
            "mse_b  [0.04187908]  mse_f: [0.03630908]   total loss: [0.07818816]\n",
            "It: 15440, Time: 1.44\n",
            "mse_b  [2.0548909]  mse_f: [0.0375065]   total loss: [2.0923975]\n",
            "It: 15480, Time: 1.45\n",
            "mse_b  [0.18026014]  mse_f: [0.03568153]   total loss: [0.21594167]\n",
            "It: 15520, Time: 1.42\n",
            "mse_b  [1.6354868]  mse_f: [0.0342501]   total loss: [1.669737]\n",
            "It: 15560, Time: 1.42\n",
            "mse_b  [0.78156203]  mse_f: [0.03583799]   total loss: [0.81740004]\n",
            "It: 15600, Time: 1.44\n",
            "mse_b  [0.8113459]  mse_f: [0.03565154]   total loss: [0.84699744]\n",
            "It: 15640, Time: 1.42\n",
            "mse_b  [1.551022]  mse_f: [0.03350677]   total loss: [1.5845288]\n",
            "It: 15680, Time: 1.44\n",
            "mse_b  [0.19411823]  mse_f: [0.03359674]   total loss: [0.22771497]\n",
            "It: 15720, Time: 1.39\n",
            "mse_b  [1.9562523]  mse_f: [0.03475353]   total loss: [1.9910059]\n",
            "It: 15760, Time: 1.44\n",
            "mse_b  [0.02383918]  mse_f: [0.03329066]   total loss: [0.05712984]\n",
            "It: 15800, Time: 1.42\n",
            "mse_b  [1.8435758]  mse_f: [0.03188766]   total loss: [1.8754635]\n",
            "It: 15840, Time: 1.41\n",
            "mse_b  [0.38509473]  mse_f: [0.03286202]   total loss: [0.41795674]\n",
            "It: 15880, Time: 1.43\n",
            "mse_b  [1.3085375]  mse_f: [0.03317074]   total loss: [1.3417082]\n",
            "It: 15920, Time: 1.39\n",
            "mse_b  [1.0376977]  mse_f: [0.03076808]   total loss: [1.0684657]\n",
            "It: 15960, Time: 1.43\n",
            "mse_b  [0.5755288]  mse_f: [0.03068023]   total loss: [0.60620904]\n",
            "It: 16000, Time: 1.39\n",
            "mse_b  [1.7483884]  mse_f: [0.03204378]   total loss: [1.7804322]\n",
            "It: 16040, Time: 1.48\n",
            "mse_b  [0.06855015]  mse_f: [0.03067145]   total loss: [0.0992216]\n",
            "It: 16080, Time: 1.40\n",
            "mse_b  [1.9121091]  mse_f: [0.02937271]   total loss: [1.9414818]\n",
            "It: 16120, Time: 1.36\n",
            "mse_b  [0.2529218]  mse_f: [0.03014371]   total loss: [0.2830655]\n",
            "It: 16160, Time: 1.37\n",
            "mse_b  [1.3471532]  mse_f: [0.03037224]   total loss: [1.3775254]\n",
            "It: 16200, Time: 1.36\n",
            "mse_b  [0.9408132]  mse_f: [0.02824569]   total loss: [0.9690589]\n",
            "It: 16240, Time: 1.33\n",
            "mse_b  [0.5325105]  mse_f: [0.02804875]   total loss: [0.5605593]\n",
            "It: 16280, Time: 1.32\n",
            "mse_b  [1.7583971]  mse_f: [0.02925249]   total loss: [1.7876496]\n",
            "It: 16320, Time: 1.33\n",
            "mse_b  [0.03327729]  mse_f: [0.02792746]   total loss: [0.06120475]\n",
            "It: 16360, Time: 1.33\n",
            "mse_b  [1.8492013]  mse_f: [0.02640574]   total loss: [1.875607]\n",
            "It: 16400, Time: 1.34\n",
            "mse_b  [0.2587965]  mse_f: [0.02761766]   total loss: [0.28641418]\n",
            "It: 16440, Time: 1.35\n",
            "mse_b  [1.391851]  mse_f: [0.02755424]   total loss: [1.4194052]\n",
            "It: 16480, Time: 1.32\n",
            "mse_b  [0.91062385]  mse_f: [0.02567475]   total loss: [0.9362986]\n",
            "It: 16520, Time: 1.33\n",
            "mse_b  [0.5770728]  mse_f: [0.02560651]   total loss: [0.6026793]\n",
            "It: 16560, Time: 1.30\n",
            "mse_b  [1.628761]  mse_f: [0.0265055]   total loss: [1.6552665]\n",
            "It: 16600, Time: 1.31\n",
            "mse_b  [0.06017807]  mse_f: [0.02533868]   total loss: [0.08551675]\n",
            "It: 16640, Time: 1.30\n",
            "mse_b  [1.8499092]  mse_f: [0.02410666]   total loss: [1.8740158]\n",
            "It: 16680, Time: 1.32\n",
            "mse_b  [0.13654716]  mse_f: [0.02485704]   total loss: [0.1614042]\n",
            "It: 16720, Time: 1.31\n",
            "mse_b  [1.4891593]  mse_f: [0.02496868]   total loss: [1.514128]\n",
            "It: 16760, Time: 1.29\n",
            "mse_b  [0.73336256]  mse_f: [0.02324808]   total loss: [0.75661063]\n",
            "It: 16800, Time: 1.28\n",
            "mse_b  [0.7362595]  mse_f: [0.02299103]   total loss: [0.7592505]\n",
            "It: 16840, Time: 1.28\n",
            "mse_b  [1.510789]  mse_f: [0.02402617]   total loss: [1.5348152]\n",
            "It: 16880, Time: 1.29\n",
            "mse_b  [0.05280995]  mse_f: [0.022934]   total loss: [0.07574395]\n",
            "It: 16920, Time: 1.28\n",
            "mse_b  [1.8139079]  mse_f: [0.02178752]   total loss: [1.8356954]\n",
            "It: 16960, Time: 1.28\n",
            "mse_b  [0.22001933]  mse_f: [0.02241138]   total loss: [0.2424307]\n",
            "It: 17000, Time: 1.31\n",
            "mse_b  [1.2643969]  mse_f: [0.02256324]   total loss: [1.2869601]\n",
            "It: 17040, Time: 1.26\n",
            "mse_b  [0.88957584]  mse_f: [0.02090633]   total loss: [0.91048217]\n",
            "It: 17080, Time: 1.25\n",
            "mse_b  [0.49522084]  mse_f: [0.02064532]   total loss: [0.51586616]\n",
            "It: 17120, Time: 1.62\n",
            "mse_b  [1.6108326]  mse_f: [0.02157993]   total loss: [1.6324126]\n",
            "It: 17160, Time: 1.46\n",
            "mse_b  [0.01435384]  mse_f: [0.02082676]   total loss: [0.0351806]\n",
            "It: 17200, Time: 1.25\n",
            "mse_b  [1.7626483]  mse_f: [0.01924706]   total loss: [1.7818954]\n",
            "It: 17240, Time: 1.25\n",
            "mse_b  [0.3157714]  mse_f: [0.02058432]   total loss: [0.33635572]\n",
            "It: 17280, Time: 1.24\n",
            "mse_b  [1.1906778]  mse_f: [0.02045999]   total loss: [1.2111378]\n",
            "It: 17320, Time: 1.21\n",
            "mse_b  [1.0092899]  mse_f: [0.01895324]   total loss: [1.0282431]\n",
            "It: 17360, Time: 1.22\n",
            "mse_b  [0.40727872]  mse_f: [0.01906529]   total loss: [0.426344]\n",
            "It: 17400, Time: 1.22\n",
            "mse_b  [1.6604886]  mse_f: [0.01997055]   total loss: [1.6804591]\n",
            "It: 17440, Time: 1.19\n",
            "mse_b  [0.01126854]  mse_f: [0.01885627]   total loss: [0.03012481]\n",
            "It: 17480, Time: 1.22\n",
            "mse_b  [1.5903662]  mse_f: [0.01814577]   total loss: [1.608512]\n",
            "It: 17520, Time: 1.19\n",
            "mse_b  [0.5449903]  mse_f: [0.01893169]   total loss: [0.563922]\n",
            "It: 17560, Time: 1.19\n",
            "mse_b  [0.7682494]  mse_f: [0.01875029]   total loss: [0.7869997]\n",
            "It: 17600, Time: 1.18\n",
            "mse_b  [1.3606664]  mse_f: [0.01761672]   total loss: [1.3782831]\n",
            "It: 17640, Time: 1.20\n",
            "mse_b  [0.08077051]  mse_f: [0.01765015]   total loss: [0.09842065]\n",
            "It: 17680, Time: 1.18\n",
            "mse_b  [1.7169702]  mse_f: [0.01836706]   total loss: [1.7353373]\n",
            "It: 17720, Time: 1.16\n",
            "mse_b  [0.09317282]  mse_f: [0.01741114]   total loss: [0.11058396]\n",
            "It: 17760, Time: 1.15\n",
            "mse_b  [1.3968693]  mse_f: [0.01678986]   total loss: [1.4136592]\n",
            "It: 17800, Time: 1.18\n",
            "mse_b  [0.66649956]  mse_f: [0.01765719]   total loss: [0.6841568]\n",
            "It: 17840, Time: 1.15\n",
            "mse_b  [0.58200616]  mse_f: [0.01745071]   total loss: [0.59945685]\n",
            "It: 17880, Time: 1.18\n",
            "mse_b  [1.4066288]  mse_f: [0.01635328]   total loss: [1.4229821]\n",
            "It: 17920, Time: 1.19\n",
            "mse_b  [0.06583262]  mse_f: [0.01649754]   total loss: [0.08233017]\n",
            "It: 17960, Time: 1.18\n",
            "mse_b  [1.6661378]  mse_f: [0.01716948]   total loss: [1.6833073]\n",
            "It: 18000, Time: 1.15\n",
            "mse_b  [0.15458576]  mse_f: [0.01629215]   total loss: [0.17087792]\n",
            "It: 18040, Time: 1.13\n",
            "mse_b  [1.289397]  mse_f: [0.01562949]   total loss: [1.3050265]\n",
            "It: 18080, Time: 1.12\n",
            "mse_b  [0.72449666]  mse_f: [0.01648013]   total loss: [0.7409768]\n",
            "It: 18120, Time: 1.10\n",
            "mse_b  [0.5598567]  mse_f: [0.01648933]   total loss: [0.57634604]\n",
            "It: 18160, Time: 1.10\n",
            "mse_b  [1.4051332]  mse_f: [0.01501762]   total loss: [1.4201509]\n",
            "It: 18200, Time: 1.09\n",
            "mse_b  [0.04178897]  mse_f: [0.01574228]   total loss: [0.05753125]\n",
            "It: 18240, Time: 1.10\n",
            "mse_b  [1.6759245]  mse_f: [0.01603158]   total loss: [1.6919562]\n",
            "It: 18280, Time: 1.12\n",
            "mse_b  [0.3626282]  mse_f: [0.01519418]   total loss: [0.37782237]\n",
            "It: 18320, Time: 1.07\n",
            "mse_b  [0.9631216]  mse_f: [0.01462516]   total loss: [0.9777468]\n",
            "It: 18360, Time: 1.10\n",
            "mse_b  [1.213113]  mse_f: [0.01564772]   total loss: [1.2287607]\n",
            "It: 18400, Time: 1.07\n",
            "mse_b  [0.12899235]  mse_f: [0.01540073]   total loss: [0.14439307]\n",
            "It: 18440, Time: 1.07\n",
            "mse_b  [1.6687198]  mse_f: [0.01405555]   total loss: [1.6827753]\n",
            "It: 18480, Time: 1.06\n",
            "mse_b  [0.16128545]  mse_f: [0.0152887]   total loss: [0.17657414]\n",
            "It: 18520, Time: 1.09\n",
            "mse_b  [1.2419608]  mse_f: [0.0151658]   total loss: [1.2571266]\n",
            "It: 18560, Time: 1.05\n",
            "mse_b  [0.92780995]  mse_f: [0.01407661]   total loss: [0.94188654]\n",
            "It: 18600, Time: 1.06\n",
            "mse_b  [0.36681995]  mse_f: [0.01436017]   total loss: [0.3811801]\n",
            "It: 18640, Time: 1.06\n",
            "mse_b  [1.5508522]  mse_f: [0.01483861]   total loss: [1.5656908]\n",
            "It: 18680, Time: 1.05\n",
            "mse_b  [0.03828046]  mse_f: [0.0144493]   total loss: [0.05272976]\n",
            "It: 18720, Time: 1.05\n",
            "mse_b  [1.4769518]  mse_f: [0.01358329]   total loss: [1.4905351]\n",
            "It: 18760, Time: 1.05\n",
            "mse_b  [0.5637768]  mse_f: [0.01462889]   total loss: [0.5784057]\n",
            "It: 18800, Time: 1.03\n",
            "mse_b  [0.712734]  mse_f: [0.01450018]   total loss: [0.7272342]\n",
            "It: 18840, Time: 1.03\n",
            "mse_b  [1.4054718]  mse_f: [0.0133165]   total loss: [1.4187883]\n",
            "It: 18880, Time: 1.05\n",
            "mse_b  [0.03675165]  mse_f: [0.01405188]   total loss: [0.05080353]\n",
            "It: 18920, Time: 1.04\n",
            "mse_b  [1.4890227]  mse_f: [0.01414016]   total loss: [1.5031629]\n",
            "It: 18960, Time: 1.00\n",
            "mse_b  [0.75413173]  mse_f: [0.01376418]   total loss: [0.76789594]\n",
            "It: 19000, Time: 1.00\n",
            "mse_b  [0.24967696]  mse_f: [0.01507156]   total loss: [0.2647485]\n",
            "It: 19040, Time: 1.04\n",
            "mse_b  [0.1260015]  mse_f: [0.01307048]   total loss: [0.13907199]\n",
            "It: 19080, Time: 1.05\n",
            "mse_b  [0.8561483]  mse_f: [0.01311339]   total loss: [0.8692617]\n",
            "It: 19120, Time: 1.02\n",
            "mse_b  [1.3280154]  mse_f: [0.01316645]   total loss: [1.3411819]\n",
            "It: 19160, Time: 1.01\n",
            "mse_b  [0.01482898]  mse_f: [0.0132091]   total loss: [0.02803808]\n",
            "It: 19200, Time: 1.00\n",
            "mse_b  [1.602108]  mse_f: [0.01177759]   total loss: [1.6138856]\n",
            "It: 19240, Time: 1.01\n",
            "mse_b  [0.19900915]  mse_f: [0.01354752]   total loss: [0.21255668]\n",
            "It: 19280, Time: 1.00\n",
            "mse_b  [1.3789407]  mse_f: [0.01321503]   total loss: [1.3921558]\n",
            "It: 19320, Time: 1.00\n",
            "mse_b  [0.8752299]  mse_f: [0.01239393]   total loss: [0.88762385]\n",
            "It: 19360, Time: 0.99\n",
            "mse_b  [0.33513203]  mse_f: [0.01260875]   total loss: [0.3477408]\n",
            "It: 19400, Time: 1.00\n",
            "mse_b  [1.5554649]  mse_f: [0.0129012]   total loss: [1.568366]\n",
            "It: 19440, Time: 0.97\n",
            "mse_b  [0.08913545]  mse_f: [0.01250487]   total loss: [0.10164032]\n",
            "It: 19480, Time: 0.99\n",
            "mse_b  [1.1765882]  mse_f: [0.01177898]   total loss: [1.1883671]\n",
            "It: 19520, Time: 0.99\n",
            "mse_b  [0.73214304]  mse_f: [0.01254379]   total loss: [0.74468684]\n",
            "It: 19560, Time: 1.00\n",
            "mse_b  [0.3513275]  mse_f: [0.01274258]   total loss: [0.3640701]\n",
            "It: 19600, Time: 0.97\n",
            "mse_b  [1.3912035]  mse_f: [0.01157094]   total loss: [1.4027745]\n",
            "It: 19640, Time: 0.96\n",
            "mse_b  [0.07705083]  mse_f: [0.01237927]   total loss: [0.08943009]\n",
            "It: 19680, Time: 0.99\n",
            "mse_b  [1.2785467]  mse_f: [0.0123625]   total loss: [1.2909092]\n",
            "It: 19720, Time: 0.96\n",
            "mse_b  [0.62001115]  mse_f: [0.01167017]   total loss: [0.6316813]\n",
            "It: 19760, Time: 0.96\n",
            "mse_b  [0.45378283]  mse_f: [0.01162153]   total loss: [0.46540436]\n",
            "It: 19800, Time: 0.98\n",
            "mse_b  [1.2946483]  mse_f: [0.01223241]   total loss: [1.3068807]\n",
            "It: 19840, Time: 0.99\n",
            "mse_b  [0.04712199]  mse_f: [0.01167511]   total loss: [0.0587971]\n",
            "It: 19880, Time: 0.96\n",
            "mse_b  [1.2555745]  mse_f: [0.01145471]   total loss: [1.2670292]\n",
            "It: 19920, Time: 0.96\n",
            "mse_b  [0.58554745]  mse_f: [0.01179399]   total loss: [0.5973414]\n",
            "It: 19960, Time: 0.98\n",
            "mse_b  [0.27220032]  mse_f: [0.01180652]   total loss: [0.28400683]\n",
            "It: 20000, Time: 0.94\n",
            "mse_b  [1.3978997]  mse_f: [0.01111095]   total loss: [1.4090106]\n",
            "It: 20040, Time: 0.96\n",
            "mse_b  [0.16675194]  mse_f: [0.01139636]   total loss: [0.1781483]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-f3ceb9a323be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m MSE_b1, MSE_f1, weightu, weightf, loss_saman, list_model_fi= fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu,sai_exact1,\n\u001b[0m\u001b[1;32m    349\u001b[0m                                                    \u001b[0mfi_exact1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_exact1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_exact1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp_exact1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_exact1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT_exact1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_star1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                                                    \u001b[0mxtt2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtt3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtt4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytt2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytt3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytt4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxbb1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxbb2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxbb3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxbb4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mybb1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mybb2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mybb3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mybb4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-f3ceb9a323be>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, sai_exact1, fi_exact1, u_exact1, v_exact1, p_exact1, c_exact1, T_exact1, X_star, xtt1, xtt2, xtt3, xtt4, ytt1, ytt2, ytt3, ytt4, xbb1, xbb2, xbb3, xbb4, ybb1, ybb2, ybb3, ybb4, xrr, yrr, xll1, yll1, xll2, yll2, tf_iter, tf_iter2, newton_iter1, newton_iter2)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             loss_value, mse_b, mse_f, grads, grads_ub, grads_fu = grad(u_model, x_f_batch, y_f_batch,\n\u001b[0m\u001b[1;32m    262\u001b[0m                                                                        \u001b[0mxb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                                                                        \u001b[0mub_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_ub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "loss2=[]\n",
        "for i in range(len(loss_saman)):\n",
        "  loss2.append(loss_saman[i][0].numpy())\n",
        "print(\"Min: \", pd.Series(loss2).idxmin())\n",
        "print(\"Max: \", pd.Series(loss2).idxmax())\n",
        "print('loss_min',loss_saman[pd.Series(loss2).idxmin()])\n",
        "\n",
        "model_fi=list_model_fi[pd.Series(loss2).idxmin()]"
      ],
      "metadata": {
        "id": "pDnTEd9LooZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_fi.save('my_fi_model_Lx6_Asym_2top_2bott.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvny2RufTP8C",
        "outputId": "8c177a19-deac-432c-9e40-d09ce8aac49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model('my_fi_model.h5')\n",
        "model_fi= new_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS_sLYzJTgsZ",
        "outputId": "11b969b7-503e-4d58-cf4f-d9c83fd53038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualizing of predicted electric field"
      ],
      "metadata": {
        "id": "BI-Pk5lHhJ4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Set up meshgrid\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "N=100\n",
        "x1 = (np.linspace(0, xmax, N+1)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, N+1)).flatten()[:, None]\n",
        "X1, Y1 = np.meshgrid(x1, y1)\n",
        "\n",
        "X_star1 = np.hstack((X1.flatten()[:, None], Y1.flatten()[:, None]))\n",
        "\n",
        "X_star = tf.convert_to_tensor(X_star1, dtype=tf.float32)\n",
        "#up, vp, pp = u_x_model(X_star[:, 0:1], X_star[:, 1:2])\n",
        "\n",
        "UU=model_fi(tf.concat([X_star[:, 0:1], X_star[:, 1:2]],1))\n",
        "uuu2=tf.reshape(UU,shape=[tf.shape(UU).numpy()[0]])\n",
        "U = uuu2.numpy().reshape(N+1,N+1)\n",
        "plt. contourf(X1, Y1, U,15, cmap='rainbow');\n",
        "plt.colorbar();\n",
        "\n",
        "\n",
        "x1 = (np.linspace(0, xmax, N+1)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, N+1)).flatten()[:, None]\n",
        "X1, Y1 = np.meshgrid(x1, y1)\n",
        "fig, ax = plt.subplots(figsize=(4, 1))\n",
        "ax.contourf(X1,Y1,U,30,cmap='rainbow')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "-dsIjJUqUD0x",
        "outputId": "b5848577-bb96-46c4-c119-023d6c3f5ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.contour.QuadContourSet at 0x7f88bbbe8fa0>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x72 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xcor1=x_f[0 * N_f:(0 * N_f + N_f), ]\n",
        "ycor1=y_f[0 * N_f:(0 * N_f + N_f), ]\n",
        "sai_ref = model_sai(tf.concat([xcor1,ycor1],1))\n",
        "\n",
        "fi_ref = model_fi(tf.concat([xcor1,ycor1],1))"
      ],
      "metadata": {
        "id": "RzUJEB7V0C-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#distribution of eletric layer with respect to distance from the wall"
      ],
      "metadata": {
        "id": "yY3srM5thQhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.scatter(sai_ref,xcor1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "SXiXMsXFWBKu",
        "outputId": "e510472d-c0f3-4b0e-c19a-97a57b0b83da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f88ba7dff40>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_f,sai_ref,fi_ref"
      ],
      "metadata": {
        "id": "uHDiSx3Kcj7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculation of derivative of fi with respect to x and y"
      ],
      "metadata": {
        "id": "6wxgPeXFhZNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape(persistent=True)as tape:\n",
        "  tape.watch(xcor1)\n",
        "  tape.watch(xcor1)\n",
        "  fi1_ref = model_fi(tf.stack([xcor1,ycor1],axis=1))\n",
        "  fi_ref_x=tape.gradient(fi1_ref,xcor1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvzZPGl01To4",
        "outputId": "2bad3446-d519-48b9-b51a-d5102add8367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Solving Navier-Stoke euqation for finding velocity, pressure"
      ],
      "metadata": {
        "id": "FBW0GEvdiiSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NEW NS with importing fi and sai\n",
        "#NEW NS\n",
        "#########\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DTYPE='float32'\n",
        "tf.keras.backend.set_floatx(DTYPE)\n",
        "# Set number of data points\n",
        "#N_0 = 100\n",
        "#N_b = 100\n",
        "#N_r = 5000\n",
        "\n",
        "N_b = Nu1\n",
        "N_r = N_f\n",
        "\n",
        "\n",
        "# Set boundary\n",
        "xmin = 0.\n",
        "xmax = xmax\n",
        "ymin = 0.\n",
        "ymax = 1.\n",
        "# Set constants\n",
        "pi = tf.constant(np.pi, dtype=DTYPE)\n",
        "viscosity = .01/pi\n",
        "\n",
        "# Define initial condition\n",
        "def fun_u_0(x):\n",
        "    return -tf.sin(pi * x)\n",
        "\n",
        "# Define boundary condition\n",
        "def fun_u_b(x, y):\n",
        "    n = x.shape[0]\n",
        "    return tf.zeros((n,1), dtype=DTYPE)\n",
        "\n",
        "# Define residual of the PDE\n",
        "def Nsx(x, y, u, v, u_x, u_y, v_x, v_y, u_xx, u_yy, v_xx, v_yy, p_x, p_y):\n",
        "    return -u*u_x-v*u_y-p_x+(1.0/1.0)*(u_xx+u_yy)\n",
        "#    return -p_x+(1.0/1.0)*(u_xx+u_yy)\n",
        "\n",
        "\n",
        "def Nsy(x, y, u, v, u_x, u_y, v_x, v_y, u_xx, u_yy, v_xx, v_yy, p_x, p_y):\n",
        "    return -u*v_x-v*v_y-p_y+(1.0/1.0)*(v_xx+v_yy)\n",
        "#    return -p_y+(1.0/1.0)*(v_xx+v_yy)\n",
        "\n",
        "def Cont(u, v, u_x, u_y, v_x, v_y):\n",
        "    return u_x+v_y\n",
        "\n",
        "# Lower bounds\n",
        "lb = tf.constant([xmin, ymin], dtype=DTYPE)\n",
        "# Upper bounds\n",
        "ub = tf.constant([xmax, ymax], dtype=DTYPE)\n",
        "\n",
        "# Set random seed for reproducible results\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# Draw uniform sample points for initial boundary data\n",
        "x_0 = lb[0] + (ub[0] - lb[0]) * tf.keras.backend.random_bernoulli((N_b,1), 0.5, dtype=DTYPE)\n",
        "y_0 = tf.random.uniform((N_b,1), lb[1], ub[1], dtype=DTYPE)\n",
        "X_0 = tf.concat([x_0, y_0], axis=1)\n",
        "\n",
        "# Evaluate intitial condition at x_0\n",
        "u_0 = fun_u_0(x_0)\n",
        "\n",
        "# Boundary data\n",
        "x_b = tf.random.uniform((N_b,1), lb[0], ub[0], dtype=DTYPE)\n",
        "y_b = lb[1] + (ub[1] - lb[1]) * tf.keras.backend.random_bernoulli((N_b,1), 0.5, dtype=DTYPE)\n",
        "X_b = tf.concat([x_b, y_b], axis=1)\n",
        "\n",
        "# Evaluate boundary condition at (t_b,x_b)\n",
        "u_b = fun_u_b(x_b, y_b)\n",
        "\n",
        "\n",
        "# Draw uniformly sampled collocation points\n",
        "x_r = tf.random.uniform((N_r,1), lb[0], ub[0], dtype=DTYPE)\n",
        "y_r = tf.random.uniform((N_r,1), lb[1], ub[1], dtype=DTYPE)\n",
        "X_r = tf.concat([x_r, y_r], axis=1)\n",
        "\n",
        "# Collect boundary and inital data in lists\n",
        "X_data = [X_0, X_b]\n",
        "u_data = [u_0, u_b]\n",
        "\n",
        "\n",
        "##################\n",
        "#################\n",
        "lo=np.zeros((N_b,1))\n",
        "#top\n",
        "xx=[]\n",
        "yy=[]\n",
        "for i in range(N_b):\n",
        "      if X_data[1][i,1].numpy() == 1.0:\n",
        "        xx.append(X_data[1][i,0].numpy())\n",
        "        yy.append(X_data[1][i,1].numpy())\n",
        "xtt=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "ytt=tf.zeros((len(yy),1), dtype=DTYPE)\n",
        "xtt=tf.add(xtt[:,0],xx)\n",
        "ytt=tf.add(ytt[:,0],yy)\n",
        "utt=tf.ones((len(xx),1), dtype=DTYPE)\n",
        "vtt=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "\n",
        "#bottom\n",
        "xx=[]\n",
        "yy=[]\n",
        "for i in range(N_b):\n",
        "      if X_data[1][i,1].numpy() == 0.0:\n",
        "        xx.append(X_data[1][i,0].numpy())\n",
        "        yy.append(X_data[1][i,1].numpy())\n",
        "xbb=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "ybb=tf.zeros((len(yy),1), dtype=DTYPE)\n",
        "xbb=tf.add(xbb[:,0],xx)\n",
        "ybb=tf.add(ybb[:,0],yy)\n",
        "ubb=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "vbb=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "#left\n",
        "xx=[]\n",
        "yy=[]\n",
        "for i in range(N_b):\n",
        "      if X_data[0][i,0].numpy() == 0:\n",
        "        xx.append(X_data[0][i,0].numpy())\n",
        "        yy.append(X_data[0][i,1].numpy())\n",
        "xll=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "yll=tf.zeros((len(yy),1), dtype=DTYPE)\n",
        "xll=tf.add(xll[:,0],xx)\n",
        "yll=tf.add(yll[:,0],yy)\n",
        "ull=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "vll=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "#right\n",
        "\n",
        "xx=[]\n",
        "yy=[]\n",
        "for i in range(N_b):\n",
        "      if X_data[0][i,0].numpy() == xmax:\n",
        "        xx.append(X_data[0][i,0].numpy())\n",
        "        yy.append(X_data[0][i,1].numpy())\n",
        "xrr=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "yrr=tf.zeros((len(yy),1), dtype=DTYPE)\n",
        "xrr=tf.add(xrr[:,0],xx)\n",
        "yrr=tf.add(yrr[:,0],yy)\n",
        "urr=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "vrr=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "xtt=tf.reshape(xtt,shape=[tf.shape(xtt).numpy()[0],1])\n",
        "ytt=tf.reshape(ytt,shape=[tf.shape(ytt).numpy()[0],1])\n",
        "\n",
        "xbb=tf.reshape(xbb,shape=[tf.shape(xbb).numpy()[0],1])\n",
        "ybb=tf.reshape(ybb,shape=[tf.shape(ybb).numpy()[0],1])\n",
        "\n",
        "xrr=tf.reshape(xrr,shape=[tf.shape(xrr).numpy()[0],1])\n",
        "yrr=tf.reshape(yrr,shape=[tf.shape(yrr).numpy()[0],1])\n",
        "\n",
        "xll=tf.reshape(xll,shape=[tf.shape(xll).numpy()[0],1])\n",
        "yll=tf.reshape(yll,shape=[tf.shape(yll).numpy()[0],1])\n",
        "\n",
        "xbound=tf.concat([xtt,xbb,xrr,xll],0)\n",
        "ybound=tf.concat([ytt,ybb,yrr,yll],0)\n",
        "plt.scatter(xbound,ybound)\n",
        "\n",
        "ubound=tf.concat([utt,ubb,urr,ull],0)\n",
        "vbound=tf.concat([vtt,vbb,vrr,vll],0)\n",
        "\n",
        "xb=xbound\n",
        "yb=ybound\n",
        "\n",
        "ub=ubound\n",
        "vb=vbound\n",
        "\n",
        "##################\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Oct 24 13:02:32 2021\n",
        "\n",
        "@author: SAMAN\n",
        "\"\"\"\n",
        "# import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import scipy.io\n",
        "import math\n",
        "import matplotlib.gridspec as gridspec\n",
        "#from plotting import newfig\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras import layers, activations\n",
        "from scipy.interpolate import griddata\n",
        "#from eager_lbfgs import lbfgs, Struct\n",
        "from pyDOE import lhs\n",
        "\n",
        "\n",
        "gh = tf.constant(0.01, dtype=tf.float32)\n",
        "gg = tf.constant(9.8, dtype=tf.float32)\n",
        "weight_ub = tf.Variable([8.0], dtype=tf.float32)  # weight_ub = tf.Variable([8.0], dtype=tf.float32)\n",
        "weight_fu = tf.Variable([1.0], dtype=tf.float32)  # weight_fu = tf.Variable([1.0], dtype=tf.float32)\n",
        "layer_sizes = [2, 20, 20, 20, 20, 20, 20, 20, 3]\n",
        "sizes_w = []\n",
        "sizes_b = []\n",
        "loss_saman=[]\n",
        "list_model_u=[]\n",
        "for i, width in enumerate(layer_sizes):\n",
        "    if i != 1:\n",
        "        sizes_w.append(int(width * layer_sizes[1]))\n",
        "        sizes_b.append(int(width if i != 0 else layer_sizes[1]))\n",
        "\n",
        "\n",
        "\n",
        "# L-BFGS weight getting and setting from https://github.com/pierremtb/PINNs-TF2.0\n",
        "\n",
        "def set_weights(model, w, sizes_w, sizes_b):  # 重新设置参数\n",
        "\n",
        "    for i, layer in enumerate(model.layers[1:len(sizes_w) + 1]):\n",
        "        start_weights = sum(sizes_w[:i]) + sum(sizes_b[:i])\n",
        "        end_weights = sum(sizes_w[:i + 1]) + sum(sizes_b[:i])\n",
        "        weights = w[start_weights:end_weights]\n",
        "        w_div = int(sizes_w[i] / sizes_b[i])\n",
        "        weights = tf.reshape(weights, [w_div, sizes_b[i]])\n",
        "        biases = w[end_weights:end_weights + sizes_b[i]]\n",
        "        weights_biases = [weights, biases]\n",
        "        layer.set_weights(weights_biases)\n",
        "\n",
        "\n",
        "def get_weights(model):\n",
        "    w = []\n",
        "    for layer in model.layers[1:len(sizes_w) + 1]:\n",
        "        weights_biases = layer.get_weights()\n",
        "        weights = weights_biases[0].flatten()\n",
        "        biases = weights_biases[1]\n",
        "        w.extend(weights)\n",
        "        w.extend(biases)\n",
        "    w = tf.convert_to_tensor(w)\n",
        "    return w\n",
        "\n",
        "def xavier_init(layer_sizes):\n",
        "    in_dim = layer_sizes[0]\n",
        "    out_dim = layer_sizes[1]\n",
        "    xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "    return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "\n",
        "def neural_net(layer_sizes):\n",
        "\n",
        "    input_tensor = keras.Input(shape=(layer_sizes[0],))\n",
        "\n",
        "    hide_layer_list = []\n",
        "    flag = True\n",
        "    for width in layer_sizes[1:-1]:\n",
        "        if flag:\n",
        "            x = layers.Dense(\n",
        "                width, activation=tf.nn.tanh,\n",
        "                kernel_initializer=\"glorot_normal\")(input_tensor)\n",
        "            flag = False\n",
        "        else:\n",
        "            x = layers.Dense(\n",
        "                width, activation=tf.nn.tanh,\n",
        "                kernel_initializer=\"glorot_normal\")(x)\n",
        "    output_tensor = layers.Dense(layer_sizes[-1], activation=None,kernel_initializer=\"glorot_normal\")(x)\n",
        "    print(\"xxxxxxxxxxxxxx\")\n",
        "    output0 = output_tensor[:, 0:1]\n",
        "    output1 = output_tensor[:, 1:2]\n",
        "    output2 = output_tensor[:, 2:3]\n",
        "\n",
        "    model_output = keras.models.Model(input_tensor, [output0, output1, output2])\n",
        "\n",
        "    return model_output\n",
        "\n",
        "# initialize the NN\n",
        "u_model = neural_net(layer_sizes)\n",
        "# view the NN\n",
        "u_model.summary()\n",
        "\n",
        "def gh1(pp):\n",
        "  return pp\n",
        "\n",
        "\n",
        "# define the loss\n",
        "def loss(x_f_batch, y_f_batch, xb, yb, ub, vb, weight_ub,weight_fu,x_top,y_top,x_bottom,y_bottom,x_right,y_right,x_left,y_left):\n",
        "\n",
        "    f_u_pred, f_v_pred, div_pred = f_model(x_f_batch, y_f_batch)\n",
        "\n",
        "\n",
        "    u_pred, v_pred, p_pred = u_model(tf.concat([xb, yb], 1))\n",
        "\n",
        "    #mse_b = 100*weight_ub*(tf.reduce_sum(tf.square(u_pred - ub)) + tf.reduce_sum(tf.square(v_pred - vb)))\n",
        "    #mse_b = 1*weight_ub*(tf.reduce_sum(tf.square(u_pred - ub)) + tf.reduce_sum(tf.square(v_pred - vb)))\n",
        "    loss_2 = loss_bd(x_top,y_top,x_bottom,y_bottom,x_right,y_right,x_left,y_left)\n",
        "    ###mse_b = loss_bd(x_top,y_top,x_bottom,y_bottom,x_right,y_right,x_left,y_left)*weight_ub\n",
        "    #mse_b = loss_2*weight_ub#+ mse_b\n",
        "\n",
        "    #mse_f = weight_fu*(tf.reduce_sum(tf.square(f_u_pred)) + tf.reduce_sum(tf.square(f_v_pred)) + tf.reduce_sum(tf.square(div_pred)))\n",
        "    loss3=(tf.reduce_sum(tf.square(f_u_pred)) + tf.reduce_sum(tf.square(f_v_pred)) + tf.reduce_sum(tf.square(div_pred)))\n",
        "    ###mse_f=(tf.reduce_sum(tf.square(f_u_pred)) + tf.reduce_sum(tf.square(f_v_pred)) + tf.reduce_sum(tf.square(div_pred)))*weight_fu\n",
        "    #weight_ub = tf.cond(loss3>loss_2,lambda:[loss_2/loss3],lambda:[1.0])\n",
        "    #weight_fu = tf.cond(loss_2>loss3, lambda:[1.0],lambda:[loss3/loss_2])\n",
        "\n",
        "\n",
        "\n",
        "    #weight_ub.assign([1.0])\n",
        "    #weight_fu.assign([1.0])\n",
        "\n",
        "    mse_b = loss_2 * weight_ub\n",
        "    mse_f = loss3\n",
        "\n",
        "    tf.print('mse_b ======',mse_b)\n",
        "\n",
        "    #tf.print('reduce_max',tf.reduce_max(f_u_pred))\n",
        "    #tf.print('min or max',tf.math.minimum(f_u_pred))\n",
        "    return mse_b + mse_f, mse_b, mse_f\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def loss_bd(xtop,ytop,xbottom,ybottom,xright,yright,xleft,yleft):\n",
        "  utop, vtop, ptop = u_model(tf.concat([xtop, ytop],1))\n",
        "  ubottom, vbottom, pbottom = u_model(tf.concat([xbottom, ybottom],1))\n",
        "  uright, vright, pright = u_model(tf.concat([xright, yright],1))\n",
        "  uleft, vleft, pleft = u_model(tf.concat([xleft, yleft],1))\n",
        "\n",
        "  #tf.print('=======================pbottom',tf.shape(pbottom),'ybottom',tf.shape(ybottom))\n",
        "#  loss_bd = tf.reduce_sum(tf.square(tf.gradients(ptop, ytop)[0]))+tf.reduce_sum(tf.square(tf.gradients(pbottom, ybottom)[0])) \\\n",
        "#  + tf.reduce_sum(tf.square(pright))+tf.reduce_sum(tf.square(tf.gradients(pleft, xleft)[0])) \\\n",
        "#  + tf.reduce_sum(tf.square(uleft-1))+tf.reduce_sum(tf.square(utop))+tf.reduce_sum(tf.square(ubottom))  \\\n",
        "#  + tf.reduce_sum(tf.square(tf.gradients(uright, xright)[0]))  \\\n",
        "#  + tf.reduce_sum(tf.square(vleft))+tf.reduce_sum(tf.square(vtop))+tf.reduce_sum(tf.square(vbottom)) \\\n",
        "#  + tf.reduce_sum(tf.square(tf.gradients(vright, xright)[0]))\n",
        "\n",
        "\n",
        "  loss_bd = tf.reduce_sum(tf.square(tf.gradients(ptop, ptop)[0]))+tf.reduce_sum(tf.square(tf.gradients(pbottom, ybottom)[0])) \\\n",
        "  + tf.reduce_sum(tf.square(pright))+tf.reduce_sum(tf.square(tf.gradients(pleft, xleft)[0])) \\\n",
        "  + tf.reduce_sum(tf.square(uleft-1))+tf.reduce_sum(tf.square(utop))+tf.reduce_sum(tf.square(ubottom))  \\\n",
        "  + tf.reduce_sum(tf.square(tf.gradients(uright,xright)[0]))\\\n",
        "  + tf.reduce_sum(tf.square(vleft))+tf.reduce_sum(tf.square(vtop))+tf.reduce_sum(tf.square(vbottom))\\\n",
        "  + tf.reduce_sum(tf.square(tf.gradients(vright,xright)[0]))\n",
        "  return loss_bd\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def f_model(x, y):\n",
        "    u, v, p = u_model(tf.concat([x, y],1))\n",
        "\n",
        "    #saii=model_sai(x,y)\n",
        "    #fii=model_fi(x,y)\n",
        "\n",
        "    #fii_x=tf.gradients(fii,x)[0]\n",
        "\n",
        "\n",
        "    #u_t = tf.gradients(u, t)[0]\n",
        "    u_x = tf.gradients(u, x)[0]\n",
        "    u_y = tf.gradients(u, y)[0]\n",
        "    u_xx = tf.gradients(u_x, x)[0]\n",
        "    u_yy = tf.gradients(u_y, y)[0]\n",
        "\n",
        "    #v_t = tf.gradients(v, t)[0]\n",
        "    v_x = tf.gradients(v, x)[0]\n",
        "    v_y = tf.gradients(v, y)[0]\n",
        "    v_xx = tf.gradients(v_x, x)[0]\n",
        "    v_yy = tf.gradients(v_y, y)[0]\n",
        "\n",
        "    p_x = tf.gradients(p, x)[0]\n",
        "    p_y = tf.gradients(p, y)[0]\n",
        "\n",
        "    #bu, bv, bp = u_model(tf.concat([bx, by],1))\n",
        "    #c1=gh1(gh)\n",
        "    #c2=gh1(gg)\n",
        "    #c2 = tf.constant(9.8, dtype=tf.float32)\n",
        "\n",
        "    c1 = tf.constant(0.05, dtype=tf.float32)\n",
        "    omega=tf.constant(7.0, dtype=tf.float32)\n",
        "    Lxx=tf.constant(6.0, dtype=tf.float32)\n",
        "\n",
        "    div = u_x + v_y\n",
        "\n",
        "    #(1.0/Re)*(double(Lx)/double(Ly))*omega_h*omega_h*sai[i][j]*(fi[i+1][j]-fi[i-1][j])/double(2*dx);//inja kamel shavad\n",
        "    f_u = u*u_x + v*u_y + p_x -c1*(u_xx + u_yy) - c1*omega*omega*sai_ref*fi_ref_x*Lxx #+ kk # c33*c22*sai*fii_x/c44# - ((np.pi)*tf.cos(np.pi*x)*tf.cos(np.pi*y)*tf.sin(t) - tf.cos(np.pi*y)*(tf.sin(np.pi*x))**2*tf.sin(np.pi*y)*tf.cos(t) + \\\n",
        "    #c1*(2*(np.pi)**2*(tf.cos(np.pi*x))**2*tf.cos(np.pi*y)*tf.sin(np.pi*y)*tf.sin(t) - 6*(np.pi)**2*tf.cos(np.pi*y)*(tf.sin(np.pi*x))**2*tf.sin(np.pi*y)*tf.sin(t)) - \\\n",
        "    #tf.cos(np.pi*x)*tf.sin(np.pi*x)*(tf.sin(np.pi*y))**2*tf.sin(t)*(np.pi*(tf.cos(np.pi*y))**2*(tf.sin(np.pi*x))**2*tf.sin(t) - np.pi*(tf.sin(np.pi*x))**2*(tf.sin(np.pi*y))**2*tf.sin(t)) +\\\n",
        "    #2*np.pi*tf.cos(np.pi*x)*(tf.cos(np.pi*y))**2*(tf.sin(np.pi*x))**3*(tf.sin(np.pi*y))**2*(tf.sin(t))**2)\n",
        "\n",
        "    f_v = u*v_x + v*v_y + p_y - c1*(v_xx + v_yy) #- (tf.cos(np.pi*x)*tf.sin(np.pi*x)*(tf.sin(np.pi*y))**2*tf.cos(t) - np.pi*tf.sin(np.pi*x)*tf.sin(np.pi*y)*tf.sin(t) - \\\n",
        "    #c1*(2*(np.pi)**2*tf.cos(np.pi*x)*(tf.cos(np.pi*y))**2*tf.sin(np.pi*x)*tf.sin(t) - 6*(np.pi)**2*tf.cos(np.pi*x)*tf.sin(np.pi*x)*(tf.sin(np.pi*y))**2*tf.sin(t)) -\\\n",
        "    #tf.cos(np.pi*y)*(tf.sin(np.pi*x))**2*tf.sin(np.pi*y)*tf.sin(t)*(np.pi*(tf.cos(np.pi*x))**2*(tf.sin(np.pi*y))**2*tf.sin(t) -\\\n",
        "    #np.pi*(tf.sin(np.pi*x))**2*(tf.sin(np.pi*y))**2*tf.sin(t)) + 2*np.pi*(tf.cos(np.pi*x))**2*tf.cos(np.pi*y)*(tf.sin(np.pi*x))**2*(tf.sin(np.pi*y))**3*(tf.sin(t))**2)\n",
        "\n",
        "    return f_u, f_v, div\n",
        "\n",
        "@tf.function\n",
        "def u_x_model(x, y):\n",
        "    u, v, w = u_model(tf.concat([x, y], 1))\n",
        "    return u, v, w\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def grad(u_model, x_f_batch, y_f_batch, xb_batch, yb_batch, ub_batch, vb_batch, weight_ub,\n",
        "         weight_fu,x_top,y_top,x_bottom,y_bottom,x_right,y_right,x_left,y_left):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "        loss_value, mse_b, mse_f = loss(x_f_batch, y_f_batch,  xb_batch, yb_batch, ub_batch,\n",
        "                                        vb_batch, weight_ub, weight_fu,x_top,y_top,x_bottom,y_bottom,x_right,y_right,x_left,y_left)\n",
        "        grads = tape.gradient(loss_value, u_model.trainable_variables)\n",
        "\n",
        "        grads_ub = tape.gradient(loss_value, weight_ub)\n",
        "\n",
        "        grads_fu = tape.gradient(loss_value, weight_fu)\n",
        "\n",
        "    return loss_value, mse_b, mse_f, grads, grads_ub, grads_fu\n",
        "\n",
        "\n",
        "def fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star,xtt,ytt,xbb,ybb,xrr,yrr,xll,yll, tf_iter, tf_iter2,\n",
        "        newton_iter1, newton_iter2):\n",
        "\n",
        "    batch_sz = N_f\n",
        "    n_batches = N_f // batch_sz\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    tf_optimizer = tf.keras.optimizers.Adam(lr=0.005, beta_1=.99) #tf_optimizer = tf.keras.optimizers.Adam(lr=0.005, beta_1=.99)\n",
        "    tf_optimizer_weights = tf.keras.optimizers.Adam(lr=0.003, beta_1=.99)\n",
        "    tf_optimizer_u = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
        "\n",
        "    tf.print(f\"weight_ub: {weight_ub}  weight_fu: {weight_fu}\")\n",
        "    print(\"starting Adam training\")\n",
        "\n",
        "    a = np.random.rand(1000)\n",
        "    loss_history = list(a)\n",
        "    MSE_b0 = list(a)\n",
        "    MSE_f0 = list(a)\n",
        "\n",
        "\n",
        "    MSE_b1 = []\n",
        "    MSE_f1 = []\n",
        "\n",
        "    weightu = []\n",
        "    weightf = []\n",
        "    # For mini-batch (if used)\n",
        "    for epoch in range(tf_iter):\n",
        "        for i in range(n_batches):\n",
        "            xb_batch = xb\n",
        "            yb_batch = yb\n",
        "\n",
        "            ub_batch = ub\n",
        "            vb_batch = vb\n",
        "\n",
        "            x_top=xtt\n",
        "            y_top=ytt\n",
        "\n",
        "            x_bottom=xbb\n",
        "            y_bottom=ybb\n",
        "\n",
        "            x_right=xrr\n",
        "            y_right=yrr\n",
        "\n",
        "            x_left=xll\n",
        "            y_left=yll\n",
        "\n",
        "\n",
        "            x_f_batch = x_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "            y_f_batch = y_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "            t_f_batch = t_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "\n",
        "            loss_value, mse_b, mse_f, grads, grads_ub, grads_fu = grad(u_model, x_f_batch, y_f_batch,\n",
        "                                                                       xb_batch, yb_batch,\n",
        "                                                                       ub_batch, vb_batch, weight_ub,\n",
        "                                                                       weight_fu,\n",
        "                                                                       x_top,y_top,x_bottom,y_bottom,\n",
        "                                                                       x_right,y_right,x_left,y_left)\n",
        "\n",
        "            tf_optimizer.apply_gradients(zip(grads, u_model.trainable_variables))\n",
        "            MSE_b0.append(mse_b)\n",
        "            MSE_f0.append(mse_f)\n",
        "\n",
        "            loss_history.append(loss_value)\n",
        "            loss_saman.append(loss_value)\n",
        "            list_model_u.append(u_model)\n",
        "#            if loss_history[-1] < loss_history[-2] and loss_history[-2] < loss_history[-3] and loss_history[-1] < \\\n",
        "#                    loss_history[-10]:\n",
        "#                tf_optimizer_weights.apply_gradients(zip([-grads_fu], [weight_fu]))\n",
        "#                tf_optimizer_u.apply_gradients(zip([-grads_ub], [weight_ub]))\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('It: %d, Time: %.2f' % (epoch, elapsed))\n",
        "            tf.print(f\"mse_b  {mse_b}  mse_f: {mse_f}   total loss: {loss_value}\")\n",
        "\n",
        "            wu = weight_ub.numpy()\n",
        "            wf = weight_fu.numpy()\n",
        "\n",
        "            MSE_b1.append(mse_b)\n",
        "            MSE_f1.append(mse_f)\n",
        "\n",
        "            weightu.append(wu)\n",
        "            weightf.append(wf)\n",
        "\n",
        "            start_time = time.time()\n",
        "    tf.print(f\"weight_ub: {weight_ub}  weight_fu: {weight_fu}\")\n",
        "    u_pred, v_pred, p_pred = predict(X_star)\n",
        "    tf.print('epoch',epoch,'loss',loss_value)\n",
        "    #error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
        "    #print('Error u: %e' % (error_u))\n",
        "    #error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
        "    #print('Error v: %e' % (error_v))\n",
        "    #print(\"Starting L-BFGS training\")\n",
        "\n",
        "    '''\n",
        "    loss_and_flat_grad = get_loss_and_flat_grad(x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch,\n",
        "                                                vb_batch, weight_ub, weight_fu)\n",
        "\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "          get_weights(u_model),\n",
        "          Struct(), maxIter=newton_iter1, learningRate=0.8)\n",
        "\n",
        "    u_pred, v_pred, p_pred = predict(X_star)\n",
        "    error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
        "    print('Error u: %e' % (error_u))\n",
        "    error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
        "    print('Error v: %e' % (error_v))\n",
        "\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "          get_weights(u_model),\n",
        "          Struct(), maxIter=newton_iter2, learningRate=0.8)\n",
        "    '''\n",
        "    return MSE_b1, MSE_f1,  weightu, weightf,loss_saman\n",
        "\n",
        "# L-BFGS implementation from https://github.com/pierremtb/PINNs-TF2.0\n",
        "def get_loss_and_flat_grad(x_f_batch, y_f_batch , xb_batch, yb_batch,\n",
        "                           ub_batch, vb_batch,weight_ub, weight_fu):\n",
        "    def loss_and_flat_grad(w):\n",
        "        with tf.GradientTape() as tape:\n",
        "            set_weights(u_model, w, sizes_w, sizes_b)\n",
        "            loss_value, _, _ = loss(x_f_batch, y_f_batch, xb_batch, yb_batch, ub_batch, vb_batch, weight_ub, weight_fu)\n",
        "        grad = tape.gradient(loss_value, u_model.trainable_variables)\n",
        "        grad_flat = []\n",
        "        for g in grad:\n",
        "            grad_flat.append(tf.reshape(g, [-1]))\n",
        "        grad_flat = tf.concat(grad_flat, 0)\n",
        "        # print(loss_value, grad_flat)\n",
        "        return loss_value, grad_flat\n",
        "\n",
        "    return loss_and_flat_grad\n",
        "\n",
        "\n",
        "def predict(X_star):\n",
        "    X_star = tf.convert_to_tensor(X_star, dtype=tf.float32)\n",
        "    u_star, v_star, p_star = u_x_model(X_star[:, 0:1], X_star[:, 1:2])\n",
        "    return u_star.numpy(), v_star.numpy(), p_star.numpy()\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "MSE_b1, MSE_f1, weightu, weightf, loss_saman, list_model_u = fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star1,xtt,ytt,xbb,ybb,xrr,yrr,xll,yll,\n",
        "                                                                 tf_iter=300000,\n",
        "                                                                 tf_iter2=1000, newton_iter1=5000,newton_iter2=15000)\n",
        "\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.4f' % (elapsed))\n"
      ],
      "metadata": {
        "id": "jkzGuQAIdWhl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d6bb86b1-9b8b-49e4-b793-886fdf0dbf7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xxxxxxxxxxxxxx\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 2)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_48 (Dense)               (None, 20)           60          ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " dense_49 (Dense)               (None, 20)           420         ['dense_48[0][0]']               \n",
            "                                                                                                  \n",
            " dense_50 (Dense)               (None, 20)           420         ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            " dense_51 (Dense)               (None, 20)           420         ['dense_50[0][0]']               \n",
            "                                                                                                  \n",
            " dense_52 (Dense)               (None, 20)           420         ['dense_51[0][0]']               \n",
            "                                                                                                  \n",
            " dense_53 (Dense)               (None, 20)           420         ['dense_52[0][0]']               \n",
            "                                                                                                  \n",
            " dense_54 (Dense)               (None, 20)           420         ['dense_53[0][0]']               \n",
            "                                                                                                  \n",
            " dense_55 (Dense)               (None, 3)            63          ['dense_54[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_10 (S  (None, 1)           0           ['dense_55[0][0]']               \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_11 (S  (None, 1)           0           ['dense_55[0][0]']               \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_12 (S  (None, 1)           0           ['dense_55[0][0]']               \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,643\n",
            "Trainable params: 2,643\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "weight_ub: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([8.], dtype=float32)>  weight_fu: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>\n",
            "starting Adam training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "mse_b  [870.49426]  mse_f: 4.674972057342529   total loss: [875.16925]\n",
            "mse_b ====== [872.417114]\n",
            "It: 52654, Time: 0.03\n",
            "mse_b  [872.4171]  mse_f: 4.777572154998779   total loss: [877.1947]\n",
            "mse_b ====== [867.375122]\n",
            "It: 52655, Time: 0.03\n",
            "mse_b  [867.3751]  mse_f: 3.3015589714050293   total loss: [870.6767]\n",
            "mse_b ====== [869.21051]\n",
            "It: 52656, Time: 0.03\n",
            "mse_b  [869.2105]  mse_f: 4.195425033569336   total loss: [873.40594]\n",
            "mse_b ====== [873.10376]\n",
            "It: 52657, Time: 0.04\n",
            "mse_b  [873.10376]  mse_f: 4.5998125076293945   total loss: [877.70355]\n",
            "mse_b ====== [867.900696]\n",
            "It: 52658, Time: 0.03\n",
            "mse_b  [867.9007]  mse_f: 4.236873149871826   total loss: [872.1376]\n",
            "mse_b ====== [867.964905]\n",
            "It: 52659, Time: 0.03\n",
            "mse_b  [867.9649]  mse_f: 3.8473238945007324   total loss: [871.81226]\n",
            "mse_b ====== [872.243]\n",
            "It: 52660, Time: 0.03\n",
            "mse_b  [872.243]  mse_f: 4.734940528869629   total loss: [876.9779]\n",
            "mse_b ====== [868.67749]\n",
            "It: 52661, Time: 0.03\n",
            "mse_b  [868.6775]  mse_f: 4.112226486206055   total loss: [872.78973]\n",
            "mse_b ====== [867.627625]\n",
            "It: 52662, Time: 0.03\n",
            "mse_b  [867.6276]  mse_f: 4.666852951049805   total loss: [872.2945]\n",
            "mse_b ====== [871.270874]\n",
            "It: 52663, Time: 0.03\n",
            "mse_b  [871.2709]  mse_f: 4.421438694000244   total loss: [875.6923]\n",
            "mse_b ====== [868.602234]\n",
            "It: 52664, Time: 0.04\n",
            "mse_b  [868.60223]  mse_f: 4.09507942199707   total loss: [872.6973]\n",
            "mse_b ====== [867.370483]\n",
            "It: 52665, Time: 0.03\n",
            "mse_b  [867.3705]  mse_f: 4.844677448272705   total loss: [872.21515]\n",
            "mse_b ====== [870.608765]\n",
            "It: 52666, Time: 0.04\n",
            "mse_b  [870.60876]  mse_f: 4.950203895568848   total loss: [875.55896]\n",
            "mse_b ====== [868.660889]\n",
            "It: 52667, Time: 0.03\n",
            "mse_b  [868.6609]  mse_f: 3.788499355316162   total loss: [872.4494]\n",
            "mse_b ====== [867.293884]\n",
            "It: 52668, Time: 0.03\n",
            "mse_b  [867.2939]  mse_f: 4.867803573608398   total loss: [872.1617]\n",
            "mse_b ====== [870.055908]\n",
            "It: 52669, Time: 0.04\n",
            "mse_b  [870.0559]  mse_f: 5.369566440582275   total loss: [875.4255]\n",
            "mse_b ====== [868.095093]\n",
            "It: 52670, Time: 0.03\n",
            "mse_b  [868.0951]  mse_f: 3.8496434688568115   total loss: [871.94476]\n",
            "mse_b ====== [867.8703]\n",
            "It: 52671, Time: 0.03\n",
            "mse_b  [867.8703]  mse_f: 4.6575398445129395   total loss: [872.52783]\n",
            "mse_b ====== [869.847168]\n",
            "It: 52672, Time: 0.03\n",
            "mse_b  [869.84717]  mse_f: 5.198480129241943   total loss: [875.04565]\n",
            "mse_b ====== [867.638184]\n",
            "It: 52673, Time: 0.03\n",
            "mse_b  [867.6382]  mse_f: 3.692548990249634   total loss: [871.33075]\n",
            "mse_b ====== [867.834595]\n",
            "It: 52674, Time: 0.03\n",
            "mse_b  [867.8346]  mse_f: 4.874044418334961   total loss: [872.7086]\n",
            "mse_b ====== [869.960815]\n",
            "It: 52675, Time: 0.03\n",
            "mse_b  [869.9608]  mse_f: 5.445498466491699   total loss: [875.4063]\n",
            "mse_b ====== [867.340942]\n",
            "It: 52676, Time: 0.03\n",
            "mse_b  [867.34094]  mse_f: 3.6732680797576904   total loss: [871.0142]\n",
            "mse_b ====== [868.115173]\n",
            "It: 52677, Time: 0.03\n",
            "mse_b  [868.1152]  mse_f: 4.370501518249512   total loss: [872.48566]\n",
            "mse_b ====== [868.970886]\n",
            "It: 52678, Time: 0.03\n",
            "mse_b  [868.9709]  mse_f: 6.285438537597656   total loss: [875.25635]\n",
            "mse_b ====== [867.176331]\n",
            "It: 52679, Time: 0.03\n",
            "mse_b  [867.17633]  mse_f: 3.776909351348877   total loss: [870.95325]\n",
            "mse_b ====== [867.94342]\n",
            "It: 52680, Time: 0.03\n",
            "mse_b  [867.9434]  mse_f: 4.965822219848633   total loss: [872.90924]\n",
            "mse_b ====== [869.365051]\n",
            "It: 52681, Time: 0.03\n",
            "mse_b  [869.36505]  mse_f: 5.330639362335205   total loss: [874.6957]\n",
            "mse_b ====== [867.221497]\n",
            "It: 52682, Time: 0.03\n",
            "mse_b  [867.2215]  mse_f: 3.4185755252838135   total loss: [870.6401]\n",
            "mse_b ====== [868.379639]\n",
            "It: 52683, Time: 0.03\n",
            "mse_b  [868.37964]  mse_f: 5.314508438110352   total loss: [873.69415]\n",
            "mse_b ====== [868.488831]\n",
            "It: 52684, Time: 0.04\n",
            "mse_b  [868.48883]  mse_f: 5.383032321929932   total loss: [873.8719]\n",
            "mse_b ====== [866.826233]\n",
            "It: 52685, Time: 0.03\n",
            "mse_b  [866.82623]  mse_f: 3.520148992538452   total loss: [870.3464]\n",
            "mse_b ====== [868.541748]\n",
            "It: 52686, Time: 0.05\n",
            "mse_b  [868.54175]  mse_f: 5.332757949829102   total loss: [873.8745]\n",
            "mse_b ====== [868.081665]\n",
            "It: 52687, Time: 0.03\n",
            "mse_b  [868.08167]  mse_f: 5.944642066955566   total loss: [874.0263]\n",
            "mse_b ====== [866.854553]\n",
            "It: 52688, Time: 0.03\n",
            "mse_b  [866.85455]  mse_f: 3.1722569465637207   total loss: [870.0268]\n",
            "mse_b ====== [868.554321]\n",
            "It: 52689, Time: 0.03\n",
            "mse_b  [868.5543]  mse_f: 5.446396827697754   total loss: [874.00073]\n",
            "mse_b ====== [867.776184]\n",
            "It: 52690, Time: 0.03\n",
            "mse_b  [867.7762]  mse_f: 5.307339191436768   total loss: [873.0835]\n",
            "mse_b ====== [867.146057]\n",
            "It: 52691, Time: 0.03\n",
            "mse_b  [867.14606]  mse_f: 3.4552161693573   total loss: [870.60126]\n",
            "mse_b ====== [868.840393]\n",
            "It: 52692, Time: 0.03\n",
            "mse_b  [868.8404]  mse_f: 5.158666610717773   total loss: [873.9991]\n",
            "mse_b ====== [867.442566]\n",
            "It: 52693, Time: 0.04\n",
            "mse_b  [867.44257]  mse_f: 4.630796432495117   total loss: [872.07336]\n",
            "mse_b ====== [867.605042]\n",
            "It: 52694, Time: 0.03\n",
            "mse_b  [867.60504]  mse_f: 3.5300612449645996   total loss: [871.13513]\n",
            "mse_b ====== [869.529968]\n",
            "It: 52695, Time: 0.03\n",
            "mse_b  [869.52997]  mse_f: 4.941644668579102   total loss: [874.4716]\n",
            "mse_b ====== [866.911804]\n",
            "It: 52696, Time: 0.03\n",
            "mse_b  [866.9118]  mse_f: 4.202117919921875   total loss: [871.1139]\n",
            "mse_b ====== [868.260498]\n",
            "It: 52697, Time: 0.03\n",
            "mse_b  [868.2605]  mse_f: 3.744619846343994   total loss: [872.0051]\n",
            "mse_b ====== [868.158264]\n",
            "It: 52698, Time: 0.03\n",
            "mse_b  [868.15826]  mse_f: 5.522669792175293   total loss: [873.6809]\n",
            "mse_b ====== [866.869751]\n",
            "It: 52699, Time: 0.03\n",
            "mse_b  [866.86975]  mse_f: 4.128227710723877   total loss: [870.998]\n",
            "mse_b ====== [868.483093]\n",
            "It: 52700, Time: 0.03\n",
            "mse_b  [868.4831]  mse_f: 4.260660648345947   total loss: [872.7438]\n",
            "mse_b ====== [868.128113]\n",
            "It: 52701, Time: 0.03\n",
            "mse_b  [868.1281]  mse_f: 4.808095455169678   total loss: [872.9362]\n",
            "mse_b ====== [866.91864]\n",
            "It: 52702, Time: 0.03\n",
            "mse_b  [866.91864]  mse_f: 3.808713436126709   total loss: [870.72736]\n",
            "mse_b ====== [868.891418]\n",
            "It: 52703, Time: 0.03\n",
            "mse_b  [868.8914]  mse_f: 4.501173973083496   total loss: [873.3926]\n",
            "mse_b ====== [867.660156]\n",
            "It: 52704, Time: 0.03\n",
            "mse_b  [867.66016]  mse_f: 4.802937984466553   total loss: [872.4631]\n",
            "mse_b ====== [867.249451]\n",
            "It: 52705, Time: 0.03\n",
            "mse_b  [867.24945]  mse_f: 3.552086591720581   total loss: [870.8015]\n",
            "mse_b ====== [869.300232]\n",
            "It: 52706, Time: 0.03\n",
            "mse_b  [869.30023]  mse_f: 4.097348690032959   total loss: [873.3976]\n",
            "mse_b ====== [867.442566]\n",
            "It: 52707, Time: 0.03\n",
            "mse_b  [867.44257]  mse_f: 4.251639366149902   total loss: [871.6942]\n",
            "mse_b ====== [868.162231]\n",
            "It: 52708, Time: 0.03\n",
            "mse_b  [868.16223]  mse_f: 3.3261632919311523   total loss: [871.4884]\n",
            "mse_b ====== [869.273376]\n",
            "It: 52709, Time: 0.03\n",
            "mse_b  [869.2734]  mse_f: 4.075108528137207   total loss: [873.3485]\n",
            "mse_b ====== [866.84552]\n",
            "It: 52710, Time: 0.03\n",
            "mse_b  [866.8455]  mse_f: 3.7289140224456787   total loss: [870.57446]\n",
            "mse_b ====== [868.468628]\n",
            "It: 52711, Time: 0.03\n",
            "mse_b  [868.4686]  mse_f: 3.8265230655670166   total loss: [872.29517]\n",
            "mse_b ====== [868.862671]\n",
            "It: 52712, Time: 0.03\n",
            "mse_b  [868.8627]  mse_f: 4.403154373168945   total loss: [873.2658]\n",
            "mse_b ====== [866.747742]\n",
            "It: 52713, Time: 0.03\n",
            "mse_b  [866.74774]  mse_f: 3.00669002532959   total loss: [869.75446]\n",
            "mse_b ====== [868.877563]\n",
            "It: 52714, Time: 0.03\n",
            "mse_b  [868.87756]  mse_f: 4.2801055908203125   total loss: [873.15765]\n",
            "mse_b ====== [868.334778]\n",
            "It: 52715, Time: 0.03\n",
            "mse_b  [868.3348]  mse_f: 3.6143829822540283   total loss: [871.94916]\n",
            "mse_b ====== [867.050415]\n",
            "It: 52716, Time: 0.04\n",
            "mse_b  [867.0504]  mse_f: 3.4314446449279785   total loss: [870.4819]\n",
            "mse_b ====== [869.148071]\n",
            "It: 52717, Time: 0.03\n",
            "mse_b  [869.1481]  mse_f: 4.002658367156982   total loss: [873.15076]\n",
            "mse_b ====== [867.472534]\n",
            "It: 52718, Time: 0.03\n",
            "mse_b  [867.47253]  mse_f: 3.5549964904785156   total loss: [871.0275]\n",
            "mse_b ====== [867.430725]\n",
            "It: 52719, Time: 0.03\n",
            "mse_b  [867.4307]  mse_f: 3.505707263946533   total loss: [870.93646]\n",
            "mse_b ====== [868.997131]\n",
            "It: 52720, Time: 0.03\n",
            "mse_b  [868.99713]  mse_f: 4.180949687957764   total loss: [873.1781]\n",
            "mse_b ====== [867.053406]\n",
            "It: 52721, Time: 0.04\n",
            "mse_b  [867.0534]  mse_f: 3.090386390686035   total loss: [870.1438]\n",
            "mse_b ====== [867.901062]\n",
            "It: 52722, Time: 0.04\n",
            "mse_b  [867.90106]  mse_f: 4.098410606384277   total loss: [871.99945]\n",
            "mse_b ====== [868.202576]\n",
            "It: 52723, Time: 0.03\n",
            "mse_b  [868.2026]  mse_f: 4.195131301879883   total loss: [872.3977]\n",
            "mse_b ====== [866.882141]\n",
            "It: 52724, Time: 0.03\n",
            "mse_b  [866.88214]  mse_f: 3.230715036392212   total loss: [870.11285]\n",
            "mse_b ====== [868.037476]\n",
            "It: 52725, Time: 0.04\n",
            "mse_b  [868.0375]  mse_f: 4.753216743469238   total loss: [872.7907]\n",
            "mse_b ====== [867.762695]\n",
            "It: 52726, Time: 0.03\n",
            "mse_b  [867.7627]  mse_f: 3.565155267715454   total loss: [871.3279]\n",
            "mse_b ====== [866.968]\n",
            "It: 52727, Time: 0.03\n",
            "mse_b  [866.968]  mse_f: 3.4587392807006836   total loss: [870.42676]\n",
            "mse_b ====== [868.704834]\n",
            "It: 52728, Time: 0.03\n",
            "mse_b  [868.70483]  mse_f: 4.474259376525879   total loss: [873.1791]\n",
            "mse_b ====== [867.37915]\n",
            "It: 52729, Time: 0.03\n",
            "mse_b  [867.37915]  mse_f: 3.3103749752044678   total loss: [870.6895]\n",
            "mse_b ====== [867.310242]\n",
            "It: 52730, Time: 0.03\n",
            "mse_b  [867.31024]  mse_f: 3.6576781272888184   total loss: [870.9679]\n",
            "mse_b ====== [868.521362]\n",
            "It: 52731, Time: 0.03\n",
            "mse_b  [868.52136]  mse_f: 4.079208850860596   total loss: [872.6006]\n",
            "mse_b ====== [867.033081]\n",
            "It: 52732, Time: 0.03\n",
            "mse_b  [867.0331]  mse_f: 3.4737563133239746   total loss: [870.50684]\n",
            "mse_b ====== [868.157593]\n",
            "It: 52733, Time: 0.03\n",
            "mse_b  [868.1576]  mse_f: 3.506896495819092   total loss: [871.6645]\n",
            "mse_b ====== [868.391174]\n",
            "It: 52734, Time: 0.03\n",
            "mse_b  [868.3912]  mse_f: 3.4823665618896484   total loss: [871.87354]\n",
            "mse_b ====== [866.983032]\n",
            "It: 52735, Time: 0.03\n",
            "mse_b  [866.98303]  mse_f: 3.2489287853240967   total loss: [870.23193]\n",
            "mse_b ====== [868.735779]\n",
            "It: 52736, Time: 0.03\n",
            "mse_b  [868.7358]  mse_f: 3.5021281242370605   total loss: [872.2379]\n",
            "mse_b ====== [867.4823]\n",
            "It: 52737, Time: 0.03\n",
            "mse_b  [867.4823]  mse_f: 3.6530532836914062   total loss: [871.1354]\n",
            "mse_b ====== [867.334229]\n",
            "It: 52738, Time: 0.03\n",
            "mse_b  [867.3342]  mse_f: 2.943411350250244   total loss: [870.27765]\n",
            "mse_b ====== [868.794373]\n",
            "It: 52739, Time: 0.03\n",
            "mse_b  [868.7944]  mse_f: 3.6978721618652344   total loss: [872.49225]\n",
            "mse_b ====== [867.046631]\n",
            "It: 52740, Time: 0.03\n",
            "mse_b  [867.04663]  mse_f: 3.1101033687591553   total loss: [870.15674]\n",
            "mse_b ====== [867.932068]\n",
            "It: 52741, Time: 0.03\n",
            "mse_b  [867.93207]  mse_f: 3.303518295288086   total loss: [871.2356]\n",
            "mse_b ====== [868.311096]\n",
            "It: 52742, Time: 0.03\n",
            "mse_b  [868.3111]  mse_f: 3.5648891925811768   total loss: [871.876]\n",
            "mse_b ====== [866.694]\n",
            "It: 52743, Time: 0.03\n",
            "mse_b  [866.694]  mse_f: 3.3141984939575195   total loss: [870.0082]\n",
            "mse_b ====== [868.059143]\n",
            "It: 52744, Time: 0.03\n",
            "mse_b  [868.05914]  mse_f: 3.712193489074707   total loss: [871.77136]\n",
            "mse_b ====== [867.867737]\n",
            "It: 52745, Time: 0.03\n",
            "mse_b  [867.86774]  mse_f: 3.336998224258423   total loss: [871.2047]\n",
            "mse_b ====== [866.915222]\n",
            "It: 52746, Time: 0.04\n",
            "mse_b  [866.9152]  mse_f: 3.338963508605957   total loss: [870.2542]\n",
            "mse_b ====== [868.320068]\n",
            "It: 52747, Time: 0.04\n",
            "mse_b  [868.32007]  mse_f: 3.786893844604492   total loss: [872.10693]\n",
            "mse_b ====== [867.138367]\n",
            "It: 52748, Time: 0.04\n",
            "mse_b  [867.13837]  mse_f: 3.039566993713379   total loss: [870.1779]\n",
            "mse_b ====== [867.352478]\n",
            "It: 52749, Time: 0.04\n",
            "mse_b  [867.3525]  mse_f: 3.8475182056427   total loss: [871.2]\n",
            "mse_b ====== [868.020142]\n",
            "It: 52750, Time: 0.04\n",
            "mse_b  [868.02014]  mse_f: 3.548351526260376   total loss: [871.5685]\n",
            "mse_b ====== [866.958252]\n",
            "It: 52751, Time: 0.03\n",
            "mse_b  [866.95825]  mse_f: 3.115434169769287   total loss: [870.07367]\n",
            "mse_b ====== [867.632568]\n",
            "It: 52752, Time: 0.03\n",
            "mse_b  [867.63257]  mse_f: 3.6959259510040283   total loss: [871.3285]\n",
            "mse_b ====== [867.902832]\n",
            "It: 52753, Time: 0.03\n",
            "mse_b  [867.90283]  mse_f: 3.6113510131835938   total loss: [871.51416]\n",
            "mse_b ====== [866.932434]\n",
            "It: 52754, Time: 0.03\n",
            "mse_b  [866.93243]  mse_f: 3.1484551429748535   total loss: [870.0809]\n",
            "mse_b ====== [868.073792]\n",
            "It: 52755, Time: 0.03\n",
            "mse_b  [868.0738]  mse_f: 3.4749388694763184   total loss: [871.5487]\n",
            "mse_b ====== [867.231689]\n",
            "It: 52756, Time: 0.03\n",
            "mse_b  [867.2317]  mse_f: 3.4271819591522217   total loss: [870.6589]\n",
            "mse_b ====== [867.502625]\n",
            "It: 52757, Time: 0.03\n",
            "mse_b  [867.5026]  mse_f: 3.008427143096924   total loss: [870.51105]\n",
            "mse_b ====== [868.240112]\n",
            "It: 52758, Time: 0.03\n",
            "mse_b  [868.2401]  mse_f: 3.3546366691589355   total loss: [871.5947]\n",
            "mse_b ====== [866.738]\n",
            "It: 52759, Time: 0.03\n",
            "mse_b  [866.738]  mse_f: 3.2353873252868652   total loss: [869.9734]\n",
            "mse_b ====== [867.814514]\n",
            "It: 52760, Time: 0.03\n",
            "mse_b  [867.8145]  mse_f: 3.3648898601531982   total loss: [871.1794]\n",
            "mse_b ====== [867.910278]\n",
            "It: 52761, Time: 0.03\n",
            "mse_b  [867.9103]  mse_f: 3.393172264099121   total loss: [871.30347]\n",
            "mse_b ====== [866.733887]\n",
            "It: 52762, Time: 0.04\n",
            "mse_b  [866.7339]  mse_f: 3.0604686737060547   total loss: [869.7944]\n",
            "mse_b ====== [868.109314]\n",
            "It: 52763, Time: 0.03\n",
            "mse_b  [868.1093]  mse_f: 3.4206185340881348   total loss: [871.5299]\n",
            "mse_b ====== [867.359253]\n",
            "It: 52764, Time: 0.03\n",
            "mse_b  [867.35925]  mse_f: 3.239182949066162   total loss: [870.59845]\n",
            "mse_b ====== [866.97168]\n",
            "It: 52765, Time: 0.03\n",
            "mse_b  [866.9717]  mse_f: 3.1797666549682617   total loss: [870.1514]\n",
            "mse_b ====== [868.103577]\n",
            "It: 52766, Time: 0.03\n",
            "mse_b  [868.1036]  mse_f: 3.473095417022705   total loss: [871.57666]\n",
            "mse_b ====== [866.931641]\n",
            "It: 52767, Time: 0.03\n",
            "mse_b  [866.93164]  mse_f: 3.213352680206299   total loss: [870.145]\n",
            "mse_b ====== [867.269]\n",
            "It: 52768, Time: 0.03\n",
            "mse_b  [867.269]  mse_f: 3.4004065990448   total loss: [870.6694]\n",
            "mse_b ====== [867.833496]\n",
            "It: 52769, Time: 0.03\n",
            "mse_b  [867.8335]  mse_f: 3.3090083599090576   total loss: [871.1425]\n",
            "mse_b ====== [866.801453]\n",
            "It: 52770, Time: 0.03\n",
            "mse_b  [866.80145]  mse_f: 3.213334321975708   total loss: [870.0148]\n",
            "mse_b ====== [867.769165]\n",
            "It: 52771, Time: 0.03\n",
            "mse_b  [867.76917]  mse_f: 3.4409189224243164   total loss: [871.2101]\n",
            "mse_b ====== [867.216431]\n",
            "It: 52772, Time: 0.03\n",
            "mse_b  [867.21643]  mse_f: 3.4334919452667236   total loss: [870.6499]\n",
            "mse_b ====== [866.920898]\n",
            "It: 52773, Time: 0.03\n",
            "mse_b  [866.9209]  mse_f: 2.8293938636779785   total loss: [869.7503]\n",
            "mse_b ====== [868.156372]\n",
            "It: 52774, Time: 0.03\n",
            "mse_b  [868.1564]  mse_f: 3.512516975402832   total loss: [871.6689]\n",
            "mse_b ====== [866.844788]\n",
            "It: 52775, Time: 0.03\n",
            "mse_b  [866.8448]  mse_f: 3.198983907699585   total loss: [870.04376]\n",
            "mse_b ====== [867.392395]\n",
            "It: 52776, Time: 0.05\n",
            "mse_b  [867.3924]  mse_f: 3.044620990753174   total loss: [870.437]\n",
            "mse_b ====== [867.879395]\n",
            "It: 52777, Time: 0.03\n",
            "mse_b  [867.8794]  mse_f: 3.1773974895477295   total loss: [871.05676]\n",
            "mse_b ====== [866.728943]\n",
            "It: 52778, Time: 0.03\n",
            "mse_b  [866.72894]  mse_f: 3.0744972229003906   total loss: [869.80347]\n",
            "mse_b ====== [867.988159]\n",
            "It: 52779, Time: 0.03\n",
            "mse_b  [867.98816]  mse_f: 3.1379854679107666   total loss: [871.12616]\n",
            "mse_b ====== [867.282166]\n",
            "It: 52780, Time: 0.03\n",
            "mse_b  [867.28217]  mse_f: 3.070399761199951   total loss: [870.35254]\n",
            "mse_b ====== [867.040039]\n",
            "It: 52781, Time: 0.03\n",
            "mse_b  [867.04004]  mse_f: 2.8880343437194824   total loss: [869.9281]\n",
            "mse_b ====== [868.011108]\n",
            "It: 52782, Time: 0.03\n",
            "mse_b  [868.0111]  mse_f: 3.22041392326355   total loss: [871.2315]\n",
            "mse_b ====== [866.793091]\n",
            "It: 52783, Time: 0.03\n",
            "mse_b  [866.7931]  mse_f: 3.0341062545776367   total loss: [869.8272]\n",
            "mse_b ====== [867.266541]\n",
            "It: 52784, Time: 0.03\n",
            "mse_b  [867.26654]  mse_f: 3.333617687225342   total loss: [870.60016]\n",
            "mse_b ====== [867.711609]\n",
            "It: 52785, Time: 0.03\n",
            "mse_b  [867.7116]  mse_f: 3.0595672130584717   total loss: [870.7712]\n",
            "mse_b ====== [866.659363]\n",
            "It: 52786, Time: 0.03\n",
            "mse_b  [866.65936]  mse_f: 3.2267110347747803   total loss: [869.88605]\n",
            "mse_b ====== [867.522827]\n",
            "It: 52787, Time: 0.03\n",
            "mse_b  [867.5228]  mse_f: 3.2778384685516357   total loss: [870.80066]\n",
            "mse_b ====== [867.183594]\n",
            "It: 52788, Time: 0.03\n",
            "mse_b  [867.1836]  mse_f: 3.139686107635498   total loss: [870.3233]\n",
            "mse_b ====== [866.757874]\n",
            "It: 52789, Time: 0.04\n",
            "mse_b  [866.7579]  mse_f: 3.119285821914673   total loss: [869.87714]\n",
            "mse_b ====== [867.792664]\n",
            "It: 52790, Time: 0.03\n",
            "mse_b  [867.79266]  mse_f: 3.146284341812134   total loss: [870.93896]\n",
            "mse_b ====== [866.902832]\n",
            "It: 52791, Time: 0.03\n",
            "mse_b  [866.90283]  mse_f: 3.04133677482605   total loss: [869.94415]\n",
            "mse_b ====== [867.30304]\n",
            "It: 52792, Time: 0.03\n",
            "mse_b  [867.30304]  mse_f: 2.9987728595733643   total loss: [870.3018]\n",
            "mse_b ====== [867.563843]\n",
            "It: 52793, Time: 0.03\n",
            "mse_b  [867.56384]  mse_f: 3.1485719680786133   total loss: [870.7124]\n",
            "mse_b ====== [866.722961]\n",
            "It: 52794, Time: 0.04\n",
            "mse_b  [866.72296]  mse_f: 3.011802911758423   total loss: [869.73474]\n",
            "mse_b ====== [867.619934]\n",
            "It: 52795, Time: 0.03\n",
            "mse_b  [867.61993]  mse_f: 3.096285820007324   total loss: [870.71625]\n",
            "mse_b ====== [867.195435]\n",
            "It: 52796, Time: 0.03\n",
            "mse_b  [867.19543]  mse_f: 3.01776385307312   total loss: [870.2132]\n",
            "mse_b ====== [866.848083]\n",
            "It: 52797, Time: 0.04\n",
            "mse_b  [866.8481]  mse_f: 2.865828275680542   total loss: [869.7139]\n",
            "mse_b ====== [867.889771]\n",
            "It: 52798, Time: 0.03\n",
            "mse_b  [867.8898]  mse_f: 3.1096458435058594   total loss: [870.9994]\n",
            "mse_b ====== [866.781677]\n",
            "It: 52799, Time: 0.03\n",
            "mse_b  [866.7817]  mse_f: 2.977570056915283   total loss: [869.7593]\n",
            "mse_b ====== [867.135925]\n",
            "It: 52800, Time: 0.03\n",
            "mse_b  [867.1359]  mse_f: 3.038130521774292   total loss: [870.1741]\n",
            "mse_b ====== [867.247375]\n",
            "It: 52801, Time: 0.03\n",
            "mse_b  [867.2474]  mse_f: 3.4491541385650635   total loss: [870.69653]\n",
            "mse_b ====== [866.726379]\n",
            "It: 52802, Time: 0.03\n",
            "mse_b  [866.7264]  mse_f: 3.1485843658447266   total loss: [869.87494]\n",
            "mse_b ====== [867.428467]\n",
            "It: 52803, Time: 0.04\n",
            "mse_b  [867.42847]  mse_f: 3.5170481204986572   total loss: [870.9455]\n",
            "mse_b ====== [867.609131]\n",
            "It: 52804, Time: 0.04\n",
            "mse_b  [867.60913]  mse_f: 3.5358264446258545   total loss: [871.14496]\n",
            "mse_b ====== [866.951904]\n",
            "It: 52805, Time: 0.03\n",
            "mse_b  [866.9519]  mse_f: 4.782142639160156   total loss: [871.7341]\n",
            "mse_b ====== [867.59552]\n",
            "It: 52806, Time: 0.03\n",
            "mse_b  [867.5955]  mse_f: 6.075327396392822   total loss: [873.67084]\n",
            "mse_b ====== [867.168274]\n",
            "It: 52807, Time: 0.03\n",
            "mse_b  [867.1683]  mse_f: 4.706779956817627   total loss: [871.87506]\n",
            "mse_b ====== [867.03949]\n",
            "It: 52808, Time: 0.03\n",
            "mse_b  [867.0395]  mse_f: 3.244513988494873   total loss: [870.284]\n",
            "mse_b ====== [867.508179]\n",
            "It: 52809, Time: 0.03\n",
            "mse_b  [867.5082]  mse_f: 3.9086408615112305   total loss: [871.4168]\n",
            "mse_b ====== [866.878235]\n",
            "It: 52810, Time: 0.03\n",
            "mse_b  [866.87823]  mse_f: 4.937504768371582   total loss: [871.81573]\n",
            "mse_b ====== [868.403809]\n",
            "It: 52811, Time: 0.03\n",
            "mse_b  [868.4038]  mse_f: 6.240363121032715   total loss: [874.64417]\n",
            "mse_b ====== [868.053284]\n",
            "It: 52812, Time: 0.03\n",
            "mse_b  [868.0533]  mse_f: 5.207292556762695   total loss: [873.26056]\n",
            "mse_b ====== [866.891479]\n",
            "It: 52813, Time: 0.03\n",
            "mse_b  [866.8915]  mse_f: 4.450180530548096   total loss: [871.3417]\n",
            "mse_b ====== [868.540771]\n",
            "It: 52814, Time: 0.03\n",
            "mse_b  [868.5408]  mse_f: 4.556121349334717   total loss: [873.09686]\n",
            "mse_b ====== [868.59259]\n",
            "It: 52815, Time: 0.03\n",
            "mse_b  [868.5926]  mse_f: 5.24444580078125   total loss: [873.83704]\n",
            "mse_b ====== [867.352356]\n",
            "It: 52816, Time: 0.03\n",
            "mse_b  [867.35236]  mse_f: 5.619950294494629   total loss: [872.9723]\n",
            "mse_b ====== [867.996948]\n",
            "It: 52817, Time: 0.03\n",
            "mse_b  [867.99695]  mse_f: 4.431507110595703   total loss: [872.42847]\n",
            "mse_b ====== [867.282349]\n",
            "It: 52818, Time: 0.04\n",
            "mse_b  [867.28235]  mse_f: 5.1715898513793945   total loss: [872.4539]\n",
            "mse_b ====== [867.444885]\n",
            "It: 52819, Time: 0.03\n",
            "mse_b  [867.4449]  mse_f: 4.997434139251709   total loss: [872.4423]\n",
            "mse_b ====== [867.370178]\n",
            "It: 52820, Time: 0.03\n",
            "mse_b  [867.3702]  mse_f: 4.398319244384766   total loss: [871.7685]\n",
            "mse_b ====== [868.053223]\n",
            "It: 52821, Time: 0.03\n",
            "mse_b  [868.0532]  mse_f: 5.300703525543213   total loss: [873.35394]\n",
            "mse_b ====== [867.255432]\n",
            "It: 52822, Time: 0.03\n",
            "mse_b  [867.25543]  mse_f: 4.913985729217529   total loss: [872.16943]\n",
            "mse_b ====== [867.051086]\n",
            "It: 52823, Time: 0.03\n",
            "mse_b  [867.0511]  mse_f: 4.294244766235352   total loss: [871.34534]\n",
            "mse_b ====== [866.980347]\n",
            "It: 52824, Time: 0.03\n",
            "mse_b  [866.98035]  mse_f: 5.433587074279785   total loss: [872.41394]\n",
            "mse_b ====== [867.51239]\n",
            "It: 52825, Time: 0.03\n",
            "mse_b  [867.5124]  mse_f: 5.694607257843018   total loss: [873.207]\n",
            "mse_b ====== [867.21167]\n",
            "It: 52826, Time: 0.03\n",
            "mse_b  [867.2117]  mse_f: 3.6559247970581055   total loss: [870.8676]\n",
            "mse_b ====== [867.382568]\n",
            "It: 52827, Time: 0.03\n",
            "mse_b  [867.38257]  mse_f: 4.226718902587891   total loss: [871.6093]\n",
            "mse_b ====== [867.627563]\n",
            "It: 52828, Time: 0.03\n",
            "mse_b  [867.62756]  mse_f: 5.498786449432373   total loss: [873.12634]\n",
            "mse_b ====== [867.216248]\n",
            "It: 52829, Time: 0.04\n",
            "mse_b  [867.21625]  mse_f: 4.059144496917725   total loss: [871.2754]\n",
            "mse_b ====== [867.215088]\n",
            "It: 52830, Time: 0.03\n",
            "mse_b  [867.2151]  mse_f: 4.5179948806762695   total loss: [871.7331]\n",
            "mse_b ====== [867.340576]\n",
            "It: 52831, Time: 0.03\n",
            "mse_b  [867.3406]  mse_f: 4.384003639221191   total loss: [871.7246]\n",
            "mse_b ====== [867.584412]\n",
            "It: 52832, Time: 0.03\n",
            "mse_b  [867.5844]  mse_f: 4.732615947723389   total loss: [872.317]\n",
            "mse_b ====== [867.345642]\n",
            "It: 52833, Time: 0.03\n",
            "mse_b  [867.34564]  mse_f: 3.9963860511779785   total loss: [871.34204]\n",
            "mse_b ====== [867.207153]\n",
            "It: 52834, Time: 0.03\n",
            "mse_b  [867.20715]  mse_f: 3.7206473350524902   total loss: [870.9278]\n",
            "mse_b ====== [868.285767]\n",
            "It: 52835, Time: 0.03\n",
            "mse_b  [868.28577]  mse_f: 4.765178203582764   total loss: [873.05096]\n",
            "mse_b ====== [867.270142]\n",
            "It: 52836, Time: 0.03\n",
            "mse_b  [867.27014]  mse_f: 3.708099842071533   total loss: [870.9783]\n",
            "mse_b ====== [867.303833]\n",
            "It: 52837, Time: 0.03\n",
            "mse_b  [867.30383]  mse_f: 3.705371379852295   total loss: [871.0092]\n",
            "mse_b ====== [868.091614]\n",
            "It: 52838, Time: 0.03\n",
            "mse_b  [868.0916]  mse_f: 5.293483257293701   total loss: [873.3851]\n",
            "mse_b ====== [867.222717]\n",
            "It: 52839, Time: 0.03\n",
            "mse_b  [867.2227]  mse_f: 4.861937999725342   total loss: [872.08466]\n",
            "mse_b ====== [867.354858]\n",
            "It: 52840, Time: 0.03\n",
            "mse_b  [867.35486]  mse_f: 4.3963541984558105   total loss: [871.7512]\n",
            "mse_b ====== [867.207764]\n",
            "It: 52841, Time: 0.03\n",
            "mse_b  [867.20776]  mse_f: 4.455483913421631   total loss: [871.66327]\n",
            "mse_b ====== [867.351624]\n",
            "It: 52842, Time: 0.03\n",
            "mse_b  [867.3516]  mse_f: 5.289470672607422   total loss: [872.6411]\n",
            "mse_b ====== [867.28772]\n",
            "It: 52843, Time: 0.03\n",
            "mse_b  [867.2877]  mse_f: 3.8409085273742676   total loss: [871.1286]\n",
            "mse_b ====== [867.220642]\n",
            "It: 52844, Time: 0.04\n",
            "mse_b  [867.22064]  mse_f: 4.330475807189941   total loss: [871.55115]\n",
            "mse_b ====== [867.303833]\n",
            "It: 52845, Time: 0.03\n",
            "mse_b  [867.30383]  mse_f: 4.951918125152588   total loss: [872.25574]\n",
            "mse_b ====== [867.209534]\n",
            "It: 52846, Time: 0.03\n",
            "mse_b  [867.20953]  mse_f: 4.524559020996094   total loss: [871.7341]\n",
            "mse_b ====== [866.761]\n",
            "It: 52847, Time: 0.03\n",
            "mse_b  [866.761]  mse_f: 4.260970592498779   total loss: [871.022]\n",
            "mse_b ====== [867.717346]\n",
            "It: 52848, Time: 0.03\n",
            "mse_b  [867.71735]  mse_f: 4.354050636291504   total loss: [872.0714]\n",
            "mse_b ====== [867.278442]\n",
            "It: 52849, Time: 0.03\n",
            "mse_b  [867.27844]  mse_f: 4.848520278930664   total loss: [872.12695]\n",
            "mse_b ====== [866.977234]\n",
            "It: 52850, Time: 0.03\n",
            "mse_b  [866.97723]  mse_f: 3.4883604049682617   total loss: [870.4656]\n",
            "mse_b ====== [867.23645]\n",
            "It: 52851, Time: 0.03\n",
            "mse_b  [867.23645]  mse_f: 4.749253273010254   total loss: [871.9857]\n",
            "mse_b ====== [867.068054]\n",
            "It: 52852, Time: 0.03\n",
            "mse_b  [867.06805]  mse_f: 5.127342224121094   total loss: [872.1954]\n",
            "mse_b ====== [867.059204]\n",
            "It: 52853, Time: 0.03\n",
            "mse_b  [867.0592]  mse_f: 3.3244218826293945   total loss: [870.3836]\n",
            "mse_b ====== [867.299133]\n",
            "It: 52854, Time: 0.03\n",
            "mse_b  [867.29913]  mse_f: 4.9260172843933105   total loss: [872.22516]\n",
            "mse_b ====== [867.268127]\n",
            "It: 52855, Time: 0.03\n",
            "mse_b  [867.2681]  mse_f: 4.0564727783203125   total loss: [871.3246]\n",
            "mse_b ====== [867.551147]\n",
            "It: 52856, Time: 0.03\n",
            "mse_b  [867.55115]  mse_f: 4.005002021789551   total loss: [871.55615]\n",
            "mse_b ====== [867.071411]\n",
            "It: 52857, Time: 0.03\n",
            "mse_b  [867.0714]  mse_f: 4.363561630249023   total loss: [871.435]\n",
            "mse_b ====== [867.610657]\n",
            "It: 52858, Time: 0.03\n",
            "mse_b  [867.61066]  mse_f: 3.4870095252990723   total loss: [871.09766]\n",
            "mse_b ====== [867.438]\n",
            "It: 52859, Time: 0.03\n",
            "mse_b  [867.438]  mse_f: 4.78873348236084   total loss: [872.22675]\n",
            "mse_b ====== [867.132568]\n",
            "It: 52860, Time: 0.03\n",
            "mse_b  [867.13257]  mse_f: 3.5562520027160645   total loss: [870.68884]\n",
            "mse_b ====== [867.681641]\n",
            "It: 52861, Time: 0.03\n",
            "mse_b  [867.68164]  mse_f: 3.989842414855957   total loss: [871.6715]\n",
            "mse_b ====== [867.376038]\n",
            "It: 52862, Time: 0.03\n",
            "mse_b  [867.37604]  mse_f: 4.059349060058594   total loss: [871.43536]\n",
            "mse_b ====== [867.365662]\n",
            "It: 52863, Time: 0.03\n",
            "mse_b  [867.36566]  mse_f: 3.555434465408325   total loss: [870.9211]\n",
            "mse_b ====== [867.943298]\n",
            "It: 52864, Time: 0.03\n",
            "mse_b  [867.9433]  mse_f: 4.154390811920166   total loss: [872.0977]\n",
            "mse_b ====== [867.127502]\n",
            "It: 52865, Time: 0.03\n",
            "mse_b  [867.1275]  mse_f: 3.033804178237915   total loss: [870.1613]\n",
            "mse_b ====== [867.26]\n",
            "It: 52866, Time: 0.03\n",
            "mse_b  [867.26]  mse_f: 4.555747032165527   total loss: [871.81573]\n",
            "mse_b ====== [867.85321]\n",
            "It: 52867, Time: 0.03\n",
            "mse_b  [867.8532]  mse_f: 3.9790737628936768   total loss: [871.8323]\n",
            "mse_b ====== [866.881531]\n",
            "It: 52868, Time: 0.03\n",
            "mse_b  [866.88153]  mse_f: 3.3160338401794434   total loss: [870.1976]\n",
            "mse_b ====== [867.767639]\n",
            "It: 52869, Time: 0.03\n",
            "mse_b  [867.76764]  mse_f: 3.949587106704712   total loss: [871.7172]\n",
            "mse_b ====== [867.719604]\n",
            "It: 52870, Time: 0.04\n",
            "mse_b  [867.7196]  mse_f: 3.5721261501312256   total loss: [871.29175]\n",
            "mse_b ====== [866.792297]\n",
            "It: 52871, Time: 0.04\n",
            "mse_b  [866.7923]  mse_f: 4.102982521057129   total loss: [870.89526]\n",
            "mse_b ====== [867.631348]\n",
            "It: 52872, Time: 0.04\n",
            "mse_b  [867.63135]  mse_f: 3.3064417839050293   total loss: [870.9378]\n",
            "mse_b ====== [867.404053]\n",
            "It: 52873, Time: 0.04\n",
            "mse_b  [867.40405]  mse_f: 4.026093006134033   total loss: [871.4302]\n",
            "mse_b ====== [867.143]\n",
            "It: 52874, Time: 0.03\n",
            "mse_b  [867.143]  mse_f: 4.180022239685059   total loss: [871.323]\n",
            "mse_b ====== [867.300598]\n",
            "It: 52875, Time: 0.03\n",
            "mse_b  [867.3006]  mse_f: 3.166350841522217   total loss: [870.4669]\n",
            "mse_b ====== [866.861206]\n",
            "It: 52876, Time: 0.03\n",
            "mse_b  [866.8612]  mse_f: 4.577001571655273   total loss: [871.43823]\n",
            "mse_b ====== [867.147949]\n",
            "It: 52877, Time: 0.04\n",
            "mse_b  [867.14795]  mse_f: 3.696326732635498   total loss: [870.8443]\n",
            "mse_b ====== [867.189758]\n",
            "It: 52878, Time: 0.03\n",
            "mse_b  [867.18976]  mse_f: 4.0911173820495605   total loss: [871.2809]\n",
            "mse_b ====== [866.942261]\n",
            "It: 52879, Time: 0.03\n",
            "mse_b  [866.94226]  mse_f: 3.720428466796875   total loss: [870.6627]\n",
            "mse_b ====== [867.02124]\n",
            "It: 52880, Time: 0.04\n",
            "mse_b  [867.02124]  mse_f: 3.8605728149414062   total loss: [870.88184]\n",
            "mse_b ====== [866.886047]\n",
            "It: 52881, Time: 0.04\n",
            "mse_b  [866.88605]  mse_f: 4.436592102050781   total loss: [871.32263]\n",
            "mse_b ====== [866.911072]\n",
            "It: 52882, Time: 0.04\n",
            "mse_b  [866.9111]  mse_f: 3.3787291049957275   total loss: [870.2898]\n",
            "mse_b ====== [867.251282]\n",
            "It: 52883, Time: 0.03\n",
            "mse_b  [867.2513]  mse_f: 4.026562690734863   total loss: [871.27783]\n",
            "mse_b ====== [866.945801]\n",
            "It: 52884, Time: 0.03\n",
            "mse_b  [866.9458]  mse_f: 3.7240302562713623   total loss: [870.66986]\n",
            "mse_b ====== [867.17511]\n",
            "It: 52885, Time: 0.03\n",
            "mse_b  [867.1751]  mse_f: 3.5744376182556152   total loss: [870.7496]\n",
            "mse_b ====== [867.295]\n",
            "It: 52886, Time: 0.03\n",
            "mse_b  [867.295]  mse_f: 3.801163673400879   total loss: [871.0961]\n",
            "mse_b ====== [866.866699]\n",
            "It: 52887, Time: 0.04\n",
            "mse_b  [866.8667]  mse_f: 3.2723140716552734   total loss: [870.13904]\n",
            "mse_b ====== [867.253784]\n",
            "It: 52888, Time: 0.03\n",
            "mse_b  [867.2538]  mse_f: 4.296421051025391   total loss: [871.55023]\n",
            "mse_b ====== [866.89093]\n",
            "It: 52889, Time: 0.03\n",
            "mse_b  [866.8909]  mse_f: 3.543069362640381   total loss: [870.434]\n",
            "mse_b ====== [866.904114]\n",
            "It: 52890, Time: 0.03\n",
            "mse_b  [866.9041]  mse_f: 3.427149772644043   total loss: [870.33124]\n",
            "mse_b ====== [867.227051]\n",
            "It: 52891, Time: 0.03\n",
            "mse_b  [867.22705]  mse_f: 4.267814636230469   total loss: [871.4949]\n",
            "mse_b ====== [866.895813]\n",
            "It: 52892, Time: 0.03\n",
            "mse_b  [866.8958]  mse_f: 3.2306160926818848   total loss: [870.1264]\n",
            "mse_b ====== [867.263367]\n",
            "It: 52893, Time: 0.04\n",
            "mse_b  [867.26337]  mse_f: 3.7426862716674805   total loss: [871.00604]\n",
            "mse_b ====== [867.145569]\n",
            "It: 52894, Time: 0.03\n",
            "mse_b  [867.14557]  mse_f: 3.283970832824707   total loss: [870.42957]\n",
            "mse_b ====== [867.030334]\n",
            "It: 52895, Time: 0.03\n",
            "mse_b  [867.03033]  mse_f: 3.746638536453247   total loss: [870.777]\n",
            "mse_b ====== [867.31012]\n",
            "It: 52896, Time: 0.03\n",
            "mse_b  [867.3101]  mse_f: 3.679412364959717   total loss: [870.9895]\n",
            "mse_b ====== [866.750732]\n",
            "It: 52897, Time: 0.03\n",
            "mse_b  [866.75073]  mse_f: 3.1050057411193848   total loss: [869.8557]\n",
            "mse_b ====== [867.248779]\n",
            "It: 52898, Time: 0.03\n",
            "mse_b  [867.2488]  mse_f: 3.7251408100128174   total loss: [870.97394]\n",
            "mse_b ====== [867.187439]\n",
            "It: 52899, Time: 0.03\n",
            "mse_b  [867.18744]  mse_f: 3.564161539077759   total loss: [870.7516]\n",
            "mse_b ====== [866.800415]\n",
            "It: 52900, Time: 0.03\n",
            "mse_b  [866.8004]  mse_f: 3.3202624320983887   total loss: [870.12067]\n",
            "mse_b ====== [867.074]\n",
            "It: 52901, Time: 0.03\n",
            "mse_b  [867.074]  mse_f: 3.6524767875671387   total loss: [870.72644]\n",
            "mse_b ====== [867.042358]\n",
            "It: 52902, Time: 0.03\n",
            "mse_b  [867.04236]  mse_f: 3.365884780883789   total loss: [870.40826]\n",
            "mse_b ====== [866.959473]\n",
            "It: 52903, Time: 0.03\n",
            "mse_b  [866.9595]  mse_f: 3.890361785888672   total loss: [870.84985]\n",
            "mse_b ====== [867.153564]\n",
            "It: 52904, Time: 0.03\n",
            "mse_b  [867.15356]  mse_f: 2.7721686363220215   total loss: [869.9257]\n",
            "mse_b ====== [866.956]\n",
            "It: 52905, Time: 0.04\n",
            "mse_b  [866.956]  mse_f: 3.7996716499328613   total loss: [870.7557]\n",
            "mse_b ====== [866.970459]\n",
            "It: 52906, Time: 0.03\n",
            "mse_b  [866.97046]  mse_f: 3.595078945159912   total loss: [870.56555]\n",
            "mse_b ====== [867.188]\n",
            "It: 52907, Time: 0.03\n",
            "mse_b  [867.188]  mse_f: 3.3035647869110107   total loss: [870.4916]\n",
            "mse_b ====== [866.813538]\n",
            "It: 52908, Time: 0.03\n",
            "mse_b  [866.81354]  mse_f: 3.475834369659424   total loss: [870.28937]\n",
            "mse_b ====== [866.973938]\n",
            "It: 52909, Time: 0.03\n",
            "mse_b  [866.97394]  mse_f: 3.260557174682617   total loss: [870.2345]\n",
            "mse_b ====== [866.94989]\n",
            "It: 52910, Time: 0.03\n",
            "mse_b  [866.9499]  mse_f: 4.245783805847168   total loss: [871.1957]\n",
            "mse_b ====== [866.797119]\n",
            "It: 52911, Time: 0.03\n",
            "mse_b  [866.7971]  mse_f: 3.0257697105407715   total loss: [869.8229]\n",
            "mse_b ====== [867.121277]\n",
            "It: 52912, Time: 0.03\n",
            "mse_b  [867.1213]  mse_f: 3.6306140422821045   total loss: [870.7519]\n",
            "mse_b ====== [866.985535]\n",
            "It: 52913, Time: 0.03\n",
            "mse_b  [866.98553]  mse_f: 3.513946056365967   total loss: [870.49945]\n",
            "mse_b ====== [866.878052]\n",
            "It: 52914, Time: 0.04\n",
            "mse_b  [866.87805]  mse_f: 3.1474218368530273   total loss: [870.02545]\n",
            "mse_b ====== [867.249207]\n",
            "It: 52915, Time: 0.03\n",
            "mse_b  [867.2492]  mse_f: 3.8114447593688965   total loss: [871.06067]\n",
            "mse_b ====== [866.871399]\n",
            "It: 52916, Time: 0.03\n",
            "mse_b  [866.8714]  mse_f: 3.5559940338134766   total loss: [870.42737]\n",
            "mse_b ====== [867.203308]\n",
            "It: 52917, Time: 0.03\n",
            "mse_b  [867.2033]  mse_f: 3.743825912475586   total loss: [870.94714]\n",
            "mse_b ====== [866.880066]\n",
            "It: 52918, Time: 0.03\n",
            "mse_b  [866.88007]  mse_f: 3.368515968322754   total loss: [870.2486]\n",
            "mse_b ====== [866.808105]\n",
            "It: 52919, Time: 0.03\n",
            "mse_b  [866.8081]  mse_f: 3.0674901008605957   total loss: [869.8756]\n",
            "mse_b ====== [867.11261]\n",
            "It: 52920, Time: 0.03\n",
            "mse_b  [867.1126]  mse_f: 4.381649971008301   total loss: [871.49426]\n",
            "mse_b ====== [866.856]\n",
            "It: 52921, Time: 0.03\n",
            "mse_b  [866.856]  mse_f: 3.2306089401245117   total loss: [870.0866]\n",
            "mse_b ====== [866.723877]\n",
            "It: 52922, Time: 0.03\n",
            "mse_b  [866.7239]  mse_f: 3.644136428833008   total loss: [870.36804]\n",
            "mse_b ====== [866.955505]\n",
            "It: 52923, Time: 0.03\n",
            "mse_b  [866.9555]  mse_f: 3.2738170623779297   total loss: [870.2293]\n",
            "mse_b ====== [866.829651]\n",
            "It: 52924, Time: 0.03\n",
            "mse_b  [866.82965]  mse_f: 4.016855239868164   total loss: [870.8465]\n",
            "mse_b ====== [867.21]\n",
            "It: 52925, Time: 0.04\n",
            "mse_b  [867.21]  mse_f: 3.5524230003356934   total loss: [870.76245]\n",
            "mse_b ====== [866.970032]\n",
            "It: 52926, Time: 0.03\n",
            "mse_b  [866.97003]  mse_f: 2.9161808490753174   total loss: [869.8862]\n",
            "mse_b ====== [866.751953]\n",
            "It: 52927, Time: 0.03\n",
            "mse_b  [866.75195]  mse_f: 3.653414726257324   total loss: [870.4054]\n",
            "mse_b ====== [867.05011]\n",
            "It: 52928, Time: 0.03\n",
            "mse_b  [867.0501]  mse_f: 3.4718053340911865   total loss: [870.5219]\n",
            "mse_b ====== [866.944214]\n",
            "It: 52929, Time: 0.03\n",
            "mse_b  [866.9442]  mse_f: 3.476870536804199   total loss: [870.4211]\n",
            "mse_b ====== [866.746826]\n",
            "It: 52930, Time: 0.03\n",
            "mse_b  [866.7468]  mse_f: 3.3493072986602783   total loss: [870.0961]\n",
            "mse_b ====== [867.129944]\n",
            "It: 52931, Time: 0.03\n",
            "mse_b  [867.12994]  mse_f: 3.1329445838928223   total loss: [870.2629]\n",
            "mse_b ====== [866.88324]\n",
            "It: 52932, Time: 0.04\n",
            "mse_b  [866.88324]  mse_f: 3.595935821533203   total loss: [870.4792]\n",
            "mse_b ====== [867.025635]\n",
            "It: 52933, Time: 0.03\n",
            "mse_b  [867.02563]  mse_f: 2.858279228210449   total loss: [869.8839]\n",
            "mse_b ====== [866.961243]\n",
            "It: 52934, Time: 0.04\n",
            "mse_b  [866.96124]  mse_f: 3.523043632507324   total loss: [870.4843]\n",
            "mse_b ====== [866.845642]\n",
            "It: 52935, Time: 0.03\n",
            "mse_b  [866.84564]  mse_f: 3.311795711517334   total loss: [870.1574]\n",
            "mse_b ====== [866.852417]\n",
            "It: 52936, Time: 0.03\n",
            "mse_b  [866.8524]  mse_f: 3.0599207878112793   total loss: [869.91235]\n",
            "mse_b ====== [866.983398]\n",
            "It: 52937, Time: 0.03\n",
            "mse_b  [866.9834]  mse_f: 3.5539159774780273   total loss: [870.5373]\n",
            "mse_b ====== [866.82428]\n",
            "It: 52938, Time: 0.03\n",
            "mse_b  [866.8243]  mse_f: 2.99411678314209   total loss: [869.8184]\n",
            "mse_b ====== [866.902222]\n",
            "It: 52939, Time: 0.03\n",
            "mse_b  [866.9022]  mse_f: 3.5853018760681152   total loss: [870.48755]\n",
            "mse_b ====== [866.760315]\n",
            "It: 52940, Time: 0.03\n",
            "mse_b  [866.7603]  mse_f: 2.982870101928711   total loss: [869.74316]\n",
            "mse_b ====== [866.825806]\n",
            "It: 52941, Time: 0.03\n",
            "mse_b  [866.8258]  mse_f: 3.355455160140991   total loss: [870.1813]\n",
            "mse_b ====== [867.118896]\n",
            "It: 52942, Time: 0.04\n",
            "mse_b  [867.1189]  mse_f: 3.484583616256714   total loss: [870.60345]\n",
            "mse_b ====== [866.666321]\n",
            "It: 52943, Time: 0.03\n",
            "mse_b  [866.6663]  mse_f: 2.9342904090881348   total loss: [869.6006]\n",
            "mse_b ====== [866.854492]\n",
            "It: 52944, Time: 0.03\n",
            "mse_b  [866.8545]  mse_f: 3.2558677196502686   total loss: [870.11035]\n",
            "mse_b ====== [866.923645]\n",
            "It: 52945, Time: 0.03\n",
            "mse_b  [866.92365]  mse_f: 3.337998867034912   total loss: [870.26166]\n",
            "mse_b ====== [866.899597]\n",
            "It: 52946, Time: 0.03\n",
            "mse_b  [866.8996]  mse_f: 3.226663589477539   total loss: [870.1263]\n",
            "mse_b ====== [866.860962]\n",
            "It: 52947, Time: 0.04\n",
            "mse_b  [866.86096]  mse_f: 3.2533326148986816   total loss: [870.1143]\n",
            "mse_b ====== [866.942078]\n",
            "It: 52948, Time: 0.03\n",
            "mse_b  [866.9421]  mse_f: 3.0147438049316406   total loss: [869.95685]\n",
            "mse_b ====== [866.888184]\n",
            "It: 52949, Time: 0.03\n",
            "mse_b  [866.8882]  mse_f: 3.376833438873291   total loss: [870.265]\n",
            "mse_b ====== [866.910828]\n",
            "It: 52950, Time: 0.04\n",
            "mse_b  [866.9108]  mse_f: 2.961655616760254   total loss: [869.8725]\n",
            "mse_b ====== [866.7453]\n",
            "It: 52951, Time: 0.03\n",
            "mse_b  [866.7453]  mse_f: 3.2172107696533203   total loss: [869.9625]\n",
            "mse_b ====== [866.856506]\n",
            "It: 52952, Time: 0.03\n",
            "mse_b  [866.8565]  mse_f: 3.2484285831451416   total loss: [870.1049]\n",
            "mse_b ====== [867.098633]\n",
            "It: 52953, Time: 0.03\n",
            "mse_b  [867.09863]  mse_f: 3.0932564735412598   total loss: [870.1919]\n",
            "mse_b ====== [866.811279]\n",
            "It: 52954, Time: 0.03\n",
            "mse_b  [866.8113]  mse_f: 3.081686496734619   total loss: [869.89294]\n",
            "mse_b ====== [866.800598]\n",
            "It: 52955, Time: 0.03\n",
            "mse_b  [866.8006]  mse_f: 2.7891340255737305   total loss: [869.5897]\n",
            "mse_b ====== [866.984314]\n",
            "It: 52956, Time: 0.03\n",
            "mse_b  [866.9843]  mse_f: 3.3665030002593994   total loss: [870.3508]\n",
            "mse_b ====== [866.692139]\n",
            "It: 52957, Time: 0.03\n",
            "mse_b  [866.69214]  mse_f: 2.8648996353149414   total loss: [869.55707]\n",
            "mse_b ====== [866.848816]\n",
            "It: 52958, Time: 0.03\n",
            "mse_b  [866.8488]  mse_f: 3.0988609790802   total loss: [869.9477]\n",
            "mse_b ====== [866.792175]\n",
            "It: 52959, Time: 0.03\n",
            "mse_b  [866.7922]  mse_f: 3.134158134460449   total loss: [869.92633]\n",
            "mse_b ====== [866.647217]\n",
            "It: 52960, Time: 0.03\n",
            "mse_b  [866.6472]  mse_f: 3.2485737800598145   total loss: [869.8958]\n",
            "mse_b ====== [866.961853]\n",
            "It: 52961, Time: 0.03\n",
            "mse_b  [866.96185]  mse_f: 3.095123052597046   total loss: [870.05695]\n",
            "mse_b ====== [866.753967]\n",
            "It: 52962, Time: 0.03\n",
            "mse_b  [866.75397]  mse_f: 2.82588267326355   total loss: [869.57983]\n",
            "mse_b ====== [866.740051]\n",
            "It: 52963, Time: 0.03\n",
            "mse_b  [866.74005]  mse_f: 3.1490094661712646   total loss: [869.88904]\n",
            "mse_b ====== [867.02533]\n",
            "It: 52964, Time: 0.03\n",
            "mse_b  [867.0253]  mse_f: 3.041721820831299   total loss: [870.0671]\n",
            "mse_b ====== [866.655396]\n",
            "It: 52965, Time: 0.03\n",
            "mse_b  [866.6554]  mse_f: 2.901066780090332   total loss: [869.55646]\n",
            "mse_b ====== [866.861389]\n",
            "It: 52966, Time: 0.03\n",
            "mse_b  [866.8614]  mse_f: 3.108985424041748   total loss: [869.9704]\n",
            "mse_b ====== [866.779541]\n",
            "It: 52967, Time: 0.04\n",
            "mse_b  [866.77954]  mse_f: 3.1532554626464844   total loss: [869.9328]\n",
            "mse_b ====== [866.739075]\n",
            "It: 52968, Time: 0.03\n",
            "mse_b  [866.7391]  mse_f: 3.1956887245178223   total loss: [869.93475]\n",
            "mse_b ====== [866.805176]\n",
            "It: 52969, Time: 0.03\n",
            "mse_b  [866.8052]  mse_f: 2.826423406600952   total loss: [869.6316]\n",
            "mse_b ====== [866.948608]\n",
            "It: 52970, Time: 0.03\n",
            "mse_b  [866.9486]  mse_f: 3.007227897644043   total loss: [869.9558]\n",
            "mse_b ====== [866.699585]\n",
            "It: 52971, Time: 0.03\n",
            "mse_b  [866.6996]  mse_f: 3.000993490219116   total loss: [869.70056]\n",
            "mse_b ====== [866.798767]\n",
            "It: 52972, Time: 0.04\n",
            "mse_b  [866.79877]  mse_f: 2.991612434387207   total loss: [869.7904]\n",
            "mse_b ====== [866.720581]\n",
            "It: 52973, Time: 0.03\n",
            "mse_b  [866.7206]  mse_f: 3.161048173904419   total loss: [869.88165]\n",
            "mse_b ====== [866.743958]\n",
            "It: 52974, Time: 0.03\n",
            "mse_b  [866.74396]  mse_f: 3.046776294708252   total loss: [869.7907]\n",
            "mse_b ====== [866.831238]\n",
            "It: 52975, Time: 0.03\n",
            "mse_b  [866.83124]  mse_f: 3.5312557220458984   total loss: [870.3625]\n",
            "mse_b ====== [866.7005]\n",
            "It: 52976, Time: 0.03\n",
            "mse_b  [866.7005]  mse_f: 3.2691996097564697   total loss: [869.9697]\n",
            "mse_b ====== [867.091431]\n",
            "It: 52977, Time: 0.03\n",
            "mse_b  [867.09143]  mse_f: 3.2031946182250977   total loss: [870.2946]\n",
            "mse_b ====== [867.022156]\n",
            "It: 52978, Time: 0.03\n",
            "mse_b  [867.02216]  mse_f: 3.461428165435791   total loss: [870.4836]\n",
            "mse_b ====== [866.918091]\n",
            "It: 52979, Time: 0.04\n",
            "mse_b  [866.9181]  mse_f: 3.030186891555786   total loss: [869.9483]\n",
            "mse_b ====== [866.742615]\n",
            "It: 52980, Time: 0.04\n",
            "mse_b  [866.7426]  mse_f: 3.2971351146698   total loss: [870.03973]\n",
            "mse_b ====== [866.898743]\n",
            "It: 52981, Time: 0.03\n",
            "mse_b  [866.89874]  mse_f: 2.934500217437744   total loss: [869.83325]\n",
            "mse_b ====== [866.772034]\n",
            "It: 52982, Time: 0.03\n",
            "mse_b  [866.77203]  mse_f: 2.8782474994659424   total loss: [869.65027]\n",
            "mse_b ====== [866.807251]\n",
            "It: 52983, Time: 0.03\n",
            "mse_b  [866.80725]  mse_f: 2.911604404449463   total loss: [869.7189]\n",
            "mse_b ====== [866.798035]\n",
            "It: 52984, Time: 0.03\n",
            "mse_b  [866.79803]  mse_f: 2.8425331115722656   total loss: [869.64056]\n",
            "mse_b ====== [866.675537]\n",
            "It: 52985, Time: 0.03\n",
            "mse_b  [866.67554]  mse_f: 3.007094383239746   total loss: [869.6826]\n",
            "mse_b ====== [866.762695]\n",
            "It: 52986, Time: 0.03\n",
            "mse_b  [866.7627]  mse_f: 2.729226589202881   total loss: [869.49194]\n",
            "mse_b ====== [866.854675]\n",
            "It: 52987, Time: 0.03\n",
            "mse_b  [866.8547]  mse_f: 2.977539539337158   total loss: [869.8322]\n",
            "mse_b ====== [866.612366]\n",
            "It: 52988, Time: 0.03\n",
            "mse_b  [866.61237]  mse_f: 2.859764575958252   total loss: [869.4721]\n",
            "mse_b ====== [866.787598]\n",
            "It: 52989, Time: 0.03\n",
            "mse_b  [866.7876]  mse_f: 3.180962562561035   total loss: [869.96857]\n",
            "mse_b ====== [866.637085]\n",
            "It: 52990, Time: 0.03\n",
            "mse_b  [866.6371]  mse_f: 3.0112290382385254   total loss: [869.6483]\n",
            "mse_b ====== [866.84845]\n",
            "It: 52991, Time: 0.03\n",
            "mse_b  [866.84845]  mse_f: 3.0892388820648193   total loss: [869.9377]\n",
            "mse_b ====== [866.98645]\n",
            "It: 52992, Time: 0.03\n",
            "mse_b  [866.98645]  mse_f: 3.0224051475524902   total loss: [870.00885]\n",
            "mse_b ====== [866.544]\n",
            "It: 52993, Time: 0.03\n",
            "mse_b  [866.544]  mse_f: 2.9801523685455322   total loss: [869.5242]\n",
            "mse_b ====== [866.814697]\n",
            "It: 52994, Time: 0.03\n",
            "mse_b  [866.8147]  mse_f: 3.1125693321228027   total loss: [869.92725]\n",
            "mse_b ====== [866.878479]\n",
            "It: 52995, Time: 0.03\n",
            "mse_b  [866.8785]  mse_f: 3.085069179534912   total loss: [869.96356]\n",
            "mse_b ====== [866.794495]\n",
            "It: 52996, Time: 0.03\n",
            "mse_b  [866.7945]  mse_f: 2.878549337387085   total loss: [869.67303]\n",
            "mse_b ====== [866.880188]\n",
            "It: 52997, Time: 0.03\n",
            "mse_b  [866.8802]  mse_f: 3.107163906097412   total loss: [869.98737]\n",
            "mse_b ====== [866.816589]\n",
            "It: 52998, Time: 0.04\n",
            "mse_b  [866.8166]  mse_f: 3.0367767810821533   total loss: [869.8534]\n",
            "mse_b ====== [866.961365]\n",
            "It: 52999, Time: 0.03\n",
            "mse_b  [866.96136]  mse_f: 3.276733160018921   total loss: [870.2381]\n",
            "mse_b ====== [866.745728]\n",
            "It: 53000, Time: 0.03\n",
            "mse_b  [866.7457]  mse_f: 3.001929759979248   total loss: [869.7477]\n",
            "mse_b ====== [866.659546]\n",
            "It: 53001, Time: 0.03\n",
            "mse_b  [866.65955]  mse_f: 3.3451497554779053   total loss: [870.0047]\n",
            "mse_b ====== [866.726501]\n",
            "It: 53002, Time: 0.03\n",
            "mse_b  [866.7265]  mse_f: 3.1275763511657715   total loss: [869.85406]\n",
            "mse_b ====== [867.035095]\n",
            "It: 53003, Time: 0.03\n",
            "mse_b  [867.0351]  mse_f: 3.087620735168457   total loss: [870.12274]\n",
            "mse_b ====== [866.745483]\n",
            "It: 53004, Time: 0.03\n",
            "mse_b  [866.7455]  mse_f: 3.2578463554382324   total loss: [870.00336]\n",
            "mse_b ====== [866.853]\n",
            "It: 53005, Time: 0.03\n",
            "mse_b  [866.853]  mse_f: 3.1196064949035645   total loss: [869.97266]\n",
            "mse_b ====== [866.939636]\n",
            "It: 53006, Time: 0.03\n",
            "mse_b  [866.93964]  mse_f: 3.6454458236694336   total loss: [870.5851]\n",
            "mse_b ====== [866.644165]\n",
            "It: 53007, Time: 0.03\n",
            "mse_b  [866.64417]  mse_f: 3.271085262298584   total loss: [869.9152]\n",
            "mse_b ====== [866.809082]\n",
            "It: 53008, Time: 0.03\n",
            "mse_b  [866.8091]  mse_f: 3.217385768890381   total loss: [870.0265]\n",
            "mse_b ====== [866.815369]\n",
            "It: 53009, Time: 0.03\n",
            "mse_b  [866.81537]  mse_f: 3.279085636138916   total loss: [870.0945]\n",
            "mse_b ====== [866.745667]\n",
            "It: 53010, Time: 0.03\n",
            "mse_b  [866.74567]  mse_f: 3.1023755073547363   total loss: [869.848]\n",
            "mse_b ====== [866.76886]\n",
            "It: 53011, Time: 0.03\n",
            "mse_b  [866.76886]  mse_f: 3.329233407974243   total loss: [870.0981]\n",
            "mse_b ====== [866.707031]\n",
            "It: 53012, Time: 0.03\n",
            "mse_b  [866.70703]  mse_f: 3.215055465698242   total loss: [869.92206]\n",
            "mse_b ====== [866.818848]\n",
            "It: 53013, Time: 0.03\n",
            "mse_b  [866.81885]  mse_f: 3.318455934524536   total loss: [870.1373]\n",
            "mse_b ====== [866.909485]\n",
            "It: 53014, Time: 0.03\n",
            "mse_b  [866.9095]  mse_f: 3.079901695251465   total loss: [869.9894]\n",
            "mse_b ====== [866.783203]\n",
            "It: 53015, Time: 0.03\n",
            "mse_b  [866.7832]  mse_f: 2.8784613609313965   total loss: [869.6617]\n",
            "mse_b ====== [866.769165]\n",
            "It: 53016, Time: 0.03\n",
            "mse_b  [866.76917]  mse_f: 2.9489235877990723   total loss: [869.7181]\n",
            "mse_b ====== [866.663]\n",
            "It: 53017, Time: 0.03\n",
            "mse_b  [866.663]  mse_f: 2.8940176963806152   total loss: [869.55707]\n",
            "mse_b ====== [866.753052]\n",
            "It: 53018, Time: 0.03\n",
            "mse_b  [866.75305]  mse_f: 3.056260108947754   total loss: [869.8093]\n",
            "mse_b ====== [866.601807]\n",
            "It: 53019, Time: 0.03\n",
            "mse_b  [866.6018]  mse_f: 2.6410317420959473   total loss: [869.24286]\n",
            "mse_b ====== [866.850098]\n",
            "It: 53020, Time: 0.03\n",
            "mse_b  [866.8501]  mse_f: 2.9504234790802   total loss: [869.80054]\n",
            "mse_b ====== [866.689758]\n",
            "It: 53021, Time: 0.03\n",
            "mse_b  [866.68976]  mse_f: 2.7355241775512695   total loss: [869.4253]\n",
            "mse_b ====== [866.561523]\n",
            "It: 53022, Time: 0.03\n",
            "mse_b  [866.5615]  mse_f: 2.862809896469116   total loss: [869.4243]\n",
            "mse_b ====== [866.756775]\n",
            "It: 53023, Time: 0.03\n",
            "mse_b  [866.7568]  mse_f: 2.885161876678467   total loss: [869.6419]\n",
            "mse_b ====== [866.649902]\n",
            "It: 53024, Time: 0.03\n",
            "mse_b  [866.6499]  mse_f: 2.7341346740722656   total loss: [869.38403]\n",
            "mse_b ====== [866.67218]\n",
            "It: 53025, Time: 0.03\n",
            "mse_b  [866.6722]  mse_f: 2.8161730766296387   total loss: [869.48834]\n",
            "mse_b ====== [866.736816]\n",
            "It: 53026, Time: 0.04\n",
            "mse_b  [866.7368]  mse_f: 2.7340633869171143   total loss: [869.4709]\n",
            "mse_b ====== [866.585693]\n",
            "It: 53027, Time: 0.03\n",
            "mse_b  [866.5857]  mse_f: 2.895442485809326   total loss: [869.48114]\n",
            "mse_b ====== [866.707092]\n",
            "It: 53028, Time: 0.03\n",
            "mse_b  [866.7071]  mse_f: 2.8469038009643555   total loss: [869.554]\n",
            "mse_b ====== [866.633362]\n",
            "It: 53029, Time: 0.03\n",
            "mse_b  [866.63336]  mse_f: 2.761002540588379   total loss: [869.39435]\n",
            "mse_b ====== [866.698669]\n",
            "It: 53030, Time: 0.03\n",
            "mse_b  [866.69867]  mse_f: 2.897624969482422   total loss: [869.5963]\n",
            "mse_b ====== [866.681]\n",
            "It: 53031, Time: 0.03\n",
            "mse_b  [866.681]  mse_f: 2.9342823028564453   total loss: [869.6153]\n",
            "mse_b ====== [866.80481]\n",
            "It: 53032, Time: 0.03\n",
            "mse_b  [866.8048]  mse_f: 3.1832990646362305   total loss: [869.9881]\n",
            "mse_b ====== [866.671326]\n",
            "It: 53033, Time: 0.03\n",
            "mse_b  [866.6713]  mse_f: 3.41310453414917   total loss: [870.0844]\n",
            "mse_b ====== [866.966919]\n",
            "It: 53034, Time: 0.03\n",
            "mse_b  [866.9669]  mse_f: 4.443885326385498   total loss: [871.4108]\n",
            "mse_b ====== [867.095215]\n",
            "It: 53035, Time: 0.03\n",
            "mse_b  [867.0952]  mse_f: 5.648445129394531   total loss: [872.74365]\n",
            "mse_b ====== [867.334961]\n",
            "It: 53036, Time: 0.03\n",
            "mse_b  [867.33496]  mse_f: 6.929959297180176   total loss: [874.2649]\n",
            "mse_b ====== [868.012]\n",
            "It: 53037, Time: 0.03\n",
            "mse_b  [868.012]  mse_f: 6.74539852142334   total loss: [874.75745]\n",
            "mse_b ====== [867.561]\n",
            "It: 53038, Time: 0.03\n",
            "mse_b  [867.561]  mse_f: 5.133463382720947   total loss: [872.69446]\n",
            "mse_b ====== [866.605408]\n",
            "It: 53039, Time: 0.03\n",
            "mse_b  [866.6054]  mse_f: 3.817270517349243   total loss: [870.42267]\n",
            "mse_b ====== [866.727051]\n",
            "It: 53040, Time: 0.03\n",
            "mse_b  [866.72705]  mse_f: 2.817991018295288   total loss: [869.54504]\n",
            "mse_b ====== [866.990051]\n",
            "It: 53041, Time: 0.03\n",
            "mse_b  [866.99005]  mse_f: 3.745666980743408   total loss: [870.7357]\n",
            "mse_b ====== [867.220642]\n",
            "It: 53042, Time: 0.03\n",
            "mse_b  [867.22064]  mse_f: 5.406464576721191   total loss: [872.62714]\n",
            "mse_b ====== [867.051819]\n",
            "It: 53043, Time: 0.03\n",
            "mse_b  [867.0518]  mse_f: 5.41998291015625   total loss: [872.4718]\n",
            "mse_b ====== [866.966248]\n",
            "It: 53044, Time: 0.03\n",
            "mse_b  [866.96625]  mse_f: 3.7308850288391113   total loss: [870.69714]\n",
            "mse_b ====== [866.632385]\n",
            "It: 53045, Time: 0.04\n",
            "mse_b  [866.6324]  mse_f: 2.9975011348724365   total loss: [869.6299]\n",
            "mse_b ====== [866.942322]\n",
            "It: 53046, Time: 0.03\n",
            "mse_b  [866.9423]  mse_f: 4.3696465492248535   total loss: [871.31195]\n",
            "mse_b ====== [867.199646]\n",
            "It: 53047, Time: 0.03\n",
            "mse_b  [867.19965]  mse_f: 5.327644348144531   total loss: [872.5273]\n",
            "mse_b ====== [867.201294]\n",
            "It: 53048, Time: 0.03\n",
            "mse_b  [867.2013]  mse_f: 4.371551513671875   total loss: [871.5729]\n",
            "mse_b ====== [867.18457]\n",
            "It: 53049, Time: 0.03\n",
            "mse_b  [867.1846]  mse_f: 3.3853163719177246   total loss: [870.5699]\n",
            "mse_b ====== [867.083862]\n",
            "It: 53050, Time: 0.03\n",
            "mse_b  [867.08386]  mse_f: 3.4202680587768555   total loss: [870.50415]\n",
            "mse_b ====== [867.084534]\n",
            "It: 53051, Time: 0.03\n",
            "mse_b  [867.08453]  mse_f: 4.315929412841797   total loss: [871.40045]\n",
            "mse_b ====== [867.051941]\n",
            "It: 53052, Time: 0.03\n",
            "mse_b  [867.05194]  mse_f: 4.174895286560059   total loss: [871.2268]\n",
            "mse_b ====== [866.75177]\n",
            "It: 53053, Time: 0.03\n",
            "mse_b  [866.7518]  mse_f: 4.12668514251709   total loss: [870.8785]\n",
            "mse_b ====== [867.057373]\n",
            "It: 53054, Time: 0.03\n",
            "mse_b  [867.0574]  mse_f: 3.7103495597839355   total loss: [870.7677]\n",
            "mse_b ====== [866.787903]\n",
            "It: 53055, Time: 0.04\n",
            "mse_b  [866.7879]  mse_f: 3.805266857147217   total loss: [870.59314]\n",
            "mse_b ====== [866.855042]\n",
            "It: 53056, Time: 0.03\n",
            "mse_b  [866.85504]  mse_f: 3.6459803581237793   total loss: [870.50104]\n",
            "mse_b ====== [867.068115]\n",
            "It: 53057, Time: 0.03\n",
            "mse_b  [867.0681]  mse_f: 4.280331611633301   total loss: [871.34845]\n",
            "mse_b ====== [867.080444]\n",
            "It: 53058, Time: 0.03\n",
            "mse_b  [867.08044]  mse_f: 4.05452823638916   total loss: [871.13495]\n",
            "mse_b ====== [866.828064]\n",
            "It: 53059, Time: 0.03\n",
            "mse_b  [866.82806]  mse_f: 3.481865406036377   total loss: [870.30994]\n",
            "mse_b ====== [866.664307]\n",
            "It: 53060, Time: 0.03\n",
            "mse_b  [866.6643]  mse_f: 3.249643325805664   total loss: [869.91394]\n",
            "mse_b ====== [866.981567]\n",
            "It: 53061, Time: 0.03\n",
            "mse_b  [866.98157]  mse_f: 4.446223735809326   total loss: [871.4278]\n",
            "mse_b ====== [867.25061]\n",
            "It: 53062, Time: 0.03\n",
            "mse_b  [867.2506]  mse_f: 4.031960487365723   total loss: [871.2826]\n",
            "mse_b ====== [866.79425]\n",
            "It: 53063, Time: 0.03\n",
            "mse_b  [866.79425]  mse_f: 3.3417105674743652   total loss: [870.136]\n",
            "mse_b ====== [866.786133]\n",
            "It: 53064, Time: 0.03\n",
            "mse_b  [866.78613]  mse_f: 3.064237594604492   total loss: [869.85034]\n",
            "mse_b ====== [867.092834]\n",
            "It: 53065, Time: 0.03\n",
            "mse_b  [867.09283]  mse_f: 4.205433368682861   total loss: [871.2983]\n",
            "mse_b ====== [867.106262]\n",
            "It: 53066, Time: 0.03\n",
            "mse_b  [867.10626]  mse_f: 4.431581020355225   total loss: [871.53784]\n",
            "mse_b ====== [866.636047]\n",
            "It: 53067, Time: 0.03\n",
            "mse_b  [866.63605]  mse_f: 3.2647476196289062   total loss: [869.9008]\n",
            "mse_b ====== [866.7]\n",
            "It: 53068, Time: 0.03\n",
            "mse_b  [866.7]  mse_f: 2.9878029823303223   total loss: [869.6878]\n",
            "mse_b ====== [866.841]\n",
            "It: 53069, Time: 0.03\n",
            "mse_b  [866.841]  mse_f: 4.470603942871094   total loss: [871.3116]\n",
            "mse_b ====== [867.122498]\n",
            "It: 53070, Time: 0.03\n",
            "mse_b  [867.1225]  mse_f: 4.094356536865234   total loss: [871.21686]\n",
            "mse_b ====== [866.74231]\n",
            "It: 53071, Time: 0.03\n",
            "mse_b  [866.7423]  mse_f: 3.046874761581421   total loss: [869.7892]\n",
            "mse_b ====== [866.808655]\n",
            "It: 53072, Time: 0.03\n",
            "mse_b  [866.80865]  mse_f: 3.204634189605713   total loss: [870.0133]\n",
            "mse_b ====== [867.221313]\n",
            "It: 53073, Time: 0.03\n",
            "mse_b  [867.2213]  mse_f: 4.029443740844727   total loss: [871.25073]\n",
            "mse_b ====== [866.783813]\n",
            "It: 53074, Time: 0.03\n",
            "mse_b  [866.7838]  mse_f: 3.735924243927002   total loss: [870.5197]\n",
            "mse_b ====== [866.622498]\n",
            "It: 53075, Time: 0.03\n",
            "mse_b  [866.6225]  mse_f: 3.0385560989379883   total loss: [869.6611]\n",
            "mse_b ====== [866.907593]\n",
            "It: 53076, Time: 0.03\n",
            "mse_b  [866.9076]  mse_f: 3.786606788635254   total loss: [870.6942]\n",
            "mse_b ====== [866.922485]\n",
            "It: 53077, Time: 0.03\n",
            "mse_b  [866.9225]  mse_f: 4.135826110839844   total loss: [871.0583]\n",
            "mse_b ====== [866.981445]\n",
            "It: 53078, Time: 0.03\n",
            "mse_b  [866.98145]  mse_f: 3.162137985229492   total loss: [870.14355]\n",
            "mse_b ====== [866.634033]\n",
            "It: 53079, Time: 0.03\n",
            "mse_b  [866.63403]  mse_f: 2.844451665878296   total loss: [869.47845]\n",
            "mse_b ====== [867.239197]\n",
            "It: 53080, Time: 0.03\n",
            "mse_b  [867.2392]  mse_f: 3.9148213863372803   total loss: [871.154]\n",
            "mse_b ====== [866.952942]\n",
            "It: 53081, Time: 0.03\n",
            "mse_b  [866.95294]  mse_f: 4.053666114807129   total loss: [871.0066]\n",
            "mse_b ====== [866.761902]\n",
            "It: 53082, Time: 0.03\n",
            "mse_b  [866.7619]  mse_f: 3.023824691772461   total loss: [869.7857]\n",
            "mse_b ====== [866.617798]\n",
            "It: 53083, Time: 0.04\n",
            "mse_b  [866.6178]  mse_f: 3.2080495357513428   total loss: [869.82587]\n",
            "mse_b ====== [867.042603]\n",
            "It: 53084, Time: 0.03\n",
            "mse_b  [867.0426]  mse_f: 3.917936086654663   total loss: [870.9605]\n",
            "mse_b ====== [866.924683]\n",
            "It: 53085, Time: 0.03\n",
            "mse_b  [866.9247]  mse_f: 3.5264458656311035   total loss: [870.4511]\n",
            "mse_b ====== [866.698]\n",
            "It: 53086, Time: 0.03\n",
            "mse_b  [866.698]  mse_f: 2.962451934814453   total loss: [869.66046]\n",
            "mse_b ====== [866.808777]\n",
            "It: 53087, Time: 0.03\n",
            "mse_b  [866.8088]  mse_f: 3.5162625312805176   total loss: [870.325]\n",
            "mse_b ====== [867.1745]\n",
            "It: 53088, Time: 0.03\n",
            "mse_b  [867.1745]  mse_f: 3.66278338432312   total loss: [870.8373]\n",
            "mse_b ====== [866.723511]\n",
            "It: 53089, Time: 0.03\n",
            "mse_b  [866.7235]  mse_f: 3.0011239051818848   total loss: [869.7246]\n",
            "mse_b ====== [866.84906]\n",
            "It: 53090, Time: 0.03\n",
            "mse_b  [866.84906]  mse_f: 2.899038553237915   total loss: [869.7481]\n",
            "mse_b ====== [866.749756]\n",
            "It: 53091, Time: 0.03\n",
            "mse_b  [866.74976]  mse_f: 3.7350316047668457   total loss: [870.4848]\n",
            "mse_b ====== [866.826599]\n",
            "It: 53092, Time: 0.03\n",
            "mse_b  [866.8266]  mse_f: 3.938944101333618   total loss: [870.76556]\n",
            "mse_b ====== [866.685547]\n",
            "It: 53093, Time: 0.03\n",
            "mse_b  [866.68555]  mse_f: 2.819025993347168   total loss: [869.5046]\n",
            "mse_b ====== [866.65033]\n",
            "It: 53094, Time: 0.03\n",
            "mse_b  [866.6503]  mse_f: 2.9810307025909424   total loss: [869.63135]\n",
            "mse_b ====== [866.996765]\n",
            "It: 53095, Time: 0.03\n",
            "mse_b  [866.99677]  mse_f: 3.7645037174224854   total loss: [870.7613]\n",
            "mse_b ====== [866.879639]\n",
            "It: 53096, Time: 0.03\n",
            "mse_b  [866.87964]  mse_f: 3.3190035820007324   total loss: [870.19867]\n",
            "mse_b ====== [866.553772]\n",
            "It: 53097, Time: 0.03\n",
            "mse_b  [866.5538]  mse_f: 2.9377894401550293   total loss: [869.4916]\n",
            "mse_b ====== [866.95752]\n",
            "It: 53098, Time: 0.03\n",
            "mse_b  [866.9575]  mse_f: 3.1328258514404297   total loss: [870.09033]\n",
            "mse_b ====== [866.879822]\n",
            "It: 53099, Time: 0.03\n",
            "mse_b  [866.8798]  mse_f: 3.825451135635376   total loss: [870.70526]\n",
            "mse_b ====== [866.663269]\n",
            "It: 53100, Time: 0.04\n",
            "mse_b  [866.66327]  mse_f: 3.016909599304199   total loss: [869.6802]\n",
            "mse_b ====== [866.568176]\n",
            "It: 53101, Time: 0.03\n",
            "mse_b  [866.5682]  mse_f: 2.81697940826416   total loss: [869.38513]\n",
            "mse_b ====== [866.907]\n",
            "It: 53102, Time: 0.03\n",
            "mse_b  [866.907]  mse_f: 3.508190631866455   total loss: [870.41516]\n",
            "mse_b ====== [866.965698]\n",
            "It: 53103, Time: 0.03\n",
            "mse_b  [866.9657]  mse_f: 3.4531259536743164   total loss: [870.4188]\n",
            "mse_b ====== [866.600281]\n",
            "It: 53104, Time: 0.03\n",
            "mse_b  [866.6003]  mse_f: 2.8648805618286133   total loss: [869.46515]\n",
            "mse_b ====== [866.693359]\n",
            "It: 53105, Time: 0.03\n",
            "mse_b  [866.69336]  mse_f: 2.9574294090270996   total loss: [869.6508]\n",
            "mse_b ====== [866.91]\n",
            "It: 53106, Time: 0.03\n",
            "mse_b  [866.91]  mse_f: 3.5065231323242188   total loss: [870.4165]\n",
            "mse_b ====== [866.63269]\n",
            "It: 53107, Time: 0.03\n",
            "mse_b  [866.6327]  mse_f: 3.467653274536133   total loss: [870.10034]\n",
            "mse_b ====== [866.60675]\n",
            "It: 53108, Time: 0.03\n",
            "mse_b  [866.60675]  mse_f: 2.6145756244659424   total loss: [869.2213]\n",
            "mse_b ====== [866.590332]\n",
            "It: 53109, Time: 0.04\n",
            "mse_b  [866.59033]  mse_f: 3.478661060333252   total loss: [870.069]\n",
            "mse_b ====== [866.827942]\n",
            "It: 53110, Time: 0.03\n",
            "mse_b  [866.82794]  mse_f: 3.556581497192383   total loss: [870.3845]\n",
            "mse_b ====== [866.844]\n",
            "It: 53111, Time: 0.04\n",
            "mse_b  [866.844]  mse_f: 2.8652615547180176   total loss: [869.7092]\n",
            "mse_b ====== [866.565]\n",
            "It: 53112, Time: 0.03\n",
            "mse_b  [866.565]  mse_f: 2.6751697063446045   total loss: [869.2402]\n",
            "mse_b ====== [866.867065]\n",
            "It: 53113, Time: 0.03\n",
            "mse_b  [866.86707]  mse_f: 3.3882648944854736   total loss: [870.2553]\n",
            "mse_b ====== [866.774292]\n",
            "It: 53114, Time: 0.03\n",
            "mse_b  [866.7743]  mse_f: 3.561448574066162   total loss: [870.33575]\n",
            "mse_b ====== [866.608215]\n",
            "It: 53115, Time: 0.03\n",
            "mse_b  [866.6082]  mse_f: 2.8114495277404785   total loss: [869.4197]\n",
            "mse_b ====== [866.730347]\n",
            "It: 53116, Time: 0.03\n",
            "mse_b  [866.73035]  mse_f: 2.9842281341552734   total loss: [869.7146]\n",
            "mse_b ====== [866.804749]\n",
            "It: 53117, Time: 0.03\n",
            "mse_b  [866.80475]  mse_f: 3.6382102966308594   total loss: [870.44293]\n",
            "mse_b ====== [866.672]\n",
            "It: 53118, Time: 0.03\n",
            "mse_b  [866.672]  mse_f: 3.2263541221618652   total loss: [869.8984]\n",
            "mse_b ====== [866.609924]\n",
            "It: 53119, Time: 0.03\n",
            "mse_b  [866.6099]  mse_f: 2.7508318424224854   total loss: [869.3608]\n",
            "mse_b ====== [866.710144]\n",
            "It: 53120, Time: 0.03\n",
            "mse_b  [866.71014]  mse_f: 3.097216844558716   total loss: [869.8074]\n",
            "mse_b ====== [866.710388]\n",
            "It: 53121, Time: 0.03\n",
            "mse_b  [866.7104]  mse_f: 3.5835094451904297   total loss: [870.2939]\n",
            "mse_b ====== [866.673767]\n",
            "It: 53122, Time: 0.03\n",
            "mse_b  [866.67377]  mse_f: 2.7899303436279297   total loss: [869.4637]\n",
            "mse_b ====== [866.62677]\n",
            "It: 53123, Time: 0.03\n",
            "mse_b  [866.6268]  mse_f: 2.784566879272461   total loss: [869.4113]\n",
            "mse_b ====== [866.682678]\n",
            "It: 53124, Time: 0.03\n",
            "mse_b  [866.6827]  mse_f: 3.45393967628479   total loss: [870.1366]\n",
            "mse_b ====== [866.776855]\n",
            "It: 53125, Time: 0.03\n",
            "mse_b  [866.77686]  mse_f: 3.509413957595825   total loss: [870.28625]\n",
            "mse_b ====== [866.731689]\n",
            "It: 53126, Time: 0.03\n",
            "mse_b  [866.7317]  mse_f: 3.031418800354004   total loss: [869.7631]\n",
            "mse_b ====== [866.794861]\n",
            "It: 53127, Time: 0.04\n",
            "mse_b  [866.79486]  mse_f: 3.7463371753692627   total loss: [870.5412]\n",
            "mse_b ====== [867.386597]\n",
            "It: 53128, Time: 0.03\n",
            "mse_b  [867.3866]  mse_f: 4.464283466339111   total loss: [871.8509]\n",
            "mse_b ====== [867.139221]\n",
            "It: 53129, Time: 0.03\n",
            "mse_b  [867.1392]  mse_f: 4.837308883666992   total loss: [871.9765]\n",
            "mse_b ====== [866.999207]\n",
            "It: 53130, Time: 0.03\n",
            "mse_b  [866.9992]  mse_f: 5.236368179321289   total loss: [872.2356]\n",
            "mse_b ====== [867.437134]\n",
            "It: 53131, Time: 0.03\n",
            "mse_b  [867.43713]  mse_f: 5.7558441162109375   total loss: [873.193]\n",
            "mse_b ====== [867.404663]\n",
            "It: 53132, Time: 0.03\n",
            "mse_b  [867.40466]  mse_f: 6.18794059753418   total loss: [873.5926]\n",
            "mse_b ====== [866.976807]\n",
            "It: 53133, Time: 0.03\n",
            "mse_b  [866.9768]  mse_f: 4.709633827209473   total loss: [871.68646]\n",
            "mse_b ====== [866.757]\n",
            "It: 53134, Time: 0.03\n",
            "mse_b  [866.757]  mse_f: 3.326158046722412   total loss: [870.0832]\n",
            "mse_b ====== [866.686584]\n",
            "It: 53135, Time: 0.03\n",
            "mse_b  [866.6866]  mse_f: 3.555229902267456   total loss: [870.2418]\n",
            "mse_b ====== [866.821655]\n",
            "It: 53136, Time: 0.03\n",
            "mse_b  [866.82166]  mse_f: 3.9520554542541504   total loss: [870.7737]\n",
            "mse_b ====== [867.041199]\n",
            "It: 53137, Time: 0.03\n",
            "mse_b  [867.0412]  mse_f: 4.917945861816406   total loss: [871.95917]\n",
            "mse_b ====== [867.172546]\n",
            "It: 53138, Time: 0.03\n",
            "mse_b  [867.17255]  mse_f: 6.073015213012695   total loss: [873.24554]\n",
            "mse_b ====== [867.290588]\n",
            "It: 53139, Time: 0.04\n",
            "mse_b  [867.2906]  mse_f: 4.978849411010742   total loss: [872.2694]\n",
            "mse_b ====== [867.069]\n",
            "It: 53140, Time: 0.03\n",
            "mse_b  [867.069]  mse_f: 3.187556266784668   total loss: [870.25653]\n",
            "mse_b ====== [866.706909]\n",
            "It: 53141, Time: 0.03\n",
            "mse_b  [866.7069]  mse_f: 2.9901232719421387   total loss: [869.697]\n",
            "mse_b ====== [867.303772]\n",
            "It: 53142, Time: 0.03\n",
            "mse_b  [867.3038]  mse_f: 4.60374641418457   total loss: [871.90753]\n",
            "mse_b ====== [867.541504]\n",
            "It: 53143, Time: 0.03\n",
            "mse_b  [867.5415]  mse_f: 5.0846848487854   total loss: [872.62616]\n",
            "mse_b ====== [867.092468]\n",
            "It: 53144, Time: 0.03\n",
            "mse_b  [867.09247]  mse_f: 4.004744529724121   total loss: [871.0972]\n",
            "mse_b ====== [866.996887]\n",
            "It: 53145, Time: 0.03\n",
            "mse_b  [866.9969]  mse_f: 3.0835657119750977   total loss: [870.08044]\n",
            "mse_b ====== [867.156128]\n",
            "It: 53146, Time: 0.03\n",
            "mse_b  [867.1561]  mse_f: 3.7891972064971924   total loss: [870.9453]\n",
            "mse_b ====== [866.928101]\n",
            "It: 53147, Time: 0.03\n",
            "mse_b  [866.9281]  mse_f: 4.371280670166016   total loss: [871.2994]\n",
            "mse_b ====== [866.898376]\n",
            "It: 53148, Time: 0.03\n",
            "mse_b  [866.8984]  mse_f: 4.30409049987793   total loss: [871.20245]\n",
            "mse_b ====== [866.72467]\n",
            "It: 53149, Time: 0.03\n",
            "mse_b  [866.7247]  mse_f: 4.000197410583496   total loss: [870.72485]\n",
            "mse_b ====== [866.910217]\n",
            "It: 53150, Time: 0.03\n",
            "mse_b  [866.9102]  mse_f: 3.485152006149292   total loss: [870.3954]\n",
            "mse_b ====== [866.646301]\n",
            "It: 53151, Time: 0.03\n",
            "mse_b  [866.6463]  mse_f: 3.8841724395751953   total loss: [870.53046]\n",
            "mse_b ====== [866.870178]\n",
            "It: 53152, Time: 0.03\n",
            "mse_b  [866.8702]  mse_f: 4.741324424743652   total loss: [871.6115]\n",
            "mse_b ====== [867.016846]\n",
            "It: 53153, Time: 0.03\n",
            "mse_b  [867.01685]  mse_f: 4.074710845947266   total loss: [871.09155]\n",
            "mse_b ====== [866.789062]\n",
            "It: 53154, Time: 0.03\n",
            "mse_b  [866.78906]  mse_f: 2.968991756439209   total loss: [869.75806]\n",
            "mse_b ====== [866.756348]\n",
            "It: 53155, Time: 0.03\n",
            "mse_b  [866.75635]  mse_f: 3.6649112701416016   total loss: [870.42126]\n",
            "mse_b ====== [866.986267]\n",
            "It: 53156, Time: 0.03\n",
            "mse_b  [866.98627]  mse_f: 4.844158172607422   total loss: [871.83044]\n",
            "mse_b ====== [866.778259]\n",
            "It: 53157, Time: 0.03\n",
            "mse_b  [866.77826]  mse_f: 4.093992233276367   total loss: [870.87225]\n",
            "mse_b ====== [866.627258]\n",
            "It: 53158, Time: 0.03\n",
            "mse_b  [866.62726]  mse_f: 2.8819031715393066   total loss: [869.50916]\n",
            "mse_b ====== [866.716248]\n",
            "It: 53159, Time: 0.03\n",
            "mse_b  [866.71625]  mse_f: 3.9600090980529785   total loss: [870.6763]\n",
            "mse_b ====== [866.941528]\n",
            "It: 53160, Time: 0.03\n",
            "mse_b  [866.9415]  mse_f: 4.332033157348633   total loss: [871.27356]\n",
            "mse_b ====== [867.079651]\n",
            "It: 53161, Time: 0.03\n",
            "mse_b  [867.07965]  mse_f: 3.647977352142334   total loss: [870.7276]\n",
            "mse_b ====== [866.820801]\n",
            "It: 53162, Time: 0.03\n",
            "mse_b  [866.8208]  mse_f: 3.1530356407165527   total loss: [869.9738]\n",
            "mse_b ====== [867.022156]\n",
            "It: 53163, Time: 0.03\n",
            "mse_b  [867.02216]  mse_f: 3.4972431659698486   total loss: [870.5194]\n",
            "mse_b ====== [867.058716]\n",
            "It: 53164, Time: 0.03\n",
            "mse_b  [867.0587]  mse_f: 3.7865076065063477   total loss: [870.8452]\n",
            "mse_b ====== [867.192871]\n",
            "It: 53165, Time: 0.03\n",
            "mse_b  [867.1929]  mse_f: 3.8563995361328125   total loss: [871.04926]\n",
            "mse_b ====== [866.895752]\n",
            "It: 53166, Time: 0.03\n",
            "mse_b  [866.89575]  mse_f: 4.145180702209473   total loss: [871.04095]\n",
            "mse_b ====== [866.907227]\n",
            "It: 53167, Time: 0.04\n",
            "mse_b  [866.9072]  mse_f: 3.563110113143921   total loss: [870.47034]\n",
            "mse_b ====== [866.784241]\n",
            "It: 53168, Time: 0.03\n",
            "mse_b  [866.78424]  mse_f: 2.97900652885437   total loss: [869.76324]\n",
            "mse_b ====== [867.153809]\n",
            "It: 53169, Time: 0.03\n",
            "mse_b  [867.1538]  mse_f: 4.016743183135986   total loss: [871.17053]\n",
            "mse_b ====== [867.089355]\n",
            "It: 53170, Time: 0.03\n",
            "mse_b  [867.08936]  mse_f: 4.30419397354126   total loss: [871.39355]\n",
            "mse_b ====== [866.935242]\n",
            "It: 53171, Time: 0.03\n",
            "mse_b  [866.93524]  mse_f: 3.3064212799072266   total loss: [870.24164]\n",
            "mse_b ====== [866.602295]\n",
            "It: 53172, Time: 0.03\n",
            "mse_b  [866.6023]  mse_f: 2.806276798248291   total loss: [869.40857]\n",
            "mse_b ====== [867.615234]\n",
            "It: 53173, Time: 0.03\n",
            "mse_b  [867.61523]  mse_f: 3.4227898120880127   total loss: [871.038]\n",
            "mse_b ====== [866.919]\n",
            "It: 53174, Time: 0.03\n",
            "mse_b  [866.919]  mse_f: 4.183015823364258   total loss: [871.10205]\n",
            "mse_b ====== [866.747864]\n",
            "It: 53175, Time: 0.03\n",
            "mse_b  [866.74786]  mse_f: 3.279573678970337   total loss: [870.02747]\n",
            "mse_b ====== [866.715393]\n",
            "It: 53176, Time: 0.03\n",
            "mse_b  [866.7154]  mse_f: 3.311389684677124   total loss: [870.0268]\n",
            "mse_b ====== [867.103]\n",
            "It: 53177, Time: 0.03\n",
            "mse_b  [867.103]  mse_f: 3.742447853088379   total loss: [870.84546]\n",
            "mse_b ====== [866.817383]\n",
            "It: 53178, Time: 0.03\n",
            "mse_b  [866.8174]  mse_f: 3.5392417907714844   total loss: [870.3566]\n",
            "mse_b ====== [866.847656]\n",
            "It: 53179, Time: 0.03\n",
            "mse_b  [866.84766]  mse_f: 3.7085423469543457   total loss: [870.5562]\n",
            "mse_b ====== [866.97467]\n",
            "It: 53180, Time: 0.03\n",
            "mse_b  [866.9747]  mse_f: 3.558896541595459   total loss: [870.53357]\n",
            "mse_b ====== [866.915405]\n",
            "It: 53181, Time: 0.03\n",
            "mse_b  [866.9154]  mse_f: 2.9955296516418457   total loss: [869.91095]\n",
            "mse_b ====== [866.863892]\n",
            "It: 53182, Time: 0.03\n",
            "mse_b  [866.8639]  mse_f: 3.3623998165130615   total loss: [870.2263]\n",
            "mse_b ====== [867.342773]\n",
            "It: 53183, Time: 0.03\n",
            "mse_b  [867.3428]  mse_f: 3.816460609436035   total loss: [871.15924]\n",
            "mse_b ====== [866.846436]\n",
            "It: 53184, Time: 0.03\n",
            "mse_b  [866.84644]  mse_f: 3.5748915672302246   total loss: [870.4213]\n",
            "mse_b ====== [866.756348]\n",
            "It: 53185, Time: 0.03\n",
            "mse_b  [866.75635]  mse_f: 2.68874192237854   total loss: [869.44507]\n",
            "mse_b ====== [866.934448]\n",
            "It: 53186, Time: 0.03\n",
            "mse_b  [866.93445]  mse_f: 3.540980815887451   total loss: [870.4754]\n",
            "mse_b ====== [866.990662]\n",
            "It: 53187, Time: 0.03\n",
            "mse_b  [866.99066]  mse_f: 4.096682071685791   total loss: [871.08734]\n",
            "mse_b ====== [866.658325]\n",
            "It: 53188, Time: 0.03\n",
            "mse_b  [866.6583]  mse_f: 3.3658688068389893   total loss: [870.0242]\n",
            "mse_b ====== [866.690247]\n",
            "It: 53189, Time: 0.03\n",
            "mse_b  [866.69025]  mse_f: 2.784640312194824   total loss: [869.4749]\n",
            "mse_b ====== [866.993225]\n",
            "It: 53190, Time: 0.03\n",
            "mse_b  [866.9932]  mse_f: 3.528696060180664   total loss: [870.5219]\n",
            "mse_b ====== [866.765381]\n",
            "It: 53191, Time: 0.03\n",
            "mse_b  [866.7654]  mse_f: 3.761730670928955   total loss: [870.5271]\n",
            "mse_b ====== [866.712219]\n",
            "It: 53192, Time: 0.03\n",
            "mse_b  [866.7122]  mse_f: 3.29892635345459   total loss: [870.01117]\n",
            "mse_b ====== [866.864746]\n",
            "It: 53193, Time: 0.03\n",
            "mse_b  [866.86475]  mse_f: 2.9250941276550293   total loss: [869.78986]\n",
            "mse_b ====== [866.697571]\n",
            "It: 53194, Time: 0.03\n",
            "mse_b  [866.6976]  mse_f: 3.5962717533111572   total loss: [870.2938]\n",
            "mse_b ====== [866.596741]\n",
            "It: 53195, Time: 0.04\n",
            "mse_b  [866.59674]  mse_f: 3.5082850456237793   total loss: [870.10504]\n",
            "mse_b ====== [866.926636]\n",
            "It: 53196, Time: 0.03\n",
            "mse_b  [866.92664]  mse_f: 3.427842140197754   total loss: [870.3545]\n",
            "mse_b ====== [866.842468]\n",
            "It: 53197, Time: 0.03\n",
            "mse_b  [866.84247]  mse_f: 3.3877930641174316   total loss: [870.2303]\n",
            "mse_b ====== [866.785828]\n",
            "It: 53198, Time: 0.03\n",
            "mse_b  [866.7858]  mse_f: 2.8814656734466553   total loss: [869.6673]\n",
            "mse_b ====== [866.752197]\n",
            "It: 53199, Time: 0.03\n",
            "mse_b  [866.7522]  mse_f: 3.165097951889038   total loss: [869.9173]\n",
            "mse_b ====== [867.056702]\n",
            "It: 53200, Time: 0.03\n",
            "mse_b  [867.0567]  mse_f: 3.8854968547821045   total loss: [870.9422]\n",
            "mse_b ====== [866.95282]\n",
            "It: 53201, Time: 0.03\n",
            "mse_b  [866.9528]  mse_f: 3.394399404525757   total loss: [870.3472]\n",
            "mse_b ====== [866.690063]\n",
            "It: 53202, Time: 0.03\n",
            "mse_b  [866.69006]  mse_f: 2.722975492477417   total loss: [869.413]\n",
            "mse_b ====== [866.60968]\n",
            "It: 53203, Time: 0.03\n",
            "mse_b  [866.6097]  mse_f: 3.207627296447754   total loss: [869.8173]\n",
            "mse_b ====== [867.091187]\n",
            "It: 53204, Time: 0.03\n",
            "mse_b  [867.0912]  mse_f: 3.6069555282592773   total loss: [870.6981]\n",
            "mse_b ====== [866.970337]\n",
            "It: 53205, Time: 0.03\n",
            "mse_b  [866.97034]  mse_f: 3.232802391052246   total loss: [870.2031]\n",
            "mse_b ====== [866.664917]\n",
            "It: 53206, Time: 0.03\n",
            "mse_b  [866.6649]  mse_f: 3.0809226036071777   total loss: [869.74585]\n",
            "mse_b ====== [866.933167]\n",
            "It: 53207, Time: 0.03\n",
            "mse_b  [866.93317]  mse_f: 3.073169231414795   total loss: [870.00635]\n",
            "mse_b ====== [866.710327]\n",
            "It: 53208, Time: 0.03\n",
            "mse_b  [866.7103]  mse_f: 3.2375357151031494   total loss: [869.9479]\n",
            "mse_b ====== [866.865784]\n",
            "It: 53209, Time: 0.03\n",
            "mse_b  [866.8658]  mse_f: 3.234478712081909   total loss: [870.1003]\n",
            "mse_b ====== [866.796509]\n",
            "It: 53210, Time: 0.03\n",
            "mse_b  [866.7965]  mse_f: 3.727670669555664   total loss: [870.5242]\n",
            "mse_b ====== [866.625122]\n",
            "It: 53211, Time: 0.03\n",
            "mse_b  [866.6251]  mse_f: 3.433623790740967   total loss: [870.0587]\n",
            "mse_b ====== [866.591125]\n",
            "It: 53212, Time: 0.03\n",
            "mse_b  [866.5911]  mse_f: 2.7295994758605957   total loss: [869.32074]\n",
            "mse_b ====== [866.624878]\n",
            "It: 53213, Time: 0.03\n",
            "mse_b  [866.6249]  mse_f: 3.3356730937957764   total loss: [869.9606]\n",
            "mse_b ====== [866.648804]\n",
            "It: 53214, Time: 0.03\n",
            "mse_b  [866.6488]  mse_f: 4.1290812492370605   total loss: [870.7779]\n",
            "mse_b ====== [866.74469]\n",
            "It: 53215, Time: 0.03\n",
            "mse_b  [866.7447]  mse_f: 3.391132354736328   total loss: [870.1358]\n",
            "mse_b ====== [866.575]\n",
            "It: 53216, Time: 0.03\n",
            "mse_b  [866.575]  mse_f: 2.6825485229492188   total loss: [869.25757]\n",
            "mse_b ====== [866.682373]\n",
            "It: 53217, Time: 0.03\n",
            "mse_b  [866.6824]  mse_f: 3.30397891998291   total loss: [869.9863]\n",
            "mse_b ====== [866.78595]\n",
            "It: 53218, Time: 0.03\n",
            "mse_b  [866.78595]  mse_f: 3.4739413261413574   total loss: [870.2599]\n",
            "mse_b ====== [866.768555]\n",
            "It: 53219, Time: 0.03\n",
            "mse_b  [866.76855]  mse_f: 3.2695319652557373   total loss: [870.0381]\n",
            "mse_b ====== [866.944519]\n",
            "It: 53220, Time: 0.03\n",
            "mse_b  [866.9445]  mse_f: 3.1515307426452637   total loss: [870.09607]\n",
            "mse_b ====== [866.77063]\n",
            "It: 53221, Time: 0.03\n",
            "mse_b  [866.7706]  mse_f: 3.2109804153442383   total loss: [869.9816]\n",
            "mse_b ====== [866.631042]\n",
            "It: 53222, Time: 0.03\n",
            "mse_b  [866.63104]  mse_f: 2.97422456741333   total loss: [869.6053]\n",
            "mse_b ====== [866.912537]\n",
            "It: 53223, Time: 0.03\n",
            "mse_b  [866.91254]  mse_f: 3.262648820877075   total loss: [870.1752]\n",
            "mse_b ====== [867.011719]\n",
            "It: 53224, Time: 0.03\n",
            "mse_b  [867.0117]  mse_f: 3.6185195446014404   total loss: [870.63025]\n",
            "mse_b ====== [866.736633]\n",
            "It: 53225, Time: 0.03\n",
            "mse_b  [866.73663]  mse_f: 3.5586702823638916   total loss: [870.2953]\n",
            "mse_b ====== [866.578674]\n",
            "It: 53226, Time: 0.03\n",
            "mse_b  [866.5787]  mse_f: 2.651397705078125   total loss: [869.2301]\n",
            "mse_b ====== [866.816345]\n",
            "It: 53227, Time: 0.03\n",
            "mse_b  [866.81635]  mse_f: 3.0248770713806152   total loss: [869.84125]\n",
            "mse_b ====== [866.905701]\n",
            "It: 53228, Time: 0.03\n",
            "mse_b  [866.9057]  mse_f: 3.706136703491211   total loss: [870.6118]\n",
            "mse_b ====== [866.799377]\n",
            "It: 53229, Time: 0.03\n",
            "mse_b  [866.7994]  mse_f: 3.7493624687194824   total loss: [870.54877]\n",
            "mse_b ====== [866.62384]\n",
            "It: 53230, Time: 0.03\n",
            "mse_b  [866.62384]  mse_f: 3.333956003189087   total loss: [869.9578]\n",
            "mse_b ====== [866.656616]\n",
            "It: 53231, Time: 0.03\n",
            "mse_b  [866.6566]  mse_f: 3.2407658100128174   total loss: [869.8974]\n",
            "mse_b ====== [866.902893]\n",
            "It: 53232, Time: 0.03\n",
            "mse_b  [866.9029]  mse_f: 2.9742631912231445   total loss: [869.87714]\n",
            "mse_b ====== [866.635132]\n",
            "It: 53233, Time: 0.03\n",
            "mse_b  [866.63513]  mse_f: 3.353350877761841   total loss: [869.98846]\n",
            "mse_b ====== [866.888611]\n",
            "It: 53234, Time: 0.03\n",
            "mse_b  [866.8886]  mse_f: 3.780202865600586   total loss: [870.6688]\n",
            "mse_b ====== [866.964111]\n",
            "It: 53235, Time: 0.03\n",
            "mse_b  [866.9641]  mse_f: 3.8113582134246826   total loss: [870.77545]\n",
            "mse_b ====== [866.672729]\n",
            "It: 53236, Time: 0.04\n",
            "mse_b  [866.6727]  mse_f: 2.907998561859131   total loss: [869.58075]\n",
            "mse_b ====== [866.52533]\n",
            "It: 53237, Time: 0.03\n",
            "mse_b  [866.5253]  mse_f: 2.786515235900879   total loss: [869.3118]\n",
            "mse_b ====== [866.727478]\n",
            "It: 53238, Time: 0.03\n",
            "mse_b  [866.7275]  mse_f: 3.612724542617798   total loss: [870.3402]\n",
            "mse_b ====== [866.995667]\n",
            "It: 53239, Time: 0.03\n",
            "mse_b  [866.99567]  mse_f: 3.7202320098876953   total loss: [870.7159]\n",
            "mse_b ====== [866.74353]\n",
            "It: 53240, Time: 0.03\n",
            "mse_b  [866.7435]  mse_f: 3.320192813873291   total loss: [870.0637]\n",
            "mse_b ====== [866.706848]\n",
            "It: 53241, Time: 0.03\n",
            "mse_b  [866.70685]  mse_f: 3.107034683227539   total loss: [869.8139]\n",
            "mse_b ====== [866.828064]\n",
            "It: 53242, Time: 0.03\n",
            "mse_b  [866.82806]  mse_f: 3.07511830329895   total loss: [869.9032]\n",
            "mse_b ====== [866.716919]\n",
            "It: 53243, Time: 0.03\n",
            "mse_b  [866.7169]  mse_f: 2.8718056678771973   total loss: [869.58875]\n",
            "mse_b ====== [866.721252]\n",
            "It: 53244, Time: 0.03\n",
            "mse_b  [866.72125]  mse_f: 3.2902534008026123   total loss: [870.01154]\n",
            "mse_b ====== [867.109375]\n",
            "It: 53245, Time: 0.03\n",
            "mse_b  [867.1094]  mse_f: 3.6839513778686523   total loss: [870.79333]\n",
            "mse_b ====== [866.800964]\n",
            "It: 53246, Time: 0.03\n",
            "mse_b  [866.80096]  mse_f: 3.470184326171875   total loss: [870.2711]\n",
            "mse_b ====== [866.516846]\n",
            "It: 53247, Time: 0.03\n",
            "mse_b  [866.51685]  mse_f: 2.693237781524658   total loss: [869.2101]\n",
            "mse_b ====== [866.674866]\n",
            "It: 53248, Time: 0.03\n",
            "mse_b  [866.67487]  mse_f: 2.969358205795288   total loss: [869.6442]\n",
            "mse_b ====== [866.862793]\n",
            "It: 53249, Time: 0.03\n",
            "mse_b  [866.8628]  mse_f: 3.319477081298828   total loss: [870.18225]\n",
            "mse_b ====== [866.7]\n",
            "It: 53250, Time: 0.04\n",
            "mse_b  [866.7]  mse_f: 3.460886240005493   total loss: [870.1609]\n",
            "mse_b ====== [866.752747]\n",
            "It: 53251, Time: 0.04\n",
            "mse_b  [866.75275]  mse_f: 3.400022029876709   total loss: [870.1528]\n",
            "mse_b ====== [867.000122]\n",
            "It: 53252, Time: 0.04\n",
            "mse_b  [867.0001]  mse_f: 3.2857775688171387   total loss: [870.2859]\n",
            "mse_b ====== [866.631897]\n",
            "It: 53253, Time: 0.03\n",
            "mse_b  [866.6319]  mse_f: 2.929678440093994   total loss: [869.5616]\n",
            "mse_b ====== [866.585083]\n",
            "It: 53254, Time: 0.04\n",
            "mse_b  [866.5851]  mse_f: 2.740553855895996   total loss: [869.3256]\n",
            "mse_b ====== [866.864]\n",
            "It: 53255, Time: 0.03\n",
            "mse_b  [866.864]  mse_f: 3.2860302925109863   total loss: [870.15]\n",
            "mse_b ====== [866.826233]\n",
            "It: 53256, Time: 0.03\n",
            "mse_b  [866.82623]  mse_f: 3.995722532272339   total loss: [870.82196]\n",
            "mse_b ====== [866.652222]\n",
            "It: 53257, Time: 0.03\n",
            "mse_b  [866.6522]  mse_f: 3.2654218673706055   total loss: [869.91766]\n",
            "mse_b ====== [866.605042]\n",
            "It: 53258, Time: 0.03\n",
            "mse_b  [866.60504]  mse_f: 2.979548454284668   total loss: [869.5846]\n",
            "mse_b ====== [866.716]\n",
            "It: 53259, Time: 0.03\n",
            "mse_b  [866.716]  mse_f: 2.9891891479492188   total loss: [869.7052]\n",
            "mse_b ====== [866.787903]\n",
            "It: 53260, Time: 0.03\n",
            "mse_b  [866.7879]  mse_f: 3.135068655014038   total loss: [869.923]\n",
            "mse_b ====== [866.690613]\n",
            "It: 53261, Time: 0.04\n",
            "mse_b  [866.6906]  mse_f: 3.4024059772491455   total loss: [870.093]\n",
            "mse_b ====== [866.778076]\n",
            "It: 53262, Time: 0.03\n",
            "mse_b  [866.7781]  mse_f: 3.8189432621002197   total loss: [870.59705]\n",
            "mse_b ====== [866.984253]\n",
            "It: 53263, Time: 0.03\n",
            "mse_b  [866.98425]  mse_f: 3.278921604156494   total loss: [870.2632]\n",
            "mse_b ====== [866.683533]\n",
            "It: 53264, Time: 0.04\n",
            "mse_b  [866.68353]  mse_f: 2.7618532180786133   total loss: [869.4454]\n",
            "mse_b ====== [866.607727]\n",
            "It: 53265, Time: 0.04\n",
            "mse_b  [866.6077]  mse_f: 2.6317338943481445   total loss: [869.23944]\n",
            "mse_b ====== [866.769409]\n",
            "It: 53266, Time: 0.03\n",
            "mse_b  [866.7694]  mse_f: 3.511087417602539   total loss: [870.2805]\n",
            "mse_b ====== [866.911]\n",
            "It: 53267, Time: 0.03\n",
            "mse_b  [866.911]  mse_f: 3.649345636367798   total loss: [870.56036]\n",
            "mse_b ====== [866.880615]\n",
            "It: 53268, Time: 0.03\n",
            "mse_b  [866.8806]  mse_f: 3.362588882446289   total loss: [870.2432]\n",
            "mse_b ====== [866.726746]\n",
            "It: 53269, Time: 0.04\n",
            "mse_b  [866.72675]  mse_f: 3.146803855895996   total loss: [869.87354]\n",
            "mse_b ====== [866.619629]\n",
            "It: 53270, Time: 0.03\n",
            "mse_b  [866.6196]  mse_f: 3.1049256324768066   total loss: [869.72455]\n",
            "mse_b ====== [866.585388]\n",
            "It: 53271, Time: 0.03\n",
            "mse_b  [866.5854]  mse_f: 2.844210624694824   total loss: [869.4296]\n",
            "mse_b ====== [866.755493]\n",
            "It: 53272, Time: 0.03\n",
            "mse_b  [866.7555]  mse_f: 3.157409191131592   total loss: [869.9129]\n",
            "mse_b ====== [866.735596]\n",
            "It: 53273, Time: 0.03\n",
            "mse_b  [866.7356]  mse_f: 3.880007743835449   total loss: [870.6156]\n",
            "mse_b ====== [866.608032]\n",
            "It: 53274, Time: 0.03\n",
            "mse_b  [866.60803]  mse_f: 4.011423110961914   total loss: [870.61945]\n",
            "mse_b ====== [866.61438]\n",
            "It: 53275, Time: 0.03\n",
            "mse_b  [866.6144]  mse_f: 2.9694178104400635   total loss: [869.5838]\n",
            "mse_b ====== [866.515747]\n",
            "It: 53276, Time: 0.03\n",
            "mse_b  [866.51575]  mse_f: 2.819298267364502   total loss: [869.335]\n",
            "mse_b ====== [866.629]\n",
            "It: 53277, Time: 0.03\n",
            "mse_b  [866.629]  mse_f: 3.195495367050171   total loss: [869.8245]\n",
            "mse_b ====== [866.716919]\n",
            "It: 53278, Time: 0.03\n",
            "mse_b  [866.7169]  mse_f: 3.4298574924468994   total loss: [870.1468]\n",
            "mse_b ====== [866.709]\n",
            "It: 53279, Time: 0.04\n",
            "mse_b  [866.709]  mse_f: 3.6181602478027344   total loss: [870.32715]\n",
            "mse_b ====== [867.013062]\n",
            "It: 53280, Time: 0.03\n",
            "mse_b  [867.01306]  mse_f: 3.7818474769592285   total loss: [870.7949]\n",
            "mse_b ====== [866.806335]\n",
            "It: 53281, Time: 0.03\n",
            "mse_b  [866.80634]  mse_f: 3.40999698638916   total loss: [870.2163]\n",
            "mse_b ====== [866.556763]\n",
            "It: 53282, Time: 0.03\n",
            "mse_b  [866.55676]  mse_f: 2.7448978424072266   total loss: [869.30164]\n",
            "mse_b ====== [866.584961]\n",
            "It: 53283, Time: 0.03\n",
            "mse_b  [866.58496]  mse_f: 2.892845392227173   total loss: [869.4778]\n",
            "mse_b ====== [866.966431]\n",
            "It: 53284, Time: 0.03\n",
            "mse_b  [866.96643]  mse_f: 3.6497116088867188   total loss: [870.61615]\n",
            "mse_b ====== [867.060852]\n",
            "It: 53285, Time: 0.03\n",
            "mse_b  [867.06085]  mse_f: 3.7637388706207275   total loss: [870.8246]\n",
            "mse_b ====== [866.664551]\n",
            "It: 53286, Time: 0.03\n",
            "mse_b  [866.66455]  mse_f: 4.063079357147217   total loss: [870.7276]\n",
            "mse_b ====== [866.681396]\n",
            "It: 53287, Time: 0.03\n",
            "mse_b  [866.6814]  mse_f: 3.5609846115112305   total loss: [870.2424]\n",
            "mse_b ====== [866.68811]\n",
            "It: 53288, Time: 0.03\n",
            "mse_b  [866.6881]  mse_f: 3.012089729309082   total loss: [869.7002]\n",
            "mse_b ====== [866.594604]\n",
            "It: 53289, Time: 0.03\n",
            "mse_b  [866.5946]  mse_f: 3.0086917877197266   total loss: [869.6033]\n",
            "mse_b ====== [866.79895]\n",
            "It: 53290, Time: 0.03\n",
            "mse_b  [866.79895]  mse_f: 3.7636051177978516   total loss: [870.56256]\n",
            "mse_b ====== [866.704468]\n",
            "It: 53291, Time: 0.03\n",
            "mse_b  [866.70447]  mse_f: 4.908255577087402   total loss: [871.61273]\n",
            "mse_b ====== [866.849487]\n",
            "It: 53292, Time: 0.03\n",
            "mse_b  [866.8495]  mse_f: 4.4307708740234375   total loss: [871.2803]\n",
            "mse_b ====== [866.781433]\n",
            "It: 53293, Time: 0.03\n",
            "mse_b  [866.78143]  mse_f: 3.007535219192505   total loss: [869.78894]\n",
            "mse_b ====== [866.545959]\n",
            "It: 53294, Time: 0.03\n",
            "mse_b  [866.54596]  mse_f: 2.939866542816162   total loss: [869.48584]\n",
            "mse_b ====== [866.88385]\n",
            "It: 53295, Time: 0.03\n",
            "mse_b  [866.88385]  mse_f: 3.486236095428467   total loss: [870.37006]\n",
            "mse_b ====== [866.939697]\n",
            "It: 53296, Time: 0.03\n",
            "mse_b  [866.9397]  mse_f: 3.9740936756134033   total loss: [870.9138]\n",
            "mse_b ====== [867.385803]\n",
            "It: 53297, Time: 0.03\n",
            "mse_b  [867.3858]  mse_f: 4.170429229736328   total loss: [871.5562]\n",
            "mse_b ====== [866.846558]\n",
            "It: 53298, Time: 0.03\n",
            "mse_b  [866.84656]  mse_f: 4.40176248550415   total loss: [871.2483]\n",
            "mse_b ====== [866.61145]\n",
            "It: 53299, Time: 0.03\n",
            "mse_b  [866.61145]  mse_f: 2.9718384742736816   total loss: [869.5833]\n",
            "mse_b ====== [866.655701]\n",
            "It: 53300, Time: 0.03\n",
            "mse_b  [866.6557]  mse_f: 2.8406667709350586   total loss: [869.49634]\n",
            "mse_b ====== [867.107239]\n",
            "It: 53301, Time: 0.03\n",
            "mse_b  [867.10724]  mse_f: 3.948763847351074   total loss: [871.056]\n",
            "mse_b ====== [867.039673]\n",
            "It: 53302, Time: 0.03\n",
            "mse_b  [867.0397]  mse_f: 4.851297855377197   total loss: [871.891]\n",
            "mse_b ====== [866.856323]\n",
            "It: 53303, Time: 0.03\n",
            "mse_b  [866.8563]  mse_f: 4.639899730682373   total loss: [871.4962]\n",
            "mse_b ====== [866.614319]\n",
            "It: 53304, Time: 0.03\n",
            "mse_b  [866.6143]  mse_f: 3.2028865814208984   total loss: [869.8172]\n",
            "mse_b ====== [866.83374]\n",
            "It: 53305, Time: 0.03\n",
            "mse_b  [866.83374]  mse_f: 2.946723461151123   total loss: [869.78046]\n",
            "mse_b ====== [867.198608]\n",
            "It: 53306, Time: 0.03\n",
            "mse_b  [867.1986]  mse_f: 4.268837928771973   total loss: [871.46747]\n",
            "mse_b ====== [866.985535]\n",
            "It: 53307, Time: 0.04\n",
            "mse_b  [866.98553]  mse_f: 5.458639621734619   total loss: [872.44415]\n",
            "mse_b ====== [867.083069]\n",
            "It: 53308, Time: 0.03\n",
            "mse_b  [867.08307]  mse_f: 5.01546573638916   total loss: [872.0985]\n",
            "mse_b ====== [866.707886]\n",
            "It: 53309, Time: 0.03\n",
            "mse_b  [866.7079]  mse_f: 3.6966073513031006   total loss: [870.4045]\n",
            "mse_b ====== [866.769592]\n",
            "It: 53310, Time: 0.03\n",
            "mse_b  [866.7696]  mse_f: 3.0886034965515137   total loss: [869.8582]\n",
            "mse_b ====== [866.918884]\n",
            "It: 53311, Time: 0.03\n",
            "mse_b  [866.9189]  mse_f: 4.903634071350098   total loss: [871.8225]\n",
            "mse_b ====== [867.140747]\n",
            "It: 53312, Time: 0.04\n",
            "mse_b  [867.14075]  mse_f: 5.968746185302734   total loss: [873.1095]\n",
            "mse_b ====== [867.068542]\n",
            "It: 53313, Time: 0.03\n",
            "mse_b  [867.06854]  mse_f: 4.274446487426758   total loss: [871.343]\n",
            "mse_b ====== [866.60437]\n",
            "It: 53314, Time: 0.03\n",
            "mse_b  [866.6044]  mse_f: 2.706930160522461   total loss: [869.3113]\n",
            "mse_b ====== [866.682251]\n",
            "It: 53315, Time: 0.03\n",
            "mse_b  [866.68225]  mse_f: 4.03936243057251   total loss: [870.7216]\n",
            "mse_b ====== [867.261475]\n",
            "It: 53316, Time: 0.04\n",
            "mse_b  [867.2615]  mse_f: 5.4939470291137695   total loss: [872.75543]\n",
            "mse_b ====== [867.074463]\n",
            "It: 53317, Time: 0.04\n",
            "mse_b  [867.07446]  mse_f: 4.6282734870910645   total loss: [871.70276]\n",
            "mse_b ====== [866.629272]\n",
            "It: 53318, Time: 0.03\n",
            "mse_b  [866.6293]  mse_f: 2.868514060974121   total loss: [869.4978]\n",
            "mse_b ====== [866.674805]\n",
            "It: 53319, Time: 0.04\n",
            "mse_b  [866.6748]  mse_f: 3.4700398445129395   total loss: [870.14484]\n",
            "mse_b ====== [867.253357]\n",
            "It: 53320, Time: 0.03\n",
            "mse_b  [867.25336]  mse_f: 4.904717445373535   total loss: [872.1581]\n",
            "mse_b ====== [867.168701]\n",
            "It: 53321, Time: 0.04\n",
            "mse_b  [867.1687]  mse_f: 4.562693119049072   total loss: [871.7314]\n",
            "mse_b ====== [866.664673]\n",
            "It: 53322, Time: 0.04\n",
            "mse_b  [866.6647]  mse_f: 2.7909393310546875   total loss: [869.4556]\n",
            "mse_b ====== [866.985168]\n",
            "It: 53323, Time: 0.04\n",
            "mse_b  [866.98517]  mse_f: 3.414576768875122   total loss: [870.3997]\n",
            "mse_b ====== [867.287048]\n",
            "It: 53324, Time: 0.05\n",
            "mse_b  [867.28705]  mse_f: 4.714900016784668   total loss: [872.00195]\n",
            "mse_b ====== [867.007141]\n",
            "It: 53325, Time: 0.03\n",
            "mse_b  [867.00714]  mse_f: 4.527951240539551   total loss: [871.5351]\n",
            "mse_b ====== [866.744568]\n",
            "It: 53326, Time: 0.03\n",
            "mse_b  [866.74457]  mse_f: 3.459596633911133   total loss: [870.20416]\n",
            "mse_b ====== [866.95929]\n",
            "It: 53327, Time: 0.03\n",
            "mse_b  [866.9593]  mse_f: 3.2498462200164795   total loss: [870.2091]\n",
            "mse_b ====== [867.033447]\n",
            "It: 53328, Time: 0.03\n",
            "mse_b  [867.03345]  mse_f: 4.342617511749268   total loss: [871.37604]\n",
            "mse_b ====== [867.057922]\n",
            "It: 53329, Time: 0.03\n",
            "mse_b  [867.0579]  mse_f: 4.710700988769531   total loss: [871.7686]\n",
            "mse_b ====== [866.852783]\n",
            "It: 53330, Time: 0.03\n",
            "mse_b  [866.8528]  mse_f: 3.396376132965088   total loss: [870.24915]\n",
            "mse_b ====== [866.795227]\n",
            "It: 53331, Time: 0.03\n",
            "mse_b  [866.7952]  mse_f: 3.058879852294922   total loss: [869.8541]\n",
            "mse_b ====== [866.820312]\n",
            "It: 53332, Time: 0.04\n",
            "mse_b  [866.8203]  mse_f: 4.373530864715576   total loss: [871.19385]\n",
            "mse_b ====== [867.022217]\n",
            "It: 53333, Time: 0.03\n",
            "mse_b  [867.0222]  mse_f: 4.307418346405029   total loss: [871.32965]\n",
            "mse_b ====== [866.886658]\n",
            "It: 53334, Time: 0.03\n",
            "mse_b  [866.88666]  mse_f: 3.1918790340423584   total loss: [870.07855]\n",
            "mse_b ====== [866.615479]\n",
            "It: 53335, Time: 0.03\n",
            "mse_b  [866.6155]  mse_f: 3.196784734725952   total loss: [869.81226]\n",
            "mse_b ====== [866.851135]\n",
            "It: 53336, Time: 0.03\n",
            "mse_b  [866.85114]  mse_f: 4.395462989807129   total loss: [871.2466]\n",
            "mse_b ====== [867.068237]\n",
            "It: 53337, Time: 0.03\n",
            "mse_b  [867.06824]  mse_f: 4.477630615234375   total loss: [871.5459]\n",
            "mse_b ====== [867.076538]\n",
            "It: 53338, Time: 0.03\n",
            "mse_b  [867.07654]  mse_f: 2.9358208179473877   total loss: [870.0123]\n",
            "mse_b ====== [866.681458]\n",
            "It: 53339, Time: 0.03\n",
            "mse_b  [866.68146]  mse_f: 3.0139153003692627   total loss: [869.6954]\n",
            "mse_b ====== [867.342651]\n",
            "It: 53340, Time: 0.03\n",
            "mse_b  [867.34265]  mse_f: 4.382566452026367   total loss: [871.7252]\n",
            "mse_b ====== [866.893921]\n",
            "It: 53341, Time: 0.03\n",
            "mse_b  [866.8939]  mse_f: 4.463169097900391   total loss: [871.3571]\n",
            "mse_b ====== [866.573425]\n",
            "It: 53342, Time: 0.03\n",
            "mse_b  [866.5734]  mse_f: 2.908646583557129   total loss: [869.48206]\n",
            "mse_b ====== [866.64917]\n",
            "It: 53343, Time: 0.03\n",
            "mse_b  [866.6492]  mse_f: 3.3174829483032227   total loss: [869.9667]\n",
            "mse_b ====== [867.232056]\n",
            "It: 53344, Time: 0.03\n",
            "mse_b  [867.23206]  mse_f: 4.518209934234619   total loss: [871.75024]\n",
            "mse_b ====== [866.863708]\n",
            "It: 53345, Time: 0.03\n",
            "mse_b  [866.8637]  mse_f: 3.7081949710845947   total loss: [870.5719]\n",
            "mse_b ====== [866.580566]\n",
            "It: 53346, Time: 0.03\n",
            "mse_b  [866.58057]  mse_f: 2.6713948249816895   total loss: [869.25195]\n",
            "mse_b ====== [866.797668]\n",
            "It: 53347, Time: 0.03\n",
            "mse_b  [866.79767]  mse_f: 3.950187921524048   total loss: [870.74786]\n",
            "mse_b ====== [867.154419]\n",
            "It: 53348, Time: 0.03\n",
            "mse_b  [867.1544]  mse_f: 4.255174160003662   total loss: [871.4096]\n",
            "mse_b ====== [866.888916]\n",
            "It: 53349, Time: 0.04\n",
            "mse_b  [866.8889]  mse_f: 3.357074737548828   total loss: [870.246]\n",
            "mse_b ====== [866.8349]\n",
            "It: 53350, Time: 0.03\n",
            "mse_b  [866.8349]  mse_f: 2.72635817527771   total loss: [869.5613]\n",
            "mse_b ====== [866.976685]\n",
            "It: 53351, Time: 0.03\n",
            "mse_b  [866.9767]  mse_f: 3.8048081398010254   total loss: [870.7815]\n",
            "mse_b ====== [866.798462]\n",
            "It: 53352, Time: 0.03\n",
            "mse_b  [866.79846]  mse_f: 4.414123058319092   total loss: [871.2126]\n",
            "mse_b ====== [866.718689]\n",
            "It: 53353, Time: 0.03\n",
            "mse_b  [866.7187]  mse_f: 3.1485743522644043   total loss: [869.86725]\n",
            "mse_b ====== [866.84967]\n",
            "It: 53354, Time: 0.03\n",
            "mse_b  [866.8497]  mse_f: 2.9369657039642334   total loss: [869.7866]\n",
            "mse_b ====== [867.092896]\n",
            "It: 53355, Time: 0.03\n",
            "mse_b  [867.0929]  mse_f: 3.8655734062194824   total loss: [870.9585]\n",
            "mse_b ====== [866.784058]\n",
            "It: 53356, Time: 0.03\n",
            "mse_b  [866.78406]  mse_f: 3.695769786834717   total loss: [870.4798]\n",
            "mse_b ====== [866.74707]\n",
            "It: 53357, Time: 0.03\n",
            "mse_b  [866.7471]  mse_f: 2.9779436588287354   total loss: [869.72504]\n",
            "mse_b ====== [866.892944]\n",
            "It: 53358, Time: 0.03\n",
            "mse_b  [866.89294]  mse_f: 3.403895854949951   total loss: [870.2968]\n",
            "mse_b ====== [867.136719]\n",
            "It: 53359, Time: 0.03\n",
            "mse_b  [867.1367]  mse_f: 3.760681390762329   total loss: [870.8974]\n",
            "mse_b ====== [866.81012]\n",
            "It: 53360, Time: 0.05\n",
            "mse_b  [866.8101]  mse_f: 3.270430088043213   total loss: [870.08057]\n",
            "mse_b ====== [867.023499]\n",
            "It: 53361, Time: 0.03\n",
            "mse_b  [867.0235]  mse_f: 2.965161085128784   total loss: [869.98865]\n",
            "mse_b ====== [866.897949]\n",
            "It: 53362, Time: 0.03\n",
            "mse_b  [866.89795]  mse_f: 3.565626382827759   total loss: [870.46356]\n",
            "mse_b ====== [866.814453]\n",
            "It: 53363, Time: 0.03\n",
            "mse_b  [866.81445]  mse_f: 3.353980779647827   total loss: [870.16846]\n",
            "mse_b ====== [866.818359]\n",
            "It: 53364, Time: 0.04\n",
            "mse_b  [866.81836]  mse_f: 3.4003169536590576   total loss: [870.2187]\n",
            "mse_b ====== [867.038574]\n",
            "It: 53365, Time: 0.03\n",
            "mse_b  [867.0386]  mse_f: 3.062291383743286   total loss: [870.1009]\n",
            "mse_b ====== [866.902405]\n",
            "It: 53366, Time: 0.03\n",
            "mse_b  [866.9024]  mse_f: 3.244576930999756   total loss: [870.147]\n",
            "mse_b ====== [866.820251]\n",
            "It: 53367, Time: 0.03\n",
            "mse_b  [866.82025]  mse_f: 3.274261236190796   total loss: [870.0945]\n",
            "mse_b ====== [866.895386]\n",
            "It: 53368, Time: 0.03\n",
            "mse_b  [866.8954]  mse_f: 3.2806410789489746   total loss: [870.176]\n",
            "mse_b ====== [866.797241]\n",
            "It: 53369, Time: 0.03\n",
            "mse_b  [866.79724]  mse_f: 3.3758809566497803   total loss: [870.1731]\n",
            "mse_b ====== [866.913269]\n",
            "It: 53370, Time: 0.03\n",
            "mse_b  [866.91327]  mse_f: 3.245059013366699   total loss: [870.1583]\n",
            "mse_b ====== [866.865112]\n",
            "It: 53371, Time: 0.03\n",
            "mse_b  [866.8651]  mse_f: 3.118492603302002   total loss: [869.9836]\n",
            "mse_b ====== [867.107605]\n",
            "It: 53372, Time: 0.03\n",
            "mse_b  [867.1076]  mse_f: 3.239060878753662   total loss: [870.3467]\n",
            "mse_b ====== [866.79364]\n",
            "It: 53373, Time: 0.03\n",
            "mse_b  [866.79364]  mse_f: 3.4699606895446777   total loss: [870.2636]\n",
            "mse_b ====== [866.756531]\n",
            "It: 53374, Time: 0.03\n",
            "mse_b  [866.75653]  mse_f: 2.856698513031006   total loss: [869.6132]\n",
            "mse_b ====== [866.941772]\n",
            "It: 53375, Time: 0.03\n",
            "mse_b  [866.9418]  mse_f: 3.1353769302368164   total loss: [870.07715]\n",
            "mse_b ====== [866.868469]\n",
            "It: 53376, Time: 0.03\n",
            "mse_b  [866.86847]  mse_f: 3.6637179851531982   total loss: [870.53217]\n",
            "mse_b ====== [866.637878]\n",
            "It: 53377, Time: 0.03\n",
            "mse_b  [866.6379]  mse_f: 3.0766332149505615   total loss: [869.71454]\n",
            "mse_b ====== [866.782898]\n",
            "It: 53378, Time: 0.03\n",
            "mse_b  [866.7829]  mse_f: 2.8094661235809326   total loss: [869.59235]\n",
            "mse_b ====== [866.803833]\n",
            "It: 53379, Time: 0.03\n",
            "mse_b  [866.80383]  mse_f: 3.56848406791687   total loss: [870.3723]\n",
            "mse_b ====== [866.897339]\n",
            "It: 53380, Time: 0.03\n",
            "mse_b  [866.89734]  mse_f: 3.3932156562805176   total loss: [870.2905]\n",
            "mse_b ====== [866.777]\n",
            "It: 53381, Time: 0.03\n",
            "mse_b  [866.777]  mse_f: 2.7623450756073   total loss: [869.5393]\n",
            "mse_b ====== [866.800171]\n",
            "It: 53382, Time: 0.03\n",
            "mse_b  [866.8002]  mse_f: 2.8129401206970215   total loss: [869.6131]\n",
            "mse_b ====== [867.105103]\n",
            "It: 53383, Time: 0.03\n",
            "mse_b  [867.1051]  mse_f: 3.4736056327819824   total loss: [870.57874]\n",
            "mse_b ====== [866.714233]\n",
            "It: 53384, Time: 0.04\n",
            "mse_b  [866.71423]  mse_f: 3.3309478759765625   total loss: [870.04517]\n",
            "mse_b ====== [866.621704]\n",
            "It: 53385, Time: 0.03\n",
            "mse_b  [866.6217]  mse_f: 2.7199530601501465   total loss: [869.3417]\n",
            "mse_b ====== [866.8078]\n",
            "It: 53386, Time: 0.03\n",
            "mse_b  [866.8078]  mse_f: 3.0707929134368896   total loss: [869.8786]\n",
            "mse_b ====== [866.812317]\n",
            "It: 53387, Time: 0.04\n",
            "mse_b  [866.8123]  mse_f: 3.774440288543701   total loss: [870.58673]\n",
            "mse_b ====== [866.552307]\n",
            "It: 53388, Time: 0.03\n",
            "mse_b  [866.5523]  mse_f: 3.2866382598876953   total loss: [869.8389]\n",
            "mse_b ====== [866.584045]\n",
            "It: 53389, Time: 0.03\n",
            "mse_b  [866.58405]  mse_f: 2.6908152103424072   total loss: [869.27484]\n",
            "mse_b ====== [866.580811]\n",
            "It: 53390, Time: 0.04\n",
            "mse_b  [866.5808]  mse_f: 3.596818447113037   total loss: [870.1776]\n",
            "mse_b ====== [866.792603]\n",
            "It: 53391, Time: 0.03\n",
            "mse_b  [866.7926]  mse_f: 3.464730978012085   total loss: [870.2573]\n",
            "mse_b ====== [866.56958]\n",
            "It: 53392, Time: 0.03\n",
            "mse_b  [866.5696]  mse_f: 2.969749927520752   total loss: [869.5393]\n",
            "mse_b ====== [866.765076]\n",
            "It: 53393, Time: 0.03\n",
            "mse_b  [866.7651]  mse_f: 3.0691771507263184   total loss: [869.8342]\n",
            "mse_b ====== [866.823608]\n",
            "It: 53394, Time: 0.03\n",
            "mse_b  [866.8236]  mse_f: 3.4188218116760254   total loss: [870.24243]\n",
            "mse_b ====== [866.564087]\n",
            "It: 53395, Time: 0.03\n",
            "mse_b  [866.5641]  mse_f: 2.993533134460449   total loss: [869.5576]\n",
            "mse_b ====== [866.573364]\n",
            "It: 53396, Time: 0.03\n",
            "mse_b  [866.57336]  mse_f: 2.926133871078491   total loss: [869.4995]\n",
            "mse_b ====== [866.834412]\n",
            "It: 53397, Time: 0.03\n",
            "mse_b  [866.8344]  mse_f: 3.2942941188812256   total loss: [870.1287]\n",
            "mse_b ====== [866.829895]\n",
            "It: 53398, Time: 0.03\n",
            "mse_b  [866.8299]  mse_f: 3.1888904571533203   total loss: [870.0188]\n",
            "mse_b ====== [866.554382]\n",
            "It: 53399, Time: 0.03\n",
            "mse_b  [866.5544]  mse_f: 2.7225427627563477   total loss: [869.2769]\n",
            "mse_b ====== [866.648]\n",
            "It: 53400, Time: 0.03\n",
            "mse_b  [866.648]  mse_f: 2.779789924621582   total loss: [869.4278]\n",
            "mse_b ====== [866.803467]\n",
            "It: 53401, Time: 0.03\n",
            "mse_b  [866.80347]  mse_f: 3.5656566619873047   total loss: [870.36914]\n",
            "mse_b ====== [866.79071]\n",
            "It: 53402, Time: 0.04\n",
            "mse_b  [866.7907]  mse_f: 2.925445318222046   total loss: [869.7161]\n",
            "mse_b ====== [866.518188]\n",
            "It: 53403, Time: 0.03\n",
            "mse_b  [866.5182]  mse_f: 2.5798356533050537   total loss: [869.098]\n",
            "mse_b ====== [866.761292]\n",
            "It: 53404, Time: 0.03\n",
            "mse_b  [866.7613]  mse_f: 3.1593117713928223   total loss: [869.9206]\n",
            "mse_b ====== [866.786255]\n",
            "It: 53405, Time: 0.03\n",
            "mse_b  [866.78625]  mse_f: 3.3381080627441406   total loss: [870.1244]\n",
            "mse_b ====== [866.565369]\n",
            "It: 53406, Time: 0.03\n",
            "mse_b  [866.56537]  mse_f: 2.8222579956054688   total loss: [869.38763]\n",
            "mse_b ====== [866.595947]\n",
            "It: 53407, Time: 0.03\n",
            "mse_b  [866.59595]  mse_f: 3.0926876068115234   total loss: [869.68866]\n",
            "mse_b ====== [866.86731]\n",
            "It: 53408, Time: 0.03\n",
            "mse_b  [866.8673]  mse_f: 3.1434457302093506   total loss: [870.01074]\n",
            "mse_b ====== [866.549683]\n",
            "It: 53409, Time: 0.03\n",
            "mse_b  [866.5497]  mse_f: 3.0511789321899414   total loss: [869.6009]\n",
            "mse_b ====== [866.628235]\n",
            "It: 53410, Time: 0.03\n",
            "mse_b  [866.62823]  mse_f: 2.8181726932525635   total loss: [869.4464]\n",
            "mse_b ====== [866.66217]\n",
            "It: 53411, Time: 0.03\n",
            "mse_b  [866.6622]  mse_f: 3.528201103210449   total loss: [870.19037]\n",
            "mse_b ====== [866.56427]\n",
            "It: 53412, Time: 0.03\n",
            "mse_b  [866.5643]  mse_f: 3.8241403102874756   total loss: [870.3884]\n",
            "mse_b ====== [866.536438]\n",
            "It: 53413, Time: 0.03\n",
            "mse_b  [866.53644]  mse_f: 2.852993965148926   total loss: [869.3894]\n",
            "mse_b ====== [866.509338]\n",
            "It: 53414, Time: 0.04\n",
            "mse_b  [866.50934]  mse_f: 2.6973519325256348   total loss: [869.20667]\n",
            "mse_b ====== [866.662415]\n",
            "It: 53415, Time: 0.03\n",
            "mse_b  [866.6624]  mse_f: 3.4477944374084473   total loss: [870.1102]\n",
            "mse_b ====== [866.627]\n",
            "It: 53416, Time: 0.04\n",
            "mse_b  [866.627]  mse_f: 3.420585870742798   total loss: [870.0476]\n",
            "mse_b ====== [866.627319]\n",
            "It: 53417, Time: 0.03\n",
            "mse_b  [866.6273]  mse_f: 3.0944666862487793   total loss: [869.7218]\n",
            "mse_b ====== [866.680542]\n",
            "It: 53418, Time: 0.03\n",
            "mse_b  [866.68054]  mse_f: 3.6804847717285156   total loss: [870.361]\n",
            "mse_b ====== [866.965881]\n",
            "It: 53419, Time: 0.03\n",
            "mse_b  [866.9659]  mse_f: 3.010640859603882   total loss: [869.9765]\n",
            "mse_b ====== [866.510376]\n",
            "It: 53420, Time: 0.03\n",
            "mse_b  [866.5104]  mse_f: 2.6585023403167725   total loss: [869.1689]\n",
            "mse_b ====== [866.68]\n",
            "It: 53421, Time: 0.03\n",
            "mse_b  [866.68]  mse_f: 2.959737777709961   total loss: [869.6397]\n",
            "mse_b ====== [866.940796]\n",
            "It: 53422, Time: 0.03\n",
            "mse_b  [866.9408]  mse_f: 3.6593761444091797   total loss: [870.60016]\n",
            "mse_b ====== [866.840942]\n",
            "It: 53423, Time: 0.03\n",
            "mse_b  [866.84094]  mse_f: 3.864595413208008   total loss: [870.70557]\n",
            "mse_b ====== [866.662537]\n",
            "It: 53424, Time: 0.03\n",
            "mse_b  [866.66254]  mse_f: 3.1930065155029297   total loss: [869.8555]\n",
            "mse_b ====== [866.595032]\n",
            "It: 53425, Time: 0.03\n",
            "mse_b  [866.59503]  mse_f: 3.149230480194092   total loss: [869.74426]\n",
            "mse_b ====== [866.762451]\n",
            "It: 53426, Time: 0.03\n",
            "mse_b  [866.76245]  mse_f: 3.1742172241210938   total loss: [869.93665]\n",
            "mse_b ====== [866.661926]\n",
            "It: 53427, Time: 0.03\n",
            "mse_b  [866.6619]  mse_f: 3.3029050827026367   total loss: [869.96484]\n",
            "mse_b ====== [866.702]\n",
            "It: 53428, Time: 0.04\n",
            "mse_b  [866.702]  mse_f: 3.867572069168091   total loss: [870.5696]\n",
            "mse_b ====== [866.842712]\n",
            "It: 53429, Time: 0.03\n",
            "mse_b  [866.8427]  mse_f: 4.475099563598633   total loss: [871.3178]\n",
            "mse_b ====== [866.726562]\n",
            "It: 53430, Time: 0.03\n",
            "mse_b  [866.72656]  mse_f: 3.3209171295166016   total loss: [870.0475]\n",
            "mse_b ====== [866.487122]\n",
            "It: 53431, Time: 0.04\n",
            "mse_b  [866.4871]  mse_f: 2.6476926803588867   total loss: [869.1348]\n",
            "mse_b ====== [866.797119]\n",
            "It: 53432, Time: 0.03\n",
            "mse_b  [866.7971]  mse_f: 3.7955899238586426   total loss: [870.5927]\n",
            "mse_b ====== [866.924438]\n",
            "It: 53433, Time: 0.03\n",
            "mse_b  [866.92444]  mse_f: 4.823634147644043   total loss: [871.74805]\n",
            "mse_b ====== [867.00177]\n",
            "It: 53434, Time: 0.03\n",
            "mse_b  [867.0018]  mse_f: 4.207573413848877   total loss: [871.20935]\n",
            "mse_b ====== [866.800659]\n",
            "It: 53435, Time: 0.03\n",
            "mse_b  [866.80066]  mse_f: 3.6261088848114014   total loss: [870.42676]\n",
            "mse_b ====== [866.760864]\n",
            "It: 53436, Time: 0.03\n",
            "mse_b  [866.76086]  mse_f: 3.209803819656372   total loss: [869.97064]\n",
            "mse_b ====== [866.749512]\n",
            "It: 53437, Time: 0.03\n",
            "mse_b  [866.7495]  mse_f: 3.647268533706665   total loss: [870.3968]\n",
            "mse_b ====== [867.159668]\n",
            "It: 53438, Time: 0.04\n",
            "mse_b  [867.15967]  mse_f: 4.743040084838867   total loss: [871.9027]\n",
            "mse_b ====== [867.436157]\n",
            "It: 53439, Time: 0.03\n",
            "mse_b  [867.43616]  mse_f: 5.314769744873047   total loss: [872.7509]\n",
            "mse_b ====== [866.978394]\n",
            "It: 53440, Time: 0.03\n",
            "mse_b  [866.9784]  mse_f: 4.69928503036499   total loss: [871.6777]\n",
            "mse_b ====== [866.754822]\n",
            "It: 53441, Time: 0.03\n",
            "mse_b  [866.7548]  mse_f: 3.17509126663208   total loss: [869.92993]\n",
            "mse_b ====== [867.05072]\n",
            "It: 53442, Time: 0.04\n",
            "mse_b  [867.0507]  mse_f: 3.7992396354675293   total loss: [870.85]\n",
            "mse_b ====== [868.155212]\n",
            "It: 53443, Time: 0.03\n",
            "mse_b  [868.1552]  mse_f: 5.536235809326172   total loss: [873.69147]\n",
            "mse_b ====== [867.339233]\n",
            "It: 53444, Time: 0.03\n",
            "mse_b  [867.33923]  mse_f: 5.38780403137207   total loss: [872.72705]\n",
            "mse_b ====== [866.623169]\n",
            "It: 53445, Time: 0.03\n",
            "mse_b  [866.62317]  mse_f: 3.5062668323516846   total loss: [870.12946]\n",
            "mse_b ====== [866.796509]\n",
            "It: 53446, Time: 0.04\n",
            "mse_b  [866.7965]  mse_f: 3.910672426223755   total loss: [870.70715]\n",
            "mse_b ====== [867.475708]\n",
            "It: 53447, Time: 0.04\n",
            "mse_b  [867.4757]  mse_f: 4.505515098571777   total loss: [871.9812]\n",
            "mse_b ====== [867.175354]\n",
            "It: 53448, Time: 0.04\n",
            "mse_b  [867.17535]  mse_f: 4.906958103179932   total loss: [872.08234]\n",
            "mse_b ====== [866.805237]\n",
            "It: 53449, Time: 0.04\n",
            "mse_b  [866.80524]  mse_f: 4.3426055908203125   total loss: [871.1478]\n",
            "mse_b ====== [866.782776]\n",
            "It: 53450, Time: 0.04\n",
            "mse_b  [866.7828]  mse_f: 3.181858539581299   total loss: [869.96466]\n",
            "mse_b ====== [866.870361]\n",
            "It: 53451, Time: 0.04\n",
            "mse_b  [866.87036]  mse_f: 4.48073148727417   total loss: [871.3511]\n",
            "mse_b ====== [866.983032]\n",
            "It: 53452, Time: 0.04\n",
            "mse_b  [866.98303]  mse_f: 4.925095081329346   total loss: [871.90814]\n",
            "mse_b ====== [866.708496]\n",
            "It: 53453, Time: 0.04\n",
            "mse_b  [866.7085]  mse_f: 4.166639804840088   total loss: [870.8751]\n",
            "mse_b ====== [866.882507]\n",
            "It: 53454, Time: 0.04\n",
            "mse_b  [866.8825]  mse_f: 3.1302242279052734   total loss: [870.01276]\n",
            "mse_b ====== [866.68988]\n",
            "It: 53455, Time: 0.03\n",
            "mse_b  [866.6899]  mse_f: 4.559427738189697   total loss: [871.2493]\n",
            "mse_b ====== [866.969849]\n",
            "It: 53456, Time: 0.03\n",
            "mse_b  [866.96985]  mse_f: 5.176034927368164   total loss: [872.1459]\n",
            "mse_b ====== [866.788879]\n",
            "It: 53457, Time: 0.03\n",
            "mse_b  [866.7889]  mse_f: 4.27644157409668   total loss: [871.0653]\n",
            "mse_b ====== [866.853638]\n",
            "It: 53458, Time: 0.04\n",
            "mse_b  [866.85364]  mse_f: 3.4363224506378174   total loss: [870.29]\n",
            "mse_b ====== [866.834106]\n",
            "It: 53459, Time: 0.03\n",
            "mse_b  [866.8341]  mse_f: 3.762089490890503   total loss: [870.5962]\n",
            "mse_b ====== [866.747]\n",
            "It: 53460, Time: 0.04\n",
            "mse_b  [866.747]  mse_f: 5.523042678833008   total loss: [872.2701]\n",
            "mse_b ====== [866.723389]\n",
            "It: 53461, Time: 0.03\n",
            "mse_b  [866.7234]  mse_f: 4.007408142089844   total loss: [870.7308]\n",
            "mse_b ====== [866.830322]\n",
            "It: 53462, Time: 0.03\n",
            "mse_b  [866.8303]  mse_f: 2.9343960285186768   total loss: [869.7647]\n",
            "mse_b ====== [867.039673]\n",
            "It: 53463, Time: 0.03\n",
            "mse_b  [867.0397]  mse_f: 4.421145439147949   total loss: [871.4608]\n",
            "mse_b ====== [867.03656]\n",
            "It: 53464, Time: 0.03\n",
            "mse_b  [867.03656]  mse_f: 4.047828197479248   total loss: [871.0844]\n",
            "mse_b ====== [866.879395]\n",
            "It: 53465, Time: 0.03\n",
            "mse_b  [866.8794]  mse_f: 3.31276798248291   total loss: [870.19214]\n",
            "mse_b ====== [866.9729]\n",
            "It: 53466, Time: 0.03\n",
            "mse_b  [866.9729]  mse_f: 3.436652183532715   total loss: [870.40955]\n",
            "mse_b ====== [866.80072]\n",
            "It: 53467, Time: 0.04\n",
            "mse_b  [866.8007]  mse_f: 4.5426130294799805   total loss: [871.3433]\n",
            "mse_b ====== [866.869934]\n",
            "It: 53468, Time: 0.04\n",
            "mse_b  [866.86993]  mse_f: 3.6896488666534424   total loss: [870.5596]\n",
            "mse_b ====== [867.184692]\n",
            "It: 53469, Time: 0.03\n",
            "mse_b  [867.1847]  mse_f: 3.408961296081543   total loss: [870.5936]\n",
            "mse_b ====== [867.196655]\n",
            "It: 53470, Time: 0.03\n",
            "mse_b  [867.19666]  mse_f: 3.653203010559082   total loss: [870.84985]\n",
            "mse_b ====== [866.608704]\n",
            "It: 53471, Time: 0.03\n",
            "mse_b  [866.6087]  mse_f: 3.7595295906066895   total loss: [870.3682]\n",
            "mse_b ====== [867.163757]\n",
            "It: 53472, Time: 0.04\n",
            "mse_b  [867.16376]  mse_f: 4.008676528930664   total loss: [871.1724]\n",
            "mse_b ====== [867.365173]\n",
            "It: 53473, Time: 0.03\n",
            "mse_b  [867.3652]  mse_f: 3.2115354537963867   total loss: [870.5767]\n",
            "mse_b ====== [866.710571]\n",
            "It: 53474, Time: 0.03\n",
            "mse_b  [866.7106]  mse_f: 3.405276298522949   total loss: [870.11584]\n",
            "mse_b ====== [866.847107]\n",
            "It: 53475, Time: 0.03\n",
            "mse_b  [866.8471]  mse_f: 4.005672931671143   total loss: [870.8528]\n",
            "mse_b ====== [867.455]\n",
            "It: 53476, Time: 0.03\n",
            "mse_b  [867.455]  mse_f: 3.11682391166687   total loss: [870.57184]\n",
            "mse_b ====== [867.469849]\n",
            "It: 53477, Time: 0.03\n",
            "mse_b  [867.46985]  mse_f: 3.2451395988464355   total loss: [870.71497]\n",
            "mse_b ====== [866.68396]\n",
            "It: 53478, Time: 0.03\n",
            "mse_b  [866.68396]  mse_f: 3.440415620803833   total loss: [870.1244]\n",
            "mse_b ====== [867.165894]\n",
            "It: 53479, Time: 0.03\n",
            "mse_b  [867.1659]  mse_f: 3.4186103343963623   total loss: [870.58453]\n",
            "mse_b ====== [867.886719]\n",
            "It: 53480, Time: 0.03\n",
            "mse_b  [867.8867]  mse_f: 3.3886008262634277   total loss: [871.2753]\n",
            "mse_b ====== [867.129517]\n",
            "It: 53481, Time: 0.03\n",
            "mse_b  [867.1295]  mse_f: 3.2012460231781006   total loss: [870.33075]\n",
            "mse_b ====== [866.79071]\n",
            "It: 53482, Time: 0.03\n",
            "mse_b  [866.7907]  mse_f: 3.1417760848999023   total loss: [869.9325]\n",
            "mse_b ====== [867.761963]\n",
            "It: 53483, Time: 0.03\n",
            "mse_b  [867.76196]  mse_f: 3.3372130393981934   total loss: [871.0992]\n",
            "mse_b ====== [867.829]\n",
            "It: 53484, Time: 0.03\n",
            "mse_b  [867.829]  mse_f: 3.6540985107421875   total loss: [871.4831]\n",
            "mse_b ====== [866.640137]\n",
            "It: 53485, Time: 0.03\n",
            "mse_b  [866.64014]  mse_f: 2.706658124923706   total loss: [869.3468]\n",
            "mse_b ====== [866.945496]\n",
            "It: 53486, Time: 0.03\n",
            "mse_b  [866.9455]  mse_f: 3.3680524826049805   total loss: [870.31354]\n",
            "mse_b ====== [867.75946]\n",
            "It: 53487, Time: 0.03\n",
            "mse_b  [867.75946]  mse_f: 3.5696959495544434   total loss: [871.32916]\n",
            "mse_b ====== [867.118408]\n",
            "It: 53488, Time: 0.03\n",
            "mse_b  [867.1184]  mse_f: 2.9312829971313477   total loss: [870.0497]\n",
            "mse_b ====== [866.656128]\n",
            "It: 53489, Time: 0.04\n",
            "mse_b  [866.6561]  mse_f: 3.2337777614593506   total loss: [869.8899]\n",
            "mse_b ====== [867.383301]\n",
            "It: 53490, Time: 0.03\n",
            "mse_b  [867.3833]  mse_f: 3.307734489440918   total loss: [870.69104]\n",
            "mse_b ====== [867.546326]\n",
            "It: 53491, Time: 0.03\n",
            "mse_b  [867.5463]  mse_f: 3.3480193614959717   total loss: [870.89435]\n",
            "mse_b ====== [866.852]\n",
            "It: 53492, Time: 0.03\n",
            "mse_b  [866.852]  mse_f: 3.168717384338379   total loss: [870.0207]\n",
            "mse_b ====== [867.031677]\n",
            "It: 53493, Time: 0.03\n",
            "mse_b  [867.0317]  mse_f: 3.123234510421753   total loss: [870.1549]\n",
            "mse_b ====== [867.278503]\n",
            "It: 53494, Time: 0.04\n",
            "mse_b  [867.2785]  mse_f: 3.3906047344207764   total loss: [870.6691]\n",
            "mse_b ====== [867.363708]\n",
            "It: 53495, Time: 0.03\n",
            "mse_b  [867.3637]  mse_f: 3.3711612224578857   total loss: [870.73486]\n",
            "mse_b ====== [866.668152]\n",
            "It: 53496, Time: 0.03\n",
            "mse_b  [866.66815]  mse_f: 3.758063554763794   total loss: [870.4262]\n",
            "mse_b ====== [866.847534]\n",
            "It: 53497, Time: 0.03\n",
            "mse_b  [866.84753]  mse_f: 2.9745547771453857   total loss: [869.8221]\n",
            "mse_b ====== [867.263062]\n",
            "It: 53498, Time: 0.03\n",
            "mse_b  [867.26306]  mse_f: 3.285358190536499   total loss: [870.5484]\n",
            "mse_b ====== [866.951]\n",
            "It: 53499, Time: 0.03\n",
            "mse_b  [866.951]  mse_f: 3.873772621154785   total loss: [870.82477]\n",
            "mse_b ====== [866.740723]\n",
            "It: 53500, Time: 0.03\n",
            "mse_b  [866.7407]  mse_f: 3.1682727336883545   total loss: [869.909]\n",
            "mse_b ====== [866.947266]\n",
            "It: 53501, Time: 0.03\n",
            "mse_b  [866.94727]  mse_f: 3.1758480072021484   total loss: [870.1231]\n",
            "mse_b ====== [866.9375]\n",
            "It: 53502, Time: 0.03\n",
            "mse_b  [866.9375]  mse_f: 3.508056163787842   total loss: [870.44556]\n",
            "mse_b ====== [866.823]\n",
            "It: 53503, Time: 0.03\n",
            "mse_b  [866.823]  mse_f: 4.095180988311768   total loss: [870.91815]\n",
            "mse_b ====== [866.918945]\n",
            "It: 53504, Time: 0.03\n",
            "mse_b  [866.91895]  mse_f: 3.2699966430664062   total loss: [870.18896]\n",
            "mse_b ====== [866.826111]\n",
            "It: 53505, Time: 0.03\n",
            "mse_b  [866.8261]  mse_f: 2.9407684803009033   total loss: [869.7669]\n",
            "mse_b ====== [867.13092]\n",
            "It: 53506, Time: 0.04\n",
            "mse_b  [867.1309]  mse_f: 3.5453643798828125   total loss: [870.6763]\n",
            "mse_b ====== [866.662537]\n",
            "It: 53507, Time: 0.03\n",
            "mse_b  [866.66254]  mse_f: 4.413677215576172   total loss: [871.07623]\n",
            "mse_b ====== [866.561218]\n",
            "It: 53508, Time: 0.03\n",
            "mse_b  [866.5612]  mse_f: 3.2068347930908203   total loss: [869.76807]\n",
            "mse_b ====== [866.623474]\n",
            "It: 53509, Time: 0.03\n",
            "mse_b  [866.6235]  mse_f: 2.9374260902404785   total loss: [869.5609]\n",
            "mse_b ====== [866.781067]\n",
            "It: 53510, Time: 0.03\n",
            "mse_b  [866.78107]  mse_f: 4.4163618087768555   total loss: [871.19745]\n",
            "mse_b ====== [866.694214]\n",
            "It: 53511, Time: 0.03\n",
            "mse_b  [866.6942]  mse_f: 3.6166129112243652   total loss: [870.31085]\n",
            "mse_b ====== [866.560791]\n",
            "It: 53512, Time: 0.03\n",
            "mse_b  [866.5608]  mse_f: 2.9477028846740723   total loss: [869.5085]\n",
            "mse_b ====== [866.701782]\n",
            "It: 53513, Time: 0.03\n",
            "mse_b  [866.7018]  mse_f: 3.5744001865386963   total loss: [870.2762]\n",
            "mse_b ====== [867.009705]\n",
            "It: 53514, Time: 0.04\n",
            "mse_b  [867.0097]  mse_f: 3.6018996238708496   total loss: [870.61163]\n",
            "mse_b ====== [866.63623]\n",
            "It: 53515, Time: 0.03\n",
            "mse_b  [866.6362]  mse_f: 3.4174771308898926   total loss: [870.0537]\n",
            "mse_b ====== [867.107422]\n",
            "It: 53516, Time: 0.04\n",
            "mse_b  [867.1074]  mse_f: 2.908600330352783   total loss: [870.01605]\n",
            "mse_b ====== [866.768921]\n",
            "It: 53517, Time: 0.03\n",
            "mse_b  [866.7689]  mse_f: 3.459428310394287   total loss: [870.22833]\n",
            "mse_b ====== [866.649048]\n",
            "It: 53518, Time: 0.04\n",
            "mse_b  [866.64905]  mse_f: 3.511756420135498   total loss: [870.1608]\n",
            "mse_b ====== [866.78949]\n",
            "It: 53519, Time: 0.04\n",
            "mse_b  [866.7895]  mse_f: 3.5512304306030273   total loss: [870.3407]\n",
            "mse_b ====== [867.082642]\n",
            "It: 53520, Time: 0.05\n",
            "mse_b  [867.08264]  mse_f: 3.424736976623535   total loss: [870.5074]\n",
            "mse_b ====== [866.803162]\n",
            "It: 53521, Time: 0.04\n",
            "mse_b  [866.80316]  mse_f: 3.0401339530944824   total loss: [869.8433]\n",
            "mse_b ====== [866.714966]\n",
            "It: 53522, Time: 0.04\n",
            "mse_b  [866.71497]  mse_f: 3.1747097969055176   total loss: [869.88965]\n",
            "mse_b ====== [866.924194]\n",
            "It: 53523, Time: 0.03\n",
            "mse_b  [866.9242]  mse_f: 3.5389719009399414   total loss: [870.4632]\n",
            "mse_b ====== [866.814148]\n",
            "It: 53524, Time: 0.03\n",
            "mse_b  [866.81415]  mse_f: 3.7443888187408447   total loss: [870.55853]\n",
            "mse_b ====== [866.710327]\n",
            "It: 53525, Time: 0.03\n",
            "mse_b  [866.7103]  mse_f: 2.8433284759521484   total loss: [869.55365]\n",
            "mse_b ====== [866.674377]\n",
            "It: 53526, Time: 0.03\n",
            "mse_b  [866.6744]  mse_f: 2.776855945587158   total loss: [869.45123]\n",
            "mse_b ====== [867.209106]\n",
            "It: 53527, Time: 0.03\n",
            "mse_b  [867.2091]  mse_f: 3.4661622047424316   total loss: [870.6753]\n",
            "mse_b ====== [866.696533]\n",
            "It: 53528, Time: 0.03\n",
            "mse_b  [866.69653]  mse_f: 3.5827505588531494   total loss: [870.2793]\n",
            "mse_b ====== [866.601379]\n",
            "It: 53529, Time: 0.03\n",
            "mse_b  [866.6014]  mse_f: 2.729478359222412   total loss: [869.3309]\n",
            "mse_b ====== [866.669617]\n",
            "It: 53530, Time: 0.04\n",
            "mse_b  [866.6696]  mse_f: 3.147327423095703   total loss: [869.81696]\n",
            "mse_b ====== [866.836609]\n",
            "It: 53531, Time: 0.03\n",
            "mse_b  [866.8366]  mse_f: 3.8437538146972656   total loss: [870.68036]\n",
            "mse_b ====== [866.838074]\n",
            "It: 53532, Time: 0.03\n",
            "mse_b  [866.8381]  mse_f: 3.4581151008605957   total loss: [870.2962]\n",
            "mse_b ====== [866.675537]\n",
            "It: 53533, Time: 0.03\n",
            "mse_b  [866.67554]  mse_f: 3.4865341186523438   total loss: [870.16205]\n",
            "mse_b ====== [866.622192]\n",
            "It: 53534, Time: 0.03\n",
            "mse_b  [866.6222]  mse_f: 3.192934274673462   total loss: [869.8151]\n",
            "mse_b ====== [866.674561]\n",
            "It: 53535, Time: 0.03\n",
            "mse_b  [866.67456]  mse_f: 3.1169910430908203   total loss: [869.79156]\n",
            "mse_b ====== [866.879395]\n",
            "It: 53536, Time: 0.03\n",
            "mse_b  [866.8794]  mse_f: 3.872096061706543   total loss: [870.75146]\n",
            "mse_b ====== [866.859802]\n",
            "It: 53537, Time: 0.04\n",
            "mse_b  [866.8598]  mse_f: 3.595994472503662   total loss: [870.4558]\n",
            "mse_b ====== [866.657288]\n",
            "It: 53538, Time: 0.03\n",
            "mse_b  [866.6573]  mse_f: 3.160853385925293   total loss: [869.8181]\n",
            "mse_b ====== [866.712097]\n",
            "It: 53539, Time: 0.04\n",
            "mse_b  [866.7121]  mse_f: 2.856386423110962   total loss: [869.5685]\n",
            "mse_b ====== [866.818298]\n",
            "It: 53540, Time: 0.04\n",
            "mse_b  [866.8183]  mse_f: 3.6079843044281006   total loss: [870.4263]\n",
            "mse_b ====== [866.787415]\n",
            "It: 53541, Time: 0.03\n",
            "mse_b  [866.7874]  mse_f: 3.681102752685547   total loss: [870.4685]\n",
            "mse_b ====== [866.696655]\n",
            "It: 53542, Time: 0.03\n",
            "mse_b  [866.69666]  mse_f: 3.1176629066467285   total loss: [869.81433]\n",
            "mse_b ====== [866.80365]\n",
            "It: 53543, Time: 0.03\n",
            "mse_b  [866.80365]  mse_f: 2.7038521766662598   total loss: [869.5075]\n",
            "mse_b ====== [866.761902]\n",
            "It: 53544, Time: 0.03\n",
            "mse_b  [866.7619]  mse_f: 3.3736321926116943   total loss: [870.13556]\n",
            "mse_b ====== [866.628418]\n",
            "It: 53545, Time: 0.03\n",
            "mse_b  [866.6284]  mse_f: 3.65877103805542   total loss: [870.2872]\n",
            "mse_b ====== [866.766174]\n",
            "It: 53546, Time: 0.03\n",
            "mse_b  [866.7662]  mse_f: 2.9960579872131348   total loss: [869.7622]\n",
            "mse_b ====== [866.915649]\n",
            "It: 53547, Time: 0.03\n",
            "mse_b  [866.91565]  mse_f: 2.916717052459717   total loss: [869.83234]\n",
            "mse_b ====== [866.635498]\n",
            "It: 53548, Time: 0.03\n",
            "mse_b  [866.6355]  mse_f: 3.10483455657959   total loss: [869.74036]\n",
            "mse_b ====== [866.702]\n",
            "It: 53549, Time: 0.03\n",
            "mse_b  [866.702]  mse_f: 3.2288477420806885   total loss: [869.93085]\n",
            "mse_b ====== [867.217224]\n",
            "It: 53550, Time: 0.04\n",
            "mse_b  [867.2172]  mse_f: 3.209618330001831   total loss: [870.4268]\n",
            "mse_b ====== [867.020874]\n",
            "It: 53551, Time: 0.03\n",
            "mse_b  [867.0209]  mse_f: 3.031724452972412   total loss: [870.0526]\n",
            "mse_b ====== [866.513428]\n",
            "It: 53552, Time: 0.03\n",
            "mse_b  [866.5134]  mse_f: 2.6570754051208496   total loss: [869.17053]\n",
            "mse_b ====== [866.864075]\n",
            "It: 53553, Time: 0.03\n",
            "mse_b  [866.8641]  mse_f: 3.111553430557251   total loss: [869.97565]\n",
            "mse_b ====== [867.461182]\n",
            "It: 53554, Time: 0.03\n",
            "mse_b  [867.4612]  mse_f: 3.4742965698242188   total loss: [870.9355]\n",
            "mse_b ====== [867.066589]\n",
            "It: 53555, Time: 0.03\n",
            "mse_b  [867.0666]  mse_f: 3.1398420333862305   total loss: [870.2064]\n",
            "mse_b ====== [866.521606]\n",
            "It: 53556, Time: 0.03\n",
            "mse_b  [866.5216]  mse_f: 2.892054557800293   total loss: [869.41364]\n",
            "mse_b ====== [866.901794]\n",
            "It: 53557, Time: 0.03\n",
            "mse_b  [866.9018]  mse_f: 2.884476661682129   total loss: [869.78625]\n",
            "mse_b ====== [867.186462]\n",
            "It: 53558, Time: 0.03\n",
            "mse_b  [867.18646]  mse_f: 3.1715307235717773   total loss: [870.358]\n",
            "mse_b ====== [866.816162]\n",
            "It: 53559, Time: 0.04\n",
            "mse_b  [866.81616]  mse_f: 3.7353804111480713   total loss: [870.5515]\n",
            "mse_b ====== [866.601868]\n",
            "It: 53560, Time: 0.03\n",
            "mse_b  [866.60187]  mse_f: 3.3608202934265137   total loss: [869.9627]\n",
            "mse_b ====== [866.860474]\n",
            "It: 53561, Time: 0.04\n",
            "mse_b  [866.8605]  mse_f: 3.0955028533935547   total loss: [869.956]\n",
            "mse_b ====== [866.846375]\n",
            "It: 53562, Time: 0.03\n",
            "mse_b  [866.8464]  mse_f: 2.831162691116333   total loss: [869.67755]\n",
            "mse_b ====== [866.56012]\n",
            "It: 53563, Time: 0.03\n",
            "mse_b  [866.5601]  mse_f: 3.2345802783966064   total loss: [869.7947]\n",
            "mse_b ====== [866.802734]\n",
            "It: 53564, Time: 0.03\n",
            "mse_b  [866.80273]  mse_f: 3.818997621536255   total loss: [870.6217]\n",
            "mse_b ====== [866.910217]\n",
            "It: 53565, Time: 0.03\n",
            "mse_b  [866.9102]  mse_f: 3.4584200382232666   total loss: [870.36865]\n",
            "mse_b ====== [866.598938]\n",
            "It: 53566, Time: 0.03\n",
            "mse_b  [866.59894]  mse_f: 2.7685012817382812   total loss: [869.36743]\n",
            "mse_b ====== [866.610168]\n",
            "It: 53567, Time: 0.03\n",
            "mse_b  [866.61017]  mse_f: 2.876333475112915   total loss: [869.4865]\n",
            "mse_b ====== [866.711792]\n",
            "It: 53568, Time: 0.03\n",
            "mse_b  [866.7118]  mse_f: 3.5597729682922363   total loss: [870.27155]\n",
            "mse_b ====== [866.786255]\n",
            "It: 53569, Time: 0.03\n",
            "mse_b  [866.78625]  mse_f: 3.3023409843444824   total loss: [870.0886]\n",
            "mse_b ====== [866.584412]\n",
            "It: 53570, Time: 0.03\n",
            "mse_b  [866.5844]  mse_f: 3.245231866836548   total loss: [869.82965]\n",
            "mse_b ====== [866.625427]\n",
            "It: 53571, Time: 0.03\n",
            "mse_b  [866.6254]  mse_f: 2.9113965034484863   total loss: [869.5368]\n",
            "mse_b ====== [866.77179]\n",
            "It: 53572, Time: 0.03\n",
            "mse_b  [866.7718]  mse_f: 2.798776149749756   total loss: [869.57056]\n",
            "mse_b ====== [866.831665]\n",
            "It: 53573, Time: 0.04\n",
            "mse_b  [866.83167]  mse_f: 3.167275905609131   total loss: [869.99896]\n",
            "mse_b ====== [866.68866]\n",
            "It: 53574, Time: 0.04\n",
            "mse_b  [866.68866]  mse_f: 3.25972056388855   total loss: [869.94836]\n",
            "mse_b ====== [866.668396]\n",
            "It: 53575, Time: 0.03\n",
            "mse_b  [866.6684]  mse_f: 3.117767095565796   total loss: [869.78613]\n",
            "mse_b ====== [866.50946]\n",
            "It: 53576, Time: 0.03\n",
            "mse_b  [866.50946]  mse_f: 2.7208309173583984   total loss: [869.2303]\n",
            "mse_b ====== [866.644531]\n",
            "It: 53577, Time: 0.03\n",
            "mse_b  [866.64453]  mse_f: 3.099539279937744   total loss: [869.7441]\n",
            "mse_b ====== [866.875183]\n",
            "It: 53578, Time: 0.03\n",
            "mse_b  [866.8752]  mse_f: 3.2872848510742188   total loss: [870.1625]\n",
            "mse_b ====== [866.654175]\n",
            "It: 53579, Time: 0.03\n",
            "mse_b  [866.6542]  mse_f: 3.3191757202148438   total loss: [869.9733]\n",
            "mse_b ====== [866.553345]\n",
            "It: 53580, Time: 0.03\n",
            "mse_b  [866.55334]  mse_f: 2.6911280155181885   total loss: [869.24445]\n",
            "mse_b ====== [866.568237]\n",
            "It: 53581, Time: 0.03\n",
            "mse_b  [866.56824]  mse_f: 3.100520610809326   total loss: [869.66876]\n",
            "mse_b ====== [866.740845]\n",
            "It: 53582, Time: 0.03\n",
            "mse_b  [866.74084]  mse_f: 3.488706111907959   total loss: [870.22955]\n",
            "mse_b ====== [866.926331]\n",
            "It: 53583, Time: 0.03\n",
            "mse_b  [866.92633]  mse_f: 3.2648427486419678   total loss: [870.19116]\n",
            "mse_b ====== [866.755]\n",
            "It: 53584, Time: 0.03\n",
            "mse_b  [866.755]  mse_f: 3.8827645778656006   total loss: [870.63776]\n",
            "mse_b ====== [866.536743]\n",
            "It: 53585, Time: 0.04\n",
            "mse_b  [866.53674]  mse_f: 3.303306818008423   total loss: [869.84]\n",
            "mse_b ====== [866.584351]\n",
            "It: 53586, Time: 0.03\n",
            "mse_b  [866.58435]  mse_f: 2.651299476623535   total loss: [869.23566]\n",
            "mse_b ====== [866.624268]\n",
            "It: 53587, Time: 0.03\n",
            "mse_b  [866.62427]  mse_f: 3.480593204498291   total loss: [870.10486]\n",
            "mse_b ====== [866.989136]\n",
            "It: 53588, Time: 0.03\n",
            "mse_b  [866.98914]  mse_f: 4.567714691162109   total loss: [871.5568]\n",
            "mse_b ====== [866.787]\n",
            "It: 53589, Time: 0.03\n",
            "mse_b  [866.787]  mse_f: 4.629445552825928   total loss: [871.41644]\n",
            "mse_b ====== [866.627319]\n",
            "It: 53590, Time: 0.03\n",
            "mse_b  [866.6273]  mse_f: 3.1440091133117676   total loss: [869.7713]\n",
            "mse_b ====== [866.575562]\n",
            "It: 53591, Time: 0.03\n",
            "mse_b  [866.57556]  mse_f: 2.9783596992492676   total loss: [869.5539]\n",
            "mse_b ====== [866.695557]\n",
            "It: 53592, Time: 0.03\n",
            "mse_b  [866.69556]  mse_f: 3.980726718902588   total loss: [870.6763]\n",
            "mse_b ====== [866.986633]\n",
            "It: 53593, Time: 0.03\n",
            "mse_b  [866.98663]  mse_f: 4.78214168548584   total loss: [871.7688]\n",
            "mse_b ====== [867.082825]\n",
            "It: 53594, Time: 0.03\n",
            "mse_b  [867.0828]  mse_f: 4.182300090789795   total loss: [871.26514]\n",
            "mse_b ====== [866.932]\n",
            "It: 53595, Time: 0.03\n",
            "mse_b  [866.932]  mse_f: 3.378251552581787   total loss: [870.31024]\n",
            "mse_b ====== [866.522766]\n",
            "It: 53596, Time: 0.03\n",
            "mse_b  [866.52277]  mse_f: 3.188875675201416   total loss: [869.7117]\n",
            "mse_b ====== [866.913391]\n",
            "It: 53597, Time: 0.03\n",
            "mse_b  [866.9134]  mse_f: 3.489604949951172   total loss: [870.403]\n",
            "mse_b ====== [867.208862]\n",
            "It: 53598, Time: 0.03\n",
            "mse_b  [867.20886]  mse_f: 4.598445415496826   total loss: [871.8073]\n",
            "mse_b ====== [867.196899]\n",
            "It: 53599, Time: 0.03\n",
            "mse_b  [867.1969]  mse_f: 4.016730308532715   total loss: [871.2136]\n",
            "mse_b ====== [866.714844]\n",
            "It: 53600, Time: 0.03\n",
            "mse_b  [866.71484]  mse_f: 2.838965892791748   total loss: [869.55383]\n",
            "mse_b ====== [866.902649]\n",
            "It: 53601, Time: 0.03\n",
            "mse_b  [866.90265]  mse_f: 2.833251953125   total loss: [869.7359]\n",
            "mse_b ====== [867.009277]\n",
            "It: 53602, Time: 0.04\n",
            "mse_b  [867.0093]  mse_f: 3.937033176422119   total loss: [870.9463]\n",
            "mse_b ====== [866.799316]\n",
            "It: 53603, Time: 0.03\n",
            "mse_b  [866.7993]  mse_f: 4.195642948150635   total loss: [870.99493]\n",
            "mse_b ====== [866.757263]\n",
            "It: 53604, Time: 0.03\n",
            "mse_b  [866.75726]  mse_f: 3.124638795852661   total loss: [869.8819]\n",
            "mse_b ====== [866.704651]\n",
            "It: 53605, Time: 0.03\n",
            "mse_b  [866.70465]  mse_f: 2.88645601272583   total loss: [869.5911]\n",
            "mse_b ====== [866.795837]\n",
            "It: 53606, Time: 0.04\n",
            "mse_b  [866.79584]  mse_f: 3.414430856704712   total loss: [870.21027]\n",
            "mse_b ====== [866.78009]\n",
            "It: 53607, Time: 0.03\n",
            "mse_b  [866.7801]  mse_f: 4.2091827392578125   total loss: [870.98926]\n",
            "mse_b ====== [866.879089]\n",
            "It: 53608, Time: 0.03\n",
            "mse_b  [866.8791]  mse_f: 3.3491458892822266   total loss: [870.2282]\n",
            "mse_b ====== [866.83844]\n",
            "It: 53609, Time: 0.03\n",
            "mse_b  [866.83844]  mse_f: 2.7892003059387207   total loss: [869.6276]\n",
            "mse_b ====== [866.753662]\n",
            "It: 53610, Time: 0.03\n",
            "mse_b  [866.75366]  mse_f: 3.147721529006958   total loss: [869.90137]\n",
            "mse_b ====== [866.872253]\n",
            "It: 53611, Time: 0.03\n",
            "mse_b  [866.87225]  mse_f: 3.8781824111938477   total loss: [870.7504]\n",
            "mse_b ====== [866.931335]\n",
            "It: 53612, Time: 0.04\n",
            "mse_b  [866.93134]  mse_f: 4.068526268005371   total loss: [870.9999]\n",
            "mse_b ====== [866.627686]\n",
            "It: 53613, Time: 0.03\n",
            "mse_b  [866.6277]  mse_f: 2.877398729324341   total loss: [869.50507]\n",
            "mse_b ====== [866.5271]\n",
            "It: 53614, Time: 0.03\n",
            "mse_b  [866.5271]  mse_f: 2.8819289207458496   total loss: [869.40906]\n",
            "mse_b ====== [867.023193]\n",
            "It: 53615, Time: 0.03\n",
            "mse_b  [867.0232]  mse_f: 4.079384803771973   total loss: [871.1026]\n",
            "mse_b ====== [867.008545]\n",
            "It: 53616, Time: 0.03\n",
            "mse_b  [867.00854]  mse_f: 4.3462419509887695   total loss: [871.3548]\n",
            "mse_b ====== [866.588684]\n",
            "It: 53617, Time: 0.03\n",
            "mse_b  [866.5887]  mse_f: 3.2569847106933594   total loss: [869.84564]\n",
            "mse_b ====== [866.61792]\n",
            "It: 53618, Time: 0.03\n",
            "mse_b  [866.6179]  mse_f: 2.6875524520874023   total loss: [869.3055]\n",
            "mse_b ====== [866.836426]\n",
            "It: 53619, Time: 0.03\n",
            "mse_b  [866.8364]  mse_f: 3.822800874710083   total loss: [870.65924]\n",
            "mse_b ====== [866.818298]\n",
            "It: 53620, Time: 0.03\n",
            "mse_b  [866.8183]  mse_f: 4.142727851867676   total loss: [870.961]\n",
            "mse_b ====== [866.685608]\n",
            "It: 53621, Time: 0.03\n",
            "mse_b  [866.6856]  mse_f: 3.6364688873291016   total loss: [870.3221]\n",
            "mse_b ====== [866.691528]\n",
            "It: 53622, Time: 0.03\n",
            "mse_b  [866.6915]  mse_f: 2.701235771179199   total loss: [869.39276]\n",
            "mse_b ====== [866.81073]\n",
            "It: 53623, Time: 0.03\n",
            "mse_b  [866.8107]  mse_f: 3.35453200340271   total loss: [870.1653]\n",
            "mse_b ====== [866.734558]\n",
            "It: 53624, Time: 0.03\n",
            "mse_b  [866.73456]  mse_f: 3.9007866382598877   total loss: [870.6353]\n",
            "mse_b ====== [866.84729]\n",
            "It: 53625, Time: 0.03\n",
            "mse_b  [866.8473]  mse_f: 3.435393810272217   total loss: [870.28265]\n",
            "mse_b ====== [866.871399]\n",
            "It: 53626, Time: 0.03\n",
            "mse_b  [866.8714]  mse_f: 2.936237335205078   total loss: [869.8076]\n",
            "mse_b ====== [866.605957]\n",
            "It: 53627, Time: 0.03\n",
            "mse_b  [866.60596]  mse_f: 3.046663284301758   total loss: [869.65265]\n",
            "mse_b ====== [866.807922]\n",
            "It: 53628, Time: 0.03\n",
            "mse_b  [866.8079]  mse_f: 3.5319015979766846   total loss: [870.33984]\n",
            "mse_b ====== [867.21991]\n",
            "It: 53629, Time: 0.03\n",
            "mse_b  [867.2199]  mse_f: 3.340228796005249   total loss: [870.5601]\n",
            "mse_b ====== [866.89325]\n",
            "It: 53630, Time: 0.03\n",
            "mse_b  [866.89325]  mse_f: 3.01938533782959   total loss: [869.91266]\n",
            "mse_b ====== [866.619507]\n",
            "It: 53631, Time: 0.04\n",
            "mse_b  [866.6195]  mse_f: 2.772569179534912   total loss: [869.3921]\n",
            "mse_b ====== [866.768188]\n",
            "It: 53632, Time: 0.03\n",
            "mse_b  [866.7682]  mse_f: 3.5170655250549316   total loss: [870.2853]\n",
            "mse_b ====== [867.216736]\n",
            "It: 53633, Time: 0.03\n",
            "mse_b  [867.21674]  mse_f: 3.8615918159484863   total loss: [871.0783]\n",
            "mse_b ====== [866.881165]\n",
            "It: 53634, Time: 0.03\n",
            "mse_b  [866.88116]  mse_f: 2.988142967224121   total loss: [869.8693]\n",
            "mse_b ====== [866.639526]\n",
            "It: 53635, Time: 0.03\n",
            "mse_b  [866.6395]  mse_f: 2.973829746246338   total loss: [869.61334]\n",
            "mse_b ====== [866.752869]\n",
            "It: 53636, Time: 0.03\n",
            "mse_b  [866.75287]  mse_f: 3.2112765312194824   total loss: [869.9642]\n",
            "mse_b ====== [866.983704]\n",
            "It: 53637, Time: 0.03\n",
            "mse_b  [866.9837]  mse_f: 3.4896883964538574   total loss: [870.4734]\n",
            "mse_b ====== [866.76123]\n",
            "It: 53638, Time: 0.03\n",
            "mse_b  [866.7612]  mse_f: 3.2844693660736084   total loss: [870.0457]\n",
            "mse_b ====== [866.743591]\n",
            "It: 53639, Time: 0.03\n",
            "mse_b  [866.7436]  mse_f: 2.885244369506836   total loss: [869.62885]\n",
            "mse_b ====== [866.75]\n",
            "It: 53640, Time: 0.03\n",
            "mse_b  [866.75]  mse_f: 3.0470871925354004   total loss: [869.79706]\n",
            "mse_b ====== [866.708679]\n",
            "It: 53641, Time: 0.03\n",
            "mse_b  [866.7087]  mse_f: 3.2761967182159424   total loss: [869.98486]\n",
            "mse_b ====== [866.607361]\n",
            "It: 53642, Time: 0.04\n",
            "mse_b  [866.60736]  mse_f: 3.453659772872925   total loss: [870.06104]\n",
            "mse_b ====== [866.524292]\n",
            "It: 53643, Time: 0.03\n",
            "mse_b  [866.5243]  mse_f: 3.2494735717773438   total loss: [869.77374]\n",
            "mse_b ====== [866.742188]\n",
            "It: 53644, Time: 0.03\n",
            "mse_b  [866.7422]  mse_f: 2.973123073577881   total loss: [869.71533]\n",
            "mse_b ====== [866.587646]\n",
            "It: 53645, Time: 0.03\n",
            "mse_b  [866.58765]  mse_f: 3.0254735946655273   total loss: [869.6131]\n",
            "mse_b ====== [866.723938]\n",
            "It: 53646, Time: 0.03\n",
            "mse_b  [866.72394]  mse_f: 3.3721561431884766   total loss: [870.09607]\n",
            "mse_b ====== [866.587646]\n",
            "It: 53647, Time: 0.03\n",
            "mse_b  [866.58765]  mse_f: 3.547353982925415   total loss: [870.135]\n",
            "mse_b ====== [866.574036]\n",
            "It: 53648, Time: 0.03\n",
            "mse_b  [866.57404]  mse_f: 2.585096836090088   total loss: [869.1591]\n",
            "mse_b ====== [866.559753]\n",
            "It: 53649, Time: 0.03\n",
            "mse_b  [866.55975]  mse_f: 3.1342480182647705   total loss: [869.69403]\n",
            "mse_b ====== [866.766235]\n",
            "It: 53650, Time: 0.03\n",
            "mse_b  [866.76624]  mse_f: 3.4992666244506836   total loss: [870.2655]\n",
            "mse_b ====== [866.617737]\n",
            "It: 53651, Time: 0.03\n",
            "mse_b  [866.61774]  mse_f: 3.424987554550171   total loss: [870.0427]\n",
            "mse_b ====== [866.548462]\n",
            "It: 53652, Time: 0.03\n",
            "mse_b  [866.54846]  mse_f: 2.733808994293213   total loss: [869.2823]\n",
            "mse_b ====== [866.56488]\n",
            "It: 53653, Time: 0.03\n",
            "mse_b  [866.5649]  mse_f: 2.938693046569824   total loss: [869.5036]\n",
            "mse_b ====== [866.594727]\n",
            "It: 53654, Time: 0.03\n",
            "mse_b  [866.5947]  mse_f: 3.4701242446899414   total loss: [870.0649]\n",
            "mse_b ====== [866.658936]\n",
            "It: 53655, Time: 0.03\n",
            "mse_b  [866.65894]  mse_f: 3.1599912643432617   total loss: [869.8189]\n",
            "mse_b ====== [866.615173]\n",
            "It: 53656, Time: 0.04\n",
            "mse_b  [866.6152]  mse_f: 2.9760560989379883   total loss: [869.59125]\n",
            "mse_b ====== [866.637451]\n",
            "It: 53657, Time: 0.03\n",
            "mse_b  [866.63745]  mse_f: 2.7128190994262695   total loss: [869.3503]\n",
            "mse_b ====== [866.608459]\n",
            "It: 53658, Time: 0.05\n",
            "mse_b  [866.60846]  mse_f: 3.1182336807250977   total loss: [869.7267]\n",
            "mse_b ====== [866.65387]\n",
            "It: 53659, Time: 0.03\n",
            "mse_b  [866.6539]  mse_f: 2.9055874347686768   total loss: [869.55945]\n",
            "mse_b ====== [866.791626]\n",
            "It: 53660, Time: 0.04\n",
            "mse_b  [866.7916]  mse_f: 2.656020164489746   total loss: [869.44763]\n",
            "mse_b ====== [866.6297]\n",
            "It: 53661, Time: 0.03\n",
            "mse_b  [866.6297]  mse_f: 3.0202014446258545   total loss: [869.6499]\n",
            "mse_b ====== [866.442261]\n",
            "It: 53662, Time: 0.03\n",
            "mse_b  [866.44226]  mse_f: 2.751291275024414   total loss: [869.19354]\n",
            "mse_b ====== [866.744873]\n",
            "It: 53663, Time: 0.03\n",
            "mse_b  [866.7449]  mse_f: 2.8700146675109863   total loss: [869.61487]\n",
            "mse_b ====== [866.823669]\n",
            "It: 53664, Time: 0.03\n",
            "mse_b  [866.82367]  mse_f: 2.8731117248535156   total loss: [869.6968]\n",
            "mse_b ====== [866.702637]\n",
            "It: 53665, Time: 0.03\n",
            "mse_b  [866.70264]  mse_f: 2.7529172897338867   total loss: [869.45557]\n",
            "mse_b ====== [866.476318]\n",
            "It: 53666, Time: 0.03\n",
            "mse_b  [866.4763]  mse_f: 2.602372169494629   total loss: [869.0787]\n",
            "mse_b ====== [866.769165]\n",
            "It: 53667, Time: 0.03\n",
            "mse_b  [866.76917]  mse_f: 2.8873066902160645   total loss: [869.6565]\n",
            "mse_b ====== [866.915649]\n",
            "It: 53668, Time: 0.04\n",
            "mse_b  [866.91565]  mse_f: 2.8824775218963623   total loss: [869.79816]\n",
            "mse_b ====== [866.57135]\n",
            "It: 53669, Time: 0.03\n",
            "mse_b  [866.57135]  mse_f: 2.7777271270751953   total loss: [869.34906]\n",
            "mse_b ====== [866.534]\n",
            "It: 53670, Time: 0.03\n",
            "mse_b  [866.534]  mse_f: 2.8633246421813965   total loss: [869.39734]\n",
            "mse_b ====== [866.751099]\n",
            "It: 53671, Time: 0.03\n",
            "mse_b  [866.7511]  mse_f: 2.5937747955322266   total loss: [869.34485]\n",
            "mse_b ====== [866.82251]\n",
            "It: 53672, Time: 0.03\n",
            "mse_b  [866.8225]  mse_f: 2.934335708618164   total loss: [869.75684]\n",
            "mse_b ====== [866.530334]\n",
            "It: 53673, Time: 0.03\n",
            "mse_b  [866.53033]  mse_f: 2.8526389598846436   total loss: [869.383]\n",
            "mse_b ====== [866.529541]\n",
            "It: 53674, Time: 0.03\n",
            "mse_b  [866.52954]  mse_f: 2.9258687496185303   total loss: [869.4554]\n",
            "mse_b ====== [866.701721]\n",
            "It: 53675, Time: 0.03\n",
            "mse_b  [866.7017]  mse_f: 3.0888500213623047   total loss: [869.7906]\n",
            "mse_b ====== [866.618591]\n",
            "It: 53676, Time: 0.03\n",
            "mse_b  [866.6186]  mse_f: 2.6813530921936035   total loss: [869.2999]\n",
            "mse_b ====== [866.443726]\n",
            "It: 53677, Time: 0.03\n",
            "mse_b  [866.4437]  mse_f: 2.7288808822631836   total loss: [869.1726]\n",
            "mse_b ====== [866.491089]\n",
            "It: 53678, Time: 0.03\n",
            "mse_b  [866.4911]  mse_f: 2.972332239151001   total loss: [869.46344]\n",
            "mse_b ====== [866.741638]\n",
            "It: 53679, Time: 0.03\n",
            "mse_b  [866.74164]  mse_f: 3.115673303604126   total loss: [869.8573]\n",
            "mse_b ====== [866.637329]\n",
            "It: 53680, Time: 0.04\n",
            "mse_b  [866.6373]  mse_f: 2.8937387466430664   total loss: [869.53107]\n",
            "mse_b ====== [866.5177]\n",
            "It: 53681, Time: 0.03\n",
            "mse_b  [866.5177]  mse_f: 3.1135482788085938   total loss: [869.6312]\n",
            "mse_b ====== [866.576599]\n",
            "It: 53682, Time: 0.03\n",
            "mse_b  [866.5766]  mse_f: 2.7424979209899902   total loss: [869.3191]\n",
            "mse_b ====== [866.645508]\n",
            "It: 53683, Time: 0.03\n",
            "mse_b  [866.6455]  mse_f: 2.756422996520996   total loss: [869.4019]\n",
            "mse_b ====== [866.56012]\n",
            "It: 53684, Time: 0.03\n",
            "mse_b  [866.5601]  mse_f: 2.888115406036377   total loss: [869.44824]\n",
            "mse_b ====== [866.648804]\n",
            "It: 53685, Time: 0.03\n",
            "mse_b  [866.6488]  mse_f: 2.8835065364837646   total loss: [869.5323]\n",
            "mse_b ====== [866.704712]\n",
            "It: 53686, Time: 0.03\n",
            "mse_b  [866.7047]  mse_f: 3.4569344520568848   total loss: [870.1616]\n",
            "mse_b ====== [866.669617]\n",
            "It: 53687, Time: 0.03\n",
            "mse_b  [866.6696]  mse_f: 2.929779052734375   total loss: [869.59937]\n",
            "mse_b ====== [866.575073]\n",
            "It: 53688, Time: 0.03\n",
            "mse_b  [866.5751]  mse_f: 2.8033270835876465   total loss: [869.3784]\n",
            "mse_b ====== [866.451111]\n",
            "It: 53689, Time: 0.03\n",
            "mse_b  [866.4511]  mse_f: 2.9614713191986084   total loss: [869.4126]\n",
            "mse_b ====== [866.458862]\n",
            "It: 53690, Time: 0.03\n",
            "mse_b  [866.45886]  mse_f: 2.812492847442627   total loss: [869.27136]\n",
            "mse_b ====== [866.476379]\n",
            "It: 53691, Time: 0.03\n",
            "mse_b  [866.4764]  mse_f: 2.780651569366455   total loss: [869.257]\n",
            "mse_b ====== [866.515503]\n",
            "It: 53692, Time: 0.03\n",
            "mse_b  [866.5155]  mse_f: 3.0870320796966553   total loss: [869.60254]\n",
            "mse_b ====== [866.667603]\n",
            "It: 53693, Time: 0.03\n",
            "mse_b  [866.6676]  mse_f: 3.0361926555633545   total loss: [869.7038]\n",
            "mse_b ====== [866.474]\n",
            "It: 53694, Time: 0.04\n",
            "mse_b  [866.474]  mse_f: 2.8833565711975098   total loss: [869.35736]\n",
            "mse_b ====== [866.469849]\n",
            "It: 53695, Time: 0.03\n",
            "mse_b  [866.46985]  mse_f: 2.9260826110839844   total loss: [869.39594]\n",
            "mse_b ====== [866.528198]\n",
            "It: 53696, Time: 0.03\n",
            "mse_b  [866.5282]  mse_f: 2.705714702606201   total loss: [869.2339]\n",
            "mse_b ====== [866.443604]\n",
            "It: 53697, Time: 0.03\n",
            "mse_b  [866.4436]  mse_f: 3.0748276710510254   total loss: [869.51843]\n",
            "mse_b ====== [866.457214]\n",
            "It: 53698, Time: 0.03\n",
            "mse_b  [866.4572]  mse_f: 2.642629384994507   total loss: [869.09985]\n",
            "mse_b ====== [866.393555]\n",
            "It: 53699, Time: 0.03\n",
            "mse_b  [866.39355]  mse_f: 2.616393566131592   total loss: [869.00995]\n",
            "mse_b ====== [866.519653]\n",
            "It: 53700, Time: 0.03\n",
            "mse_b  [866.51965]  mse_f: 2.977492332458496   total loss: [869.49713]\n",
            "mse_b ====== [866.573547]\n",
            "It: 53701, Time: 0.04\n",
            "mse_b  [866.57355]  mse_f: 2.792117118835449   total loss: [869.36566]\n",
            "mse_b ====== [866.428406]\n",
            "It: 53702, Time: 0.03\n",
            "mse_b  [866.4284]  mse_f: 2.999997138977051   total loss: [869.4284]\n",
            "mse_b ====== [866.456]\n",
            "It: 53703, Time: 0.03\n",
            "mse_b  [866.456]  mse_f: 3.1509580612182617   total loss: [869.60693]\n",
            "mse_b ====== [866.581604]\n",
            "It: 53704, Time: 0.03\n",
            "mse_b  [866.5816]  mse_f: 3.298269033432007   total loss: [869.8799]\n",
            "mse_b ====== [866.529785]\n",
            "It: 53705, Time: 0.03\n",
            "mse_b  [866.5298]  mse_f: 3.0787878036499023   total loss: [869.6086]\n",
            "mse_b ====== [866.444092]\n",
            "It: 53706, Time: 0.03\n",
            "mse_b  [866.4441]  mse_f: 3.1741650104522705   total loss: [869.6183]\n",
            "mse_b ====== [866.579224]\n",
            "It: 53707, Time: 0.04\n",
            "mse_b  [866.5792]  mse_f: 3.0893545150756836   total loss: [869.6686]\n",
            "mse_b ====== [866.675598]\n",
            "It: 53708, Time: 0.03\n",
            "mse_b  [866.6756]  mse_f: 3.637319564819336   total loss: [870.3129]\n",
            "mse_b ====== [866.726379]\n",
            "It: 53709, Time: 0.03\n",
            "mse_b  [866.7264]  mse_f: 3.279367446899414   total loss: [870.00574]\n",
            "mse_b ====== [866.597839]\n",
            "It: 53710, Time: 0.03\n",
            "mse_b  [866.59784]  mse_f: 3.105760097503662   total loss: [869.7036]\n",
            "mse_b ====== [866.489563]\n",
            "It: 53711, Time: 0.03\n",
            "mse_b  [866.48956]  mse_f: 3.268944025039673   total loss: [869.7585]\n",
            "mse_b ====== [866.580139]\n",
            "It: 53712, Time: 0.04\n",
            "mse_b  [866.58014]  mse_f: 2.92592453956604   total loss: [869.50604]\n",
            "mse_b ====== [866.526489]\n",
            "It: 53713, Time: 0.04\n",
            "mse_b  [866.5265]  mse_f: 2.956918478012085   total loss: [869.4834]\n",
            "mse_b ====== [866.524719]\n",
            "It: 53714, Time: 0.03\n",
            "mse_b  [866.5247]  mse_f: 2.8865976333618164   total loss: [869.4113]\n",
            "mse_b ====== [866.672485]\n",
            "It: 53715, Time: 0.03\n",
            "mse_b  [866.6725]  mse_f: 2.570223808288574   total loss: [869.24274]\n",
            "mse_b ====== [866.438293]\n",
            "It: 53716, Time: 0.03\n",
            "mse_b  [866.4383]  mse_f: 2.9455111026763916   total loss: [869.3838]\n",
            "mse_b ====== [866.490356]\n",
            "It: 53717, Time: 0.03\n",
            "mse_b  [866.49036]  mse_f: 2.7637696266174316   total loss: [869.25415]\n",
            "mse_b ====== [866.637085]\n",
            "It: 53718, Time: 0.03\n",
            "mse_b  [866.6371]  mse_f: 2.8138608932495117   total loss: [869.4509]\n",
            "mse_b ====== [866.585388]\n",
            "It: 53719, Time: 0.03\n",
            "mse_b  [866.5854]  mse_f: 3.310850143432617   total loss: [869.89624]\n",
            "mse_b ====== [866.604919]\n",
            "It: 53720, Time: 0.03\n",
            "mse_b  [866.6049]  mse_f: 3.1464650630950928   total loss: [869.7514]\n",
            "mse_b ====== [866.536499]\n",
            "It: 53721, Time: 0.03\n",
            "mse_b  [866.5365]  mse_f: 3.1035425662994385   total loss: [869.64]\n",
            "mse_b ====== [866.715759]\n",
            "It: 53722, Time: 0.03\n",
            "mse_b  [866.71576]  mse_f: 3.38314151763916   total loss: [870.0989]\n",
            "mse_b ====== [866.692078]\n",
            "It: 53723, Time: 0.03\n",
            "mse_b  [866.6921]  mse_f: 3.361192464828491   total loss: [870.0533]\n",
            "mse_b ====== [866.567871]\n",
            "It: 53724, Time: 0.03\n",
            "mse_b  [866.5679]  mse_f: 3.459672212600708   total loss: [870.0275]\n",
            "mse_b ====== [866.533386]\n",
            "It: 53725, Time: 0.03\n",
            "mse_b  [866.5334]  mse_f: 3.4627671241760254   total loss: [869.99615]\n",
            "mse_b ====== [866.63623]\n",
            "It: 53726, Time: 0.03\n",
            "mse_b  [866.6362]  mse_f: 2.9517736434936523   total loss: [869.588]\n",
            "mse_b ====== [866.599731]\n",
            "It: 53727, Time: 0.03\n",
            "mse_b  [866.59973]  mse_f: 2.9860076904296875   total loss: [869.58575]\n",
            "mse_b ====== [866.516602]\n",
            "It: 53728, Time: 0.03\n",
            "mse_b  [866.5166]  mse_f: 2.6956076622009277   total loss: [869.2122]\n",
            "mse_b ====== [866.406128]\n",
            "It: 53729, Time: 0.04\n",
            "mse_b  [866.4061]  mse_f: 2.6550886631011963   total loss: [869.0612]\n",
            "mse_b ====== [866.52594]\n",
            "It: 53730, Time: 0.04\n",
            "mse_b  [866.52594]  mse_f: 2.886460542678833   total loss: [869.4124]\n",
            "mse_b ====== [866.593079]\n",
            "It: 53731, Time: 0.03\n",
            "mse_b  [866.5931]  mse_f: 2.7593560218811035   total loss: [869.3524]\n",
            "mse_b ====== [866.50293]\n",
            "It: 53732, Time: 0.03\n",
            "mse_b  [866.5029]  mse_f: 2.813063144683838   total loss: [869.316]\n",
            "mse_b ====== [866.462463]\n",
            "It: 53733, Time: 0.03\n",
            "mse_b  [866.46246]  mse_f: 3.010772943496704   total loss: [869.47327]\n",
            "mse_b ====== [866.466919]\n",
            "It: 53734, Time: 0.03\n",
            "mse_b  [866.4669]  mse_f: 3.061131000518799   total loss: [869.5281]\n",
            "mse_b ====== [866.5401]\n",
            "It: 53735, Time: 0.04\n",
            "mse_b  [866.5401]  mse_f: 3.1148033142089844   total loss: [869.6549]\n",
            "mse_b ====== [866.453674]\n",
            "It: 53736, Time: 0.03\n",
            "mse_b  [866.4537]  mse_f: 3.2014822959899902   total loss: [869.65515]\n",
            "mse_b ====== [866.555]\n",
            "It: 53737, Time: 0.03\n",
            "mse_b  [866.555]  mse_f: 2.760828971862793   total loss: [869.3158]\n",
            "mse_b ====== [866.558838]\n",
            "It: 53738, Time: 0.03\n",
            "mse_b  [866.55884]  mse_f: 2.994880199432373   total loss: [869.5537]\n",
            "mse_b ====== [866.477722]\n",
            "It: 53739, Time: 0.03\n",
            "mse_b  [866.4777]  mse_f: 2.8905797004699707   total loss: [869.3683]\n",
            "mse_b ====== [866.430115]\n",
            "It: 53740, Time: 0.05\n",
            "mse_b  [866.4301]  mse_f: 2.7673797607421875   total loss: [869.1975]\n",
            "mse_b ====== [866.450134]\n",
            "It: 53741, Time: 0.03\n",
            "mse_b  [866.45013]  mse_f: 3.2151403427124023   total loss: [869.6653]\n",
            "mse_b ====== [866.407166]\n",
            "It: 53742, Time: 0.03\n",
            "mse_b  [866.40717]  mse_f: 2.8306784629821777   total loss: [869.23785]\n",
            "mse_b ====== [866.419495]\n",
            "It: 53743, Time: 0.03\n",
            "mse_b  [866.4195]  mse_f: 2.860480785369873   total loss: [869.27997]\n",
            "mse_b ====== [866.436768]\n",
            "It: 53744, Time: 0.03\n",
            "mse_b  [866.43677]  mse_f: 2.894411563873291   total loss: [869.3312]\n",
            "mse_b ====== [866.468567]\n",
            "It: 53745, Time: 0.03\n",
            "mse_b  [866.46857]  mse_f: 2.8800017833709717   total loss: [869.3486]\n",
            "mse_b ====== [866.40741]\n",
            "It: 53746, Time: 0.03\n",
            "mse_b  [866.4074]  mse_f: 3.0430245399475098   total loss: [869.45044]\n",
            "mse_b ====== [866.564819]\n",
            "It: 53747, Time: 0.03\n",
            "mse_b  [866.5648]  mse_f: 2.817200183868408   total loss: [869.382]\n",
            "mse_b ====== [866.427429]\n",
            "It: 53748, Time: 0.03\n",
            "mse_b  [866.4274]  mse_f: 2.885179042816162   total loss: [869.3126]\n",
            "mse_b ====== [866.660767]\n",
            "It: 53749, Time: 0.03\n",
            "mse_b  [866.66077]  mse_f: 3.105555534362793   total loss: [869.7663]\n",
            "mse_b ====== [866.470154]\n",
            "It: 53750, Time: 0.03\n",
            "mse_b  [866.47015]  mse_f: 3.5333786010742188   total loss: [870.00354]\n",
            "mse_b ====== [866.700378]\n",
            "It: 53751, Time: 0.03\n",
            "mse_b  [866.7004]  mse_f: 3.6521658897399902   total loss: [870.35254]\n",
            "mse_b ====== [866.931824]\n",
            "It: 53752, Time: 0.03\n",
            "mse_b  [866.9318]  mse_f: 4.483481407165527   total loss: [871.4153]\n",
            "mse_b ====== [866.996887]\n",
            "It: 53753, Time: 0.03\n",
            "mse_b  [866.9969]  mse_f: 4.68898868560791   total loss: [871.68585]\n",
            "mse_b ====== [866.986206]\n",
            "It: 53754, Time: 0.03\n",
            "mse_b  [866.9862]  mse_f: 5.1868085861206055   total loss: [872.17303]\n",
            "mse_b ====== [867.147827]\n",
            "It: 53755, Time: 0.03\n",
            "mse_b  [867.1478]  mse_f: 5.301943778991699   total loss: [872.44977]\n",
            "mse_b ====== [867.005676]\n",
            "It: 53756, Time: 0.03\n",
            "mse_b  [867.0057]  mse_f: 5.289489269256592   total loss: [872.29517]\n",
            "mse_b ====== [867.114563]\n",
            "It: 53757, Time: 0.03\n",
            "mse_b  [867.11456]  mse_f: 4.982874870300293   total loss: [872.0974]\n",
            "mse_b ====== [866.71814]\n",
            "It: 53758, Time: 0.03\n",
            "mse_b  [866.71814]  mse_f: 3.617767810821533   total loss: [870.33594]\n",
            "mse_b ====== [866.48877]\n",
            "It: 53759, Time: 0.03\n",
            "mse_b  [866.4888]  mse_f: 2.885255813598633   total loss: [869.374]\n",
            "mse_b ====== [866.738953]\n",
            "It: 53760, Time: 0.03\n",
            "mse_b  [866.73895]  mse_f: 3.5026609897613525   total loss: [870.24164]\n",
            "mse_b ====== [866.908203]\n",
            "It: 53761, Time: 0.03\n",
            "mse_b  [866.9082]  mse_f: 4.707686424255371   total loss: [871.6159]\n",
            "mse_b ====== [867.134705]\n",
            "It: 53762, Time: 0.03\n",
            "mse_b  [867.1347]  mse_f: 5.922145843505859   total loss: [873.0568]\n",
            "mse_b ====== [866.909851]\n",
            "It: 53763, Time: 0.03\n",
            "mse_b  [866.90985]  mse_f: 6.247120380401611   total loss: [873.157]\n",
            "mse_b ====== [866.669373]\n",
            "It: 53764, Time: 0.03\n",
            "mse_b  [866.6694]  mse_f: 3.9131319522857666   total loss: [870.5825]\n",
            "mse_b ====== [866.650146]\n",
            "It: 53765, Time: 0.03\n",
            "mse_b  [866.65015]  mse_f: 3.0228538513183594   total loss: [869.673]\n",
            "mse_b ====== [866.913818]\n",
            "It: 53766, Time: 0.03\n",
            "mse_b  [866.9138]  mse_f: 4.991203308105469   total loss: [871.905]\n",
            "mse_b ====== [867.384827]\n",
            "It: 53767, Time: 0.03\n",
            "mse_b  [867.3848]  mse_f: 5.908378601074219   total loss: [873.2932]\n",
            "mse_b ====== [867.281738]\n",
            "It: 53768, Time: 0.04\n",
            "mse_b  [867.28174]  mse_f: 6.521568298339844   total loss: [873.8033]\n",
            "mse_b ====== [867.032654]\n",
            "It: 53769, Time: 0.04\n",
            "mse_b  [867.03265]  mse_f: 3.817399501800537   total loss: [870.85004]\n",
            "mse_b ====== [866.77887]\n",
            "It: 53770, Time: 0.03\n",
            "mse_b  [866.7789]  mse_f: 3.3308377265930176   total loss: [870.1097]\n",
            "mse_b ====== [867.848938]\n",
            "It: 53771, Time: 0.03\n",
            "mse_b  [867.84894]  mse_f: 5.2401885986328125   total loss: [873.0891]\n",
            "mse_b ====== [867.680115]\n",
            "It: 53772, Time: 0.03\n",
            "mse_b  [867.6801]  mse_f: 6.753442764282227   total loss: [874.43353]\n",
            "mse_b ====== [866.939148]\n",
            "It: 53773, Time: 0.03\n",
            "mse_b  [866.93915]  mse_f: 5.077793121337891   total loss: [872.01697]\n",
            "mse_b ====== [867.302]\n",
            "It: 53774, Time: 0.03\n",
            "mse_b  [867.302]  mse_f: 3.265195846557617   total loss: [870.5672]\n",
            "mse_b ====== [867.554932]\n",
            "It: 53775, Time: 0.03\n",
            "mse_b  [867.55493]  mse_f: 3.728930950164795   total loss: [871.2839]\n",
            "mse_b ====== [867.668396]\n",
            "It: 53776, Time: 0.03\n",
            "mse_b  [867.6684]  mse_f: 5.888947010040283   total loss: [873.5574]\n",
            "mse_b ====== [866.8479]\n",
            "It: 53777, Time: 0.03\n",
            "mse_b  [866.8479]  mse_f: 5.812008857727051   total loss: [872.6599]\n",
            "mse_b ====== [867.031128]\n",
            "It: 53778, Time: 0.03\n",
            "mse_b  [867.0311]  mse_f: 3.4622228145599365   total loss: [870.49335]\n",
            "mse_b ====== [867.475342]\n",
            "It: 53779, Time: 0.03\n",
            "mse_b  [867.47534]  mse_f: 3.5527472496032715   total loss: [871.0281]\n",
            "mse_b ====== [867.449158]\n",
            "It: 53780, Time: 0.03\n",
            "mse_b  [867.44916]  mse_f: 5.446072578430176   total loss: [872.8952]\n",
            "mse_b ====== [866.733887]\n",
            "It: 53781, Time: 0.03\n",
            "mse_b  [866.7339]  mse_f: 5.706977844238281   total loss: [872.44086]\n",
            "mse_b ====== [866.793701]\n",
            "It: 53782, Time: 0.03\n",
            "mse_b  [866.7937]  mse_f: 3.6630237102508545   total loss: [870.4567]\n",
            "mse_b ====== [867.625366]\n",
            "It: 53783, Time: 0.03\n",
            "mse_b  [867.62537]  mse_f: 3.644522190093994   total loss: [871.2699]\n",
            "mse_b ====== [867.710388]\n",
            "It: 53784, Time: 0.03\n",
            "mse_b  [867.7104]  mse_f: 4.737297534942627   total loss: [872.4477]\n",
            "mse_b ====== [867.266907]\n",
            "It: 53785, Time: 0.04\n",
            "mse_b  [867.2669]  mse_f: 5.373813629150391   total loss: [872.64075]\n",
            "mse_b ====== [866.989136]\n",
            "It: 53786, Time: 0.03\n",
            "mse_b  [866.98914]  mse_f: 4.35353946685791   total loss: [871.34265]\n",
            "mse_b ====== [867.412781]\n",
            "It: 53787, Time: 0.03\n",
            "mse_b  [867.4128]  mse_f: 3.004091739654541   total loss: [870.4169]\n",
            "mse_b ====== [867.489441]\n",
            "It: 53788, Time: 0.03\n",
            "mse_b  [867.48944]  mse_f: 4.836259841918945   total loss: [872.3257]\n",
            "mse_b ====== [867.21106]\n",
            "It: 53789, Time: 0.03\n",
            "mse_b  [867.21106]  mse_f: 5.747313976287842   total loss: [872.9584]\n",
            "mse_b ====== [867.158875]\n",
            "It: 53790, Time: 0.03\n",
            "mse_b  [867.1589]  mse_f: 4.440113067626953   total loss: [871.599]\n",
            "mse_b ====== [866.94812]\n",
            "It: 53791, Time: 0.03\n",
            "mse_b  [866.9481]  mse_f: 3.53680419921875   total loss: [870.4849]\n",
            "mse_b ====== [867.383]\n",
            "It: 53792, Time: 0.03\n",
            "mse_b  [867.383]  mse_f: 4.3345627784729   total loss: [871.7175]\n",
            "mse_b ====== [867.104492]\n",
            "It: 53793, Time: 0.03\n",
            "mse_b  [867.1045]  mse_f: 5.0325703620910645   total loss: [872.1371]\n",
            "mse_b ====== [866.904602]\n",
            "It: 53794, Time: 0.03\n",
            "mse_b  [866.9046]  mse_f: 4.458497047424316   total loss: [871.3631]\n",
            "mse_b ====== [866.965881]\n",
            "It: 53795, Time: 0.03\n",
            "mse_b  [866.9659]  mse_f: 3.8826851844787598   total loss: [870.8486]\n",
            "mse_b ====== [867.033875]\n",
            "It: 53796, Time: 0.03\n",
            "mse_b  [867.0339]  mse_f: 3.658761739730835   total loss: [870.6926]\n",
            "mse_b ====== [867.26]\n",
            "It: 53797, Time: 0.03\n",
            "mse_b  [867.26]  mse_f: 5.4839982986450195   total loss: [872.744]\n",
            "mse_b ====== [866.764465]\n",
            "It: 53798, Time: 0.03\n",
            "mse_b  [866.76447]  mse_f: 4.716128349304199   total loss: [871.4806]\n",
            "mse_b ====== [866.873474]\n",
            "It: 53799, Time: 0.03\n",
            "mse_b  [866.8735]  mse_f: 3.1809353828430176   total loss: [870.0544]\n",
            "mse_b ====== [866.975952]\n",
            "It: 53800, Time: 0.03\n",
            "mse_b  [866.97595]  mse_f: 4.470032215118408   total loss: [871.446]\n",
            "mse_b ====== [867.273254]\n",
            "It: 53801, Time: 0.03\n",
            "mse_b  [867.27325]  mse_f: 5.195880889892578   total loss: [872.4691]\n",
            "mse_b ====== [867.242554]\n",
            "It: 53802, Time: 0.03\n",
            "mse_b  [867.24255]  mse_f: 3.85294771194458   total loss: [871.0955]\n",
            "mse_b ====== [866.810547]\n",
            "It: 53803, Time: 0.03\n",
            "mse_b  [866.81055]  mse_f: 3.4850149154663086   total loss: [870.29553]\n",
            "mse_b ====== [867.065918]\n",
            "It: 53804, Time: 0.04\n",
            "mse_b  [867.0659]  mse_f: 4.536381244659424   total loss: [871.6023]\n",
            "mse_b ====== [866.795349]\n",
            "It: 53805, Time: 0.03\n",
            "mse_b  [866.79535]  mse_f: 4.968635559082031   total loss: [871.764]\n",
            "mse_b ====== [866.91748]\n",
            "It: 53806, Time: 0.03\n",
            "mse_b  [866.9175]  mse_f: 3.8054580688476562   total loss: [870.72296]\n",
            "mse_b ====== [866.729248]\n",
            "It: 53807, Time: 0.03\n",
            "mse_b  [866.72925]  mse_f: 4.242122650146484   total loss: [870.9714]\n",
            "mse_b ====== [866.658264]\n",
            "It: 53808, Time: 0.03\n",
            "mse_b  [866.65826]  mse_f: 4.248010158538818   total loss: [870.90625]\n",
            "mse_b ====== [866.978149]\n",
            "It: 53809, Time: 0.03\n",
            "mse_b  [866.97815]  mse_f: 4.9837446212768555   total loss: [871.9619]\n",
            "mse_b ====== [867.028259]\n",
            "It: 53810, Time: 0.03\n",
            "mse_b  [867.02826]  mse_f: 3.1406455039978027   total loss: [870.1689]\n",
            "mse_b ====== [867.006531]\n",
            "It: 53811, Time: 0.03\n",
            "mse_b  [867.00653]  mse_f: 3.818619728088379   total loss: [870.82513]\n",
            "mse_b ====== [866.82074]\n",
            "It: 53812, Time: 0.03\n",
            "mse_b  [866.82074]  mse_f: 4.815013885498047   total loss: [871.63574]\n",
            "mse_b ====== [866.847534]\n",
            "It: 53813, Time: 0.03\n",
            "mse_b  [866.84753]  mse_f: 4.345353126525879   total loss: [871.1929]\n",
            "mse_b ====== [866.970398]\n",
            "It: 53814, Time: 0.03\n",
            "mse_b  [866.9704]  mse_f: 3.0564441680908203   total loss: [870.02686]\n",
            "mse_b ====== [867.193604]\n",
            "It: 53815, Time: 0.03\n",
            "mse_b  [867.1936]  mse_f: 3.8914220333099365   total loss: [871.085]\n",
            "mse_b ====== [867.274414]\n",
            "It: 53816, Time: 0.03\n",
            "mse_b  [867.2744]  mse_f: 4.789278984069824   total loss: [872.0637]\n",
            "mse_b ====== [866.734497]\n",
            "It: 53817, Time: 0.03\n",
            "mse_b  [866.7345]  mse_f: 3.395463466644287   total loss: [870.12994]\n",
            "mse_b ====== [866.746094]\n",
            "It: 53818, Time: 0.03\n",
            "mse_b  [866.7461]  mse_f: 3.4593544006347656   total loss: [870.20544]\n",
            "mse_b ====== [867.233826]\n",
            "It: 53819, Time: 0.03\n",
            "mse_b  [867.2338]  mse_f: 4.490695953369141   total loss: [871.72455]\n",
            "mse_b ====== [866.998]\n",
            "It: 53820, Time: 0.03\n",
            "mse_b  [866.998]  mse_f: 4.140445709228516   total loss: [871.1384]\n",
            "mse_b ====== [866.832336]\n",
            "It: 53821, Time: 0.03\n",
            "mse_b  [866.83234]  mse_f: 3.7487263679504395   total loss: [870.58105]\n",
            "mse_b ====== [866.710327]\n",
            "It: 53822, Time: 0.04\n",
            "mse_b  [866.7103]  mse_f: 3.431837320327759   total loss: [870.14215]\n",
            "mse_b ====== [867.454]\n",
            "It: 53823, Time: 0.04\n",
            "mse_b  [867.454]  mse_f: 4.070193767547607   total loss: [871.5242]\n",
            "mse_b ====== [867.044]\n",
            "It: 53824, Time: 0.03\n",
            "mse_b  [867.044]  mse_f: 4.188830852508545   total loss: [871.23285]\n",
            "mse_b ====== [866.590637]\n",
            "It: 53825, Time: 0.04\n",
            "mse_b  [866.59064]  mse_f: 4.042607307434082   total loss: [870.63324]\n",
            "mse_b ====== [866.673035]\n",
            "It: 53826, Time: 0.03\n",
            "mse_b  [866.67303]  mse_f: 3.452120065689087   total loss: [870.1252]\n",
            "mse_b ====== [866.885498]\n",
            "It: 53827, Time: 0.03\n",
            "mse_b  [866.8855]  mse_f: 4.381826877593994   total loss: [871.26733]\n",
            "mse_b ====== [867.161621]\n",
            "It: 53828, Time: 0.03\n",
            "mse_b  [867.1616]  mse_f: 4.395802974700928   total loss: [871.55743]\n",
            "mse_b ====== [866.974854]\n",
            "It: 53829, Time: 0.03\n",
            "mse_b  [866.97485]  mse_f: 3.239912509918213   total loss: [870.2148]\n",
            "mse_b ====== [866.809]\n",
            "It: 53830, Time: 0.03\n",
            "mse_b  [866.809]  mse_f: 4.003117561340332   total loss: [870.81213]\n",
            "mse_b ====== [866.719543]\n",
            "It: 53831, Time: 0.03\n",
            "mse_b  [866.71954]  mse_f: 4.472313404083252   total loss: [871.19183]\n",
            "mse_b ====== [866.748352]\n",
            "It: 53832, Time: 0.03\n",
            "mse_b  [866.74835]  mse_f: 4.551573753356934   total loss: [871.2999]\n",
            "mse_b ====== [867.200073]\n",
            "It: 53833, Time: 0.03\n",
            "mse_b  [867.2001]  mse_f: 3.66617488861084   total loss: [870.8663]\n",
            "mse_b ====== [866.617798]\n",
            "It: 53834, Time: 0.03\n",
            "mse_b  [866.6178]  mse_f: 3.8776750564575195   total loss: [870.4955]\n",
            "mse_b ====== [866.916138]\n",
            "It: 53835, Time: 0.03\n",
            "mse_b  [866.91614]  mse_f: 4.072770118713379   total loss: [870.9889]\n",
            "mse_b ====== [866.930298]\n",
            "It: 53836, Time: 0.03\n",
            "mse_b  [866.9303]  mse_f: 4.417498588562012   total loss: [871.3478]\n",
            "mse_b ====== [867.223328]\n",
            "It: 53837, Time: 0.03\n",
            "mse_b  [867.2233]  mse_f: 3.63077712059021   total loss: [870.8541]\n",
            "mse_b ====== [866.720764]\n",
            "It: 53838, Time: 0.03\n",
            "mse_b  [866.72076]  mse_f: 3.478299617767334   total loss: [870.19904]\n",
            "mse_b ====== [867.247498]\n",
            "It: 53839, Time: 0.03\n",
            "mse_b  [867.2475]  mse_f: 3.8198750019073486   total loss: [871.0674]\n",
            "mse_b ====== [867.159241]\n",
            "It: 53840, Time: 0.03\n",
            "mse_b  [867.15924]  mse_f: 4.353710174560547   total loss: [871.51294]\n",
            "mse_b ====== [866.977417]\n",
            "It: 53841, Time: 0.03\n",
            "mse_b  [866.9774]  mse_f: 3.1757450103759766   total loss: [870.15314]\n",
            "mse_b ====== [867.122192]\n",
            "It: 53842, Time: 0.03\n",
            "mse_b  [867.1222]  mse_f: 3.3806607723236084   total loss: [870.50287]\n",
            "mse_b ====== [867.812744]\n",
            "It: 53843, Time: 0.03\n",
            "mse_b  [867.81274]  mse_f: 3.5766444206237793   total loss: [871.3894]\n",
            "mse_b ====== [867.44751]\n",
            "It: 53844, Time: 0.03\n",
            "mse_b  [867.4475]  mse_f: 3.778867483139038   total loss: [871.2264]\n",
            "mse_b ====== [866.584717]\n",
            "It: 53845, Time: 0.03\n",
            "mse_b  [866.5847]  mse_f: 3.3505160808563232   total loss: [869.93524]\n",
            "mse_b ====== [866.856262]\n",
            "It: 53846, Time: 0.03\n",
            "mse_b  [866.85626]  mse_f: 3.696566581726074   total loss: [870.55286]\n",
            "mse_b ====== [867.933655]\n",
            "It: 53847, Time: 0.03\n",
            "mse_b  [867.93365]  mse_f: 3.4180314540863037   total loss: [871.3517]\n",
            "mse_b ====== [867.286194]\n",
            "It: 53848, Time: 0.03\n",
            "mse_b  [867.2862]  mse_f: 3.9260783195495605   total loss: [871.2123]\n",
            "mse_b ====== [866.507507]\n",
            "It: 53849, Time: 0.03\n",
            "mse_b  [866.5075]  mse_f: 3.1152303218841553   total loss: [869.62274]\n",
            "mse_b ====== [867.180115]\n",
            "It: 53850, Time: 0.03\n",
            "mse_b  [867.1801]  mse_f: 3.045947551727295   total loss: [870.2261]\n",
            "mse_b ====== [867.723]\n",
            "It: 53851, Time: 0.03\n",
            "mse_b  [867.723]  mse_f: 3.8421523571014404   total loss: [871.5652]\n",
            "mse_b ====== [867.095825]\n",
            "It: 53852, Time: 0.04\n",
            "mse_b  [867.0958]  mse_f: 3.6587934494018555   total loss: [870.75464]\n",
            "mse_b ====== [866.511353]\n",
            "It: 53853, Time: 0.03\n",
            "mse_b  [866.51135]  mse_f: 2.7230677604675293   total loss: [869.23444]\n",
            "mse_b ====== [866.989563]\n",
            "It: 53854, Time: 0.03\n",
            "mse_b  [866.98956]  mse_f: 3.729278087615967   total loss: [870.7188]\n",
            "mse_b ====== [867.705322]\n",
            "It: 53855, Time: 0.03\n",
            "mse_b  [867.7053]  mse_f: 4.091329574584961   total loss: [871.79663]\n",
            "mse_b ====== [867.144287]\n",
            "It: 53856, Time: 0.03\n",
            "mse_b  [867.1443]  mse_f: 3.6741671562194824   total loss: [870.8185]\n",
            "mse_b ====== [866.541565]\n",
            "It: 53857, Time: 0.03\n",
            "mse_b  [866.54156]  mse_f: 3.0544521808624268   total loss: [869.596]\n",
            "mse_b ====== [867.046875]\n",
            "It: 53858, Time: 0.03\n",
            "mse_b  [867.0469]  mse_f: 3.6029977798461914   total loss: [870.6499]\n",
            "mse_b ====== [867.546448]\n",
            "It: 53859, Time: 0.03\n",
            "mse_b  [867.54645]  mse_f: 3.9798178672790527   total loss: [871.52625]\n",
            "mse_b ====== [867.113159]\n",
            "It: 53860, Time: 0.03\n",
            "mse_b  [867.11316]  mse_f: 3.905177593231201   total loss: [871.0183]\n",
            "mse_b ====== [866.526367]\n",
            "It: 53861, Time: 0.03\n",
            "mse_b  [866.52637]  mse_f: 3.210944652557373   total loss: [869.7373]\n",
            "mse_b ====== [866.951538]\n",
            "It: 53862, Time: 0.03\n",
            "mse_b  [866.95154]  mse_f: 3.360248565673828   total loss: [870.31177]\n",
            "mse_b ====== [867.382812]\n",
            "It: 53863, Time: 0.03\n",
            "mse_b  [867.3828]  mse_f: 3.790466070175171   total loss: [871.1733]\n",
            "mse_b ====== [867.296753]\n",
            "It: 53864, Time: 0.03\n",
            "mse_b  [867.29675]  mse_f: 3.432332992553711   total loss: [870.72906]\n",
            "mse_b ====== [866.799927]\n",
            "It: 53865, Time: 0.03\n",
            "mse_b  [866.7999]  mse_f: 3.050396680831909   total loss: [869.85034]\n",
            "mse_b ====== [866.861]\n",
            "It: 53866, Time: 0.04\n",
            "mse_b  [866.861]  mse_f: 3.4555869102478027   total loss: [870.3166]\n",
            "mse_b ====== [867.079285]\n",
            "It: 53867, Time: 0.03\n",
            "mse_b  [867.0793]  mse_f: 3.959174156188965   total loss: [871.03845]\n",
            "mse_b ====== [866.979553]\n",
            "It: 53868, Time: 0.04\n",
            "mse_b  [866.97955]  mse_f: 4.058856964111328   total loss: [871.0384]\n",
            "mse_b ====== [866.907227]\n",
            "It: 53869, Time: 0.03\n",
            "mse_b  [866.9072]  mse_f: 3.210766315460205   total loss: [870.118]\n",
            "mse_b ====== [866.799438]\n",
            "It: 53870, Time: 0.03\n",
            "mse_b  [866.79944]  mse_f: 3.341798782348633   total loss: [870.14124]\n",
            "mse_b ====== [866.802795]\n",
            "It: 53871, Time: 0.03\n",
            "mse_b  [866.8028]  mse_f: 4.07927131652832   total loss: [870.8821]\n",
            "mse_b ====== [866.922241]\n",
            "It: 53872, Time: 0.03\n",
            "mse_b  [866.92224]  mse_f: 4.198268890380859   total loss: [871.1205]\n",
            "mse_b ====== [866.696716]\n",
            "It: 53873, Time: 0.03\n",
            "mse_b  [866.6967]  mse_f: 3.176931858062744   total loss: [869.87366]\n",
            "mse_b ====== [866.76123]\n",
            "It: 53874, Time: 0.03\n",
            "mse_b  [866.7612]  mse_f: 3.0657503604888916   total loss: [869.82697]\n",
            "mse_b ====== [866.664917]\n",
            "It: 53875, Time: 0.04\n",
            "mse_b  [866.6649]  mse_f: 4.446005344390869   total loss: [871.1109]\n",
            "mse_b ====== [866.579407]\n",
            "It: 53876, Time: 0.03\n",
            "mse_b  [866.5794]  mse_f: 3.896040916442871   total loss: [870.47546]\n",
            "mse_b ====== [866.756531]\n",
            "It: 53877, Time: 0.03\n",
            "mse_b  [866.75653]  mse_f: 2.910479784011841   total loss: [869.667]\n",
            "mse_b ====== [866.800964]\n",
            "It: 53878, Time: 0.03\n",
            "mse_b  [866.80096]  mse_f: 3.477952003479004   total loss: [870.27893]\n",
            "mse_b ====== [866.66864]\n",
            "It: 53879, Time: 0.04\n",
            "mse_b  [866.66864]  mse_f: 4.594124794006348   total loss: [871.26276]\n",
            "mse_b ====== [866.858521]\n",
            "It: 53880, Time: 0.03\n",
            "mse_b  [866.8585]  mse_f: 3.402226448059082   total loss: [870.26074]\n",
            "mse_b ====== [866.717468]\n",
            "It: 53881, Time: 0.03\n",
            "mse_b  [866.71747]  mse_f: 3.0482120513916016   total loss: [869.7657]\n",
            "mse_b ====== [866.727234]\n",
            "It: 53882, Time: 0.04\n",
            "mse_b  [866.72723]  mse_f: 3.776170253753662   total loss: [870.5034]\n",
            "mse_b ====== [866.620544]\n",
            "It: 53883, Time: 0.03\n",
            "mse_b  [866.62054]  mse_f: 4.547947883605957   total loss: [871.1685]\n",
            "mse_b ====== [866.760376]\n",
            "It: 53884, Time: 0.03\n",
            "mse_b  [866.7604]  mse_f: 3.561441421508789   total loss: [870.32184]\n",
            "mse_b ====== [866.855591]\n",
            "It: 53885, Time: 0.03\n",
            "mse_b  [866.8556]  mse_f: 2.6852657794952393   total loss: [869.54083]\n",
            "mse_b ====== [866.883118]\n",
            "It: 53886, Time: 0.03\n",
            "mse_b  [866.8831]  mse_f: 3.628777027130127   total loss: [870.5119]\n",
            "mse_b ====== [866.857849]\n",
            "It: 53887, Time: 0.03\n",
            "mse_b  [866.85785]  mse_f: 4.006855010986328   total loss: [870.8647]\n",
            "mse_b ====== [866.844727]\n",
            "It: 53888, Time: 0.03\n",
            "mse_b  [866.8447]  mse_f: 3.092233180999756   total loss: [869.93695]\n",
            "mse_b ====== [866.806213]\n",
            "It: 53889, Time: 0.03\n",
            "mse_b  [866.8062]  mse_f: 3.069781541824341   total loss: [869.876]\n",
            "mse_b ====== [866.896729]\n",
            "It: 53890, Time: 0.03\n",
            "mse_b  [866.8967]  mse_f: 3.5102663040161133   total loss: [870.407]\n",
            "mse_b ====== [866.772095]\n",
            "It: 53891, Time: 0.03\n",
            "mse_b  [866.7721]  mse_f: 3.8492465019226074   total loss: [870.62134]\n",
            "mse_b ====== [866.861633]\n",
            "It: 53892, Time: 0.03\n",
            "mse_b  [866.86163]  mse_f: 3.3212122917175293   total loss: [870.18286]\n",
            "mse_b ====== [866.945679]\n",
            "It: 53893, Time: 0.03\n",
            "mse_b  [866.9457]  mse_f: 3.2900915145874023   total loss: [870.2358]\n",
            "mse_b ====== [866.776489]\n",
            "It: 53894, Time: 0.03\n",
            "mse_b  [866.7765]  mse_f: 3.117628335952759   total loss: [869.8941]\n",
            "mse_b ====== [866.661804]\n",
            "It: 53895, Time: 0.03\n",
            "mse_b  [866.6618]  mse_f: 3.732985496520996   total loss: [870.3948]\n",
            "mse_b ====== [866.736267]\n",
            "It: 53896, Time: 0.03\n",
            "mse_b  [866.73627]  mse_f: 3.8610148429870605   total loss: [870.5973]\n",
            "mse_b ====== [866.859741]\n",
            "It: 53897, Time: 0.03\n",
            "mse_b  [866.85974]  mse_f: 3.0029921531677246   total loss: [869.86273]\n",
            "mse_b ====== [866.736]\n",
            "It: 53898, Time: 0.03\n",
            "mse_b  [866.736]  mse_f: 3.0404577255249023   total loss: [869.7765]\n",
            "mse_b ====== [866.660889]\n",
            "It: 53899, Time: 0.03\n",
            "mse_b  [866.6609]  mse_f: 3.663238286972046   total loss: [870.3241]\n",
            "mse_b ====== [867.004883]\n",
            "It: 53900, Time: 0.03\n",
            "mse_b  [867.0049]  mse_f: 3.4199137687683105   total loss: [870.4248]\n",
            "mse_b ====== [866.701111]\n",
            "It: 53901, Time: 0.03\n",
            "mse_b  [866.7011]  mse_f: 2.9148359298706055   total loss: [869.61597]\n",
            "mse_b ====== [866.709167]\n",
            "It: 53902, Time: 0.03\n",
            "mse_b  [866.70917]  mse_f: 3.0382094383239746   total loss: [869.7474]\n",
            "mse_b ====== [866.775818]\n",
            "It: 53903, Time: 0.04\n",
            "mse_b  [866.7758]  mse_f: 3.643364429473877   total loss: [870.4192]\n",
            "mse_b ====== [866.665894]\n",
            "It: 53904, Time: 0.03\n",
            "mse_b  [866.6659]  mse_f: 3.748553991317749   total loss: [870.4144]\n",
            "mse_b ====== [866.750854]\n",
            "It: 53905, Time: 0.03\n",
            "mse_b  [866.75085]  mse_f: 2.9515912532806396   total loss: [869.70245]\n",
            "mse_b ====== [866.530579]\n",
            "It: 53906, Time: 0.03\n",
            "mse_b  [866.5306]  mse_f: 2.897470235824585   total loss: [869.42804]\n",
            "mse_b ====== [867.337463]\n",
            "It: 53907, Time: 0.04\n",
            "mse_b  [867.33746]  mse_f: 3.477614641189575   total loss: [870.81506]\n",
            "mse_b ====== [866.814331]\n",
            "It: 53908, Time: 0.03\n",
            "mse_b  [866.81433]  mse_f: 3.4906604290008545   total loss: [870.305]\n",
            "mse_b ====== [866.614685]\n",
            "It: 53909, Time: 0.03\n",
            "mse_b  [866.6147]  mse_f: 2.759243965148926   total loss: [869.3739]\n",
            "mse_b ====== [866.562805]\n",
            "It: 53910, Time: 0.03\n",
            "mse_b  [866.5628]  mse_f: 3.349224090576172   total loss: [869.91205]\n",
            "mse_b ====== [867.202576]\n",
            "It: 53911, Time: 0.03\n",
            "mse_b  [867.2026]  mse_f: 2.98756742477417   total loss: [870.1901]\n",
            "mse_b ====== [866.709534]\n",
            "It: 53912, Time: 0.03\n",
            "mse_b  [866.70953]  mse_f: 3.355642557144165   total loss: [870.0652]\n",
            "mse_b ====== [866.59021]\n",
            "It: 53913, Time: 0.03\n",
            "mse_b  [866.5902]  mse_f: 3.2295377254486084   total loss: [869.81976]\n",
            "mse_b ====== [866.793]\n",
            "It: 53914, Time: 0.03\n",
            "mse_b  [866.793]  mse_f: 3.1023764610290527   total loss: [869.8954]\n",
            "mse_b ====== [866.586243]\n",
            "It: 53915, Time: 0.03\n",
            "mse_b  [866.58624]  mse_f: 2.848876953125   total loss: [869.4351]\n",
            "mse_b ====== [866.707703]\n",
            "It: 53916, Time: 0.03\n",
            "mse_b  [866.7077]  mse_f: 3.4132235050201416   total loss: [870.1209]\n",
            "mse_b ====== [866.712524]\n",
            "It: 53917, Time: 0.03\n",
            "mse_b  [866.7125]  mse_f: 3.3369574546813965   total loss: [870.0495]\n",
            "mse_b ====== [866.51825]\n",
            "It: 53918, Time: 0.03\n",
            "mse_b  [866.51825]  mse_f: 2.83996844291687   total loss: [869.3582]\n",
            "mse_b ====== [866.519958]\n",
            "It: 53919, Time: 0.03\n",
            "mse_b  [866.51996]  mse_f: 3.2861013412475586   total loss: [869.806]\n",
            "mse_b ====== [866.584656]\n",
            "It: 53920, Time: 0.03\n",
            "mse_b  [866.58466]  mse_f: 3.285367012023926   total loss: [869.87]\n",
            "mse_b ====== [866.712708]\n",
            "It: 53921, Time: 0.03\n",
            "mse_b  [866.7127]  mse_f: 3.360325813293457   total loss: [870.07306]\n",
            "mse_b ====== [866.672546]\n",
            "It: 53922, Time: 0.03\n",
            "mse_b  [866.67255]  mse_f: 3.053635597229004   total loss: [869.7262]\n",
            "mse_b ====== [866.556213]\n",
            "It: 53923, Time: 0.03\n",
            "mse_b  [866.5562]  mse_f: 2.8679747581481934   total loss: [869.4242]\n",
            "mse_b ====== [866.519]\n",
            "It: 53924, Time: 0.03\n",
            "mse_b  [866.519]  mse_f: 3.382826089859009   total loss: [869.9018]\n",
            "mse_b ====== [866.781555]\n",
            "It: 53925, Time: 0.03\n",
            "mse_b  [866.78156]  mse_f: 3.386600971221924   total loss: [870.16815]\n",
            "mse_b ====== [866.63446]\n",
            "It: 53926, Time: 0.03\n",
            "mse_b  [866.63446]  mse_f: 3.071199893951416   total loss: [869.7057]\n",
            "mse_b ====== [866.492859]\n",
            "It: 53927, Time: 0.03\n",
            "mse_b  [866.49286]  mse_f: 2.976071357727051   total loss: [869.46893]\n",
            "mse_b ====== [866.740479]\n",
            "It: 53928, Time: 0.03\n",
            "mse_b  [866.7405]  mse_f: 3.2772488594055176   total loss: [870.0177]\n",
            "mse_b ====== [866.544861]\n",
            "It: 53929, Time: 0.03\n",
            "mse_b  [866.54486]  mse_f: 3.0467453002929688   total loss: [869.5916]\n",
            "mse_b ====== [866.682129]\n",
            "It: 53930, Time: 0.03\n",
            "mse_b  [866.6821]  mse_f: 3.042733669281006   total loss: [869.72485]\n",
            "mse_b ====== [866.442261]\n",
            "It: 53931, Time: 0.03\n",
            "mse_b  [866.44226]  mse_f: 3.691589117050171   total loss: [870.13385]\n",
            "mse_b ====== [866.505]\n",
            "It: 53932, Time: 0.03\n",
            "mse_b  [866.505]  mse_f: 2.8374714851379395   total loss: [869.34247]\n",
            "mse_b ====== [866.567505]\n",
            "It: 53933, Time: 0.03\n",
            "mse_b  [866.5675]  mse_f: 2.9655508995056152   total loss: [869.5331]\n",
            "mse_b ====== [866.786438]\n",
            "It: 53934, Time: 0.03\n",
            "mse_b  [866.78644]  mse_f: 3.0304853916168213   total loss: [869.8169]\n",
            "mse_b ====== [866.876953]\n",
            "It: 53935, Time: 0.04\n",
            "mse_b  [866.87695]  mse_f: 2.974074363708496   total loss: [869.851]\n",
            "mse_b ====== [866.443481]\n",
            "It: 53936, Time: 0.03\n",
            "mse_b  [866.4435]  mse_f: 3.0418219566345215   total loss: [869.4853]\n",
            "mse_b ====== [866.669861]\n",
            "It: 53937, Time: 0.03\n",
            "mse_b  [866.66986]  mse_f: 2.8716039657592773   total loss: [869.54144]\n",
            "mse_b ====== [866.559937]\n",
            "It: 53938, Time: 0.03\n",
            "mse_b  [866.55994]  mse_f: 2.9833831787109375   total loss: [869.54333]\n",
            "mse_b ====== [866.773865]\n",
            "It: 53939, Time: 0.03\n",
            "mse_b  [866.77386]  mse_f: 2.9923577308654785   total loss: [869.76624]\n",
            "mse_b ====== [866.527649]\n",
            "It: 53940, Time: 0.03\n",
            "mse_b  [866.52765]  mse_f: 3.1448068618774414   total loss: [869.6725]\n",
            "mse_b ====== [866.55835]\n",
            "It: 53941, Time: 0.03\n",
            "mse_b  [866.55835]  mse_f: 2.715604782104492   total loss: [869.2739]\n",
            "mse_b ====== [866.677063]\n",
            "It: 53942, Time: 0.03\n",
            "mse_b  [866.67706]  mse_f: 3.0772061347961426   total loss: [869.7543]\n",
            "mse_b ====== [866.599609]\n",
            "It: 53943, Time: 0.03\n",
            "mse_b  [866.5996]  mse_f: 3.0066447257995605   total loss: [869.60626]\n",
            "mse_b ====== [866.626038]\n",
            "It: 53944, Time: 0.03\n",
            "mse_b  [866.62604]  mse_f: 2.8114418983459473   total loss: [869.4375]\n",
            "mse_b ====== [866.631104]\n",
            "It: 53945, Time: 0.03\n",
            "mse_b  [866.6311]  mse_f: 3.4590296745300293   total loss: [870.09015]\n",
            "mse_b ====== [866.644]\n",
            "It: 53946, Time: 0.03\n",
            "mse_b  [866.644]  mse_f: 2.9330692291259766   total loss: [869.577]\n",
            "mse_b ====== [866.397522]\n",
            "It: 53947, Time: 0.03\n",
            "mse_b  [866.3975]  mse_f: 2.8904635906219482   total loss: [869.28796]\n",
            "mse_b ====== [866.491333]\n",
            "It: 53948, Time: 0.03\n",
            "mse_b  [866.49133]  mse_f: 2.9824466705322266   total loss: [869.47375]\n",
            "mse_b ====== [866.701904]\n",
            "It: 53949, Time: 0.03\n",
            "mse_b  [866.7019]  mse_f: 3.1319916248321533   total loss: [869.8339]\n",
            "mse_b ====== [866.633789]\n",
            "It: 53950, Time: 0.03\n",
            "mse_b  [866.6338]  mse_f: 2.8804712295532227   total loss: [869.5143]\n",
            "mse_b ====== [866.643921]\n",
            "It: 53951, Time: 0.03\n",
            "mse_b  [866.6439]  mse_f: 3.108699083328247   total loss: [869.7526]\n",
            "mse_b ====== [866.527893]\n",
            "It: 53952, Time: 0.03\n",
            "mse_b  [866.5279]  mse_f: 2.8696041107177734   total loss: [869.3975]\n",
            "mse_b ====== [866.645447]\n",
            "It: 53953, Time: 0.03\n",
            "mse_b  [866.64545]  mse_f: 2.8818540573120117   total loss: [869.5273]\n",
            "mse_b ====== [866.500244]\n",
            "It: 53954, Time: 0.04\n",
            "mse_b  [866.50024]  mse_f: 3.1178972721099854   total loss: [869.61816]\n",
            "mse_b ====== [866.628]\n",
            "It: 53955, Time: 0.03\n",
            "mse_b  [866.628]  mse_f: 2.8955557346343994   total loss: [869.52356]\n",
            "mse_b ====== [866.705444]\n",
            "It: 53956, Time: 0.03\n",
            "mse_b  [866.70544]  mse_f: 3.274498462677002   total loss: [869.9799]\n",
            "mse_b ====== [866.673279]\n",
            "It: 53957, Time: 0.03\n",
            "mse_b  [866.6733]  mse_f: 3.317932605743408   total loss: [869.9912]\n",
            "mse_b ====== [866.523926]\n",
            "It: 53958, Time: 0.03\n",
            "mse_b  [866.5239]  mse_f: 2.697857618331909   total loss: [869.2218]\n",
            "mse_b ====== [866.412292]\n",
            "It: 53959, Time: 0.03\n",
            "mse_b  [866.4123]  mse_f: 3.1203548908233643   total loss: [869.53265]\n",
            "mse_b ====== [866.661682]\n",
            "It: 53960, Time: 0.04\n",
            "mse_b  [866.6617]  mse_f: 2.9562206268310547   total loss: [869.6179]\n",
            "mse_b ====== [866.714844]\n",
            "It: 53961, Time: 0.03\n",
            "mse_b  [866.71484]  mse_f: 2.816591262817383   total loss: [869.53143]\n",
            "mse_b ====== [866.657227]\n",
            "It: 53962, Time: 0.04\n",
            "mse_b  [866.6572]  mse_f: 3.168654680252075   total loss: [869.82587]\n",
            "mse_b ====== [866.781799]\n",
            "It: 53963, Time: 0.04\n",
            "mse_b  [866.7818]  mse_f: 3.2814407348632812   total loss: [870.06323]\n",
            "mse_b ====== [866.591492]\n",
            "It: 53964, Time: 0.03\n",
            "mse_b  [866.5915]  mse_f: 3.026439666748047   total loss: [869.6179]\n",
            "mse_b ====== [866.465698]\n",
            "It: 53965, Time: 0.03\n",
            "mse_b  [866.4657]  mse_f: 2.9372401237487793   total loss: [869.40295]\n",
            "mse_b ====== [866.497864]\n",
            "It: 53966, Time: 0.03\n",
            "mse_b  [866.49786]  mse_f: 2.8139002323150635   total loss: [869.31177]\n",
            "mse_b ====== [866.475098]\n",
            "It: 53967, Time: 0.03\n",
            "mse_b  [866.4751]  mse_f: 3.446650981903076   total loss: [869.92175]\n",
            "mse_b ====== [866.639526]\n",
            "It: 53968, Time: 0.03\n",
            "mse_b  [866.6395]  mse_f: 3.6106653213500977   total loss: [870.2502]\n",
            "mse_b ====== [866.618469]\n",
            "It: 53969, Time: 0.03\n",
            "mse_b  [866.61847]  mse_f: 3.3123419284820557   total loss: [869.9308]\n",
            "mse_b ====== [866.570679]\n",
            "It: 53970, Time: 0.03\n",
            "mse_b  [866.5707]  mse_f: 3.0228683948516846   total loss: [869.59357]\n",
            "mse_b ====== [866.715454]\n",
            "It: 53971, Time: 0.04\n",
            "mse_b  [866.71545]  mse_f: 3.035804510116577   total loss: [869.7513]\n",
            "mse_b ====== [866.391357]\n",
            "It: 53972, Time: 0.04\n",
            "mse_b  [866.39136]  mse_f: 2.7173209190368652   total loss: [869.1087]\n",
            "mse_b ====== [866.621399]\n",
            "It: 53973, Time: 0.03\n",
            "mse_b  [866.6214]  mse_f: 2.820396900177002   total loss: [869.4418]\n",
            "mse_b ====== [866.557617]\n",
            "It: 53974, Time: 0.03\n",
            "mse_b  [866.5576]  mse_f: 3.1520376205444336   total loss: [869.70966]\n",
            "mse_b ====== [866.719238]\n",
            "It: 53975, Time: 0.03\n",
            "mse_b  [866.71924]  mse_f: 2.9609076976776123   total loss: [869.6802]\n",
            "mse_b ====== [866.514587]\n",
            "It: 53976, Time: 0.03\n",
            "mse_b  [866.5146]  mse_f: 2.9122419357299805   total loss: [869.4268]\n",
            "mse_b ====== [866.436646]\n",
            "It: 53977, Time: 0.03\n",
            "mse_b  [866.43665]  mse_f: 3.084683656692505   total loss: [869.5213]\n",
            "mse_b ====== [866.591431]\n",
            "It: 53978, Time: 0.03\n",
            "mse_b  [866.59143]  mse_f: 2.9872817993164062   total loss: [869.57874]\n",
            "mse_b ====== [866.420166]\n",
            "It: 53979, Time: 0.03\n",
            "mse_b  [866.42017]  mse_f: 2.9947359561920166   total loss: [869.4149]\n",
            "mse_b ====== [866.493286]\n",
            "It: 53980, Time: 0.03\n",
            "mse_b  [866.4933]  mse_f: 2.734097480773926   total loss: [869.22736]\n",
            "mse_b ====== [866.467]\n",
            "It: 53981, Time: 0.03\n",
            "mse_b  [866.467]  mse_f: 2.753429889678955   total loss: [869.2204]\n",
            "mse_b ====== [866.55542]\n",
            "It: 53982, Time: 0.03\n",
            "mse_b  [866.5554]  mse_f: 3.1364145278930664   total loss: [869.69183]\n",
            "mse_b ====== [866.442871]\n",
            "It: 53983, Time: 0.03\n",
            "mse_b  [866.4429]  mse_f: 2.9535536766052246   total loss: [869.3964]\n",
            "mse_b ====== [866.571106]\n",
            "It: 53984, Time: 0.03\n",
            "mse_b  [866.5711]  mse_f: 3.0276644229888916   total loss: [869.59875]\n",
            "mse_b ====== [866.84906]\n",
            "It: 53985, Time: 0.03\n",
            "mse_b  [866.84906]  mse_f: 3.3610267639160156   total loss: [870.2101]\n",
            "mse_b ====== [866.638611]\n",
            "It: 53986, Time: 0.03\n",
            "mse_b  [866.6386]  mse_f: 3.309271812438965   total loss: [869.9479]\n",
            "mse_b ====== [866.604797]\n",
            "It: 53987, Time: 0.03\n",
            "mse_b  [866.6048]  mse_f: 3.3339972496032715   total loss: [869.9388]\n",
            "mse_b ====== [866.514832]\n",
            "It: 53988, Time: 0.03\n",
            "mse_b  [866.51483]  mse_f: 3.4559383392333984   total loss: [869.97076]\n",
            "mse_b ====== [866.808]\n",
            "It: 53989, Time: 0.03\n",
            "mse_b  [866.808]  mse_f: 3.5007617473602295   total loss: [870.3087]\n",
            "mse_b ====== [866.892395]\n",
            "It: 53990, Time: 0.03\n",
            "mse_b  [866.8924]  mse_f: 3.704840660095215   total loss: [870.5972]\n",
            "mse_b ====== [866.647339]\n",
            "It: 53991, Time: 0.03\n",
            "mse_b  [866.64734]  mse_f: 3.7385473251342773   total loss: [870.38586]\n",
            "mse_b ====== [866.609314]\n",
            "It: 53992, Time: 0.03\n",
            "mse_b  [866.6093]  mse_f: 3.1499745845794678   total loss: [869.7593]\n",
            "mse_b ====== [866.467346]\n",
            "It: 53993, Time: 0.03\n",
            "mse_b  [866.46735]  mse_f: 3.2477314472198486   total loss: [869.7151]\n",
            "mse_b ====== [866.432678]\n",
            "It: 53994, Time: 0.03\n",
            "mse_b  [866.4327]  mse_f: 3.008678913116455   total loss: [869.44135]\n",
            "mse_b ====== [866.349426]\n",
            "It: 53995, Time: 0.03\n",
            "mse_b  [866.3494]  mse_f: 2.574739456176758   total loss: [868.9242]\n",
            "mse_b ====== [866.475403]\n",
            "It: 53996, Time: 0.03\n",
            "mse_b  [866.4754]  mse_f: 3.183305263519287   total loss: [869.6587]\n",
            "mse_b ====== [866.54]\n",
            "It: 53997, Time: 0.03\n",
            "mse_b  [866.54]  mse_f: 2.8741703033447266   total loss: [869.4141]\n",
            "mse_b ====== [866.648315]\n",
            "It: 53998, Time: 0.03\n",
            "mse_b  [866.6483]  mse_f: 2.9168496131896973   total loss: [869.5652]\n",
            "mse_b ====== [866.625488]\n",
            "It: 53999, Time: 0.04\n",
            "mse_b  [866.6255]  mse_f: 3.021226406097412   total loss: [869.6467]\n",
            "mse_b ====== [866.656555]\n",
            "It: 54000, Time: 0.03\n",
            "mse_b  [866.65656]  mse_f: 3.132434129714966   total loss: [869.789]\n",
            "mse_b ====== [866.607788]\n",
            "It: 54001, Time: 0.03\n",
            "mse_b  [866.6078]  mse_f: 3.197246551513672   total loss: [869.80505]\n",
            "mse_b ====== [866.41571]\n",
            "It: 54002, Time: 0.03\n",
            "mse_b  [866.4157]  mse_f: 3.094420909881592   total loss: [869.51013]\n",
            "mse_b ====== [866.534851]\n",
            "It: 54003, Time: 0.03\n",
            "mse_b  [866.53485]  mse_f: 2.646251916885376   total loss: [869.1811]\n",
            "mse_b ====== [866.453674]\n",
            "It: 54004, Time: 0.03\n",
            "mse_b  [866.4537]  mse_f: 2.834929943084717   total loss: [869.2886]\n",
            "mse_b ====== [866.549377]\n",
            "It: 54005, Time: 0.03\n",
            "mse_b  [866.5494]  mse_f: 3.0756688117980957   total loss: [869.62506]\n",
            "mse_b ====== [866.38855]\n",
            "It: 54006, Time: 0.03\n",
            "mse_b  [866.38855]  mse_f: 2.9306445121765137   total loss: [869.3192]\n",
            "mse_b ====== [866.752808]\n",
            "It: 54007, Time: 0.03\n",
            "mse_b  [866.7528]  mse_f: 3.4186370372772217   total loss: [870.17145]\n",
            "mse_b ====== [866.925903]\n",
            "It: 54008, Time: 0.03\n",
            "mse_b  [866.9259]  mse_f: 3.4834342002868652   total loss: [870.40936]\n",
            "mse_b ====== [866.68988]\n",
            "It: 54009, Time: 0.03\n",
            "mse_b  [866.6899]  mse_f: 3.3327255249023438   total loss: [870.0226]\n",
            "mse_b ====== [866.541687]\n",
            "It: 54010, Time: 0.03\n",
            "mse_b  [866.5417]  mse_f: 3.334646701812744   total loss: [869.87634]\n",
            "mse_b ====== [866.375732]\n",
            "It: 54011, Time: 0.03\n",
            "mse_b  [866.37573]  mse_f: 3.2131576538085938   total loss: [869.58887]\n",
            "mse_b ====== [866.495789]\n",
            "It: 54012, Time: 0.03\n",
            "mse_b  [866.4958]  mse_f: 3.057021379470825   total loss: [869.5528]\n",
            "mse_b ====== [866.500427]\n",
            "It: 54013, Time: 0.03\n",
            "mse_b  [866.5004]  mse_f: 2.7512011528015137   total loss: [869.25165]\n",
            "mse_b ====== [866.414551]\n",
            "It: 54014, Time: 0.03\n",
            "mse_b  [866.41455]  mse_f: 2.844954013824463   total loss: [869.2595]\n",
            "mse_b ====== [866.424377]\n",
            "It: 54015, Time: 0.03\n",
            "mse_b  [866.4244]  mse_f: 2.7779226303100586   total loss: [869.2023]\n",
            "mse_b ====== [866.432739]\n",
            "It: 54016, Time: 0.03\n",
            "mse_b  [866.43274]  mse_f: 3.2982616424560547   total loss: [869.731]\n",
            "mse_b ====== [866.579163]\n",
            "It: 54017, Time: 0.04\n",
            "mse_b  [866.57916]  mse_f: 3.14387845993042   total loss: [869.723]\n",
            "mse_b ====== [866.457092]\n",
            "It: 54018, Time: 0.03\n",
            "mse_b  [866.4571]  mse_f: 3.7903871536254883   total loss: [870.2475]\n",
            "mse_b ====== [866.895935]\n",
            "It: 54019, Time: 0.03\n",
            "mse_b  [866.89594]  mse_f: 3.981130599975586   total loss: [870.8771]\n",
            "mse_b ====== [866.737244]\n",
            "It: 54020, Time: 0.03\n",
            "mse_b  [866.73724]  mse_f: 3.7354397773742676   total loss: [870.47266]\n",
            "mse_b ====== [866.553894]\n",
            "It: 54021, Time: 0.03\n",
            "mse_b  [866.5539]  mse_f: 3.2906699180603027   total loss: [869.84454]\n",
            "mse_b ====== [866.45929]\n",
            "It: 54022, Time: 0.03\n",
            "mse_b  [866.4593]  mse_f: 3.061272382736206   total loss: [869.52057]\n",
            "mse_b ====== [866.703186]\n",
            "It: 54023, Time: 0.03\n",
            "mse_b  [866.7032]  mse_f: 2.767136573791504   total loss: [869.47034]\n",
            "mse_b ====== [866.698792]\n",
            "It: 54024, Time: 0.03\n",
            "mse_b  [866.6988]  mse_f: 2.7989871501922607   total loss: [869.4978]\n",
            "mse_b ====== [866.468323]\n",
            "It: 54025, Time: 0.03\n",
            "mse_b  [866.4683]  mse_f: 2.9890880584716797   total loss: [869.4574]\n",
            "mse_b ====== [866.644226]\n",
            "It: 54026, Time: 0.03\n",
            "mse_b  [866.6442]  mse_f: 2.8324646949768066   total loss: [869.4767]\n",
            "mse_b ====== [866.635193]\n",
            "It: 54027, Time: 0.03\n",
            "mse_b  [866.6352]  mse_f: 3.2273077964782715   total loss: [869.8625]\n",
            "mse_b ====== [866.460083]\n",
            "It: 54028, Time: 0.03\n",
            "mse_b  [866.4601]  mse_f: 3.591270685195923   total loss: [870.05133]\n",
            "mse_b ====== [866.457764]\n",
            "It: 54029, Time: 0.03\n",
            "mse_b  [866.45776]  mse_f: 3.38438081741333   total loss: [869.84216]\n",
            "mse_b ====== [866.594727]\n",
            "It: 54030, Time: 0.03\n",
            "mse_b  [866.5947]  mse_f: 3.688028335571289   total loss: [870.2828]\n",
            "mse_b ====== [866.518]\n",
            "It: 54031, Time: 0.03\n",
            "mse_b  [866.518]  mse_f: 3.2423152923583984   total loss: [869.7603]\n",
            "mse_b ====== [866.654]\n",
            "It: 54032, Time: 0.03\n",
            "mse_b  [866.654]  mse_f: 2.8566532135009766   total loss: [869.5106]\n",
            "mse_b ====== [866.427185]\n",
            "It: 54033, Time: 0.03\n",
            "mse_b  [866.4272]  mse_f: 2.8911349773406982   total loss: [869.3183]\n",
            "mse_b ====== [866.552246]\n",
            "It: 54034, Time: 0.03\n",
            "mse_b  [866.55225]  mse_f: 2.8533236980438232   total loss: [869.4056]\n",
            "mse_b ====== [866.928833]\n",
            "It: 54035, Time: 0.03\n",
            "mse_b  [866.92883]  mse_f: 3.0179519653320312   total loss: [869.9468]\n",
            "mse_b ====== [866.775452]\n",
            "It: 54036, Time: 0.04\n",
            "mse_b  [866.77545]  mse_f: 3.748532772064209   total loss: [870.524]\n",
            "mse_b ====== [866.672668]\n",
            "It: 54037, Time: 0.03\n",
            "mse_b  [866.67267]  mse_f: 3.5069820880889893   total loss: [870.1796]\n",
            "mse_b ====== [866.492371]\n",
            "It: 54038, Time: 0.03\n",
            "mse_b  [866.4924]  mse_f: 3.8227875232696533   total loss: [870.3152]\n",
            "mse_b ====== [866.461]\n",
            "It: 54039, Time: 0.03\n",
            "mse_b  [866.461]  mse_f: 3.647692918777466   total loss: [870.1087]\n",
            "mse_b ====== [866.498352]\n",
            "It: 54040, Time: 0.03\n",
            "mse_b  [866.49835]  mse_f: 2.850436210632324   total loss: [869.3488]\n",
            "mse_b ====== [866.464966]\n",
            "It: 54041, Time: 0.03\n",
            "mse_b  [866.46497]  mse_f: 2.811849355697632   total loss: [869.2768]\n",
            "mse_b ====== [866.372864]\n",
            "It: 54042, Time: 0.03\n",
            "mse_b  [866.37286]  mse_f: 3.0301239490509033   total loss: [869.403]\n",
            "mse_b ====== [866.63208]\n",
            "It: 54043, Time: 0.03\n",
            "mse_b  [866.6321]  mse_f: 3.0362389087677   total loss: [869.66833]\n",
            "mse_b ====== [866.595215]\n",
            "It: 54044, Time: 0.03\n",
            "mse_b  [866.5952]  mse_f: 3.4815001487731934   total loss: [870.0767]\n",
            "mse_b ====== [866.717224]\n",
            "It: 54045, Time: 0.04\n",
            "mse_b  [866.7172]  mse_f: 3.966691493988037   total loss: [870.6839]\n",
            "mse_b ====== [866.934937]\n",
            "It: 54046, Time: 0.03\n",
            "mse_b  [866.93494]  mse_f: 4.059313774108887   total loss: [870.99426]\n",
            "mse_b ====== [866.84967]\n",
            "It: 54047, Time: 0.03\n",
            "mse_b  [866.8497]  mse_f: 4.60418176651001   total loss: [871.45386]\n",
            "mse_b ====== [866.720825]\n",
            "It: 54048, Time: 0.03\n",
            "mse_b  [866.7208]  mse_f: 3.328238010406494   total loss: [870.0491]\n",
            "mse_b ====== [866.540771]\n",
            "It: 54049, Time: 0.03\n",
            "mse_b  [866.5408]  mse_f: 2.863502025604248   total loss: [869.4043]\n",
            "mse_b ====== [866.553406]\n",
            "It: 54050, Time: 0.03\n",
            "mse_b  [866.5534]  mse_f: 3.0794873237609863   total loss: [869.6329]\n",
            "mse_b ====== [866.897217]\n",
            "It: 54051, Time: 0.03\n",
            "mse_b  [866.8972]  mse_f: 3.6732945442199707   total loss: [870.5705]\n",
            "mse_b ====== [866.887329]\n",
            "It: 54052, Time: 0.03\n",
            "mse_b  [866.8873]  mse_f: 4.633478164672852   total loss: [871.5208]\n",
            "mse_b ====== [867.174744]\n",
            "It: 54053, Time: 0.04\n",
            "mse_b  [867.17474]  mse_f: 4.575148582458496   total loss: [871.7499]\n",
            "mse_b ====== [866.876038]\n",
            "It: 54054, Time: 0.03\n",
            "mse_b  [866.87604]  mse_f: 3.6541876792907715   total loss: [870.5302]\n",
            "mse_b ====== [866.755493]\n",
            "It: 54055, Time: 0.03\n",
            "mse_b  [866.7555]  mse_f: 2.957484722137451   total loss: [869.71295]\n",
            "mse_b ====== [866.806152]\n",
            "It: 54056, Time: 0.03\n",
            "mse_b  [866.80615]  mse_f: 3.1520962715148926   total loss: [869.95825]\n",
            "mse_b ====== [866.78717]\n",
            "It: 54057, Time: 0.03\n",
            "mse_b  [866.7872]  mse_f: 3.760565996170044   total loss: [870.5477]\n",
            "mse_b ====== [867.029]\n",
            "It: 54058, Time: 0.03\n",
            "mse_b  [867.029]  mse_f: 4.427414894104004   total loss: [871.4564]\n",
            "mse_b ====== [866.777954]\n",
            "It: 54059, Time: 0.03\n",
            "mse_b  [866.77795]  mse_f: 4.58765172958374   total loss: [871.3656]\n",
            "mse_b ====== [866.750305]\n",
            "It: 54060, Time: 0.03\n",
            "mse_b  [866.7503]  mse_f: 3.280217409133911   total loss: [870.0305]\n",
            "mse_b ====== [867.058044]\n",
            "It: 54061, Time: 0.03\n",
            "mse_b  [867.05804]  mse_f: 3.042393445968628   total loss: [870.10046]\n",
            "mse_b ====== [866.870605]\n",
            "It: 54062, Time: 0.03\n",
            "mse_b  [866.8706]  mse_f: 3.991842269897461   total loss: [870.8624]\n",
            "mse_b ====== [866.842]\n",
            "It: 54063, Time: 0.03\n",
            "mse_b  [866.842]  mse_f: 4.942332744598389   total loss: [871.7843]\n",
            "mse_b ====== [866.789]\n",
            "It: 54064, Time: 0.03\n",
            "mse_b  [866.789]  mse_f: 4.791766166687012   total loss: [871.58075]\n",
            "mse_b ====== [867.016907]\n",
            "It: 54065, Time: 0.03\n",
            "mse_b  [867.0169]  mse_f: 3.730109691619873   total loss: [870.747]\n",
            "mse_b ====== [867.080811]\n",
            "It: 54066, Time: 0.03\n",
            "mse_b  [867.0808]  mse_f: 3.187933921813965   total loss: [870.26874]\n",
            "mse_b ====== [867.227478]\n",
            "It: 54067, Time: 0.03\n",
            "mse_b  [867.2275]  mse_f: 4.558938026428223   total loss: [871.78644]\n",
            "mse_b ====== [867.420349]\n",
            "It: 54068, Time: 0.03\n",
            "mse_b  [867.42035]  mse_f: 4.445336818695068   total loss: [871.86566]\n",
            "mse_b ====== [867.112671]\n",
            "It: 54069, Time: 0.03\n",
            "mse_b  [867.1127]  mse_f: 4.216031551361084   total loss: [871.3287]\n",
            "mse_b ====== [866.803345]\n",
            "It: 54070, Time: 0.03\n",
            "mse_b  [866.80334]  mse_f: 3.5222270488739014   total loss: [870.32556]\n",
            "mse_b ====== [866.886902]\n",
            "It: 54071, Time: 0.03\n",
            "mse_b  [866.8869]  mse_f: 3.5929200649261475   total loss: [870.4798]\n",
            "mse_b ====== [867.541382]\n",
            "It: 54072, Time: 0.03\n",
            "mse_b  [867.5414]  mse_f: 3.577812433242798   total loss: [871.1192]\n",
            "mse_b ====== [868.112427]\n",
            "It: 54073, Time: 0.04\n",
            "mse_b  [868.1124]  mse_f: 3.8944733142852783   total loss: [872.0069]\n",
            "mse_b ====== [867.021423]\n",
            "It: 54074, Time: 0.03\n",
            "mse_b  [867.0214]  mse_f: 3.655442714691162   total loss: [870.6769]\n",
            "mse_b ====== [866.752258]\n",
            "It: 54075, Time: 0.03\n",
            "mse_b  [866.75226]  mse_f: 2.862551689147949   total loss: [869.6148]\n",
            "mse_b ====== [866.672607]\n",
            "It: 54076, Time: 0.03\n",
            "mse_b  [866.6726]  mse_f: 3.3812801837921143   total loss: [870.0539]\n",
            "mse_b ====== [867.552185]\n",
            "It: 54077, Time: 0.03\n",
            "mse_b  [867.5522]  mse_f: 3.909562826156616   total loss: [871.46173]\n",
            "mse_b ====== [867.932]\n",
            "It: 54078, Time: 0.03\n",
            "mse_b  [867.932]  mse_f: 3.316920042037964   total loss: [871.2489]\n",
            "mse_b ====== [867.161316]\n",
            "It: 54079, Time: 0.03\n",
            "mse_b  [867.1613]  mse_f: 3.503077983856201   total loss: [870.66437]\n",
            "mse_b ====== [866.464966]\n",
            "It: 54080, Time: 0.03\n",
            "mse_b  [866.46497]  mse_f: 2.8492746353149414   total loss: [869.3143]\n",
            "mse_b ====== [866.875122]\n",
            "It: 54081, Time: 0.03\n",
            "mse_b  [866.8751]  mse_f: 3.553560733795166   total loss: [870.4287]\n",
            "mse_b ====== [867.555969]\n",
            "It: 54082, Time: 0.03\n",
            "mse_b  [867.55597]  mse_f: 3.6216044425964355   total loss: [871.17755]\n",
            "mse_b ====== [867.475]\n",
            "It: 54083, Time: 0.03\n",
            "mse_b  [867.475]  mse_f: 3.6977906227111816   total loss: [871.1728]\n",
            "mse_b ====== [866.770386]\n",
            "It: 54084, Time: 0.03\n",
            "mse_b  [866.7704]  mse_f: 2.9319803714752197   total loss: [869.7024]\n",
            "mse_b ====== [867.034485]\n",
            "It: 54085, Time: 0.03\n",
            "mse_b  [867.0345]  mse_f: 3.3559279441833496   total loss: [870.39044]\n",
            "mse_b ====== [866.812317]\n",
            "It: 54086, Time: 0.03\n",
            "mse_b  [866.8123]  mse_f: 3.8008389472961426   total loss: [870.61316]\n",
            "mse_b ====== [867.335754]\n",
            "It: 54087, Time: 0.03\n",
            "mse_b  [867.33575]  mse_f: 3.780696392059326   total loss: [871.11646]\n",
            "mse_b ====== [867.047424]\n",
            "It: 54088, Time: 0.03\n",
            "mse_b  [867.0474]  mse_f: 3.161226749420166   total loss: [870.2087]\n",
            "mse_b ====== [866.557068]\n",
            "It: 54089, Time: 0.03\n",
            "mse_b  [866.55707]  mse_f: 3.4476499557495117   total loss: [870.0047]\n",
            "mse_b ====== [866.776733]\n",
            "It: 54090, Time: 0.03\n",
            "mse_b  [866.77673]  mse_f: 3.603304386138916   total loss: [870.38007]\n",
            "mse_b ====== [866.967]\n",
            "It: 54091, Time: 0.03\n",
            "mse_b  [866.967]  mse_f: 4.3244476318359375   total loss: [871.29144]\n",
            "mse_b ====== [866.829712]\n",
            "It: 54092, Time: 0.04\n",
            "mse_b  [866.8297]  mse_f: 3.7984066009521484   total loss: [870.6281]\n",
            "mse_b ====== [866.662903]\n",
            "It: 54093, Time: 0.04\n",
            "mse_b  [866.6629]  mse_f: 3.365079879760742   total loss: [870.02795]\n",
            "mse_b ====== [866.937256]\n",
            "It: 54094, Time: 0.03\n",
            "mse_b  [866.93726]  mse_f: 3.0341408252716064   total loss: [869.9714]\n",
            "mse_b ====== [866.917969]\n",
            "It: 54095, Time: 0.04\n",
            "mse_b  [866.91797]  mse_f: 3.7688677310943604   total loss: [870.6868]\n",
            "mse_b ====== [866.582031]\n",
            "It: 54096, Time: 0.03\n",
            "mse_b  [866.58203]  mse_f: 4.473579406738281   total loss: [871.0556]\n",
            "mse_b ====== [866.598877]\n",
            "It: 54097, Time: 0.03\n",
            "mse_b  [866.5989]  mse_f: 4.487068176269531   total loss: [871.08594]\n",
            "mse_b ====== [866.448914]\n",
            "It: 54098, Time: 0.05\n",
            "mse_b  [866.4489]  mse_f: 2.9494736194610596   total loss: [869.3984]\n",
            "mse_b ====== [866.703064]\n",
            "It: 54099, Time: 0.05\n",
            "mse_b  [866.70306]  mse_f: 3.296933650970459   total loss: [870.]\n",
            "mse_b ====== [866.623169]\n",
            "It: 54100, Time: 0.03\n",
            "mse_b  [866.62317]  mse_f: 3.6334643363952637   total loss: [870.25665]\n",
            "mse_b ====== [866.568787]\n",
            "It: 54101, Time: 0.03\n",
            "mse_b  [866.5688]  mse_f: 4.040841579437256   total loss: [870.6096]\n",
            "mse_b ====== [866.67]\n",
            "It: 54102, Time: 0.03\n",
            "mse_b  [866.67]  mse_f: 3.163889169692993   total loss: [869.83386]\n",
            "mse_b ====== [866.514038]\n",
            "It: 54103, Time: 0.03\n",
            "mse_b  [866.51404]  mse_f: 3.561128854751587   total loss: [870.0752]\n",
            "mse_b ====== [866.406555]\n",
            "It: 54104, Time: 0.03\n",
            "mse_b  [866.40656]  mse_f: 2.8516180515289307   total loss: [869.2582]\n",
            "mse_b ====== [866.492188]\n",
            "It: 54105, Time: 0.03\n",
            "mse_b  [866.4922]  mse_f: 3.9531500339508057   total loss: [870.4453]\n",
            "mse_b ====== [866.711243]\n",
            "It: 54106, Time: 0.04\n",
            "mse_b  [866.71124]  mse_f: 3.4979820251464844   total loss: [870.2092]\n",
            "mse_b ====== [866.703918]\n",
            "It: 54107, Time: 0.04\n",
            "mse_b  [866.7039]  mse_f: 3.1913678646087646   total loss: [869.89526]\n",
            "mse_b ====== [866.490356]\n",
            "It: 54108, Time: 0.03\n",
            "mse_b  [866.49036]  mse_f: 2.972926139831543   total loss: [869.46326]\n",
            "mse_b ====== [866.583923]\n",
            "It: 54109, Time: 0.03\n",
            "mse_b  [866.5839]  mse_f: 3.515920400619507   total loss: [870.09985]\n",
            "mse_b ====== [866.577454]\n",
            "It: 54110, Time: 0.03\n",
            "mse_b  [866.57745]  mse_f: 3.2856404781341553   total loss: [869.8631]\n",
            "mse_b ====== [866.833557]\n",
            "It: 54111, Time: 0.03\n",
            "mse_b  [866.83356]  mse_f: 3.3016610145568848   total loss: [870.1352]\n",
            "mse_b ====== [866.426697]\n",
            "It: 54112, Time: 0.03\n",
            "mse_b  [866.4267]  mse_f: 3.068499803543091   total loss: [869.4952]\n",
            "mse_b ====== [866.368042]\n",
            "It: 54113, Time: 0.03\n",
            "mse_b  [866.36804]  mse_f: 3.579085350036621   total loss: [869.94714]\n",
            "mse_b ====== [866.580383]\n",
            "It: 54114, Time: 0.03\n",
            "mse_b  [866.5804]  mse_f: 3.2696311473846436   total loss: [869.85004]\n",
            "mse_b ====== [866.684265]\n",
            "It: 54115, Time: 0.03\n",
            "mse_b  [866.68427]  mse_f: 3.490694761276245   total loss: [870.175]\n",
            "mse_b ====== [866.430237]\n",
            "It: 54116, Time: 0.03\n",
            "mse_b  [866.43024]  mse_f: 2.7446906566619873   total loss: [869.1749]\n",
            "mse_b ====== [866.485229]\n",
            "It: 54117, Time: 0.03\n",
            "mse_b  [866.4852]  mse_f: 3.6134226322174072   total loss: [870.09863]\n",
            "mse_b ====== [866.592896]\n",
            "It: 54118, Time: 0.04\n",
            "mse_b  [866.5929]  mse_f: 3.172151565551758   total loss: [869.7651]\n",
            "mse_b ====== [866.556]\n",
            "It: 54119, Time: 0.03\n",
            "mse_b  [866.556]  mse_f: 3.4121639728546143   total loss: [869.9682]\n",
            "mse_b ====== [866.527649]\n",
            "It: 54120, Time: 0.03\n",
            "mse_b  [866.52765]  mse_f: 3.0603623390197754   total loss: [869.588]\n",
            "mse_b ====== [866.652832]\n",
            "It: 54121, Time: 0.03\n",
            "mse_b  [866.65283]  mse_f: 3.340237617492676   total loss: [869.99304]\n",
            "mse_b ====== [866.575073]\n",
            "It: 54122, Time: 0.03\n",
            "mse_b  [866.5751]  mse_f: 2.79183292388916   total loss: [869.3669]\n",
            "mse_b ====== [866.924561]\n",
            "It: 54123, Time: 0.03\n",
            "mse_b  [866.92456]  mse_f: 2.9844377040863037   total loss: [869.909]\n",
            "mse_b ====== [866.735718]\n",
            "It: 54124, Time: 0.03\n",
            "mse_b  [866.7357]  mse_f: 3.057245969772339   total loss: [869.79297]\n",
            "mse_b ====== [866.615662]\n",
            "It: 54125, Time: 0.03\n",
            "mse_b  [866.61566]  mse_f: 3.3184220790863037   total loss: [869.9341]\n",
            "mse_b ====== [866.550476]\n",
            "It: 54126, Time: 0.04\n",
            "mse_b  [866.5505]  mse_f: 2.9347567558288574   total loss: [869.4852]\n",
            "mse_b ====== [866.52771]\n",
            "It: 54127, Time: 0.03\n",
            "mse_b  [866.5277]  mse_f: 3.2937445640563965   total loss: [869.8215]\n",
            "mse_b ====== [866.5578]\n",
            "It: 54128, Time: 0.03\n",
            "mse_b  [866.5578]  mse_f: 2.7522923946380615   total loss: [869.3101]\n",
            "mse_b ====== [866.764343]\n",
            "It: 54129, Time: 0.03\n",
            "mse_b  [866.76434]  mse_f: 3.2555816173553467   total loss: [870.0199]\n",
            "mse_b ====== [866.732849]\n",
            "It: 54130, Time: 0.03\n",
            "mse_b  [866.73285]  mse_f: 2.935521125793457   total loss: [869.6684]\n",
            "mse_b ====== [866.693665]\n",
            "It: 54131, Time: 0.03\n",
            "mse_b  [866.69366]  mse_f: 3.1679224967956543   total loss: [869.8616]\n",
            "mse_b ====== [866.353638]\n",
            "It: 54132, Time: 0.03\n",
            "mse_b  [866.35364]  mse_f: 2.863300085067749   total loss: [869.2169]\n",
            "mse_b ====== [866.615723]\n",
            "It: 54133, Time: 0.03\n",
            "mse_b  [866.6157]  mse_f: 3.0469701290130615   total loss: [869.6627]\n",
            "mse_b ====== [866.833435]\n",
            "It: 54134, Time: 0.03\n",
            "mse_b  [866.83344]  mse_f: 2.760981559753418   total loss: [869.5944]\n",
            "mse_b ====== [866.921387]\n",
            "It: 54135, Time: 0.03\n",
            "mse_b  [866.9214]  mse_f: 3.0672879219055176   total loss: [869.98865]\n",
            "mse_b ====== [866.563904]\n",
            "It: 54136, Time: 0.03\n",
            "mse_b  [866.5639]  mse_f: 2.8763267993927   total loss: [869.44025]\n",
            "mse_b ====== [866.332825]\n",
            "It: 54137, Time: 0.03\n",
            "mse_b  [866.3328]  mse_f: 3.2317678928375244   total loss: [869.5646]\n",
            "mse_b ====== [866.370544]\n",
            "It: 54138, Time: 0.03\n",
            "mse_b  [866.37054]  mse_f: 3.101562976837158   total loss: [869.4721]\n",
            "mse_b ====== [866.591125]\n",
            "It: 54139, Time: 0.03\n",
            "mse_b  [866.5911]  mse_f: 2.9497876167297363   total loss: [869.5409]\n",
            "mse_b ====== [866.664551]\n",
            "It: 54140, Time: 0.03\n",
            "mse_b  [866.66455]  mse_f: 2.7736854553222656   total loss: [869.43823]\n",
            "mse_b ====== [866.720764]\n",
            "It: 54141, Time: 0.03\n",
            "mse_b  [866.72076]  mse_f: 3.250455141067505   total loss: [869.9712]\n",
            "mse_b ====== [866.495178]\n",
            "It: 54142, Time: 0.03\n",
            "mse_b  [866.4952]  mse_f: 2.9132864475250244   total loss: [869.40845]\n",
            "mse_b ====== [866.432922]\n",
            "It: 54143, Time: 0.03\n",
            "mse_b  [866.4329]  mse_f: 3.089481830596924   total loss: [869.5224]\n",
            "mse_b ====== [866.371155]\n",
            "It: 54144, Time: 0.03\n",
            "mse_b  [866.37115]  mse_f: 2.829735279083252   total loss: [869.20087]\n",
            "mse_b ====== [866.397339]\n",
            "It: 54145, Time: 0.03\n",
            "mse_b  [866.39734]  mse_f: 3.221388339996338   total loss: [869.6187]\n",
            "mse_b ====== [866.526245]\n",
            "It: 54146, Time: 0.04\n",
            "mse_b  [866.52625]  mse_f: 2.9848837852478027   total loss: [869.5111]\n",
            "mse_b ====== [866.844666]\n",
            "It: 54147, Time: 0.03\n",
            "mse_b  [866.84467]  mse_f: 3.035889148712158   total loss: [869.88055]\n",
            "mse_b ====== [866.499268]\n",
            "It: 54148, Time: 0.04\n",
            "mse_b  [866.49927]  mse_f: 2.7445931434631348   total loss: [869.24384]\n",
            "mse_b ====== [866.410156]\n",
            "It: 54149, Time: 0.03\n",
            "mse_b  [866.41016]  mse_f: 3.0551233291625977   total loss: [869.4653]\n",
            "mse_b ====== [866.350708]\n",
            "It: 54150, Time: 0.03\n",
            "mse_b  [866.3507]  mse_f: 2.8941359519958496   total loss: [869.2449]\n",
            "mse_b ====== [866.49469]\n",
            "It: 54151, Time: 0.03\n",
            "mse_b  [866.4947]  mse_f: 3.038331985473633   total loss: [869.533]\n",
            "mse_b ====== [866.800903]\n",
            "It: 54152, Time: 0.03\n",
            "mse_b  [866.8009]  mse_f: 2.8497748374938965   total loss: [869.6507]\n",
            "mse_b ====== [866.688965]\n",
            "It: 54153, Time: 0.04\n",
            "mse_b  [866.68896]  mse_f: 3.022855758666992   total loss: [869.7118]\n",
            "mse_b ====== [866.644104]\n",
            "It: 54154, Time: 0.03\n",
            "mse_b  [866.6441]  mse_f: 2.7470595836639404   total loss: [869.3912]\n",
            "mse_b ====== [866.581543]\n",
            "It: 54155, Time: 0.03\n",
            "mse_b  [866.58154]  mse_f: 2.9580883979797363   total loss: [869.5396]\n",
            "mse_b ====== [866.333]\n",
            "It: 54156, Time: 0.03\n",
            "mse_b  [866.333]  mse_f: 2.7195498943328857   total loss: [869.05255]\n",
            "mse_b ====== [866.534729]\n",
            "It: 54157, Time: 0.03\n",
            "mse_b  [866.5347]  mse_f: 2.8456599712371826   total loss: [869.3804]\n",
            "mse_b ====== [866.717651]\n",
            "It: 54158, Time: 0.03\n",
            "mse_b  [866.71765]  mse_f: 2.858210563659668   total loss: [869.57587]\n",
            "mse_b ====== [866.83374]\n",
            "It: 54159, Time: 0.03\n",
            "mse_b  [866.83374]  mse_f: 2.8675146102905273   total loss: [869.70123]\n",
            "mse_b ====== [866.46991]\n",
            "It: 54160, Time: 0.03\n",
            "mse_b  [866.4699]  mse_f: 2.6005420684814453   total loss: [869.07043]\n",
            "mse_b ====== [866.411316]\n",
            "It: 54161, Time: 0.03\n",
            "mse_b  [866.4113]  mse_f: 3.099260091781616   total loss: [869.51056]\n",
            "mse_b ====== [866.44]\n",
            "It: 54162, Time: 0.04\n",
            "mse_b  [866.44]  mse_f: 2.8110504150390625   total loss: [869.25104]\n",
            "mse_b ====== [866.814941]\n",
            "It: 54163, Time: 0.03\n",
            "mse_b  [866.81494]  mse_f: 2.910034656524658   total loss: [869.725]\n",
            "mse_b ====== [866.704529]\n",
            "It: 54164, Time: 0.04\n",
            "mse_b  [866.7045]  mse_f: 2.996453285217285   total loss: [869.701]\n",
            "mse_b ====== [866.649963]\n",
            "It: 54165, Time: 0.03\n",
            "mse_b  [866.64996]  mse_f: 3.1924691200256348   total loss: [869.8424]\n",
            "mse_b ====== [866.495056]\n",
            "It: 54166, Time: 0.03\n",
            "mse_b  [866.49506]  mse_f: 3.2408571243286133   total loss: [869.7359]\n",
            "mse_b ====== [866.512085]\n",
            "It: 54167, Time: 0.03\n",
            "mse_b  [866.5121]  mse_f: 3.761176824569702   total loss: [870.27325]\n",
            "mse_b ====== [866.62384]\n",
            "It: 54168, Time: 0.03\n",
            "mse_b  [866.62384]  mse_f: 4.351616859436035   total loss: [870.97546]\n",
            "mse_b ====== [867.180542]\n",
            "It: 54169, Time: 0.03\n",
            "mse_b  [867.18054]  mse_f: 5.10048770904541   total loss: [872.281]\n",
            "mse_b ====== [867.595]\n",
            "It: 54170, Time: 0.03\n",
            "mse_b  [867.595]  mse_f: 5.609385013580322   total loss: [873.20435]\n",
            "mse_b ====== [867.744141]\n",
            "It: 54171, Time: 0.03\n",
            "mse_b  [867.74414]  mse_f: 5.685575485229492   total loss: [873.4297]\n",
            "mse_b ====== [867.673157]\n",
            "It: 54172, Time: 0.03\n",
            "mse_b  [867.67316]  mse_f: 5.338954925537109   total loss: [873.0121]\n",
            "mse_b ====== [867.100647]\n",
            "It: 54173, Time: 0.03\n",
            "mse_b  [867.10065]  mse_f: 4.640717506408691   total loss: [871.7414]\n",
            "mse_b ====== [866.59729]\n",
            "It: 54174, Time: 0.03\n",
            "mse_b  [866.5973]  mse_f: 3.8365962505340576   total loss: [870.4339]\n",
            "mse_b ====== [866.469116]\n",
            "It: 54175, Time: 0.03\n",
            "mse_b  [866.4691]  mse_f: 3.215256929397583   total loss: [869.6844]\n",
            "mse_b ====== [866.413513]\n",
            "It: 54176, Time: 0.03\n",
            "mse_b  [866.4135]  mse_f: 2.9417591094970703   total loss: [869.3553]\n",
            "mse_b ====== [866.79718]\n",
            "It: 54177, Time: 0.03\n",
            "mse_b  [866.7972]  mse_f: 3.579024314880371   total loss: [870.3762]\n",
            "mse_b ====== [867.049622]\n",
            "It: 54178, Time: 0.03\n",
            "mse_b  [867.0496]  mse_f: 4.523392677307129   total loss: [871.573]\n",
            "mse_b ====== [867.363892]\n",
            "It: 54179, Time: 0.03\n",
            "mse_b  [867.3639]  mse_f: 5.050207138061523   total loss: [872.4141]\n",
            "mse_b ====== [867.041443]\n",
            "It: 54180, Time: 0.03\n",
            "mse_b  [867.04144]  mse_f: 4.67421817779541   total loss: [871.71564]\n",
            "mse_b ====== [866.499573]\n",
            "It: 54181, Time: 0.04\n",
            "mse_b  [866.4996]  mse_f: 3.587125539779663   total loss: [870.0867]\n",
            "mse_b ====== [866.392151]\n",
            "It: 54182, Time: 0.03\n",
            "mse_b  [866.39215]  mse_f: 2.7113218307495117   total loss: [869.10345]\n",
            "mse_b ====== [866.607]\n",
            "It: 54183, Time: 0.03\n",
            "mse_b  [866.607]  mse_f: 3.3154852390289307   total loss: [869.9225]\n",
            "mse_b ====== [867.154236]\n",
            "It: 54184, Time: 0.03\n",
            "mse_b  [867.15424]  mse_f: 4.5589599609375   total loss: [871.7132]\n",
            "mse_b ====== [867.118713]\n",
            "It: 54185, Time: 0.03\n",
            "mse_b  [867.1187]  mse_f: 5.58076286315918   total loss: [872.69946]\n",
            "mse_b ====== [867.099609]\n",
            "It: 54186, Time: 0.03\n",
            "mse_b  [867.0996]  mse_f: 4.776693344116211   total loss: [871.8763]\n",
            "mse_b ====== [866.777344]\n",
            "It: 54187, Time: 0.03\n",
            "mse_b  [866.77734]  mse_f: 3.0695371627807617   total loss: [869.84686]\n",
            "mse_b ====== [866.519714]\n",
            "It: 54188, Time: 0.04\n",
            "mse_b  [866.5197]  mse_f: 2.8910372257232666   total loss: [869.41077]\n",
            "mse_b ====== [866.55896]\n",
            "It: 54189, Time: 0.03\n",
            "mse_b  [866.55896]  mse_f: 4.255290508270264   total loss: [870.8143]\n",
            "mse_b ====== [866.994446]\n",
            "It: 54190, Time: 0.03\n",
            "mse_b  [866.99445]  mse_f: 5.079905033111572   total loss: [872.07434]\n",
            "mse_b ====== [867.122437]\n",
            "It: 54191, Time: 0.03\n",
            "mse_b  [867.12244]  mse_f: 5.042511940002441   total loss: [872.165]\n",
            "mse_b ====== [866.947754]\n",
            "It: 54192, Time: 0.03\n",
            "mse_b  [866.94775]  mse_f: 3.6301321983337402   total loss: [870.5779]\n",
            "mse_b ====== [866.677795]\n",
            "It: 54193, Time: 0.03\n",
            "mse_b  [866.6778]  mse_f: 3.0272557735443115   total loss: [869.7051]\n",
            "mse_b ====== [866.884521]\n",
            "It: 54194, Time: 0.03\n",
            "mse_b  [866.8845]  mse_f: 3.9399328231811523   total loss: [870.82446]\n",
            "mse_b ====== [867.212219]\n",
            "It: 54195, Time: 0.03\n",
            "mse_b  [867.2122]  mse_f: 5.004380702972412   total loss: [872.2166]\n",
            "mse_b ====== [867.140747]\n",
            "It: 54196, Time: 0.03\n",
            "mse_b  [867.14075]  mse_f: 5.134963035583496   total loss: [872.2757]\n",
            "mse_b ====== [866.981567]\n",
            "It: 54197, Time: 0.03\n",
            "mse_b  [866.98157]  mse_f: 3.5917110443115234   total loss: [870.5733]\n",
            "mse_b ====== [866.972717]\n",
            "It: 54198, Time: 0.03\n",
            "mse_b  [866.9727]  mse_f: 3.218864917755127   total loss: [870.1916]\n",
            "mse_b ====== [867.831848]\n",
            "It: 54199, Time: 0.03\n",
            "mse_b  [867.83185]  mse_f: 3.7449965476989746   total loss: [871.57684]\n",
            "mse_b ====== [867.560059]\n",
            "It: 54200, Time: 0.03\n",
            "mse_b  [867.56006]  mse_f: 4.850881576538086   total loss: [872.41095]\n",
            "mse_b ====== [866.937866]\n",
            "It: 54201, Time: 0.03\n",
            "mse_b  [866.93787]  mse_f: 4.232421398162842   total loss: [871.1703]\n",
            "mse_b ====== [866.638]\n",
            "It: 54202, Time: 0.03\n",
            "mse_b  [866.638]  mse_f: 3.010802984237671   total loss: [869.6488]\n",
            "mse_b ====== [867.19458]\n",
            "It: 54203, Time: 0.03\n",
            "mse_b  [867.1946]  mse_f: 2.922603130340576   total loss: [870.1172]\n",
            "mse_b ====== [867.508118]\n",
            "It: 54204, Time: 0.03\n",
            "mse_b  [867.5081]  mse_f: 3.9248650074005127   total loss: [871.433]\n",
            "mse_b ====== [867.347229]\n",
            "It: 54205, Time: 0.03\n",
            "mse_b  [867.3472]  mse_f: 4.0596771240234375   total loss: [871.4069]\n",
            "mse_b ====== [866.684204]\n",
            "It: 54206, Time: 0.03\n",
            "mse_b  [866.6842]  mse_f: 3.193997383117676   total loss: [869.8782]\n",
            "mse_b ====== [866.468933]\n",
            "It: 54207, Time: 0.04\n",
            "mse_b  [866.46893]  mse_f: 2.9347681999206543   total loss: [869.4037]\n",
            "mse_b ====== [866.918823]\n",
            "It: 54208, Time: 0.03\n",
            "mse_b  [866.9188]  mse_f: 3.4705519676208496   total loss: [870.3894]\n",
            "mse_b ====== [867.36615]\n",
            "It: 54209, Time: 0.04\n",
            "mse_b  [867.36615]  mse_f: 4.249268531799316   total loss: [871.6154]\n",
            "mse_b ====== [867.264709]\n",
            "It: 54210, Time: 0.03\n",
            "mse_b  [867.2647]  mse_f: 3.3860180377960205   total loss: [870.65076]\n",
            "mse_b ====== [866.814941]\n",
            "It: 54211, Time: 0.03\n",
            "mse_b  [866.81494]  mse_f: 2.857231855392456   total loss: [869.6722]\n",
            "mse_b ====== [866.794739]\n",
            "It: 54212, Time: 0.03\n",
            "mse_b  [866.79474]  mse_f: 3.2689666748046875   total loss: [870.0637]\n",
            "mse_b ====== [866.890564]\n",
            "It: 54213, Time: 0.03\n",
            "mse_b  [866.89056]  mse_f: 4.276933670043945   total loss: [871.1675]\n",
            "mse_b ====== [867.151062]\n",
            "It: 54214, Time: 0.03\n",
            "mse_b  [867.15106]  mse_f: 3.451720714569092   total loss: [870.6028]\n",
            "mse_b ====== [866.751221]\n",
            "It: 54215, Time: 0.04\n",
            "mse_b  [866.7512]  mse_f: 3.3380603790283203   total loss: [870.0893]\n",
            "mse_b ====== [866.918457]\n",
            "It: 54216, Time: 0.03\n",
            "mse_b  [866.91846]  mse_f: 3.2889742851257324   total loss: [870.20746]\n",
            "mse_b ====== [866.905334]\n",
            "It: 54217, Time: 0.03\n",
            "mse_b  [866.90533]  mse_f: 4.2666425704956055   total loss: [871.172]\n",
            "mse_b ====== [866.660156]\n",
            "It: 54218, Time: 0.03\n",
            "mse_b  [866.66016]  mse_f: 3.549532175064087   total loss: [870.2097]\n",
            "mse_b ====== [866.614563]\n",
            "It: 54219, Time: 0.03\n",
            "mse_b  [866.61456]  mse_f: 3.1218905448913574   total loss: [869.73645]\n",
            "mse_b ====== [866.889648]\n",
            "It: 54220, Time: 0.03\n",
            "mse_b  [866.88965]  mse_f: 3.1230506896972656   total loss: [870.0127]\n",
            "mse_b ====== [867.029907]\n",
            "It: 54221, Time: 0.03\n",
            "mse_b  [867.0299]  mse_f: 4.389716625213623   total loss: [871.4196]\n",
            "mse_b ====== [866.858643]\n",
            "It: 54222, Time: 0.03\n",
            "mse_b  [866.85864]  mse_f: 3.8567047119140625   total loss: [870.71533]\n",
            "mse_b ====== [866.678406]\n",
            "It: 54223, Time: 0.03\n",
            "mse_b  [866.6784]  mse_f: 3.1036341190338135   total loss: [869.78204]\n",
            "mse_b ====== [866.685486]\n",
            "It: 54224, Time: 0.03\n",
            "mse_b  [866.6855]  mse_f: 2.902909278869629   total loss: [869.5884]\n",
            "mse_b ====== [866.984375]\n",
            "It: 54225, Time: 0.03\n",
            "mse_b  [866.9844]  mse_f: 3.756199359893799   total loss: [870.7406]\n",
            "mse_b ====== [866.887634]\n",
            "It: 54226, Time: 0.03\n",
            "mse_b  [866.88763]  mse_f: 3.8823633193969727   total loss: [870.77]\n",
            "mse_b ====== [867.00116]\n",
            "It: 54227, Time: 0.03\n",
            "mse_b  [867.00116]  mse_f: 3.3851611614227295   total loss: [870.3863]\n",
            "mse_b ====== [866.966187]\n",
            "It: 54228, Time: 0.03\n",
            "mse_b  [866.9662]  mse_f: 2.7148101329803467   total loss: [869.68097]\n",
            "mse_b ====== [866.745178]\n",
            "It: 54229, Time: 0.03\n",
            "mse_b  [866.7452]  mse_f: 3.452324628829956   total loss: [870.1975]\n",
            "mse_b ====== [866.606873]\n",
            "It: 54230, Time: 0.03\n",
            "mse_b  [866.6069]  mse_f: 3.8544623851776123   total loss: [870.46136]\n",
            "mse_b ====== [866.901]\n",
            "It: 54231, Time: 0.03\n",
            "mse_b  [866.901]  mse_f: 3.3826191425323486   total loss: [870.2836]\n",
            "mse_b ====== [867.139465]\n",
            "It: 54232, Time: 0.03\n",
            "mse_b  [867.13947]  mse_f: 2.7524631023406982   total loss: [869.8919]\n",
            "mse_b ====== [867.052429]\n",
            "It: 54233, Time: 0.03\n",
            "mse_b  [867.0524]  mse_f: 3.3549606800079346   total loss: [870.4074]\n",
            "mse_b ====== [866.652405]\n",
            "It: 54234, Time: 0.03\n",
            "mse_b  [866.6524]  mse_f: 3.578667163848877   total loss: [870.2311]\n",
            "mse_b ====== [866.707336]\n",
            "It: 54235, Time: 0.03\n",
            "mse_b  [866.70734]  mse_f: 3.312774896621704   total loss: [870.02014]\n",
            "mse_b ====== [866.645874]\n",
            "It: 54236, Time: 0.04\n",
            "mse_b  [866.6459]  mse_f: 3.100383758544922   total loss: [869.7463]\n",
            "mse_b ====== [867.077637]\n",
            "It: 54237, Time: 0.03\n",
            "mse_b  [867.07764]  mse_f: 2.9565787315368652   total loss: [870.03424]\n",
            "mse_b ====== [866.853088]\n",
            "It: 54238, Time: 0.03\n",
            "mse_b  [866.8531]  mse_f: 3.49629545211792   total loss: [870.34937]\n",
            "mse_b ====== [866.63739]\n",
            "It: 54239, Time: 0.03\n",
            "mse_b  [866.6374]  mse_f: 3.632782459259033   total loss: [870.2702]\n",
            "mse_b ====== [866.7146]\n",
            "It: 54240, Time: 0.03\n",
            "mse_b  [866.7146]  mse_f: 3.1053428649902344   total loss: [869.81995]\n",
            "mse_b ====== [866.894348]\n",
            "It: 54241, Time: 0.03\n",
            "mse_b  [866.89435]  mse_f: 2.93683123588562   total loss: [869.8312]\n",
            "mse_b ====== [866.73053]\n",
            "It: 54242, Time: 0.03\n",
            "mse_b  [866.7305]  mse_f: 3.3135037422180176   total loss: [870.044]\n",
            "mse_b ====== [866.785278]\n",
            "It: 54243, Time: 0.03\n",
            "mse_b  [866.7853]  mse_f: 3.424224853515625   total loss: [870.2095]\n",
            "mse_b ====== [866.585449]\n",
            "It: 54244, Time: 0.03\n",
            "mse_b  [866.58545]  mse_f: 3.757232904434204   total loss: [870.3427]\n",
            "mse_b ====== [866.744629]\n",
            "It: 54245, Time: 0.03\n",
            "mse_b  [866.7446]  mse_f: 3.2349371910095215   total loss: [869.97955]\n",
            "mse_b ====== [866.487244]\n",
            "It: 54246, Time: 0.03\n",
            "mse_b  [866.48724]  mse_f: 3.1255784034729004   total loss: [869.6128]\n",
            "mse_b ====== [866.457275]\n",
            "It: 54247, Time: 0.03\n",
            "mse_b  [866.4573]  mse_f: 3.5217044353485107   total loss: [869.979]\n",
            "mse_b ====== [866.741455]\n",
            "It: 54248, Time: 0.03\n",
            "mse_b  [866.74146]  mse_f: 3.6898019313812256   total loss: [870.4313]\n",
            "mse_b ====== [866.734863]\n",
            "It: 54249, Time: 0.04\n",
            "mse_b  [866.73486]  mse_f: 3.235346794128418   total loss: [869.9702]\n",
            "mse_b ====== [866.672485]\n",
            "It: 54250, Time: 0.04\n",
            "mse_b  [866.6725]  mse_f: 3.035670757293701   total loss: [869.7081]\n",
            "mse_b ====== [866.60553]\n",
            "It: 54251, Time: 0.03\n",
            "mse_b  [866.6055]  mse_f: 3.3232967853546143   total loss: [869.92883]\n",
            "mse_b ====== [866.598083]\n",
            "It: 54252, Time: 0.03\n",
            "mse_b  [866.5981]  mse_f: 3.605271816253662   total loss: [870.20337]\n",
            "mse_b ====== [866.460388]\n",
            "It: 54253, Time: 0.04\n",
            "mse_b  [866.4604]  mse_f: 3.3525140285491943   total loss: [869.8129]\n",
            "mse_b ====== [866.720093]\n",
            "It: 54254, Time: 0.03\n",
            "mse_b  [866.7201]  mse_f: 2.8484580516815186   total loss: [869.56854]\n",
            "mse_b ====== [866.766846]\n",
            "It: 54255, Time: 0.03\n",
            "mse_b  [866.76685]  mse_f: 3.104421615600586   total loss: [869.8713]\n",
            "mse_b ====== [867.166687]\n",
            "It: 54256, Time: 0.04\n",
            "mse_b  [867.1667]  mse_f: 3.3028712272644043   total loss: [870.46954]\n",
            "mse_b ====== [866.739563]\n",
            "It: 54257, Time: 0.05\n",
            "mse_b  [866.73956]  mse_f: 3.1238818168640137   total loss: [869.86346]\n",
            "mse_b ====== [866.338562]\n",
            "It: 54258, Time: 0.05\n",
            "mse_b  [866.33856]  mse_f: 2.865933418273926   total loss: [869.20447]\n",
            "mse_b ====== [866.620789]\n",
            "It: 54259, Time: 0.04\n",
            "mse_b  [866.6208]  mse_f: 2.7865633964538574   total loss: [869.40735]\n",
            "mse_b ====== [867.015442]\n",
            "It: 54260, Time: 0.05\n",
            "mse_b  [867.01544]  mse_f: 3.4330930709838867   total loss: [870.44855]\n",
            "mse_b ====== [867.120911]\n",
            "It: 54261, Time: 0.05\n",
            "mse_b  [867.1209]  mse_f: 3.2558116912841797   total loss: [870.3767]\n",
            "mse_b ====== [866.518127]\n",
            "It: 54262, Time: 0.05\n",
            "mse_b  [866.5181]  mse_f: 2.926051139831543   total loss: [869.44415]\n",
            "mse_b ====== [866.38562]\n",
            "It: 54263, Time: 0.04\n",
            "mse_b  [866.3856]  mse_f: 2.6357109546661377   total loss: [869.0213]\n",
            "mse_b ====== [866.856567]\n",
            "It: 54264, Time: 0.05\n",
            "mse_b  [866.85657]  mse_f: 3.0731372833251953   total loss: [869.9297]\n",
            "mse_b ====== [867.006958]\n",
            "It: 54265, Time: 0.04\n",
            "mse_b  [867.00696]  mse_f: 3.3581206798553467   total loss: [870.36505]\n",
            "mse_b ====== [866.773132]\n",
            "It: 54266, Time: 0.05\n",
            "mse_b  [866.77313]  mse_f: 3.2191481590270996   total loss: [869.9923]\n",
            "mse_b ====== [866.577271]\n",
            "It: 54267, Time: 0.04\n",
            "mse_b  [866.5773]  mse_f: 2.8804750442504883   total loss: [869.45776]\n",
            "mse_b ====== [866.580139]\n",
            "It: 54268, Time: 0.04\n",
            "mse_b  [866.58014]  mse_f: 3.049309730529785   total loss: [869.62946]\n",
            "mse_b ====== [866.744568]\n",
            "It: 54269, Time: 0.04\n",
            "mse_b  [866.74457]  mse_f: 3.0348405838012695   total loss: [869.7794]\n",
            "mse_b ====== [866.613953]\n",
            "It: 54270, Time: 0.04\n",
            "mse_b  [866.61395]  mse_f: 3.109158992767334   total loss: [869.7231]\n",
            "mse_b ====== [866.607788]\n",
            "It: 54271, Time: 0.04\n",
            "mse_b  [866.6078]  mse_f: 3.1442370414733887   total loss: [869.752]\n",
            "mse_b ====== [866.923218]\n",
            "It: 54272, Time: 0.04\n",
            "mse_b  [866.9232]  mse_f: 3.369295835494995   total loss: [870.29254]\n",
            "mse_b ====== [866.569275]\n",
            "It: 54273, Time: 0.04\n",
            "mse_b  [866.5693]  mse_f: 3.561084032058716   total loss: [870.1304]\n",
            "mse_b ====== [866.501221]\n",
            "It: 54274, Time: 0.04\n",
            "mse_b  [866.5012]  mse_f: 2.8069300651550293   total loss: [869.30817]\n",
            "mse_b ====== [866.470398]\n",
            "It: 54275, Time: 0.04\n",
            "mse_b  [866.4704]  mse_f: 2.6158177852630615   total loss: [869.08624]\n",
            "mse_b ====== [866.750427]\n",
            "It: 54276, Time: 0.04\n",
            "mse_b  [866.7504]  mse_f: 3.480898141860962   total loss: [870.2313]\n",
            "mse_b ====== [866.688965]\n",
            "It: 54277, Time: 0.04\n",
            "mse_b  [866.68896]  mse_f: 3.7746214866638184   total loss: [870.46356]\n",
            "mse_b ====== [866.569458]\n",
            "It: 54278, Time: 0.04\n",
            "mse_b  [866.56946]  mse_f: 3.2867581844329834   total loss: [869.8562]\n",
            "mse_b ====== [866.459229]\n",
            "It: 54279, Time: 0.04\n",
            "mse_b  [866.4592]  mse_f: 2.932121753692627   total loss: [869.39136]\n",
            "mse_b ====== [866.714722]\n",
            "It: 54280, Time: 0.04\n",
            "mse_b  [866.7147]  mse_f: 3.052237033843994   total loss: [869.76697]\n",
            "mse_b ====== [866.654114]\n",
            "It: 54281, Time: 0.04\n",
            "mse_b  [866.6541]  mse_f: 3.1918516159057617   total loss: [869.84595]\n",
            "mse_b ====== [866.447205]\n",
            "It: 54282, Time: 0.04\n",
            "mse_b  [866.4472]  mse_f: 3.4807257652282715   total loss: [869.9279]\n",
            "mse_b ====== [866.652161]\n",
            "It: 54283, Time: 0.04\n",
            "mse_b  [866.65216]  mse_f: 3.1154000759124756   total loss: [869.7676]\n",
            "mse_b ====== [866.60083]\n",
            "It: 54284, Time: 0.05\n",
            "mse_b  [866.6008]  mse_f: 3.446180820465088   total loss: [870.047]\n",
            "mse_b ====== [866.543884]\n",
            "It: 54285, Time: 0.04\n",
            "mse_b  [866.5439]  mse_f: 3.564051866531372   total loss: [870.1079]\n",
            "mse_b ====== [866.44574]\n",
            "It: 54286, Time: 0.04\n",
            "mse_b  [866.44574]  mse_f: 3.1216187477111816   total loss: [869.5674]\n",
            "mse_b ====== [866.661133]\n",
            "It: 54287, Time: 0.04\n",
            "mse_b  [866.66113]  mse_f: 2.6814682483673096   total loss: [869.3426]\n",
            "mse_b ====== [866.958374]\n",
            "It: 54288, Time: 0.04\n",
            "mse_b  [866.9584]  mse_f: 3.202503204345703   total loss: [870.1609]\n",
            "mse_b ====== [866.877869]\n",
            "It: 54289, Time: 0.04\n",
            "mse_b  [866.87787]  mse_f: 3.662872076034546   total loss: [870.5407]\n",
            "mse_b ====== [866.522156]\n",
            "It: 54290, Time: 0.04\n",
            "mse_b  [866.52216]  mse_f: 3.9810233116149902   total loss: [870.5032]\n",
            "mse_b ====== [866.724609]\n",
            "It: 54291, Time: 0.04\n",
            "mse_b  [866.7246]  mse_f: 3.558128833770752   total loss: [870.2827]\n",
            "mse_b ====== [867.152405]\n",
            "It: 54292, Time: 0.04\n",
            "mse_b  [867.1524]  mse_f: 3.1029911041259766   total loss: [870.2554]\n",
            "mse_b ====== [867.175598]\n",
            "It: 54293, Time: 0.04\n",
            "mse_b  [867.1756]  mse_f: 2.8430709838867188   total loss: [870.0187]\n",
            "mse_b ====== [866.733]\n",
            "It: 54294, Time: 0.04\n",
            "mse_b  [866.733]  mse_f: 3.281681537628174   total loss: [870.01465]\n",
            "mse_b ====== [866.445801]\n",
            "It: 54295, Time: 0.04\n",
            "mse_b  [866.4458]  mse_f: 3.467637062072754   total loss: [869.91345]\n",
            "mse_b ====== [867.102783]\n",
            "It: 54296, Time: 0.04\n",
            "mse_b  [867.1028]  mse_f: 3.7929887771606445   total loss: [870.89575]\n",
            "mse_b ====== [867.452148]\n",
            "It: 54297, Time: 0.04\n",
            "mse_b  [867.45215]  mse_f: 3.9865565299987793   total loss: [871.4387]\n",
            "mse_b ====== [867.223267]\n",
            "It: 54298, Time: 0.04\n",
            "mse_b  [867.22327]  mse_f: 3.2234089374542236   total loss: [870.44666]\n",
            "mse_b ====== [866.615601]\n",
            "It: 54299, Time: 0.04\n",
            "mse_b  [866.6156]  mse_f: 2.8754286766052246   total loss: [869.491]\n",
            "mse_b ====== [866.606628]\n",
            "It: 54300, Time: 0.04\n",
            "mse_b  [866.6066]  mse_f: 3.6010007858276367   total loss: [870.20764]\n",
            "mse_b ====== [867.135071]\n",
            "It: 54301, Time: 0.04\n",
            "mse_b  [867.1351]  mse_f: 3.934683322906494   total loss: [871.06976]\n",
            "mse_b ====== [867.067932]\n",
            "It: 54302, Time: 0.04\n",
            "mse_b  [867.06793]  mse_f: 4.863872051239014   total loss: [871.9318]\n",
            "mse_b ====== [866.99823]\n",
            "It: 54303, Time: 0.05\n",
            "mse_b  [866.9982]  mse_f: 4.367496013641357   total loss: [871.3657]\n",
            "mse_b ====== [867.038086]\n",
            "It: 54304, Time: 0.04\n",
            "mse_b  [867.0381]  mse_f: 3.678844451904297   total loss: [870.7169]\n",
            "mse_b ====== [866.85791]\n",
            "It: 54305, Time: 0.04\n",
            "mse_b  [866.8579]  mse_f: 3.4562065601348877   total loss: [870.3141]\n",
            "mse_b ====== [866.595276]\n",
            "It: 54306, Time: 0.04\n",
            "mse_b  [866.5953]  mse_f: 4.4872660636901855   total loss: [871.0825]\n",
            "mse_b ====== [866.664185]\n",
            "It: 54307, Time: 0.05\n",
            "mse_b  [866.6642]  mse_f: 4.86262321472168   total loss: [871.5268]\n",
            "mse_b ====== [867.042236]\n",
            "It: 54308, Time: 0.03\n",
            "mse_b  [867.04224]  mse_f: 5.354694366455078   total loss: [872.3969]\n",
            "mse_b ====== [866.881836]\n",
            "It: 54309, Time: 0.04\n",
            "mse_b  [866.88184]  mse_f: 4.1968231201171875   total loss: [871.0787]\n",
            "mse_b ====== [866.685425]\n",
            "It: 54310, Time: 0.03\n",
            "mse_b  [866.6854]  mse_f: 3.411867141723633   total loss: [870.0973]\n",
            "mse_b ====== [866.584045]\n",
            "It: 54311, Time: 0.04\n",
            "mse_b  [866.58405]  mse_f: 4.003514289855957   total loss: [870.5876]\n",
            "mse_b ====== [867.207703]\n",
            "It: 54312, Time: 0.04\n",
            "mse_b  [867.2077]  mse_f: 5.036682605743408   total loss: [872.2444]\n",
            "mse_b ====== [866.934631]\n",
            "It: 54313, Time: 0.04\n",
            "mse_b  [866.93463]  mse_f: 4.498586654663086   total loss: [871.4332]\n",
            "mse_b ====== [866.448181]\n",
            "It: 54314, Time: 0.03\n",
            "mse_b  [866.4482]  mse_f: 3.9791059494018555   total loss: [870.4273]\n",
            "mse_b ====== [866.694]\n",
            "It: 54315, Time: 0.04\n",
            "mse_b  [866.694]  mse_f: 3.095916748046875   total loss: [869.7899]\n",
            "mse_b ====== [867.0047]\n",
            "It: 54316, Time: 0.04\n",
            "mse_b  [867.0047]  mse_f: 4.499972820281982   total loss: [871.5047]\n",
            "mse_b ====== [867.198425]\n",
            "It: 54317, Time: 0.04\n",
            "mse_b  [867.1984]  mse_f: 4.936211109161377   total loss: [872.13464]\n",
            "mse_b ====== [866.690491]\n",
            "It: 54318, Time: 0.04\n",
            "mse_b  [866.6905]  mse_f: 4.213373184204102   total loss: [870.9039]\n",
            "mse_b ====== [866.549194]\n",
            "It: 54319, Time: 0.04\n",
            "mse_b  [866.5492]  mse_f: 2.787242889404297   total loss: [869.3364]\n",
            "mse_b ====== [867.085938]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-18b05e1b53c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m MSE_b1, MSE_f1, weightu, weightf, loss_saman, list_model_u = fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star1,xtt,ytt,xbb,ybb,xrr,yrr,xll,yll, \n\u001b[0m\u001b[1;32m    554\u001b[0m                                                                  \u001b[0mtf_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                                                                  tf_iter2=1000, newton_iter1=5000,newton_iter2=15000)\n",
            "\u001b[0;32m<ipython-input-58-18b05e1b53c3>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star, xtt, ytt, xbb, ybb, xrr, yrr, xll, yll, tf_iter, tf_iter2, newton_iter1, newton_iter2)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mt_f_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_sz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_sz\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_sz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             loss_value, mse_b, mse_f, grads, grads_ub, grads_fu = grad(u_model, x_f_batch, y_f_batch,\n\u001b[0m\u001b[1;32m    466\u001b[0m                                                                        \u001b[0mxb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                                                                        \u001b[0mub_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_ub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_saman[40000:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "-3Cs2i7homjS",
        "outputId": "992ae7c7-980a-45e8-8654-433b9a19aae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc4d4338310>]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_saman[0].numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMDdJRYMpemq",
        "outputId": "8f6df34a-e279-4036-a5ea-4cbee923366c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11909.92], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "loss2=[]\n",
        "for i in range(len(loss_saman)):\n",
        "  loss2.append(loss_saman[i][0].numpy())\n",
        "print(\"Min: \", pd.Series(loss2).idxmin())\n",
        "print(\"Max: \", pd.Series(loss2).idxmax())\n",
        "print('loss_min',loss_saman[pd.Series(loss2).idxmin()])\n",
        "\n",
        "model_u=list_model_u[pd.Series(loss2).idxmin()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSEv7OB15Zn0",
        "outputId": "26602843-ff08-4e9d-eaae-456419c36071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min:  53995\n",
            "Max:  0\n",
            "loss_min tf.Tensor([868.9242], shape=(1,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_u.save('my_u_model_Lx6_Asym_2top_2bott2.h5')"
      ],
      "metadata": {
        "id": "fSVX-iwW7Go9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6611581-5e14-4f7f-97d5-4df70038b8c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model('my_u_model_Lx6_Asym_2top_2bott.h5')\n",
        "model_u=new_model"
      ],
      "metadata": {
        "id": "ZQ33NJeX7Yta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ecf2eca-5890-4072-9e99-6fb1234c56f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualization of velocity"
      ],
      "metadata": {
        "id": "Fof1i2WfisrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Set up meshgrid\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "N=100\n",
        "x1 = (np.linspace(0, xmax, N+1)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, N+1)).flatten()[:, None]\n",
        "X1, Y1 = np.meshgrid(x1, y1)\n",
        "\n",
        "X_star1 = np.hstack((X1.flatten()[:, None], Y1.flatten()[:, None]))\n",
        "\n",
        "X_star = tf.convert_to_tensor(X_star1, dtype=tf.float32)\n",
        "#up, vp, pp = u_x_model(X_star[:, 0:1], X_star[:, 1:2])\n",
        "\n",
        "UU=new_model(tf.concat([X_star[:, 0:1], X_star[:, 1:2]],1))[0]\n",
        "uuu2=tf.reshape(UU,shape=[tf.shape(UU).numpy()[0]])\n",
        "U = uuu2.numpy().reshape(N+1,N+1)\n",
        "plt. contourf(X1, Y1, U,60, cmap='rainbow');\n",
        "plt.colorbar();\n",
        "\n",
        "\n",
        "x1 = (np.linspace(0, xmax, N+1)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, N+1)).flatten()[:, None]\n",
        "X1, Y1 = np.meshgrid(x1, y1)\n",
        "fig, ax = plt.subplots(figsize=(4, 1))\n",
        "ax.contourf(X1,Y1,U,60,vmax=1.0,cmap='rainbow')\n",
        "plt.axis('scaled')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "PDtBkbhDPn2E",
        "outputId": "59321195-ff1b-4c8b-e53d-2626769fb6d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 6.0, 0.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x72 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = (np.linspace(0, xmax, N+1)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, N+1)).flatten()[:, None]\n",
        "X1, Y1 = np.meshgrid(x1, y1)\n",
        "fig, ax = plt.subplots(figsize=(6, 1))\n",
        "ax.contourf(X1,Y1,U,60,vmax=0.8,cmap='rainbow')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "CW6qhYdtWYwH",
        "outputId": "b04585b3-cbb2-4207-e39f-234aaff09eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.contour.QuadContourSet at 0x7fac7013bf40>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x72 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_u = tf.keras.models.load_model('my_u_model_Lx6_Asym_2top_2bott.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6k1b_1S1_K3",
        "outputId": "16116c3c-202f-4f4b-ce97-255e1814ad19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_u(tf.concat([xcor1,ycor1],1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrLLogtq2g_z",
        "outputId": "e04ba4c8-f84c-41c6-a89f-3a5d01de4edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(10000, 1), dtype=float32, numpy=\n",
              " array([[2.059567  ],\n",
              "        [1.4083912 ],\n",
              "        [0.6796989 ],\n",
              "        ...,\n",
              "        [1.454575  ],\n",
              "        [0.67357385],\n",
              "        [1.1621233 ]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(10000, 1), dtype=float32, numpy=\n",
              " array([[ 0.00255417],\n",
              "        [-0.06266842],\n",
              "        [-0.07143642],\n",
              "        ...,\n",
              "        [ 0.34537724],\n",
              "        [-0.0191423 ],\n",
              "        [ 0.24548493]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(10000, 1), dtype=float32, numpy=\n",
              " array([[0.3821262 ],\n",
              "        [0.04062033],\n",
              "        [0.77488816],\n",
              "        ...,\n",
              "        [1.6343566 ],\n",
              "        [0.74118346],\n",
              "        [1.1893368 ]], dtype=float32)>]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xcor1=x_f[0 * N_f:(0 * N_f + N_f), ]\n",
        "ycor1=y_f[0 * N_f:(0 * N_f + N_f), ]\n",
        "u_ref, v_ref, p_ref = model_u(tf.concat([xcor1,ycor1],1))\n",
        "\n"
      ],
      "metadata": {
        "id": "dI78tPlW112J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running PINNs for solving Concentration"
      ],
      "metadata": {
        "id": "jstEVxcri3a2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Solving C for electrowetting flow\n",
        "#NEW NS with importing fi and sai\n",
        "#NEW NS\n",
        "#########\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DTYPE='float32'\n",
        "tf.keras.backend.set_floatx(DTYPE)\n",
        "# Set number of data points\n",
        "#N_0 = 100\n",
        "#N_b = 100\n",
        "#N_r = 5000\n",
        "\n",
        "N_b = Nu1\n",
        "N_r = N_f\n",
        "\n",
        "\n",
        "# Set boundary\n",
        "xmin = 0.\n",
        "xmax = 6\n",
        "ymin = 0.\n",
        "ymax = 1.\n",
        "# Set constants\n",
        "pi = tf.constant(np.pi, dtype=DTYPE)\n",
        "viscosity = .01/pi\n",
        "\n",
        "# Define initial condition\n",
        "def fun_u_0(x):\n",
        "    return -tf.sin(pi * x)\n",
        "\n",
        "# Define boundary condition\n",
        "def fun_u_b(x, y):\n",
        "    n = x.shape[0]\n",
        "    return tf.zeros((n,1), dtype=DTYPE)\n",
        "\n",
        "# Define residual of the PDE\n",
        "def Nsx(x, y, u, v, u_x, u_y, v_x, v_y, u_xx, u_yy, v_xx, v_yy, p_x, p_y):\n",
        "    return -u*u_x-v*u_y-p_x+(1.0/1.0)*(u_xx+u_yy)\n",
        "#    return -p_x+(1.0/1.0)*(u_xx+u_yy)\n",
        "\n",
        "\n",
        "def Nsy(x, y, u, v, u_x, u_y, v_x, v_y, u_xx, u_yy, v_xx, v_yy, p_x, p_y):\n",
        "    return -u*v_x-v*v_y-p_y+(1.0/1.0)*(v_xx+v_yy)\n",
        "#    return -p_y+(1.0/1.0)*(v_xx+v_yy)\n",
        "\n",
        "def Cont(u, v, u_x, u_y, v_x, v_y):\n",
        "    return u_x+v_y\n",
        "\n",
        "# Lower bounds\n",
        "lb = tf.constant([xmin, ymin], dtype=DTYPE)\n",
        "# Upper bounds\n",
        "ub = tf.constant([xmax, ymax], dtype=DTYPE)\n",
        "\n",
        "# Set random seed for reproducible results\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# Draw uniform sample points for initial boundary data\n",
        "x_0 = lb[0] + (ub[0] - lb[0]) * tf.keras.backend.random_bernoulli((N_b,1), 0.5, dtype=DTYPE)\n",
        "y_0 = tf.random.uniform((N_b,1), lb[1], ub[1], dtype=DTYPE)\n",
        "X_0 = tf.concat([x_0, y_0], axis=1)\n",
        "\n",
        "# Evaluate intitial condition at x_0\n",
        "u_0 = fun_u_0(x_0)\n",
        "\n",
        "# Boundary data\n",
        "x_b = tf.random.uniform((N_b,1), lb[0], ub[0], dtype=DTYPE)\n",
        "y_b = lb[1] + (ub[1] - lb[1]) * tf.keras.backend.random_bernoulli((N_b,1), 0.5, dtype=DTYPE)\n",
        "X_b = tf.concat([x_b, y_b], axis=1)\n",
        "\n",
        "# Evaluate boundary condition at (t_b,x_b)\n",
        "u_b = fun_u_b(x_b, y_b)\n",
        "\n",
        "\n",
        "# Draw uniformly sampled collocation points\n",
        "x_r = tf.random.uniform((N_r,1), lb[0], ub[0], dtype=DTYPE)\n",
        "y_r = tf.random.uniform((N_r,1), lb[1], ub[1], dtype=DTYPE)\n",
        "X_r = tf.concat([x_r, y_r], axis=1)\n",
        "\n",
        "# Collect boundary and inital data in lists\n",
        "X_data = [X_0, X_b]\n",
        "u_data = [u_0, u_b]\n",
        "\n",
        "\n",
        "##################\n",
        "#################\n",
        "lo=np.zeros((N_b,1))\n",
        "#top\n",
        "xx=[]\n",
        "yy=[]\n",
        "for i in range(N_b):\n",
        "      if X_data[1][i,1].numpy() == 1.0:\n",
        "        xx.append(X_data[1][i,0].numpy())\n",
        "        yy.append(X_data[1][i,1].numpy())\n",
        "xtt=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "ytt=tf.zeros((len(yy),1), dtype=DTYPE)\n",
        "xtt=tf.add(xtt[:,0],xx)\n",
        "ytt=tf.add(ytt[:,0],yy)\n",
        "utt=tf.ones((len(xx),1), dtype=DTYPE)\n",
        "vtt=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "\n",
        "#bottom\n",
        "xx=[]\n",
        "yy=[]\n",
        "for i in range(N_b):\n",
        "      if X_data[1][i,1].numpy() == 0.0:\n",
        "        xx.append(X_data[1][i,0].numpy())\n",
        "        yy.append(X_data[1][i,1].numpy())\n",
        "xbb=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "ybb=tf.zeros((len(yy),1), dtype=DTYPE)\n",
        "xbb=tf.add(xbb[:,0],xx)\n",
        "ybb=tf.add(ybb[:,0],yy)\n",
        "ubb=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "vbb=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "#left1\n",
        "xx=[]\n",
        "yy=[]\n",
        "Nb_1= 50\n",
        "xll1= tf.zeros((Nb_1,1), dtype=DTYPE)\n",
        "yll1 =tf.random.uniform((Nb_1,1),0, 0.5, dtype=DTYPE)\n",
        "#xll1=tf.add(xll1[:,0],xx)\n",
        "#yll1=tf.add(yll1[:,0],yy)\n",
        "ull1=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "vll1=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "#left2\n",
        "xx=[]\n",
        "yy=[]\n",
        "Nb_2= 50\n",
        "xll2= tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "yll2= tf.random.uniform((Nb_2,1),0.5, 1.0, dtype=DTYPE)\n",
        "#xll2=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "#yll2=tf.zeros((len(yy),1), dtype=DTYPE)\n",
        "#xll2=tf.add(xll2[:,0],xx)\n",
        "#yll2=tf.add(yll2[:,0],yy)\n",
        "ull2=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "vll2=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "#right\n",
        "\n",
        "xx=[]\n",
        "yy=[]\n",
        "for i in range(N_b):\n",
        "      if X_data[0][i,0].numpy() == xmax:\n",
        "        xx.append(X_data[0][i,0].numpy())\n",
        "        yy.append(X_data[0][i,1].numpy())\n",
        "xrr=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "yrr=tf.zeros((len(yy),1), dtype=DTYPE)\n",
        "xrr=tf.add(xrr[:,0],xx)\n",
        "yrr=tf.add(yrr[:,0],yy)\n",
        "urr=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "vrr=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "xtt=tf.reshape(xtt,shape=[tf.shape(xtt).numpy()[0],1])\n",
        "ytt=tf.reshape(ytt,shape=[tf.shape(ytt).numpy()[0],1])\n",
        "\n",
        "xbb=tf.reshape(xbb,shape=[tf.shape(xbb).numpy()[0],1])\n",
        "ybb=tf.reshape(ybb,shape=[tf.shape(ybb).numpy()[0],1])\n",
        "\n",
        "xrr=tf.reshape(xrr,shape=[tf.shape(xrr).numpy()[0],1])\n",
        "yrr=tf.reshape(yrr,shape=[tf.shape(yrr).numpy()[0],1])\n",
        "\n",
        "\n",
        "xll1=tf.reshape(xll1,shape=[tf.shape(xll1).numpy()[0],1])\n",
        "yll1=tf.reshape(yll1,shape=[tf.shape(yll1).numpy()[0],1])\n",
        "xll2=tf.reshape(xll2,shape=[tf.shape(xll2).numpy()[0],1])\n",
        "yll2=tf.reshape(yll2,shape=[tf.shape(yll2).numpy()[0],1])\n",
        "\n",
        "xbound=tf.concat([xtt,xbb,xrr,xll1,xll2],0)\n",
        "ybound=tf.concat([ytt,ybb,yrr,yll1,yll2],0)\n",
        "plt.scatter(xbound,ybound)\n",
        "\n",
        "ubound=tf.concat([utt,ubb,urr,ull1,ull2],0)\n",
        "vbound=tf.concat([vtt,vbb,vrr,vll1,vll2],0)\n",
        "\n",
        "xb=xbound\n",
        "yb=ybound\n",
        "\n",
        "ub=ubound\n",
        "vb=vbound\n",
        "\n",
        "##################\n",
        "#xcor1=x_f[0 * N_f:(0 * N_f + N_f), ]\n",
        "#ycor1=y_f[0 * N_f:(0 * N_f + N_f), ]\n",
        "#sai_ref = model_sai(tf.concat([xcor1,ycor1],1))\n",
        "\n",
        "#fi_ref = model_fi(tf.concat([xcor1,ycor1],1))\n",
        "##################\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Oct 24 13:02:32 2021\n",
        "\n",
        "@author: SAMAN\n",
        "\"\"\"\n",
        "# import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import scipy.io\n",
        "import math\n",
        "import matplotlib.gridspec as gridspec\n",
        "#from plotting import newfig\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras import layers, activations\n",
        "from scipy.interpolate import griddata\n",
        "#from eager_lbfgs import lbfgs, Struct\n",
        "from pyDOE import lhs\n",
        "\n",
        "\n",
        "gh = tf.constant(0.01, dtype=tf.float32)\n",
        "gg = tf.constant(9.8, dtype=tf.float32)\n",
        "weight_ub = tf.Variable([5.0], dtype=tf.float32)  # weight_ub = tf.Variable([8.0], dtype=tf.float32)\n",
        "weight_fu = tf.Variable([1.0], dtype=tf.float32)  # weight_fu = tf.Variable([1.0], dtype=tf.float32)\n",
        "layer_sizes = [2, 20, 20, 20, 20, 20, 20, 20, 3]\n",
        "sizes_w = []\n",
        "sizes_b = []\n",
        "loss_saman=[]\n",
        "list_model_u=[]\n",
        "for i, width in enumerate(layer_sizes):\n",
        "    if i != 1:\n",
        "        sizes_w.append(int(width * layer_sizes[1]))\n",
        "        sizes_b.append(int(width if i != 0 else layer_sizes[1]))\n",
        "\n",
        "\n",
        "\n",
        "# L-BFGS weight getting and setting from https://github.com/pierremtb/PINNs-TF2.0\n",
        "\n",
        "def set_weights(model, w, sizes_w, sizes_b):  # 重新设置参数\n",
        "\n",
        "    for i, layer in enumerate(model.layers[1:len(sizes_w) + 1]):\n",
        "        start_weights = sum(sizes_w[:i]) + sum(sizes_b[:i])\n",
        "        end_weights = sum(sizes_w[:i + 1]) + sum(sizes_b[:i])\n",
        "        weights = w[start_weights:end_weights]\n",
        "        w_div = int(sizes_w[i] / sizes_b[i])\n",
        "        weights = tf.reshape(weights, [w_div, sizes_b[i]])\n",
        "        biases = w[end_weights:end_weights + sizes_b[i]]\n",
        "        weights_biases = [weights, biases]\n",
        "        layer.set_weights(weights_biases)\n",
        "\n",
        "\n",
        "def get_weights(model):\n",
        "    w = []\n",
        "    for layer in model.layers[1:len(sizes_w) + 1]:\n",
        "        weights_biases = layer.get_weights()\n",
        "        weights = weights_biases[0].flatten()\n",
        "        biases = weights_biases[1]\n",
        "        w.extend(weights)\n",
        "        w.extend(biases)\n",
        "    w = tf.convert_to_tensor(w)\n",
        "    return w\n",
        "\n",
        "def xavier_init(layer_sizes):\n",
        "    in_dim = layer_sizes[0]\n",
        "    out_dim = layer_sizes[1]\n",
        "    xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "    return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "\n",
        "def neural_net(layer_sizes):\n",
        "\n",
        "    input_tensor = keras.Input(shape=(layer_sizes[0],))\n",
        "\n",
        "    hide_layer_list = []\n",
        "    flag = True\n",
        "    for width in layer_sizes[1:-1]:\n",
        "        if flag:\n",
        "            x = layers.Dense(\n",
        "                width, activation=tf.nn.tanh,\n",
        "                kernel_initializer=\"glorot_normal\")(input_tensor)\n",
        "            flag = False\n",
        "        else:\n",
        "            x = layers.Dense(\n",
        "                width, activation=tf.nn.tanh,\n",
        "                kernel_initializer=\"glorot_normal\")(x)\n",
        "    output_tensor = layers.Dense(layer_sizes[-1], activation=None,kernel_initializer=\"glorot_normal\")(x)\n",
        "    print(\"xxxxxxxxxxxxxx\")\n",
        "    output0 = output_tensor[:, 0:1]\n",
        "\n",
        "\n",
        "    model_output = keras.models.Model(input_tensor, [output0])\n",
        "\n",
        "    return model_output\n",
        "\n",
        "# initialize the NN\n",
        "u_model = neural_net(layer_sizes)\n",
        "# view the NN\n",
        "u_model.summary()\n",
        "\n",
        "def gh1(pp):\n",
        "  return pp\n",
        "\n",
        "\n",
        "# define the loss\n",
        "def loss(x_f_batch, y_f_batch, xb, yb, ub, vb, weight_ub,weight_fu,x_top,y_top,x_bottom,y_bottom,x_right,y_right,x_left1,y_left1,x_left2,y_left2):\n",
        "\n",
        "    f_c_pred = f_model(x_f_batch, y_f_batch)\n",
        "\n",
        "\n",
        "    c_pred = u_model(tf.concat([xb, yb], 1))\n",
        "\n",
        "    #mse_b = 100*weight_ub*(tf.reduce_sum(tf.square(u_pred - ub)) + tf.reduce_sum(tf.square(v_pred - vb)))\n",
        "    #mse_b = 1*weight_ub*(tf.reduce_sum(tf.square(u_pred - ub)) + tf.reduce_sum(tf.square(v_pred - vb)))\n",
        "    loss_2 = loss_bd(x_top,y_top,x_bottom,y_bottom,x_right,y_right,x_left1,y_left1,x_left2,y_left2)\n",
        "    #mse_b = loss_2*weight_ub#+ mse_b\n",
        "\n",
        "    #mse_f = weight_fu*(tf.reduce_sum(tf.square(f_u_pred)) + tf.reduce_sum(tf.square(f_v_pred)) + tf.reduce_sum(tf.square(div_pred)))\n",
        "    loss3=tf.reduce_sum(tf.square(f_c_pred))\n",
        "\n",
        "    #weight_ub = tf.cond(loss3>loss_2,lambda:[loss_2/loss3],lambda:[1.0])\n",
        "    #weight_fu = tf.cond(loss_2>loss3, lambda:[1.0],lambda:[loss3/loss_2])\n",
        "\n",
        "\n",
        "\n",
        "    #weight_ub.assign([1.0])\n",
        "    #weight_fu.assign([1.0])\n",
        "\n",
        "    mse_b = loss_2 * weight_ub\n",
        "    mse_f = loss3\n",
        "\n",
        "    tf.print('mse_b ======',mse_b)\n",
        "\n",
        "    #tf.print('reduce_max',tf.reduce_max(f_u_pred))\n",
        "    #tf.print('min or max',tf.math.minimum(f_u_pred))\n",
        "    return mse_b + mse_f, mse_b, mse_f\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def loss_bd(xtop,ytop,xbottom,ybottom,xright,yright,xleft1,yleft1,xleft2,yleft2):\n",
        "  ctop = u_model(tf.concat([xtop, ytop],1))\n",
        "  cbottom = u_model(tf.concat([xbottom, ybottom],1))\n",
        "  cright = u_model(tf.concat([xright, yright],1))\n",
        "  cleft1 = u_model(tf.concat([xleft1, yleft1],1))\n",
        "  cleft2 = u_model(tf.concat([xleft2, yleft2],1))\n",
        "\n",
        "\n",
        "\n",
        "  loss_bd = tf.reduce_sum(tf.square(tf.gradients(ctop, ytop)[0]))+tf.reduce_sum(tf.square(tf.gradients(cbottom, ybottom)[0])) \\\n",
        "  + tf.reduce_sum(tf.square(tf.gradients(cright,xright)[0]))+tf.reduce_sum(tf.square(cleft1-0)) +tf.reduce_sum(tf.square(cleft2-1.0)) \\\n",
        "\n",
        "  return loss_bd\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def f_model(x, y):\n",
        "    c = u_model(tf.concat([x, y],1))\n",
        "\n",
        "    c_x = tf.gradients(c, x)[0]\n",
        "    c_y = tf.gradients(c, y)[0]\n",
        "    c_xx = tf.gradients(c_x, x)[0]\n",
        "    c_yy = tf.gradients(c_y, y)[0]\n",
        "\n",
        "\n",
        "\n",
        "    Reynolds=tf.constant(100.0, dtype=tf.float32)\n",
        "    Schimit= tf.constant(10, dtype=tf.float32)\n",
        "\n",
        "    #(1.0/Re)*(double(Lx)/double(Ly))*omega_h*omega_h*sai[i][j]*(fi[i+1][j]-fi[i-1][j])/double(2*dx);//inja kamel shavad\n",
        "    f_c = u_ref*c_x + v_ref*c_y  -(1.0/Reynolds)*(1.0/Schimit)*(c_xx + c_yy)\n",
        "\n",
        "    return f_c\n",
        "\n",
        "@tf.function\n",
        "def u_x_model(x, y):\n",
        "    c= u_model(tf.concat([x, y], 1))\n",
        "    return c\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def grad(u_model, x_f_batch, y_f_batch, xb_batch, yb_batch, ub_batch, vb_batch, weight_ub,\n",
        "         weight_fu,x_top,y_top,x_bottom,y_bottom,x_right,y_right,x_left1,y_left1,x_left2,y_left2):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "        loss_value, mse_b, mse_f = loss(x_f_batch, y_f_batch,  xb_batch, yb_batch, ub_batch,\n",
        "                                        vb_batch, weight_ub, weight_fu,x_top,y_top,x_bottom,y_bottom,x_right,y_right,x_left1,y_left1,x_left2,y_left2)\n",
        "        grads = tape.gradient(loss_value, u_model.trainable_variables)\n",
        "\n",
        "        grads_ub = tape.gradient(loss_value, weight_ub)\n",
        "\n",
        "        grads_fu = tape.gradient(loss_value, weight_fu)\n",
        "\n",
        "    return loss_value, mse_b, mse_f, grads, grads_ub, grads_fu\n",
        "\n",
        "\n",
        "def fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star,xtt,ytt,xbb,ybb,xrr,yrr,xll1,yll1,xll2,yll2, tf_iter, tf_iter2,\n",
        "        newton_iter1, newton_iter2):\n",
        "\n",
        "    batch_sz = N_f\n",
        "    n_batches = N_f // batch_sz\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    tf_optimizer = tf.keras.optimizers.Adam(lr=0.005, beta_1=.99) #tf_optimizer = tf.keras.optimizers.Adam(lr=0.005, beta_1=.99)\n",
        "    tf_optimizer_weights = tf.keras.optimizers.Adam(lr=0.003, beta_1=.99)\n",
        "    tf_optimizer_u = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
        "\n",
        "    tf.print(f\"weight_ub: {weight_ub}  weight_fu: {weight_fu}\")\n",
        "    print(\"starting Adam training\")\n",
        "\n",
        "    a = np.random.rand(1000)\n",
        "    loss_history = list(a)\n",
        "    MSE_b0 = list(a)\n",
        "    MSE_f0 = list(a)\n",
        "\n",
        "\n",
        "    MSE_b1 = []\n",
        "    MSE_f1 = []\n",
        "\n",
        "    weightu = []\n",
        "    weightf = []\n",
        "    # For mini-batch (if used)\n",
        "    for epoch in range(tf_iter):\n",
        "        for i in range(n_batches):\n",
        "            xb_batch = xb\n",
        "            yb_batch = yb\n",
        "\n",
        "            ub_batch = ub\n",
        "            vb_batch = vb\n",
        "\n",
        "            x_top=xtt\n",
        "            y_top=ytt\n",
        "\n",
        "            x_bottom=xbb\n",
        "            y_bottom=ybb\n",
        "\n",
        "            x_right=xrr\n",
        "            y_right=yrr\n",
        "\n",
        "            x_left1=xll1\n",
        "            y_left1=yll1\n",
        "\n",
        "            x_left2=xll2\n",
        "            y_left2=yll2\n",
        "\n",
        "            x_f_batch = x_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "            y_f_batch = y_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "            t_f_batch = t_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "\n",
        "            loss_value, mse_b, mse_f, grads, grads_ub, grads_fu = grad(u_model, x_f_batch, y_f_batch,\n",
        "                                                                       xb_batch, yb_batch,\n",
        "                                                                       ub_batch, vb_batch, weight_ub,\n",
        "                                                                       weight_fu,\n",
        "                                                                       x_top,y_top,x_bottom,y_bottom,\n",
        "                                                                       x_right,y_right,x_left1,y_left1,x_left2,y_left2)\n",
        "\n",
        "            tf_optimizer.apply_gradients(zip(grads, u_model.trainable_variables))\n",
        "            MSE_b0.append(mse_b)\n",
        "            MSE_f0.append(mse_f)\n",
        "\n",
        "            loss_history.append(loss_value)\n",
        "            loss_saman.append(loss_value)\n",
        "            list_model_u.append(u_model)\n",
        "#            if loss_history[-1] < loss_history[-2] and loss_history[-2] < loss_history[-3] and loss_history[-1] < \\\n",
        "#                    loss_history[-10]:\n",
        "#                tf_optimizer_weights.apply_gradients(zip([-grads_fu], [weight_fu]))\n",
        "#                tf_optimizer_u.apply_gradients(zip([-grads_ub], [weight_ub]))\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('It: %d, Time: %.2f' % (epoch, elapsed))\n",
        "            tf.print(f\"mse_b  {mse_b}  mse_f: {mse_f}   total loss: {loss_value}\")\n",
        "\n",
        "            wu = weight_ub.numpy()\n",
        "            wf = weight_fu.numpy()\n",
        "\n",
        "            MSE_b1.append(mse_b)\n",
        "            MSE_f1.append(mse_f)\n",
        "\n",
        "            weightu.append(wu)\n",
        "            weightf.append(wf)\n",
        "\n",
        "            start_time = time.time()\n",
        "    tf.print(f\"weight_ub: {weight_ub}  weight_fu: {weight_fu}\")\n",
        "    c_pred= predict(X_star)\n",
        "    tf.print('epoch',epoch,'loss',loss_value)\n",
        "    #error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
        "    #print('Error u: %e' % (error_u))\n",
        "    #error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
        "    #print('Error v: %e' % (error_v))\n",
        "    #print(\"Starting L-BFGS training\")\n",
        "\n",
        "    '''\n",
        "    loss_and_flat_grad = get_loss_and_flat_grad(x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch,\n",
        "                                                vb_batch, weight_ub, weight_fu)\n",
        "\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "          get_weights(u_model),\n",
        "          Struct(), maxIter=newton_iter1, learningRate=0.8)\n",
        "\n",
        "    u_pred, v_pred, p_pred = predict(X_star)\n",
        "    error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
        "    print('Error u: %e' % (error_u))\n",
        "    error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
        "    print('Error v: %e' % (error_v))\n",
        "\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "          get_weights(u_model),\n",
        "          Struct(), maxIter=newton_iter2, learningRate=0.8)\n",
        "    '''\n",
        "    return MSE_b1, MSE_f1,  weightu, weightf,loss_saman\n",
        "\n",
        "# L-BFGS implementation from https://github.com/pierremtb/PINNs-TF2.0\n",
        "def get_loss_and_flat_grad(x_f_batch, y_f_batch , xb_batch, yb_batch,\n",
        "                           ub_batch, vb_batch,weight_ub, weight_fu):\n",
        "    def loss_and_flat_grad(w):\n",
        "        with tf.GradientTape() as tape:\n",
        "            set_weights(u_model, w, sizes_w, sizes_b)\n",
        "            loss_value, _, _ = loss(x_f_batch, y_f_batch, xb_batch, yb_batch, ub_batch, vb_batch, weight_ub, weight_fu)\n",
        "        grad = tape.gradient(loss_value, u_model.trainable_variables)\n",
        "        grad_flat = []\n",
        "        for g in grad:\n",
        "            grad_flat.append(tf.reshape(g, [-1]))\n",
        "        grad_flat = tf.concat(grad_flat, 0)\n",
        "        # print(loss_value, grad_flat)\n",
        "        return loss_value, grad_flat\n",
        "\n",
        "    return loss_and_flat_grad\n",
        "\n",
        "\n",
        "def predict(X_star):\n",
        "    X_star = tf.convert_to_tensor(X_star, dtype=tf.float32)\n",
        "    c_star = u_x_model(X_star[:, 0:1], X_star[:, 1:2])\n",
        "    return c_star.numpy()\n",
        "\n",
        "start_time = time.time()\n",
        "MSE_b1, MSE_f1, weightu, weightf, loss_saman, list_model_u = fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star1,xtt,ytt,xbb,ybb,xrr,yrr,xll1,yll1,xll2,yll2,\n",
        "                                                                 tf_iter=300000,\n",
        "                                                                 tf_iter2=1000, newton_iter1=5000,newton_iter2=15000)\n",
        "\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.4f' % (elapsed))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F4oG3eYYsV1C",
        "outputId": "240d34ed-303c-4e79-88fd-d540d9ae6ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xxxxxxxxxxxxxx\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 2)]               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 20)                60        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3)                 63        \n",
            "                                                                 \n",
            " tf.__operators__.getitem (S  (None, 1)                0         \n",
            " licingOpLambda)                                                 \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,643\n",
            "Trainable params: 2,643\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "weight_ub: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([5.], dtype=float32)>  weight_fu: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>\n",
            "starting Adam training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "mse_b  [1.2819963]  mse_f: 0.5324460864067078   total loss: [1.8144424]\n",
            "mse_b ====== [1.27976704]\n",
            "It: 2006, Time: 0.02\n",
            "mse_b  [1.279767]  mse_f: 0.5320045948028564   total loss: [1.8117716]\n",
            "mse_b ====== [1.27747595]\n",
            "It: 2007, Time: 0.02\n",
            "mse_b  [1.277476]  mse_f: 0.531452476978302   total loss: [1.8089285]\n",
            "mse_b ====== [1.27522171]\n",
            "It: 2008, Time: 0.02\n",
            "mse_b  [1.2752217]  mse_f: 0.5311967134475708   total loss: [1.8064184]\n",
            "mse_b ====== [1.2729969]\n",
            "It: 2009, Time: 0.02\n",
            "mse_b  [1.2729969]  mse_f: 0.531293511390686   total loss: [1.8042904]\n",
            "mse_b ====== [1.27071762]\n",
            "It: 2010, Time: 0.03\n",
            "mse_b  [1.2707176]  mse_f: 0.5313593745231628   total loss: [1.802077]\n",
            "mse_b ====== [1.2685349]\n",
            "It: 2011, Time: 0.02\n",
            "mse_b  [1.2685349]  mse_f: 0.5308915972709656   total loss: [1.7994266]\n",
            "mse_b ====== [1.26629806]\n",
            "It: 2012, Time: 0.02\n",
            "mse_b  [1.266298]  mse_f: 0.5302547812461853   total loss: [1.7965529]\n",
            "mse_b ====== [1.26410604]\n",
            "It: 2013, Time: 0.02\n",
            "mse_b  [1.264106]  mse_f: 0.5298395156860352   total loss: [1.7939456]\n",
            "mse_b ====== [1.26193786]\n",
            "It: 2014, Time: 0.02\n",
            "mse_b  [1.2619379]  mse_f: 0.5297917723655701   total loss: [1.7917297]\n",
            "mse_b ====== [1.25973248]\n",
            "It: 2015, Time: 0.02\n",
            "mse_b  [1.2597325]  mse_f: 0.5297790169715881   total loss: [1.7895114]\n",
            "mse_b ====== [1.25762486]\n",
            "It: 2016, Time: 0.03\n",
            "mse_b  [1.2576249]  mse_f: 0.5292844772338867   total loss: [1.7869093]\n",
            "mse_b ====== [1.25545311]\n",
            "It: 2017, Time: 0.02\n",
            "mse_b  [1.2554531]  mse_f: 0.5285639762878418   total loss: [1.7840171]\n",
            "mse_b ====== [1.25333965]\n",
            "It: 2018, Time: 0.02\n",
            "mse_b  [1.2533396]  mse_f: 0.5279624462127686   total loss: [1.7813021]\n",
            "mse_b ====== [1.25124204]\n",
            "It: 2019, Time: 0.02\n",
            "mse_b  [1.251242]  mse_f: 0.5277248620986938   total loss: [1.7789669]\n",
            "mse_b ====== [1.24912286]\n",
            "It: 2020, Time: 0.02\n",
            "mse_b  [1.2491229]  mse_f: 0.5276191234588623   total loss: [1.776742]\n",
            "mse_b ====== [1.24708951]\n",
            "It: 2021, Time: 0.02\n",
            "mse_b  [1.2470895]  mse_f: 0.5271192789077759   total loss: [1.7742088]\n",
            "mse_b ====== [1.24500179]\n",
            "It: 2022, Time: 0.02\n",
            "mse_b  [1.2450018]  mse_f: 0.5263310074806213   total loss: [1.7713327]\n",
            "mse_b ====== [1.24298453]\n",
            "It: 2023, Time: 0.02\n",
            "mse_b  [1.2429845]  mse_f: 0.5255251526832581   total loss: [1.7685096]\n",
            "mse_b ====== [1.2409724]\n",
            "It: 2024, Time: 0.03\n",
            "mse_b  [1.2409724]  mse_f: 0.5250717997550964   total loss: [1.7660441]\n",
            "mse_b ====== [1.23894382]\n",
            "It: 2025, Time: 0.03\n",
            "mse_b  [1.2389438]  mse_f: 0.5248466730117798   total loss: [1.7637905]\n",
            "mse_b ====== [1.23698807]\n",
            "It: 2026, Time: 0.02\n",
            "mse_b  [1.2369881]  mse_f: 0.5243448615074158   total loss: [1.761333]\n",
            "mse_b ====== [1.23498297]\n",
            "It: 2027, Time: 0.02\n",
            "mse_b  [1.234983]  mse_f: 0.5235304832458496   total loss: [1.7585135]\n",
            "mse_b ====== [1.23303545]\n",
            "It: 2028, Time: 0.02\n",
            "mse_b  [1.2330354]  mse_f: 0.5225728750228882   total loss: [1.7556083]\n",
            "mse_b ====== [1.23107243]\n",
            "It: 2029, Time: 0.02\n",
            "mse_b  [1.2310724]  mse_f: 0.5219229459762573   total loss: [1.7529954]\n",
            "mse_b ====== [1.22909379]\n",
            "It: 2030, Time: 0.03\n",
            "mse_b  [1.2290938]  mse_f: 0.5215747952461243   total loss: [1.7506685]\n",
            "mse_b ====== [1.22717381]\n",
            "It: 2031, Time: 0.02\n",
            "mse_b  [1.2271738]  mse_f: 0.5211101174354553   total loss: [1.7482839]\n",
            "mse_b ====== [1.22519]\n",
            "It: 2032, Time: 0.02\n",
            "mse_b  [1.22519]  mse_f: 0.5203653573989868   total loss: [1.7455554]\n",
            "mse_b ====== [1.22326076]\n",
            "It: 2033, Time: 0.02\n",
            "mse_b  [1.2232608]  mse_f: 0.5193584561347961   total loss: [1.7426193]\n",
            "mse_b ====== [1.22130489]\n",
            "It: 2034, Time: 0.02\n",
            "mse_b  [1.2213049]  mse_f: 0.5185370445251465   total loss: [1.7398419]\n",
            "mse_b ====== [1.21935976]\n",
            "It: 2035, Time: 0.02\n",
            "mse_b  [1.2193598]  mse_f: 0.5180310010910034   total loss: [1.7373908]\n",
            "mse_b ====== [1.21744239]\n",
            "It: 2036, Time: 0.02\n",
            "mse_b  [1.2174424]  mse_f: 0.5175901055335999   total loss: [1.7350326]\n",
            "mse_b ====== [1.21546888]\n",
            "It: 2037, Time: 0.02\n",
            "mse_b  [1.2154689]  mse_f: 0.5169576406478882   total loss: [1.7324265]\n",
            "mse_b ====== [1.21354663]\n",
            "It: 2038, Time: 0.02\n",
            "mse_b  [1.2135466]  mse_f: 0.5159748792648315   total loss: [1.7295215]\n",
            "mse_b ====== [1.21158743]\n",
            "It: 2039, Time: 0.02\n",
            "mse_b  [1.2115874]  mse_f: 0.5150229930877686   total loss: [1.7266104]\n",
            "mse_b ====== [1.20963264]\n",
            "It: 2040, Time: 0.02\n",
            "mse_b  [1.2096326]  mse_f: 0.5143429636955261   total loss: [1.7239757]\n",
            "mse_b ====== [1.20768487]\n",
            "It: 2041, Time: 0.02\n",
            "mse_b  [1.2076849]  mse_f: 0.513877272605896   total loss: [1.7215621]\n",
            "mse_b ====== [1.20570517]\n",
            "It: 2042, Time: 0.02\n",
            "mse_b  [1.2057052]  mse_f: 0.5133553147315979   total loss: [1.7190604]\n",
            "mse_b ====== [1.20377076]\n",
            "It: 2043, Time: 0.02\n",
            "mse_b  [1.2037708]  mse_f: 0.512479305267334   total loss: [1.7162501]\n",
            "mse_b ====== [1.20177913]\n",
            "It: 2044, Time: 0.02\n",
            "mse_b  [1.2017791]  mse_f: 0.5114877223968506   total loss: [1.7132668]\n",
            "mse_b ====== [1.19979644]\n",
            "It: 2045, Time: 0.03\n",
            "mse_b  [1.1997964]  mse_f: 0.5106301307678223   total loss: [1.7104266]\n",
            "mse_b ====== [1.19780052]\n",
            "It: 2046, Time: 0.02\n",
            "mse_b  [1.1978005]  mse_f: 0.5100616812705994   total loss: [1.7078621]\n",
            "mse_b ====== [1.1957705]\n",
            "It: 2047, Time: 0.02\n",
            "mse_b  [1.1957705]  mse_f: 0.5096232295036316   total loss: [1.7053938]\n",
            "mse_b ====== [1.193748]\n",
            "It: 2048, Time: 0.02\n",
            "mse_b  [1.193748]  mse_f: 0.5089626312255859   total loss: [1.7027106]\n",
            "mse_b ====== [1.19165444]\n",
            "It: 2049, Time: 0.03\n",
            "mse_b  [1.1916544]  mse_f: 0.5080934762954712   total loss: [1.6997479]\n",
            "mse_b ====== [1.18958187]\n",
            "It: 2050, Time: 0.02\n",
            "mse_b  [1.1895819]  mse_f: 0.5071590542793274   total loss: [1.6967409]\n",
            "mse_b ====== [1.18747258]\n",
            "It: 2051, Time: 0.02\n",
            "mse_b  [1.1874726]  mse_f: 0.50648033618927   total loss: [1.6939529]\n",
            "mse_b ====== [1.18531632]\n",
            "It: 2052, Time: 0.02\n",
            "mse_b  [1.1853163]  mse_f: 0.5060652494430542   total loss: [1.6913816]\n",
            "mse_b ====== [1.18316901]\n",
            "It: 2053, Time: 0.02\n",
            "mse_b  [1.183169]  mse_f: 0.5056191682815552   total loss: [1.6887882]\n",
            "mse_b ====== [1.18096495]\n",
            "It: 2054, Time: 0.03\n",
            "mse_b  [1.180965]  mse_f: 0.5049763917922974   total loss: [1.6859413]\n",
            "mse_b ====== [1.17875719]\n",
            "It: 2055, Time: 0.02\n",
            "mse_b  [1.1787572]  mse_f: 0.5041128993034363   total loss: [1.6828701]\n",
            "mse_b ====== [1.17648959]\n",
            "It: 2056, Time: 0.02\n",
            "mse_b  [1.1764896]  mse_f: 0.5033588409423828   total loss: [1.6798484]\n",
            "mse_b ====== [1.17419839]\n",
            "It: 2057, Time: 0.02\n",
            "mse_b  [1.1741984]  mse_f: 0.5028622150421143   total loss: [1.6770606]\n",
            "mse_b ====== [1.17188919]\n",
            "It: 2058, Time: 0.02\n",
            "mse_b  [1.1718892]  mse_f: 0.5025407075881958   total loss: [1.6744299]\n",
            "mse_b ====== [1.16950524]\n",
            "It: 2059, Time: 0.02\n",
            "mse_b  [1.1695052]  mse_f: 0.5021889209747314   total loss: [1.6716942]\n",
            "mse_b ====== [1.16712236]\n",
            "It: 2060, Time: 0.02\n",
            "mse_b  [1.1671224]  mse_f: 0.5015830397605896   total loss: [1.6687055]\n",
            "mse_b ====== [1.16467953]\n",
            "It: 2061, Time: 0.02\n",
            "mse_b  [1.1646795]  mse_f: 0.5008847117424011   total loss: [1.6655643]\n",
            "mse_b ====== [1.1622262]\n",
            "It: 2062, Time: 0.02\n",
            "mse_b  [1.1622262]  mse_f: 0.5002840757369995   total loss: [1.6625103]\n",
            "mse_b ====== [1.15973699]\n",
            "It: 2063, Time: 0.02\n",
            "mse_b  [1.159737]  mse_f: 0.4999467730522156   total loss: [1.6596837]\n",
            "mse_b ====== [1.15719175]\n",
            "It: 2064, Time: 0.02\n",
            "mse_b  [1.1571918]  mse_f: 0.4997713565826416   total loss: [1.6569631]\n",
            "mse_b ====== [1.15465748]\n",
            "It: 2065, Time: 0.02\n",
            "mse_b  [1.1546575]  mse_f: 0.4994555711746216   total loss: [1.654113]\n",
            "mse_b ====== [1.15207052]\n",
            "It: 2066, Time: 0.03\n",
            "mse_b  [1.1520705]  mse_f: 0.49895578622817993   total loss: [1.6510262]\n",
            "mse_b ====== [1.14947939]\n",
            "It: 2067, Time: 0.03\n",
            "mse_b  [1.1494794]  mse_f: 0.49834543466567993   total loss: [1.6478248]\n",
            "mse_b ====== [1.14684117]\n",
            "It: 2068, Time: 0.02\n",
            "mse_b  [1.1468412]  mse_f: 0.49788689613342285   total loss: [1.6447281]\n",
            "mse_b ====== [1.14419329]\n",
            "It: 2069, Time: 0.02\n",
            "mse_b  [1.1441933]  mse_f: 0.49765563011169434   total loss: [1.6418489]\n",
            "mse_b ====== [1.14153135]\n",
            "It: 2070, Time: 0.02\n",
            "mse_b  [1.1415313]  mse_f: 0.49751660227775574   total loss: [1.639048]\n",
            "mse_b ====== [1.13881397]\n",
            "It: 2071, Time: 0.02\n",
            "mse_b  [1.138814]  mse_f: 0.4973051846027374   total loss: [1.6361191]\n",
            "mse_b ====== [1.13611102]\n",
            "It: 2072, Time: 0.02\n",
            "mse_b  [1.136111]  mse_f: 0.49685603380203247   total loss: [1.632967]\n",
            "mse_b ====== [1.13337159]\n",
            "It: 2073, Time: 0.02\n",
            "mse_b  [1.1333716]  mse_f: 0.49634850025177   total loss: [1.6297201]\n",
            "mse_b ====== [1.13061881]\n",
            "It: 2074, Time: 0.02\n",
            "mse_b  [1.1306188]  mse_f: 0.49595966935157776   total loss: [1.6265785]\n",
            "mse_b ====== [1.12784255]\n",
            "It: 2075, Time: 0.02\n",
            "mse_b  [1.1278425]  mse_f: 0.4957919716835022   total loss: [1.6236346]\n",
            "mse_b ====== [1.12504315]\n",
            "It: 2076, Time: 0.02\n",
            "mse_b  [1.1250432]  mse_f: 0.49573615193367004   total loss: [1.6207793]\n",
            "mse_b ====== [1.1222477]\n",
            "It: 2077, Time: 0.03\n",
            "mse_b  [1.1222477]  mse_f: 0.4955478310585022   total loss: [1.6177955]\n",
            "mse_b ====== [1.11941051]\n",
            "It: 2078, Time: 0.02\n",
            "mse_b  [1.1194105]  mse_f: 0.49520474672317505   total loss: [1.6146152]\n",
            "mse_b ====== [1.11658263]\n",
            "It: 2079, Time: 0.02\n",
            "mse_b  [1.1165826]  mse_f: 0.4947424530982971   total loss: [1.611325]\n",
            "mse_b ====== [1.1137408]\n",
            "It: 2080, Time: 0.03\n",
            "mse_b  [1.1137408]  mse_f: 0.494384765625   total loss: [1.6081256]\n",
            "mse_b ====== [1.11089563]\n",
            "It: 2081, Time: 0.02\n",
            "mse_b  [1.1108956]  mse_f: 0.4942120909690857   total loss: [1.6051078]\n",
            "mse_b ====== [1.10805249]\n",
            "It: 2082, Time: 0.02\n",
            "mse_b  [1.1080525]  mse_f: 0.49413204193115234   total loss: [1.6021845]\n",
            "mse_b ====== [1.1051892]\n",
            "It: 2083, Time: 0.02\n",
            "mse_b  [1.1051892]  mse_f: 0.4939801096916199   total loss: [1.5991693]\n",
            "mse_b ====== [1.10237372]\n",
            "It: 2084, Time: 0.02\n",
            "mse_b  [1.1023737]  mse_f: 0.49358969926834106   total loss: [1.5959635]\n",
            "mse_b ====== [1.09953237]\n",
            "It: 2085, Time: 0.02\n",
            "mse_b  [1.0995324]  mse_f: 0.4930955767631531   total loss: [1.592628]\n",
            "mse_b ====== [1.09671247]\n",
            "It: 2086, Time: 0.02\n",
            "mse_b  [1.0967125]  mse_f: 0.4926309883594513   total loss: [1.5893434]\n",
            "mse_b ====== [1.09389949]\n",
            "It: 2087, Time: 0.02\n",
            "mse_b  [1.0938995]  mse_f: 0.4923173785209656   total loss: [1.5862169]\n",
            "mse_b ====== [1.0910902]\n",
            "It: 2088, Time: 0.02\n",
            "mse_b  [1.0910902]  mse_f: 0.49211302399635315   total loss: [1.5832032]\n",
            "mse_b ====== [1.08831418]\n",
            "It: 2089, Time: 0.02\n",
            "mse_b  [1.0883142]  mse_f: 0.49182742834091187   total loss: [1.5801415]\n",
            "mse_b ====== [1.08551919]\n",
            "It: 2090, Time: 0.03\n",
            "mse_b  [1.0855192]  mse_f: 0.49138104915618896   total loss: [1.5769002]\n",
            "mse_b ====== [1.08279312]\n",
            "It: 2091, Time: 0.02\n",
            "mse_b  [1.0827931]  mse_f: 0.49071794748306274   total loss: [1.5735111]\n",
            "mse_b ====== [1.0800606]\n",
            "It: 2092, Time: 0.02\n",
            "mse_b  [1.0800606]  mse_f: 0.49004966020584106   total loss: [1.5701103]\n",
            "mse_b ====== [1.07736039]\n",
            "It: 2093, Time: 0.02\n",
            "mse_b  [1.0773604]  mse_f: 0.48949292302131653   total loss: [1.5668533]\n",
            "mse_b ====== [1.07468498]\n",
            "It: 2094, Time: 0.03\n",
            "mse_b  [1.074685]  mse_f: 0.4890356659889221   total loss: [1.5637207]\n",
            "mse_b ====== [1.07202244]\n",
            "It: 2095, Time: 0.02\n",
            "mse_b  [1.0720224]  mse_f: 0.48856860399246216   total loss: [1.560591]\n",
            "mse_b ====== [1.06941211]\n",
            "It: 2096, Time: 0.02\n",
            "mse_b  [1.0694121]  mse_f: 0.4879049062728882   total loss: [1.557317]\n",
            "mse_b ====== [1.06677353]\n",
            "It: 2097, Time: 0.02\n",
            "mse_b  [1.0667735]  mse_f: 0.487102210521698   total loss: [1.5538757]\n",
            "mse_b ====== [1.06418979]\n",
            "It: 2098, Time: 0.02\n",
            "mse_b  [1.0641898]  mse_f: 0.4861904978752136   total loss: [1.5503802]\n",
            "mse_b ====== [1.06161702]\n",
            "It: 2099, Time: 0.02\n",
            "mse_b  [1.061617]  mse_f: 0.48536792397499084   total loss: [1.5469849]\n",
            "mse_b ====== [1.05903816]\n",
            "It: 2100, Time: 0.02\n",
            "mse_b  [1.0590382]  mse_f: 0.4846877157688141   total loss: [1.5437258]\n",
            "mse_b ====== [1.05649579]\n",
            "It: 2101, Time: 0.02\n",
            "mse_b  [1.0564958]  mse_f: 0.4840199947357178   total loss: [1.5405158]\n",
            "mse_b ====== [1.05394149]\n",
            "It: 2102, Time: 0.03\n",
            "mse_b  [1.0539415]  mse_f: 0.48327481746673584   total loss: [1.5372163]\n",
            "mse_b ====== [1.05144191]\n",
            "It: 2103, Time: 0.02\n",
            "mse_b  [1.0514419]  mse_f: 0.4823128879070282   total loss: [1.5337548]\n",
            "mse_b ====== [1.04889333]\n",
            "It: 2104, Time: 0.02\n",
            "mse_b  [1.0488933]  mse_f: 0.48129987716674805   total loss: [1.5301932]\n",
            "mse_b ====== [1.0463773]\n",
            "It: 2105, Time: 0.02\n",
            "mse_b  [1.0463773]  mse_f: 0.4803028106689453   total loss: [1.5266801]\n",
            "mse_b ====== [1.04386103]\n",
            "It: 2106, Time: 0.03\n",
            "mse_b  [1.043861]  mse_f: 0.4794411063194275   total loss: [1.5233021]\n",
            "mse_b ====== [1.04132187]\n",
            "It: 2107, Time: 0.02\n",
            "mse_b  [1.0413219]  mse_f: 0.4786953330039978   total loss: [1.5200171]\n",
            "mse_b ====== [1.03881]\n",
            "It: 2108, Time: 0.02\n",
            "mse_b  [1.03881]  mse_f: 0.4778856039047241   total loss: [1.5166956]\n",
            "mse_b ====== [1.03625488]\n",
            "It: 2109, Time: 0.02\n",
            "mse_b  [1.0362549]  mse_f: 0.4769776463508606   total loss: [1.5132325]\n",
            "mse_b ====== [1.03375101]\n",
            "It: 2110, Time: 0.02\n",
            "mse_b  [1.033751]  mse_f: 0.4758981466293335   total loss: [1.5096492]\n",
            "mse_b ====== [1.03118634]\n",
            "It: 2111, Time: 0.02\n",
            "mse_b  [1.0311863]  mse_f: 0.474867045879364   total loss: [1.5060534]\n",
            "mse_b ====== [1.0286212]\n",
            "It: 2112, Time: 0.02\n",
            "mse_b  [1.0286212]  mse_f: 0.47393882274627686   total loss: [1.50256]\n",
            "mse_b ====== [1.0260601]\n",
            "It: 2113, Time: 0.02\n",
            "mse_b  [1.0260601]  mse_f: 0.4731272757053375   total loss: [1.4991874]\n",
            "mse_b ====== [1.02344537]\n",
            "It: 2114, Time: 0.02\n",
            "mse_b  [1.0234454]  mse_f: 0.4723889231681824   total loss: [1.4958344]\n",
            "mse_b ====== [1.02085495]\n",
            "It: 2115, Time: 0.02\n",
            "mse_b  [1.020855]  mse_f: 0.47153526544570923   total loss: [1.4923902]\n",
            "mse_b ====== [1.01819813]\n",
            "It: 2116, Time: 0.02\n",
            "mse_b  [1.0181981]  mse_f: 0.4706145226955414   total loss: [1.4888127]\n",
            "mse_b ====== [1.01556516]\n",
            "It: 2117, Time: 0.03\n",
            "mse_b  [1.0155652]  mse_f: 0.469600647687912   total loss: [1.4851658]\n",
            "mse_b ====== [1.01286662]\n",
            "It: 2118, Time: 0.03\n",
            "mse_b  [1.0128666]  mse_f: 0.4687094986438751   total loss: [1.4815761]\n",
            "mse_b ====== [1.01014066]\n",
            "It: 2119, Time: 0.02\n",
            "mse_b  [1.0101407]  mse_f: 0.467964768409729   total loss: [1.4781054]\n",
            "mse_b ====== [1.00742376]\n",
            "It: 2120, Time: 0.02\n",
            "mse_b  [1.0074238]  mse_f: 0.46728259325027466   total loss: [1.4747064]\n",
            "mse_b ====== [1.0046314]\n",
            "It: 2121, Time: 0.02\n",
            "mse_b  [1.0046314]  mse_f: 0.46663421392440796   total loss: [1.4712656]\n",
            "mse_b ====== [1.00185823]\n",
            "It: 2122, Time: 0.02\n",
            "mse_b  [1.0018582]  mse_f: 0.46584856510162354   total loss: [1.4677068]\n",
            "mse_b ====== [0.998991728]\n",
            "It: 2123, Time: 0.02\n",
            "mse_b  [0.9989917]  mse_f: 0.4650501012802124   total loss: [1.4640418]\n",
            "mse_b ====== [0.996137142]\n",
            "It: 2124, Time: 0.02\n",
            "mse_b  [0.99613714]  mse_f: 0.4642394781112671   total loss: [1.4603766]\n",
            "mse_b ====== [0.993222117]\n",
            "It: 2125, Time: 0.02\n",
            "mse_b  [0.9932221]  mse_f: 0.463584303855896   total loss: [1.4568064]\n",
            "mse_b ====== [0.990255475]\n",
            "It: 2126, Time: 0.02\n",
            "mse_b  [0.9902555]  mse_f: 0.46307995915412903   total loss: [1.4533354]\n",
            "mse_b ====== [0.98730278]\n",
            "It: 2127, Time: 0.02\n",
            "mse_b  [0.9873028]  mse_f: 0.46257781982421875   total loss: [1.4498806]\n",
            "mse_b ====== [0.984261036]\n",
            "It: 2128, Time: 0.02\n",
            "mse_b  [0.98426104]  mse_f: 0.46208152174949646   total loss: [1.4463426]\n",
            "mse_b ====== [0.981239319]\n",
            "It: 2129, Time: 0.02\n",
            "mse_b  [0.9812393]  mse_f: 0.46144795417785645   total loss: [1.4426873]\n",
            "mse_b ====== [0.97813189]\n",
            "It: 2130, Time: 0.02\n",
            "mse_b  [0.9781319]  mse_f: 0.46084582805633545   total loss: [1.4389777]\n",
            "mse_b ====== [0.975044489]\n",
            "It: 2131, Time: 0.02\n",
            "mse_b  [0.9750445]  mse_f: 0.4602724313735962   total loss: [1.4353169]\n",
            "mse_b ====== [0.971926689]\n",
            "It: 2132, Time: 0.02\n",
            "mse_b  [0.9719267]  mse_f: 0.45982980728149414   total loss: [1.4317565]\n",
            "mse_b ====== [0.968758583]\n",
            "It: 2133, Time: 0.02\n",
            "mse_b  [0.9687586]  mse_f: 0.45950204133987427   total loss: [1.4282606]\n",
            "mse_b ====== [0.965644]\n",
            "It: 2134, Time: 0.02\n",
            "mse_b  [0.965644]  mse_f: 0.45909249782562256   total loss: [1.4247365]\n",
            "mse_b ====== [0.962435842]\n",
            "It: 2135, Time: 0.02\n",
            "mse_b  [0.96243584]  mse_f: 0.45866209268569946   total loss: [1.421098]\n",
            "mse_b ====== [0.959284425]\n",
            "It: 2136, Time: 0.03\n",
            "mse_b  [0.9592844]  mse_f: 0.4580882787704468   total loss: [1.4173727]\n",
            "mse_b ====== [0.956079]\n",
            "It: 2137, Time: 0.02\n",
            "mse_b  [0.956079]  mse_f: 0.4575663208961487   total loss: [1.4136453]\n",
            "mse_b ====== [0.952903926]\n",
            "It: 2138, Time: 0.02\n",
            "mse_b  [0.9529039]  mse_f: 0.45708170533180237   total loss: [1.4099857]\n",
            "mse_b ====== [0.949748158]\n",
            "It: 2139, Time: 0.02\n",
            "mse_b  [0.94974816]  mse_f: 0.4566735029220581   total loss: [1.4064217]\n",
            "mse_b ====== [0.946538687]\n",
            "It: 2140, Time: 0.02\n",
            "mse_b  [0.9465387]  mse_f: 0.45633241534233093   total loss: [1.4028711]\n",
            "mse_b ====== [0.943417549]\n",
            "It: 2141, Time: 0.02\n",
            "mse_b  [0.94341755]  mse_f: 0.45583629608154297   total loss: [1.3992538]\n",
            "mse_b ====== [0.940223575]\n",
            "It: 2142, Time: 0.02\n",
            "mse_b  [0.9402236]  mse_f: 0.45530766248703003   total loss: [1.3955312]\n",
            "mse_b ====== [0.937101722]\n",
            "It: 2143, Time: 0.03\n",
            "mse_b  [0.9371017]  mse_f: 0.454653263092041   total loss: [1.391755]\n",
            "mse_b ====== [0.933951795]\n",
            "It: 2144, Time: 0.03\n",
            "mse_b  [0.9339518]  mse_f: 0.4540567696094513   total loss: [1.3880086]\n",
            "mse_b ====== [0.930837095]\n",
            "It: 2145, Time: 0.02\n",
            "mse_b  [0.9308371]  mse_f: 0.4535142481327057   total loss: [1.3843514]\n",
            "mse_b ====== [0.927761495]\n",
            "It: 2146, Time: 0.02\n",
            "mse_b  [0.9277615]  mse_f: 0.4529934823513031   total loss: [1.380755]\n",
            "mse_b ====== [0.92461437]\n",
            "It: 2147, Time: 0.02\n",
            "mse_b  [0.92461437]  mse_f: 0.452517032623291   total loss: [1.3771315]\n",
            "mse_b ====== [0.921584964]\n",
            "It: 2148, Time: 0.02\n",
            "mse_b  [0.92158496]  mse_f: 0.4518398642539978   total loss: [1.3734248]\n",
            "mse_b ====== [0.918478489]\n",
            "It: 2149, Time: 0.02\n",
            "mse_b  [0.9184785]  mse_f: 0.45114850997924805   total loss: [1.369627]\n",
            "mse_b ====== [0.915453494]\n",
            "It: 2150, Time: 0.02\n",
            "mse_b  [0.9154535]  mse_f: 0.4503660202026367   total loss: [1.3658195]\n",
            "mse_b ====== [0.912403166]\n",
            "It: 2151, Time: 0.02\n",
            "mse_b  [0.91240317]  mse_f: 0.4496670961380005   total loss: [1.3620703]\n",
            "mse_b ====== [0.909365714]\n",
            "It: 2152, Time: 0.02\n",
            "mse_b  [0.9093657]  mse_f: 0.4490443766117096   total loss: [1.3584101]\n",
            "mse_b ====== [0.906363]\n",
            "It: 2153, Time: 0.03\n",
            "mse_b  [0.906363]  mse_f: 0.4484073519706726   total loss: [1.3547704]\n",
            "mse_b ====== [0.903275132]\n",
            "It: 2154, Time: 0.02\n",
            "mse_b  [0.90327513]  mse_f: 0.44781404733657837   total loss: [1.3510892]\n",
            "mse_b ====== [0.900303423]\n",
            "It: 2155, Time: 0.02\n",
            "mse_b  [0.9003034]  mse_f: 0.4470111131668091   total loss: [1.3473146]\n",
            "mse_b ====== [0.897253156]\n",
            "It: 2156, Time: 0.02\n",
            "mse_b  [0.89725316]  mse_f: 0.4462352991104126   total loss: [1.3434885]\n",
            "mse_b ====== [0.894259572]\n",
            "It: 2157, Time: 0.02\n",
            "mse_b  [0.8942596]  mse_f: 0.4454226493835449   total loss: [1.3396822]\n",
            "mse_b ====== [0.891240597]\n",
            "It: 2158, Time: 0.02\n",
            "mse_b  [0.8912406]  mse_f: 0.44471219182014465   total loss: [1.3359528]\n",
            "mse_b ====== [0.888196707]\n",
            "It: 2159, Time: 0.02\n",
            "mse_b  [0.8881967]  mse_f: 0.44409650564193726   total loss: [1.3322933]\n",
            "mse_b ====== [0.885207355]\n",
            "It: 2160, Time: 0.02\n",
            "mse_b  [0.88520736]  mse_f: 0.4434366524219513   total loss: [1.328644]\n",
            "mse_b ====== [0.882106543]\n",
            "It: 2161, Time: 0.02\n",
            "mse_b  [0.88210654]  mse_f: 0.44282281398773193   total loss: [1.3249294]\n",
            "mse_b ====== [0.879119098]\n",
            "It: 2162, Time: 0.03\n",
            "mse_b  [0.8791191]  mse_f: 0.44202208518981934   total loss: [1.3211412]\n",
            "mse_b ====== [0.876019]\n",
            "It: 2163, Time: 0.02\n",
            "mse_b  [0.876019]  mse_f: 0.4412996768951416   total loss: [1.3173187]\n",
            "mse_b ====== [0.872949839]\n",
            "It: 2164, Time: 0.02\n",
            "mse_b  [0.87294984]  mse_f: 0.44059646129608154   total loss: [1.3135463]\n",
            "mse_b ====== [0.869848251]\n",
            "It: 2165, Time: 0.02\n",
            "mse_b  [0.86984825]  mse_f: 0.43999069929122925   total loss: [1.309839]\n",
            "mse_b ====== [0.866700888]\n",
            "It: 2166, Time: 0.02\n",
            "mse_b  [0.8667009]  mse_f: 0.4394943118095398   total loss: [1.3061953]\n",
            "mse_b ====== [0.863607526]\n",
            "It: 2167, Time: 0.03\n",
            "mse_b  [0.8636075]  mse_f: 0.4389305114746094   total loss: [1.302538]\n",
            "mse_b ====== [0.860385776]\n",
            "It: 2168, Time: 0.02\n",
            "mse_b  [0.8603858]  mse_f: 0.43843188881874084   total loss: [1.2988176]\n",
            "mse_b ====== [0.857251167]\n",
            "It: 2169, Time: 0.02\n",
            "mse_b  [0.85725117]  mse_f: 0.43777576088905334   total loss: [1.2950269]\n",
            "mse_b ====== [0.853993595]\n",
            "It: 2170, Time: 0.02\n",
            "mse_b  [0.8539936]  mse_f: 0.4372400641441345   total loss: [1.2912337]\n",
            "mse_b ====== [0.850744605]\n",
            "It: 2171, Time: 0.02\n",
            "mse_b  [0.8507446]  mse_f: 0.4367420971393585   total loss: [1.2874867]\n",
            "mse_b ====== [0.847491503]\n",
            "It: 2172, Time: 0.02\n",
            "mse_b  [0.8474915]  mse_f: 0.4363137483596802   total loss: [1.2838053]\n",
            "mse_b ====== [0.844174147]\n",
            "It: 2173, Time: 0.02\n",
            "mse_b  [0.84417415]  mse_f: 0.4360021650791168   total loss: [1.2801763]\n",
            "mse_b ====== [0.84092021]\n",
            "It: 2174, Time: 0.03\n",
            "mse_b  [0.8409202]  mse_f: 0.4356003403663635   total loss: [1.2765205]\n",
            "mse_b ====== [0.837541819]\n",
            "It: 2175, Time: 0.02\n",
            "mse_b  [0.8375418]  mse_f: 0.435269296169281   total loss: [1.2728112]\n",
            "mse_b ====== [0.834262729]\n",
            "It: 2176, Time: 0.02\n",
            "mse_b  [0.8342627]  mse_f: 0.43478286266326904   total loss: [1.2690456]\n",
            "mse_b ====== [0.830873549]\n",
            "It: 2177, Time: 0.02\n",
            "mse_b  [0.83087355]  mse_f: 0.4344051480293274   total loss: [1.2652787]\n",
            "mse_b ====== [0.827532887]\n",
            "It: 2178, Time: 0.02\n",
            "mse_b  [0.8275329]  mse_f: 0.43402957916259766   total loss: [1.2615625]\n",
            "mse_b ====== [0.824217141]\n",
            "It: 2179, Time: 0.02\n",
            "mse_b  [0.82421714]  mse_f: 0.43369805812835693   total loss: [1.2579153]\n",
            "mse_b ====== [0.820835233]\n",
            "It: 2180, Time: 0.02\n",
            "mse_b  [0.82083523]  mse_f: 0.4334717392921448   total loss: [1.254307]\n",
            "mse_b ====== [0.817544758]\n",
            "It: 2181, Time: 0.02\n",
            "mse_b  [0.81754476]  mse_f: 0.43313419818878174   total loss: [1.250679]\n",
            "mse_b ====== [0.814150453]\n",
            "It: 2182, Time: 0.03\n",
            "mse_b  [0.81415045]  mse_f: 0.4328450560569763   total loss: [1.2469954]\n",
            "mse_b ====== [0.8108899]\n",
            "It: 2183, Time: 0.02\n",
            "mse_b  [0.8108899]  mse_f: 0.43237316608428955   total loss: [1.243263]\n",
            "mse_b ====== [0.80755204]\n",
            "It: 2184, Time: 0.02\n",
            "mse_b  [0.80755204]  mse_f: 0.4319864511489868   total loss: [1.2395384]\n",
            "mse_b ====== [0.804290771]\n",
            "It: 2185, Time: 0.02\n",
            "mse_b  [0.8042908]  mse_f: 0.43156886100769043   total loss: [1.2358596]\n",
            "mse_b ====== [0.801062465]\n",
            "It: 2186, Time: 0.02\n",
            "mse_b  [0.80106246]  mse_f: 0.4311891198158264   total loss: [1.2322516]\n",
            "mse_b ====== [0.797767699]\n",
            "It: 2187, Time: 0.02\n",
            "mse_b  [0.7977677]  mse_f: 0.43090641498565674   total loss: [1.2286742]\n",
            "mse_b ====== [0.794604778]\n",
            "It: 2188, Time: 0.03\n",
            "mse_b  [0.7946048]  mse_f: 0.43047234416007996   total loss: [1.2250772]\n",
            "mse_b ====== [0.791358948]\n",
            "It: 2189, Time: 0.02\n",
            "mse_b  [0.79135895]  mse_f: 0.4300725758075714   total loss: [1.2214315]\n",
            "mse_b ====== [0.788261354]\n",
            "It: 2190, Time: 0.02\n",
            "mse_b  [0.78826135]  mse_f: 0.4294860363006592   total loss: [1.2177474]\n",
            "mse_b ====== [0.785083294]\n",
            "It: 2191, Time: 0.02\n",
            "mse_b  [0.7850833]  mse_f: 0.4289804995059967   total loss: [1.2140638]\n",
            "mse_b ====== [0.781987429]\n",
            "It: 2192, Time: 0.03\n",
            "mse_b  [0.7819874]  mse_f: 0.4284409284591675   total loss: [1.2104284]\n",
            "mse_b ====== [0.778909862]\n",
            "It: 2193, Time: 0.02\n",
            "mse_b  [0.77890986]  mse_f: 0.42794525623321533   total loss: [1.206855]\n",
            "mse_b ====== [0.775785923]\n",
            "It: 2194, Time: 0.03\n",
            "mse_b  [0.7757859]  mse_f: 0.42753589153289795   total loss: [1.2033218]\n",
            "mse_b ====== [0.772821546]\n",
            "It: 2195, Time: 0.02\n",
            "mse_b  [0.77282155]  mse_f: 0.4269494414329529   total loss: [1.1997709]\n",
            "mse_b ====== [0.769733727]\n",
            "It: 2196, Time: 0.02\n",
            "mse_b  [0.7697337]  mse_f: 0.42644667625427246   total loss: [1.1961803]\n",
            "mse_b ====== [0.766783]\n",
            "It: 2197, Time: 0.03\n",
            "mse_b  [0.766783]  mse_f: 0.4257715344429016   total loss: [1.1925545]\n",
            "mse_b ====== [0.763738513]\n",
            "It: 2198, Time: 0.02\n",
            "mse_b  [0.7637385]  mse_f: 0.4251890182495117   total loss: [1.1889275]\n",
            "mse_b ====== [0.76077652]\n",
            "It: 2199, Time: 0.02\n",
            "mse_b  [0.7607765]  mse_f: 0.4245777726173401   total loss: [1.1853542]\n",
            "mse_b ====== [0.757818282]\n",
            "It: 2200, Time: 0.02\n",
            "mse_b  [0.7578183]  mse_f: 0.42401641607284546   total loss: [1.1818347]\n",
            "mse_b ====== [0.754814386]\n",
            "It: 2201, Time: 0.02\n",
            "mse_b  [0.7548144]  mse_f: 0.42354172468185425   total loss: [1.1783562]\n",
            "mse_b ====== [0.751940608]\n",
            "It: 2202, Time: 0.02\n",
            "mse_b  [0.7519406]  mse_f: 0.4229327142238617   total loss: [1.1748734]\n",
            "mse_b ====== [0.748888135]\n",
            "It: 2203, Time: 0.02\n",
            "mse_b  [0.74888813]  mse_f: 0.4224648177623749   total loss: [1.171353]\n",
            "mse_b ====== [0.745990396]\n",
            "It: 2204, Time: 0.02\n",
            "mse_b  [0.7459904]  mse_f: 0.4218076467514038   total loss: [1.167798]\n",
            "mse_b ====== [0.742984414]\n",
            "It: 2205, Time: 0.02\n",
            "mse_b  [0.7429844]  mse_f: 0.42126116156578064   total loss: [1.1642456]\n",
            "mse_b ====== [0.740038872]\n",
            "It: 2206, Time: 0.02\n",
            "mse_b  [0.7400389]  mse_f: 0.42068639397621155   total loss: [1.1607252]\n",
            "mse_b ====== [0.737075567]\n",
            "It: 2207, Time: 0.03\n",
            "mse_b  [0.73707557]  mse_f: 0.4201885163784027   total loss: [1.1572641]\n",
            "mse_b ====== [0.734036922]\n",
            "It: 2208, Time: 0.02\n",
            "mse_b  [0.7340369]  mse_f: 0.41980916261672974   total loss: [1.153846]\n",
            "mse_b ====== [0.731100559]\n",
            "It: 2209, Time: 0.02\n",
            "mse_b  [0.73110056]  mse_f: 0.41932564973831177   total loss: [1.1504261]\n",
            "mse_b ====== [0.727977693]\n",
            "It: 2210, Time: 0.02\n",
            "mse_b  [0.7279777]  mse_f: 0.4189957082271576   total loss: [1.1469734]\n",
            "mse_b ====== [0.725050867]\n",
            "It: 2211, Time: 0.02\n",
            "mse_b  [0.72505087]  mse_f: 0.4184330701828003   total loss: [1.1434839]\n",
            "mse_b ====== [0.721952379]\n",
            "It: 2212, Time: 0.02\n",
            "mse_b  [0.7219524]  mse_f: 0.4180389940738678   total loss: [1.1399914]\n",
            "mse_b ====== [0.718913555]\n",
            "It: 2213, Time: 0.02\n",
            "mse_b  [0.71891356]  mse_f: 0.41761666536331177   total loss: [1.1365302]\n",
            "mse_b ====== [0.715850592]\n",
            "It: 2214, Time: 0.03\n",
            "mse_b  [0.7158506]  mse_f: 0.41727346181869507   total loss: [1.1331241]\n",
            "mse_b ====== [0.712700129]\n",
            "It: 2215, Time: 0.02\n",
            "mse_b  [0.7127001]  mse_f: 0.4170525074005127   total loss: [1.1297526]\n",
            "mse_b ====== [0.709687173]\n",
            "It: 2216, Time: 0.03\n",
            "mse_b  [0.7096872]  mse_f: 0.4167024493217468   total loss: [1.1263896]\n",
            "mse_b ====== [0.706486404]\n",
            "It: 2217, Time: 0.02\n",
            "mse_b  [0.7064864]  mse_f: 0.41651105880737305   total loss: [1.1229975]\n",
            "mse_b ====== [0.703490138]\n",
            "It: 2218, Time: 0.02\n",
            "mse_b  [0.70349014]  mse_f: 0.41608452796936035   total loss: [1.1195747]\n",
            "mse_b ====== [0.70026511]\n",
            "It: 2219, Time: 0.02\n",
            "mse_b  [0.7002651]  mse_f: 0.41587117314338684   total loss: [1.1161363]\n",
            "mse_b ====== [0.697179437]\n",
            "It: 2220, Time: 0.02\n",
            "mse_b  [0.69717944]  mse_f: 0.4155440628528595   total loss: [1.1127235]\n",
            "mse_b ====== [0.694079459]\n",
            "It: 2221, Time: 0.03\n",
            "mse_b  [0.69407946]  mse_f: 0.41528353095054626   total loss: [1.109363]\n",
            "mse_b ====== [0.690894604]\n",
            "It: 2222, Time: 0.02\n",
            "mse_b  [0.6908946]  mse_f: 0.41514772176742554   total loss: [1.1060424]\n",
            "mse_b ====== [0.687892079]\n",
            "It: 2223, Time: 0.02\n",
            "mse_b  [0.6878921]  mse_f: 0.4148404896259308   total loss: [1.1027325]\n",
            "mse_b ====== [0.684642196]\n",
            "It: 2224, Time: 0.03\n",
            "mse_b  [0.6846422]  mse_f: 0.4147491455078125   total loss: [1.0993913]\n",
            "mse_b ====== [0.681642592]\n",
            "It: 2225, Time: 0.03\n",
            "mse_b  [0.6816426]  mse_f: 0.4143775999546051   total loss: [1.0960202]\n",
            "mse_b ====== [0.678435326]\n",
            "It: 2226, Time: 0.02\n",
            "mse_b  [0.6784353]  mse_f: 0.41419804096221924   total loss: [1.0926334]\n",
            "mse_b ====== [0.675413549]\n",
            "It: 2227, Time: 0.02\n",
            "mse_b  [0.67541355]  mse_f: 0.41385671496391296   total loss: [1.0892702]\n",
            "mse_b ====== [0.672313273]\n",
            "It: 2228, Time: 0.02\n",
            "mse_b  [0.6723133]  mse_f: 0.4136336147785187   total loss: [1.0859469]\n",
            "mse_b ====== [0.66913116]\n",
            "It: 2229, Time: 0.02\n",
            "mse_b  [0.66913116]  mse_f: 0.41352444887161255   total loss: [1.0826557]\n",
            "mse_b ====== [0.66616714]\n",
            "It: 2230, Time: 0.03\n",
            "mse_b  [0.66616714]  mse_f: 0.41321271657943726   total loss: [1.0793798]\n",
            "mse_b ====== [0.662900865]\n",
            "It: 2231, Time: 0.02\n",
            "mse_b  [0.66290087]  mse_f: 0.4131755232810974   total loss: [1.0760764]\n",
            "mse_b ====== [0.659987807]\n",
            "It: 2232, Time: 0.02\n",
            "mse_b  [0.6599878]  mse_f: 0.412746787071228   total loss: [1.0727346]\n",
            "mse_b ====== [0.656769872]\n",
            "It: 2233, Time: 0.02\n",
            "mse_b  [0.6567699]  mse_f: 0.4126117527484894   total loss: [1.0693816]\n",
            "mse_b ====== [0.653735697]\n",
            "It: 2234, Time: 0.02\n",
            "mse_b  [0.6537357]  mse_f: 0.41229957342147827   total loss: [1.0660353]\n",
            "mse_b ====== [0.650621355]\n",
            "It: 2235, Time: 0.02\n",
            "mse_b  [0.65062135]  mse_f: 0.41209596395492554   total loss: [1.0627173]\n",
            "mse_b ====== [0.647458911]\n",
            "It: 2236, Time: 0.02\n",
            "mse_b  [0.6474589]  mse_f: 0.4119735360145569   total loss: [1.0594325]\n",
            "mse_b ====== [0.644518673]\n",
            "It: 2237, Time: 0.02\n",
            "mse_b  [0.6445187]  mse_f: 0.411640465259552   total loss: [1.0561591]\n",
            "mse_b ====== [0.641230404]\n",
            "It: 2238, Time: 0.02\n",
            "mse_b  [0.6412304]  mse_f: 0.411633163690567   total loss: [1.0528636]\n",
            "mse_b ====== [0.638355]\n",
            "It: 2239, Time: 0.03\n",
            "mse_b  [0.638355]  mse_f: 0.41117462515830994   total loss: [1.0495297]\n",
            "mse_b ====== [0.635032177]\n",
            "It: 2240, Time: 0.02\n",
            "mse_b  [0.6350322]  mse_f: 0.4111339747905731   total loss: [1.0461662]\n",
            "mse_b ====== [0.632081926]\n",
            "It: 2241, Time: 0.02\n",
            "mse_b  [0.6320819]  mse_f: 0.4107212722301483   total loss: [1.0428032]\n",
            "mse_b ====== [0.628961086]\n",
            "It: 2242, Time: 0.02\n",
            "mse_b  [0.6289611]  mse_f: 0.4104995131492615   total loss: [1.0394607]\n",
            "mse_b ====== [0.62578088]\n",
            "It: 2243, Time: 0.02\n",
            "mse_b  [0.6257809]  mse_f: 0.4103660583496094   total loss: [1.0361469]\n",
            "mse_b ====== [0.622863412]\n",
            "It: 2244, Time: 0.02\n",
            "mse_b  [0.6228634]  mse_f: 0.4099770784378052   total loss: [1.0328405]\n",
            "mse_b ====== [0.619506836]\n",
            "It: 2245, Time: 0.02\n",
            "mse_b  [0.61950684]  mse_f: 0.4100160598754883   total loss: [1.0295229]\n",
            "mse_b ====== [0.616707146]\n",
            "It: 2246, Time: 0.02\n",
            "mse_b  [0.61670715]  mse_f: 0.4094579219818115   total loss: [1.026165]\n",
            "mse_b ====== [0.61335969]\n",
            "It: 2247, Time: 0.02\n",
            "mse_b  [0.6133597]  mse_f: 0.4094116687774658   total loss: [1.0227714]\n",
            "mse_b ====== [0.610490263]\n",
            "It: 2248, Time: 0.02\n",
            "mse_b  [0.61049026]  mse_f: 0.40887370705604553   total loss: [1.019364]\n",
            "mse_b ====== [0.607276917]\n",
            "It: 2249, Time: 0.02\n",
            "mse_b  [0.6072769]  mse_f: 0.40869253873825073   total loss: [1.0159695]\n",
            "mse_b ====== [0.604200304]\n",
            "It: 2250, Time: 0.02\n",
            "mse_b  [0.6042003]  mse_f: 0.4083952009677887   total loss: [1.0125955]\n",
            "mse_b ====== [0.601294518]\n",
            "It: 2251, Time: 0.02\n",
            "mse_b  [0.6012945]  mse_f: 0.40794283151626587   total loss: [1.0092373]\n",
            "mse_b ====== [0.597953498]\n",
            "It: 2252, Time: 0.02\n",
            "mse_b  [0.5979535]  mse_f: 0.4079182744026184   total loss: [1.0058718]\n",
            "mse_b ====== [0.59530592]\n",
            "It: 2253, Time: 0.03\n",
            "mse_b  [0.5953059]  mse_f: 0.4071792960166931   total loss: [1.0024853]\n",
            "mse_b ====== [0.591798723]\n",
            "It: 2254, Time: 0.02\n",
            "mse_b  [0.5917987]  mse_f: 0.4072636365890503   total loss: [0.99906236]\n",
            "mse_b ====== [0.589192748]\n",
            "It: 2255, Time: 0.02\n",
            "mse_b  [0.58919275]  mse_f: 0.40642112493515015   total loss: [0.9956139]\n",
            "mse_b ====== [0.585868597]\n",
            "It: 2256, Time: 0.04\n",
            "mse_b  [0.5858686]  mse_f: 0.4062865376472473   total loss: [0.99215513]\n",
            "mse_b ====== [0.583026648]\n",
            "It: 2257, Time: 0.02\n",
            "mse_b  [0.58302665]  mse_f: 0.4056806266307831   total loss: [0.9887073]\n",
            "mse_b ====== [0.580063403]\n",
            "It: 2258, Time: 0.03\n",
            "mse_b  [0.5800634]  mse_f: 0.4052119255065918   total loss: [0.9852753]\n",
            "mse_b ====== [0.576833248]\n",
            "It: 2259, Time: 0.02\n",
            "mse_b  [0.57683325]  mse_f: 0.4050174951553345   total loss: [0.98185074]\n",
            "mse_b ====== [0.574294865]\n",
            "It: 2260, Time: 0.02\n",
            "mse_b  [0.57429487]  mse_f: 0.40413898229599   total loss: [0.97843385]\n",
            "mse_b ====== [0.570790946]\n",
            "It: 2261, Time: 0.03\n",
            "mse_b  [0.57079095]  mse_f: 0.4042084515094757   total loss: [0.9749994]\n",
            "mse_b ====== [0.568523943]\n",
            "It: 2262, Time: 0.03\n",
            "mse_b  [0.56852394]  mse_f: 0.4030134081840515   total loss: [0.97153735]\n",
            "mse_b ====== [0.564856768]\n",
            "It: 2263, Time: 0.02\n",
            "mse_b  [0.56485677]  mse_f: 0.4031948447227478   total loss: [0.9680516]\n",
            "mse_b ====== [0.562666774]\n",
            "It: 2264, Time: 0.02\n",
            "mse_b  [0.5626668]  mse_f: 0.40187978744506836   total loss: [0.96454656]\n",
            "mse_b ====== [0.55909282]\n",
            "It: 2265, Time: 0.02\n",
            "mse_b  [0.5590928]  mse_f: 0.40193334221839905   total loss: [0.9610262]\n",
            "mse_b ====== [0.556773424]\n",
            "It: 2266, Time: 0.02\n",
            "mse_b  [0.5567734]  mse_f: 0.40073126554489136   total loss: [0.9575047]\n",
            "mse_b ====== [0.553450048]\n",
            "It: 2267, Time: 0.03\n",
            "mse_b  [0.55345005]  mse_f: 0.4005260467529297   total loss: [0.9539761]\n",
            "mse_b ====== [0.550781429]\n",
            "It: 2268, Time: 0.02\n",
            "mse_b  [0.5507814]  mse_f: 0.3996666967868805   total loss: [0.95044816]\n",
            "mse_b ====== [0.547876239]\n",
            "It: 2269, Time: 0.02\n",
            "mse_b  [0.54787624]  mse_f: 0.3990488052368164   total loss: [0.94692504]\n",
            "mse_b ====== [0.544809222]\n",
            "It: 2270, Time: 0.02\n",
            "mse_b  [0.5448092]  mse_f: 0.3985908627510071   total loss: [0.9434001]\n",
            "mse_b ====== [0.54236]\n",
            "It: 2271, Time: 0.02\n",
            "mse_b  [0.54236]  mse_f: 0.3975203037261963   total loss: [0.9398803]\n",
            "mse_b ====== [0.538776338]\n",
            "It: 2272, Time: 0.02\n",
            "mse_b  [0.53877634]  mse_f: 0.3975951671600342   total loss: [0.9363715]\n",
            "mse_b ====== [0.536915839]\n",
            "It: 2273, Time: 0.02\n",
            "mse_b  [0.53691584]  mse_f: 0.395968496799469   total loss: [0.93288434]\n",
            "mse_b ====== [0.53258872]\n",
            "It: 2274, Time: 0.03\n",
            "mse_b  [0.5325887]  mse_f: 0.3968800902366638   total loss: [0.9294688]\n",
            "mse_b ====== [0.53191644]\n",
            "It: 2275, Time: 0.02\n",
            "mse_b  [0.53191644]  mse_f: 0.3942875564098358   total loss: [0.92620397]\n",
            "mse_b ====== [0.525871694]\n",
            "It: 2276, Time: 0.02\n",
            "mse_b  [0.5258717]  mse_f: 0.3975089490413666   total loss: [0.9233806]\n",
            "mse_b ====== [0.528279305]\n",
            "It: 2277, Time: 0.02\n",
            "mse_b  [0.5282793]  mse_f: 0.3932953476905823   total loss: [0.92157465]\n",
            "mse_b ====== [0.518118858]\n",
            "It: 2278, Time: 0.02\n",
            "mse_b  [0.51811886]  mse_f: 0.4049656391143799   total loss: [0.9230845]\n",
            "mse_b ====== [0.529940605]\n",
            "It: 2279, Time: 0.02\n",
            "mse_b  [0.5299406]  mse_f: 0.4019133448600769   total loss: [0.93185395]\n",
            "mse_b ====== [0.512906909]\n",
            "It: 2280, Time: 0.02\n",
            "mse_b  [0.5129069]  mse_f: 0.45649591088294983   total loss: [0.9694028]\n",
            "mse_b ====== [0.547007442]\n",
            "It: 2281, Time: 0.03\n",
            "mse_b  [0.54700744]  mse_f: 0.48542043566703796   total loss: [1.0324279]\n",
            "mse_b ====== [0.546263397]\n",
            "It: 2282, Time: 0.03\n",
            "mse_b  [0.5462634]  mse_f: 0.6877188682556152   total loss: [1.2339823]\n",
            "mse_b ====== [0.531994]\n",
            "It: 2283, Time: 0.02\n",
            "mse_b  [0.531994]  mse_f: 0.6061944961547852   total loss: [1.1381885]\n",
            "mse_b ====== [0.513313]\n",
            "It: 2284, Time: 0.02\n",
            "mse_b  [0.513313]  mse_f: 0.5409005284309387   total loss: [1.0542135]\n",
            "mse_b ====== [0.512485862]\n",
            "It: 2285, Time: 0.02\n",
            "mse_b  [0.51248586]  mse_f: 0.4643761217594147   total loss: [0.97686195]\n",
            "mse_b ====== [0.534197152]\n",
            "It: 2286, Time: 0.02\n",
            "mse_b  [0.53419715]  mse_f: 0.41427797079086304   total loss: [0.9484751]\n",
            "mse_b ====== [0.522737145]\n",
            "It: 2287, Time: 0.02\n",
            "mse_b  [0.52273715]  mse_f: 0.5192865133285522   total loss: [1.0420237]\n",
            "mse_b ====== [0.507557094]\n",
            "It: 2288, Time: 0.02\n",
            "mse_b  [0.5075571]  mse_f: 0.609826385974884   total loss: [1.1173835]\n",
            "mse_b ====== [0.507602215]\n",
            "It: 2289, Time: 0.02\n",
            "mse_b  [0.5076022]  mse_f: 0.5749799013137817   total loss: [1.0825821]\n",
            "mse_b ====== [0.499164522]\n",
            "It: 2290, Time: 0.02\n",
            "mse_b  [0.49916452]  mse_f: 0.4525938928127289   total loss: [0.9517584]\n",
            "mse_b ====== [0.507670939]\n",
            "It: 2291, Time: 0.02\n",
            "mse_b  [0.50767094]  mse_f: 0.41220659017562866   total loss: [0.9198775]\n",
            "mse_b ====== [0.489935279]\n",
            "It: 2292, Time: 0.02\n",
            "mse_b  [0.48993528]  mse_f: 0.6198432445526123   total loss: [1.1097785]\n",
            "mse_b ====== [0.486268938]\n",
            "It: 2293, Time: 0.03\n",
            "mse_b  [0.48626894]  mse_f: 0.6404781937599182   total loss: [1.1267471]\n",
            "mse_b ====== [0.481120348]\n",
            "It: 2294, Time: 0.02\n",
            "mse_b  [0.48112035]  mse_f: 0.43814682960510254   total loss: [0.9192672]\n",
            "mse_b ====== [0.500856757]\n",
            "It: 2295, Time: 0.02\n",
            "mse_b  [0.50085676]  mse_f: 0.4316083490848541   total loss: [0.9324651]\n",
            "mse_b ====== [0.46657756]\n",
            "It: 2296, Time: 0.02\n",
            "mse_b  [0.46657756]  mse_f: 0.6022486686706543   total loss: [1.0688262]\n",
            "mse_b ====== [0.466250628]\n",
            "It: 2297, Time: 0.02\n",
            "mse_b  [0.46625063]  mse_f: 0.5153860449790955   total loss: [0.98163664]\n",
            "mse_b ====== [0.487972379]\n",
            "It: 2298, Time: 0.02\n",
            "mse_b  [0.48797238]  mse_f: 0.4043498635292053   total loss: [0.89232224]\n",
            "mse_b ====== [0.463376]\n",
            "It: 2299, Time: 0.02\n",
            "mse_b  [0.463376]  mse_f: 0.6260838508605957   total loss: [1.0894599]\n",
            "mse_b ====== [0.50332725]\n",
            "It: 2300, Time: 0.02\n",
            "mse_b  [0.50332725]  mse_f: 0.5008090734481812   total loss: [1.0041363]\n",
            "mse_b ====== [0.466507316]\n",
            "It: 2301, Time: 0.02\n",
            "mse_b  [0.46650732]  mse_f: 0.3983297049999237   total loss: [0.86483705]\n",
            "mse_b ====== [0.46748513]\n",
            "It: 2302, Time: 0.02\n",
            "mse_b  [0.46748513]  mse_f: 0.4946582317352295   total loss: [0.96214336]\n",
            "mse_b ====== [0.491331935]\n",
            "It: 2303, Time: 0.02\n",
            "mse_b  [0.49133193]  mse_f: 0.5047047138214111   total loss: [0.99603665]\n",
            "mse_b ====== [0.471691191]\n",
            "It: 2304, Time: 0.03\n",
            "mse_b  [0.4716912]  mse_f: 0.40963295102119446   total loss: [0.8813242]\n",
            "mse_b ====== [0.475157857]\n",
            "It: 2305, Time: 0.03\n",
            "mse_b  [0.47515786]  mse_f: 0.4596880078315735   total loss: [0.93484586]\n",
            "mse_b ====== [0.47250843]\n",
            "It: 2306, Time: 0.02\n",
            "mse_b  [0.47250843]  mse_f: 0.5413874983787537   total loss: [1.013896]\n",
            "mse_b ====== [0.466423213]\n",
            "It: 2307, Time: 0.02\n",
            "mse_b  [0.4664232]  mse_f: 0.38418734073638916   total loss: [0.85061055]\n",
            "mse_b ====== [0.468878597]\n",
            "It: 2308, Time: 0.02\n",
            "mse_b  [0.4688786]  mse_f: 0.42155325412750244   total loss: [0.8904319]\n",
            "mse_b ====== [0.466442704]\n",
            "It: 2309, Time: 0.02\n",
            "mse_b  [0.4664427]  mse_f: 0.5325440764427185   total loss: [0.9989868]\n",
            "mse_b ====== [0.458556563]\n",
            "It: 2310, Time: 0.02\n",
            "mse_b  [0.45855656]  mse_f: 0.40971839427948   total loss: [0.8682749]\n",
            "mse_b ====== [0.448609889]\n",
            "It: 2311, Time: 0.02\n",
            "mse_b  [0.4486099]  mse_f: 0.4344877600669861   total loss: [0.88309765]\n",
            "mse_b ====== [0.46202755]\n",
            "It: 2312, Time: 0.02\n",
            "mse_b  [0.46202755]  mse_f: 0.49633994698524475   total loss: [0.95836747]\n",
            "mse_b ====== [0.446099907]\n",
            "It: 2313, Time: 0.02\n",
            "mse_b  [0.4460999]  mse_f: 0.4019532799720764   total loss: [0.8480532]\n",
            "mse_b ====== [0.435063094]\n",
            "It: 2314, Time: 0.02\n",
            "mse_b  [0.4350631]  mse_f: 0.4106904864311218   total loss: [0.84575355]\n",
            "mse_b ====== [0.44414711]\n",
            "It: 2315, Time: 0.02\n",
            "mse_b  [0.4441471]  mse_f: 0.5126802921295166   total loss: [0.9568274]\n",
            "mse_b ====== [0.428789437]\n",
            "It: 2316, Time: 0.02\n",
            "mse_b  [0.42878944]  mse_f: 0.4435614347457886   total loss: [0.8723509]\n",
            "mse_b ====== [0.446426749]\n",
            "It: 2317, Time: 0.03\n",
            "mse_b  [0.44642675]  mse_f: 0.437938928604126   total loss: [0.8843657]\n",
            "mse_b ====== [0.430198044]\n",
            "It: 2318, Time: 0.03\n",
            "mse_b  [0.43019804]  mse_f: 0.5614200234413147   total loss: [0.99161804]\n",
            "mse_b ====== [0.455385804]\n",
            "It: 2319, Time: 0.02\n",
            "mse_b  [0.4553858]  mse_f: 0.42397892475128174   total loss: [0.8793647]\n",
            "mse_b ====== [0.45091629]\n",
            "It: 2320, Time: 0.02\n",
            "mse_b  [0.4509163]  mse_f: 0.5585371255874634   total loss: [1.0094534]\n",
            "mse_b ====== [0.436981887]\n",
            "It: 2321, Time: 0.03\n",
            "mse_b  [0.4369819]  mse_f: 0.4641246199607849   total loss: [0.9011065]\n",
            "mse_b ====== [0.446350813]\n",
            "It: 2322, Time: 0.02\n",
            "mse_b  [0.4463508]  mse_f: 0.3676617443561554   total loss: [0.8140125]\n",
            "mse_b ====== [0.447877914]\n",
            "It: 2323, Time: 0.02\n",
            "mse_b  [0.4478779]  mse_f: 0.6382512450218201   total loss: [1.0861292]\n",
            "mse_b ====== [0.454910368]\n",
            "It: 2324, Time: 0.02\n",
            "mse_b  [0.45491037]  mse_f: 0.4613873064517975   total loss: [0.9162977]\n",
            "mse_b ====== [0.441819429]\n",
            "It: 2325, Time: 0.02\n",
            "mse_b  [0.44181943]  mse_f: 0.3992057740688324   total loss: [0.84102523]\n",
            "mse_b ====== [0.437932]\n",
            "It: 2326, Time: 0.02\n",
            "mse_b  [0.437932]  mse_f: 0.45644646883010864   total loss: [0.8943785]\n",
            "mse_b ====== [0.462968171]\n",
            "It: 2327, Time: 0.02\n",
            "mse_b  [0.46296817]  mse_f: 0.41937124729156494   total loss: [0.8823394]\n",
            "mse_b ====== [0.437714756]\n",
            "It: 2328, Time: 0.02\n",
            "mse_b  [0.43771476]  mse_f: 0.5403868556022644   total loss: [0.9781016]\n",
            "mse_b ====== [0.427673399]\n",
            "It: 2329, Time: 0.03\n",
            "mse_b  [0.4276734]  mse_f: 0.5108002424240112   total loss: [0.93847364]\n",
            "mse_b ====== [0.423985481]\n",
            "It: 2330, Time: 0.03\n",
            "mse_b  [0.42398548]  mse_f: 0.36962270736694336   total loss: [0.7936082]\n",
            "mse_b ====== [0.421612412]\n",
            "It: 2331, Time: 0.03\n",
            "mse_b  [0.4216124]  mse_f: 0.4939216673374176   total loss: [0.9155341]\n",
            "mse_b ====== [0.457673788]\n",
            "It: 2332, Time: 0.02\n",
            "mse_b  [0.4576738]  mse_f: 0.5561766028404236   total loss: [1.0138505]\n",
            "mse_b ====== [0.428832948]\n",
            "It: 2333, Time: 0.02\n",
            "mse_b  [0.42883295]  mse_f: 0.5435041189193726   total loss: [0.97233707]\n",
            "mse_b ====== [0.410346448]\n",
            "It: 2334, Time: 0.02\n",
            "mse_b  [0.41034645]  mse_f: 0.48603594303131104   total loss: [0.8963824]\n",
            "mse_b ====== [0.419004917]\n",
            "It: 2335, Time: 0.02\n",
            "mse_b  [0.41900492]  mse_f: 0.5102775692939758   total loss: [0.9292825]\n",
            "mse_b ====== [0.45353356]\n",
            "It: 2336, Time: 0.02\n",
            "mse_b  [0.45353356]  mse_f: 0.8187187314033508   total loss: [1.2722523]\n",
            "mse_b ====== [0.415044785]\n",
            "It: 2337, Time: 0.02\n",
            "mse_b  [0.41504478]  mse_f: 0.37199026346206665   total loss: [0.78703505]\n",
            "mse_b ====== [0.467659563]\n",
            "It: 2338, Time: 0.02\n",
            "mse_b  [0.46765956]  mse_f: 0.7888597249984741   total loss: [1.2565193]\n",
            "mse_b ====== [0.753944635]\n",
            "It: 2339, Time: 0.03\n",
            "mse_b  [0.75394464]  mse_f: 1.2819525003433228   total loss: [2.0358973]\n",
            "mse_b ====== [0.677459538]\n",
            "It: 2340, Time: 0.02\n",
            "mse_b  [0.67745954]  mse_f: 0.8197945952415466   total loss: [1.4972541]\n",
            "mse_b ====== [0.492613167]\n",
            "It: 2341, Time: 0.02\n",
            "mse_b  [0.49261317]  mse_f: 1.3831223249435425   total loss: [1.8757355]\n",
            "mse_b ====== [0.460184067]\n",
            "It: 2342, Time: 0.02\n",
            "mse_b  [0.46018407]  mse_f: 0.4422385096549988   total loss: [0.90242255]\n",
            "mse_b ====== [0.568677664]\n",
            "It: 2343, Time: 0.02\n",
            "mse_b  [0.56867766]  mse_f: 0.9932442903518677   total loss: [1.561922]\n",
            "mse_b ====== [0.500194788]\n",
            "It: 2344, Time: 0.02\n",
            "mse_b  [0.5001948]  mse_f: 0.3924853205680847   total loss: [0.8926801]\n",
            "mse_b ====== [0.484429955]\n",
            "It: 2345, Time: 0.02\n",
            "mse_b  [0.48442996]  mse_f: 1.2159690856933594   total loss: [1.700399]\n",
            "mse_b ====== [0.528746963]\n",
            "It: 2346, Time: 0.02\n",
            "mse_b  [0.52874696]  mse_f: 0.396952360868454   total loss: [0.92569935]\n",
            "mse_b ====== [0.589788616]\n",
            "It: 2347, Time: 0.02\n",
            "mse_b  [0.5897886]  mse_f: 1.0348711013793945   total loss: [1.6246598]\n",
            "mse_b ====== [0.485279799]\n",
            "It: 2348, Time: 0.03\n",
            "mse_b  [0.4852798]  mse_f: 0.40213894844055176   total loss: [0.88741875]\n",
            "mse_b ====== [0.519219518]\n",
            "It: 2349, Time: 0.02\n",
            "mse_b  [0.5192195]  mse_f: 1.1772375106811523   total loss: [1.696457]\n",
            "mse_b ====== [0.488542855]\n",
            "It: 2350, Time: 0.03\n",
            "mse_b  [0.48854285]  mse_f: 0.37232667207717896   total loss: [0.8608695]\n",
            "mse_b ====== [0.637663901]\n",
            "It: 2351, Time: 0.02\n",
            "mse_b  [0.6376639]  mse_f: 0.8678417205810547   total loss: [1.5055056]\n",
            "mse_b ====== [0.535010695]\n",
            "It: 2352, Time: 0.02\n",
            "mse_b  [0.5350107]  mse_f: 0.5189166069030762   total loss: [1.0539273]\n",
            "mse_b ====== [0.461416394]\n",
            "It: 2353, Time: 0.02\n",
            "mse_b  [0.4614164]  mse_f: 0.7825629711151123   total loss: [1.2439793]\n",
            "mse_b ====== [0.449444205]\n",
            "It: 2354, Time: 0.02\n",
            "mse_b  [0.4494442]  mse_f: 0.5295009016990662   total loss: [0.97894514]\n",
            "mse_b ====== [0.462109685]\n",
            "It: 2355, Time: 0.02\n",
            "mse_b  [0.46210968]  mse_f: 0.7938520312309265   total loss: [1.2559617]\n",
            "mse_b ====== [0.459986031]\n",
            "It: 2356, Time: 0.02\n",
            "mse_b  [0.45998603]  mse_f: 0.6927072405815125   total loss: [1.1526933]\n",
            "mse_b ====== [0.429334193]\n",
            "It: 2357, Time: 0.03\n",
            "mse_b  [0.4293342]  mse_f: 0.6277003288269043   total loss: [1.0570345]\n",
            "mse_b ====== [0.439856201]\n",
            "It: 2358, Time: 0.02\n",
            "mse_b  [0.4398562]  mse_f: 0.769521951675415   total loss: [1.2093781]\n",
            "mse_b ====== [0.440245092]\n",
            "It: 2359, Time: 0.02\n",
            "mse_b  [0.4402451]  mse_f: 0.60009765625   total loss: [1.0403428]\n",
            "mse_b ====== [0.453528464]\n",
            "It: 2360, Time: 0.02\n",
            "mse_b  [0.45352846]  mse_f: 0.9254586696624756   total loss: [1.3789871]\n",
            "mse_b ====== [0.416057706]\n",
            "It: 2361, Time: 0.02\n",
            "mse_b  [0.4160577]  mse_f: 0.39957520365715027   total loss: [0.81563294]\n",
            "mse_b ====== [0.482783467]\n",
            "It: 2362, Time: 0.02\n",
            "mse_b  [0.48278347]  mse_f: 1.044215202331543   total loss: [1.5269986]\n",
            "mse_b ====== [0.447470516]\n",
            "It: 2363, Time: 0.02\n",
            "mse_b  [0.44747052]  mse_f: 0.40357157588005066   total loss: [0.8510421]\n",
            "mse_b ====== [0.564497709]\n",
            "It: 2364, Time: 0.03\n",
            "mse_b  [0.5644977]  mse_f: 0.9400099515914917   total loss: [1.5045077]\n",
            "mse_b ====== [0.494492084]\n",
            "It: 2365, Time: 0.03\n",
            "mse_b  [0.49449208]  mse_f: 0.46449586749076843   total loss: [0.95898795]\n",
            "mse_b ====== [0.466939688]\n",
            "It: 2366, Time: 0.02\n",
            "mse_b  [0.4669397]  mse_f: 0.7252804636955261   total loss: [1.1922202]\n",
            "mse_b ====== [0.478121579]\n",
            "It: 2367, Time: 0.02\n",
            "mse_b  [0.47812158]  mse_f: 0.6398317813873291   total loss: [1.1179533]\n",
            "mse_b ====== [0.472543716]\n",
            "It: 2368, Time: 0.02\n",
            "mse_b  [0.47254372]  mse_f: 0.5948678255081177   total loss: [1.0674115]\n",
            "mse_b ====== [0.546858549]\n",
            "It: 2369, Time: 0.02\n",
            "mse_b  [0.54685855]  mse_f: 0.7815645337104797   total loss: [1.328423]\n",
            "mse_b ====== [0.501592696]\n",
            "It: 2370, Time: 0.03\n",
            "mse_b  [0.5015927]  mse_f: 0.3671468198299408   total loss: [0.8687395]\n",
            "mse_b ====== [0.464414209]\n",
            "It: 2371, Time: 0.02\n",
            "mse_b  [0.4644142]  mse_f: 0.9504592418670654   total loss: [1.4148735]\n",
            "mse_b ====== [0.464533627]\n",
            "It: 2372, Time: 0.02\n",
            "mse_b  [0.46453363]  mse_f: 0.45421603322029114   total loss: [0.9187497]\n",
            "mse_b ====== [0.473591357]\n",
            "It: 2373, Time: 0.02\n",
            "mse_b  [0.47359136]  mse_f: 0.6648964881896973   total loss: [1.1384878]\n",
            "mse_b ====== [0.462710559]\n",
            "It: 2374, Time: 0.02\n",
            "mse_b  [0.46271056]  mse_f: 0.7972604036331177   total loss: [1.2599709]\n",
            "mse_b ====== [0.462361902]\n",
            "It: 2375, Time: 0.02\n",
            "mse_b  [0.4623619]  mse_f: 0.3826884627342224   total loss: [0.84505033]\n",
            "mse_b ====== [0.452569276]\n",
            "It: 2376, Time: 0.02\n",
            "mse_b  [0.45256928]  mse_f: 0.8447830080986023   total loss: [1.2973523]\n",
            "mse_b ====== [0.456160903]\n",
            "It: 2377, Time: 0.02\n",
            "mse_b  [0.4561609]  mse_f: 0.4212534725666046   total loss: [0.87741435]\n",
            "mse_b ====== [0.535601735]\n",
            "It: 2378, Time: 0.02\n",
            "mse_b  [0.53560174]  mse_f: 0.6995725631713867   total loss: [1.2351743]\n",
            "mse_b ====== [0.464161336]\n",
            "It: 2379, Time: 0.02\n",
            "mse_b  [0.46416134]  mse_f: 0.6066550016403198   total loss: [1.0708163]\n",
            "mse_b ====== [0.430962801]\n",
            "It: 2380, Time: 0.02\n",
            "mse_b  [0.4309628]  mse_f: 0.4649335443973541   total loss: [0.8958963]\n",
            "mse_b ====== [0.484800637]\n",
            "It: 2381, Time: 0.02\n",
            "mse_b  [0.48480064]  mse_f: 0.846871018409729   total loss: [1.3316717]\n",
            "mse_b ====== [0.41321373]\n",
            "It: 2382, Time: 0.02\n",
            "mse_b  [0.41321373]  mse_f: 0.39712852239608765   total loss: [0.81034225]\n",
            "mse_b ====== [0.509445667]\n",
            "It: 2383, Time: 0.03\n",
            "mse_b  [0.50944567]  mse_f: 0.8391255736351013   total loss: [1.3485713]\n",
            "mse_b ====== [0.461522341]\n",
            "It: 2384, Time: 0.02\n",
            "mse_b  [0.46152234]  mse_f: 0.515081524848938   total loss: [0.97660387]\n",
            "mse_b ====== [0.408179402]\n",
            "It: 2385, Time: 0.02\n",
            "mse_b  [0.4081794]  mse_f: 0.5944211483001709   total loss: [1.0026006]\n",
            "mse_b ====== [0.451400042]\n",
            "It: 2386, Time: 0.02\n",
            "mse_b  [0.45140004]  mse_f: 0.6901199817657471   total loss: [1.14152]\n",
            "mse_b ====== [0.40312472]\n",
            "It: 2387, Time: 0.03\n",
            "mse_b  [0.40312472]  mse_f: 0.4813776910305023   total loss: [0.8845024]\n",
            "mse_b ====== [0.428535283]\n",
            "It: 2388, Time: 0.02\n",
            "mse_b  [0.42853528]  mse_f: 0.8913586139678955   total loss: [1.3198938]\n",
            "mse_b ====== [0.422815233]\n",
            "It: 2389, Time: 0.02\n",
            "mse_b  [0.42281523]  mse_f: 0.3422200679779053   total loss: [0.7650353]\n",
            "mse_b ====== [0.428548306]\n",
            "It: 2390, Time: 0.03\n",
            "mse_b  [0.4285483]  mse_f: 0.7802122235298157   total loss: [1.2087605]\n",
            "mse_b ====== [0.428813964]\n",
            "It: 2391, Time: 0.02\n",
            "mse_b  [0.42881396]  mse_f: 0.6363686323165894   total loss: [1.0651826]\n",
            "mse_b ====== [0.433939666]\n",
            "It: 2392, Time: 0.02\n",
            "mse_b  [0.43393967]  mse_f: 0.4163014590740204   total loss: [0.8502411]\n",
            "mse_b ====== [0.440507472]\n",
            "It: 2393, Time: 0.02\n",
            "mse_b  [0.44050747]  mse_f: 0.8420543670654297   total loss: [1.2825618]\n",
            "mse_b ====== [0.425963759]\n",
            "It: 2394, Time: 0.02\n",
            "mse_b  [0.42596376]  mse_f: 0.3826062083244324   total loss: [0.80856997]\n",
            "mse_b ====== [0.468272865]\n",
            "It: 2395, Time: 0.02\n",
            "mse_b  [0.46827286]  mse_f: 0.7046704888343811   total loss: [1.1729434]\n",
            "mse_b ====== [0.422239959]\n",
            "It: 2396, Time: 0.02\n",
            "mse_b  [0.42223996]  mse_f: 0.49758797883987427   total loss: [0.91982794]\n",
            "mse_b ====== [0.478031039]\n",
            "It: 2397, Time: 0.02\n",
            "mse_b  [0.47803104]  mse_f: 0.43633538484573364   total loss: [0.9143664]\n",
            "mse_b ====== [0.51297456]\n",
            "It: 2398, Time: 0.02\n",
            "mse_b  [0.51297456]  mse_f: 0.6969841122627258   total loss: [1.2099587]\n",
            "mse_b ====== [0.424797356]\n",
            "It: 2399, Time: 0.02\n",
            "mse_b  [0.42479736]  mse_f: 0.36126965284347534   total loss: [0.786067]\n",
            "mse_b ====== [0.462100029]\n",
            "It: 2400, Time: 0.02\n",
            "mse_b  [0.46210003]  mse_f: 0.5596874356269836   total loss: [1.0217874]\n",
            "mse_b ====== [0.468250453]\n",
            "It: 2401, Time: 0.03\n",
            "mse_b  [0.46825045]  mse_f: 0.5455461740493774   total loss: [1.0137966]\n",
            "mse_b ====== [0.425162971]\n",
            "It: 2402, Time: 0.02\n",
            "mse_b  [0.42516297]  mse_f: 0.3912762403488159   total loss: [0.8164392]\n",
            "mse_b ====== [0.496604592]\n",
            "It: 2403, Time: 0.02\n",
            "mse_b  [0.4966046]  mse_f: 0.6768388748168945   total loss: [1.1734434]\n",
            "mse_b ====== [0.469319224]\n",
            "It: 2404, Time: 0.02\n",
            "mse_b  [0.46931922]  mse_f: 0.3945249319076538   total loss: [0.86384416]\n",
            "mse_b ====== [0.411791056]\n",
            "It: 2405, Time: 0.03\n",
            "mse_b  [0.41179106]  mse_f: 0.5532005429267883   total loss: [0.96499157]\n",
            "mse_b ====== [0.429295719]\n",
            "It: 2406, Time: 0.02\n",
            "mse_b  [0.42929572]  mse_f: 0.6411310434341431   total loss: [1.0704267]\n",
            "mse_b ====== [0.406331]\n",
            "It: 2407, Time: 0.02\n",
            "mse_b  [0.406331]  mse_f: 0.33476972579956055   total loss: [0.7411007]\n",
            "mse_b ====== [0.408171535]\n",
            "It: 2408, Time: 0.02\n",
            "mse_b  [0.40817153]  mse_f: 0.7719925045967102   total loss: [1.1801641]\n",
            "mse_b ====== [0.400180221]\n",
            "It: 2409, Time: 0.02\n",
            "mse_b  [0.40018022]  mse_f: 0.391909122467041   total loss: [0.79208934]\n",
            "mse_b ====== [0.419938]\n",
            "It: 2410, Time: 0.02\n",
            "mse_b  [0.419938]  mse_f: 0.4725213050842285   total loss: [0.8924593]\n",
            "mse_b ====== [0.414155453]\n",
            "It: 2411, Time: 0.02\n",
            "mse_b  [0.41415545]  mse_f: 0.6591204404830933   total loss: [1.0732759]\n",
            "mse_b ====== [0.396562219]\n",
            "It: 2412, Time: 0.02\n",
            "mse_b  [0.39656222]  mse_f: 0.3355312943458557   total loss: [0.7320935]\n",
            "mse_b ====== [0.402847052]\n",
            "It: 2413, Time: 0.02\n",
            "mse_b  [0.40284705]  mse_f: 0.6636365652084351   total loss: [1.0664836]\n",
            "mse_b ====== [0.382971495]\n",
            "It: 2414, Time: 0.02\n",
            "mse_b  [0.3829715]  mse_f: 0.4684298038482666   total loss: [0.8514013]\n",
            "mse_b ====== [0.43454811]\n",
            "It: 2415, Time: 0.02\n",
            "mse_b  [0.4345481]  mse_f: 0.42282596230506897   total loss: [0.8573741]\n",
            "mse_b ====== [0.434989333]\n",
            "It: 2416, Time: 0.02\n",
            "mse_b  [0.43498933]  mse_f: 0.5815231800079346   total loss: [1.0165125]\n",
            "mse_b ====== [0.389075518]\n",
            "It: 2417, Time: 0.02\n",
            "mse_b  [0.38907552]  mse_f: 0.33007705211639404   total loss: [0.71915257]\n",
            "mse_b ====== [0.467983425]\n",
            "It: 2418, Time: 0.03\n",
            "mse_b  [0.46798342]  mse_f: 0.5976400375366211   total loss: [1.0656235]\n",
            "mse_b ====== [0.439362258]\n",
            "It: 2419, Time: 0.02\n",
            "mse_b  [0.43936226]  mse_f: 0.48898234963417053   total loss: [0.9283446]\n",
            "mse_b ====== [0.377018]\n",
            "It: 2420, Time: 0.03\n",
            "mse_b  [0.377018]  mse_f: 0.32629749178886414   total loss: [0.7033155]\n",
            "mse_b ====== [0.439319849]\n",
            "It: 2421, Time: 0.02\n",
            "mse_b  [0.43931985]  mse_f: 0.6646610498428345   total loss: [1.1039809]\n",
            "mse_b ====== [0.3853423]\n",
            "It: 2422, Time: 0.02\n",
            "mse_b  [0.3853423]  mse_f: 0.32601284980773926   total loss: [0.71135515]\n",
            "mse_b ====== [0.386313349]\n",
            "It: 2423, Time: 0.03\n",
            "mse_b  [0.38631335]  mse_f: 0.5442547798156738   total loss: [0.9305681]\n",
            "mse_b ====== [0.403448462]\n",
            "It: 2424, Time: 0.02\n",
            "mse_b  [0.40344846]  mse_f: 0.5024821758270264   total loss: [0.90593064]\n",
            "mse_b ====== [0.376004755]\n",
            "It: 2425, Time: 0.03\n",
            "mse_b  [0.37600476]  mse_f: 0.34109482169151306   total loss: [0.71709955]\n",
            "mse_b ====== [0.388537616]\n",
            "It: 2426, Time: 0.03\n",
            "mse_b  [0.38853762]  mse_f: 0.6377840638160706   total loss: [1.0263216]\n",
            "mse_b ====== [0.384025872]\n",
            "It: 2427, Time: 0.02\n",
            "mse_b  [0.38402587]  mse_f: 0.32958173751831055   total loss: [0.7136076]\n",
            "mse_b ====== [0.369718671]\n",
            "It: 2428, Time: 0.02\n",
            "mse_b  [0.36971867]  mse_f: 0.479006826877594   total loss: [0.8487255]\n",
            "mse_b ====== [0.368853211]\n",
            "It: 2429, Time: 0.03\n",
            "mse_b  [0.3688532]  mse_f: 0.626796543598175   total loss: [0.99564976]\n",
            "mse_b ====== [0.374148339]\n",
            "It: 2430, Time: 0.03\n",
            "mse_b  [0.37414834]  mse_f: 0.27202948927879333   total loss: [0.6461778]\n",
            "mse_b ====== [0.396496505]\n",
            "It: 2431, Time: 0.02\n",
            "mse_b  [0.3964965]  mse_f: 0.5867661237716675   total loss: [0.98326266]\n",
            "mse_b ====== [0.370573938]\n",
            "It: 2432, Time: 0.02\n",
            "mse_b  [0.37057394]  mse_f: 0.42630594968795776   total loss: [0.7968799]\n",
            "mse_b ====== [0.389212668]\n",
            "It: 2433, Time: 0.03\n",
            "mse_b  [0.38921267]  mse_f: 0.40151238441467285   total loss: [0.79072505]\n",
            "mse_b ====== [0.381119698]\n",
            "It: 2434, Time: 0.02\n",
            "mse_b  [0.3811197]  mse_f: 0.553210973739624   total loss: [0.9343307]\n",
            "mse_b ====== [0.355742037]\n",
            "It: 2435, Time: 0.02\n",
            "mse_b  [0.35574204]  mse_f: 0.3198852241039276   total loss: [0.67562723]\n",
            "mse_b ====== [0.408139378]\n",
            "It: 2436, Time: 0.03\n",
            "mse_b  [0.40813938]  mse_f: 0.5155490636825562   total loss: [0.9236884]\n",
            "mse_b ====== [0.367327392]\n",
            "It: 2437, Time: 0.02\n",
            "mse_b  [0.3673274]  mse_f: 0.4015682637691498   total loss: [0.7688956]\n",
            "mse_b ====== [0.371373415]\n",
            "It: 2438, Time: 0.02\n",
            "mse_b  [0.37137341]  mse_f: 0.3377610445022583   total loss: [0.70913446]\n",
            "mse_b ====== [0.429079235]\n",
            "It: 2439, Time: 0.02\n",
            "mse_b  [0.42907923]  mse_f: 0.5234735012054443   total loss: [0.95255274]\n",
            "mse_b ====== [0.379578769]\n",
            "It: 2440, Time: 0.02\n",
            "mse_b  [0.37957877]  mse_f: 0.377048134803772   total loss: [0.7566269]\n",
            "mse_b ====== [0.356958061]\n",
            "It: 2441, Time: 0.02\n",
            "mse_b  [0.35695806]  mse_f: 0.34110814332962036   total loss: [0.69806623]\n",
            "mse_b ====== [0.412235677]\n",
            "It: 2442, Time: 0.02\n",
            "mse_b  [0.41223568]  mse_f: 0.5277462005615234   total loss: [0.9399819]\n",
            "mse_b ====== [0.354658812]\n",
            "It: 2443, Time: 0.02\n",
            "mse_b  [0.3546588]  mse_f: 0.30631738901138306   total loss: [0.6609762]\n",
            "mse_b ====== [0.355582774]\n",
            "It: 2444, Time: 0.02\n",
            "mse_b  [0.35558277]  mse_f: 0.541840136051178   total loss: [0.8974229]\n",
            "mse_b ====== [0.366553843]\n",
            "It: 2445, Time: 0.02\n",
            "mse_b  [0.36655384]  mse_f: 0.3859541416168213   total loss: [0.752508]\n",
            "mse_b ====== [0.350212872]\n",
            "It: 2446, Time: 0.02\n",
            "mse_b  [0.35021287]  mse_f: 0.38014882802963257   total loss: [0.7303617]\n",
            "mse_b ====== [0.357368022]\n",
            "It: 2447, Time: 0.03\n",
            "mse_b  [0.35736802]  mse_f: 0.5531244874000549   total loss: [0.91049254]\n",
            "mse_b ====== [0.353470683]\n",
            "It: 2448, Time: 0.02\n",
            "mse_b  [0.35347068]  mse_f: 0.2723137140274048   total loss: [0.6257844]\n",
            "mse_b ====== [0.346002817]\n",
            "It: 2449, Time: 0.02\n",
            "mse_b  [0.34600282]  mse_f: 0.4565196633338928   total loss: [0.8025225]\n",
            "mse_b ====== [0.343377233]\n",
            "It: 2450, Time: 0.03\n",
            "mse_b  [0.34337723]  mse_f: 0.49399998784065247   total loss: [0.8373772]\n",
            "mse_b ====== [0.354133576]\n",
            "It: 2451, Time: 0.02\n",
            "mse_b  [0.35413358]  mse_f: 0.2647823691368103   total loss: [0.6189159]\n",
            "mse_b ====== [0.363601387]\n",
            "It: 2452, Time: 0.03\n",
            "mse_b  [0.3636014]  mse_f: 0.47825461626052856   total loss: [0.841856]\n",
            "mse_b ====== [0.342966259]\n",
            "It: 2453, Time: 0.03\n",
            "mse_b  [0.34296626]  mse_f: 0.3864330053329468   total loss: [0.72939926]\n",
            "mse_b ====== [0.362957656]\n",
            "It: 2454, Time: 0.03\n",
            "mse_b  [0.36295766]  mse_f: 0.3695293068885803   total loss: [0.73248696]\n",
            "mse_b ====== [0.350700259]\n",
            "It: 2455, Time: 0.03\n",
            "mse_b  [0.35070026]  mse_f: 0.4760439097881317   total loss: [0.8267442]\n",
            "mse_b ====== [0.33162412]\n",
            "It: 2456, Time: 0.02\n",
            "mse_b  [0.33162412]  mse_f: 0.3080593943595886   total loss: [0.6396835]\n",
            "mse_b ====== [0.386071652]\n",
            "It: 2457, Time: 0.02\n",
            "mse_b  [0.38607165]  mse_f: 0.46275657415390015   total loss: [0.8488282]\n",
            "mse_b ====== [0.345806658]\n",
            "It: 2458, Time: 0.02\n",
            "mse_b  [0.34580666]  mse_f: 0.3430989384651184   total loss: [0.6889056]\n",
            "mse_b ====== [0.341050774]\n",
            "It: 2459, Time: 0.02\n",
            "mse_b  [0.34105077]  mse_f: 0.31888800859451294   total loss: [0.6599388]\n",
            "mse_b ====== [0.381600112]\n",
            "It: 2460, Time: 0.03\n",
            "mse_b  [0.3816001]  mse_f: 0.4740091562271118   total loss: [0.8556093]\n",
            "mse_b ====== [0.347468287]\n",
            "It: 2461, Time: 0.03\n",
            "mse_b  [0.3474683]  mse_f: 0.3220481872558594   total loss: [0.66951644]\n",
            "mse_b ====== [0.337452143]\n",
            "It: 2462, Time: 0.03\n",
            "mse_b  [0.33745214]  mse_f: 0.32125261425971985   total loss: [0.65870476]\n",
            "mse_b ====== [0.378183126]\n",
            "It: 2463, Time: 0.03\n",
            "mse_b  [0.37818313]  mse_f: 0.42255401611328125   total loss: [0.80073714]\n",
            "mse_b ====== [0.337230504]\n",
            "It: 2464, Time: 0.02\n",
            "mse_b  [0.3372305]  mse_f: 0.30033501982688904   total loss: [0.6375655]\n",
            "mse_b ====== [0.339899182]\n",
            "It: 2465, Time: 0.02\n",
            "mse_b  [0.33989918]  mse_f: 0.4395301342010498   total loss: [0.7794293]\n",
            "mse_b ====== [0.354913414]\n",
            "It: 2466, Time: 0.02\n",
            "mse_b  [0.3549134]  mse_f: 0.3287621736526489   total loss: [0.6836756]\n",
            "mse_b ====== [0.3388156]\n",
            "It: 2467, Time: 0.02\n",
            "mse_b  [0.3388156]  mse_f: 0.3472367823123932   total loss: [0.6860524]\n",
            "mse_b ====== [0.333423883]\n",
            "It: 2468, Time: 0.02\n",
            "mse_b  [0.33342388]  mse_f: 0.47099769115448   total loss: [0.80442154]\n",
            "mse_b ====== [0.330426872]\n",
            "It: 2469, Time: 0.02\n",
            "mse_b  [0.33042687]  mse_f: 0.2774049639701843   total loss: [0.60783184]\n",
            "mse_b ====== [0.325323194]\n",
            "It: 2470, Time: 0.02\n",
            "mse_b  [0.3253232]  mse_f: 0.38246163725852966   total loss: [0.70778483]\n",
            "mse_b ====== [0.319477022]\n",
            "It: 2471, Time: 0.03\n",
            "mse_b  [0.31947702]  mse_f: 0.46487703919410706   total loss: [0.7843541]\n",
            "mse_b ====== [0.319055974]\n",
            "It: 2472, Time: 0.02\n",
            "mse_b  [0.31905597]  mse_f: 0.2518547773361206   total loss: [0.57091075]\n",
            "mse_b ====== [0.322572917]\n",
            "It: 2473, Time: 0.02\n",
            "mse_b  [0.32257292]  mse_f: 0.40282806754112244   total loss: [0.725401]\n",
            "mse_b ====== [0.309176177]\n",
            "It: 2474, Time: 0.02\n",
            "mse_b  [0.30917618]  mse_f: 0.3696545660495758   total loss: [0.67883074]\n",
            "mse_b ====== [0.311681688]\n",
            "It: 2475, Time: 0.02\n",
            "mse_b  [0.3116817]  mse_f: 0.34286195039749146   total loss: [0.65454364]\n",
            "mse_b ====== [0.299628615]\n",
            "It: 2476, Time: 0.03\n",
            "mse_b  [0.29962862]  mse_f: 0.40432584285736084   total loss: [0.70395446]\n",
            "mse_b ====== [0.312018335]\n",
            "It: 2477, Time: 0.02\n",
            "mse_b  [0.31201833]  mse_f: 0.28995904326438904   total loss: [0.60197735]\n",
            "mse_b ====== [0.377720714]\n",
            "It: 2478, Time: 0.02\n",
            "mse_b  [0.3777207]  mse_f: 0.41209712624549866   total loss: [0.7898178]\n",
            "mse_b ====== [0.305035979]\n",
            "It: 2479, Time: 0.02\n",
            "mse_b  [0.30503598]  mse_f: 0.2591060698032379   total loss: [0.56414205]\n",
            "mse_b ====== [0.307324439]\n",
            "It: 2480, Time: 0.02\n",
            "mse_b  [0.30732444]  mse_f: 0.3725891709327698   total loss: [0.67991364]\n",
            "mse_b ====== [0.328693539]\n",
            "It: 2481, Time: 0.02\n",
            "mse_b  [0.32869354]  mse_f: 0.4166310727596283   total loss: [0.7453246]\n",
            "mse_b ====== [0.297932208]\n",
            "It: 2482, Time: 0.03\n",
            "mse_b  [0.2979322]  mse_f: 0.2773028612136841   total loss: [0.57523507]\n",
            "mse_b ====== [0.327311605]\n",
            "It: 2483, Time: 0.02\n",
            "mse_b  [0.3273116]  mse_f: 0.3346978425979614   total loss: [0.6620095]\n",
            "mse_b ====== [0.329200774]\n",
            "It: 2484, Time: 0.02\n",
            "mse_b  [0.32920077]  mse_f: 0.3166998326778412   total loss: [0.6459006]\n",
            "mse_b ====== [0.305194139]\n",
            "It: 2485, Time: 0.02\n",
            "mse_b  [0.30519414]  mse_f: 0.3397018313407898   total loss: [0.644896]\n",
            "mse_b ====== [0.323497504]\n",
            "It: 2486, Time: 0.02\n",
            "mse_b  [0.3234975]  mse_f: 0.32427820563316345   total loss: [0.6477757]\n",
            "mse_b ====== [0.321715057]\n",
            "It: 2487, Time: 0.02\n",
            "mse_b  [0.32171506]  mse_f: 0.26752668619155884   total loss: [0.58924174]\n",
            "mse_b ====== [0.305692345]\n",
            "It: 2488, Time: 0.02\n",
            "mse_b  [0.30569234]  mse_f: 0.40319254994392395   total loss: [0.7088849]\n",
            "mse_b ====== [0.303544641]\n",
            "It: 2489, Time: 0.03\n",
            "mse_b  [0.30354464]  mse_f: 0.30626416206359863   total loss: [0.6098088]\n",
            "mse_b ====== [0.303336203]\n",
            "It: 2490, Time: 0.03\n",
            "mse_b  [0.3033362]  mse_f: 0.2734951972961426   total loss: [0.5768314]\n",
            "mse_b ====== [0.302236944]\n",
            "It: 2491, Time: 0.02\n",
            "mse_b  [0.30223694]  mse_f: 0.4308805465698242   total loss: [0.73311746]\n",
            "mse_b ====== [0.303038418]\n",
            "It: 2492, Time: 0.02\n",
            "mse_b  [0.30303842]  mse_f: 0.2797397971153259   total loss: [0.5827782]\n",
            "mse_b ====== [0.306836247]\n",
            "It: 2493, Time: 0.03\n",
            "mse_b  [0.30683625]  mse_f: 0.3085477352142334   total loss: [0.615384]\n",
            "mse_b ====== [0.287604272]\n",
            "It: 2494, Time: 0.02\n",
            "mse_b  [0.28760427]  mse_f: 0.3448109030723572   total loss: [0.6324152]\n",
            "mse_b ====== [0.292545587]\n",
            "It: 2495, Time: 0.02\n",
            "mse_b  [0.2925456]  mse_f: 0.32846879959106445   total loss: [0.62101436]\n",
            "mse_b ====== [0.286424935]\n",
            "It: 2496, Time: 0.03\n",
            "mse_b  [0.28642493]  mse_f: 0.34250444173812866   total loss: [0.6289294]\n",
            "mse_b ====== [0.280903548]\n",
            "It: 2497, Time: 0.03\n",
            "mse_b  [0.28090355]  mse_f: 0.2707475423812866   total loss: [0.5516511]\n",
            "mse_b ====== [0.335032701]\n",
            "It: 2498, Time: 0.02\n",
            "mse_b  [0.3350327]  mse_f: 0.3472965359687805   total loss: [0.68232924]\n",
            "mse_b ====== [0.2961514]\n",
            "It: 2499, Time: 0.02\n",
            "mse_b  [0.2961514]  mse_f: 0.2544371485710144   total loss: [0.55058855]\n",
            "mse_b ====== [0.276532561]\n",
            "It: 2500, Time: 0.02\n",
            "mse_b  [0.27653256]  mse_f: 0.29440805315971375   total loss: [0.5709406]\n",
            "mse_b ====== [0.29408738]\n",
            "It: 2501, Time: 0.02\n",
            "mse_b  [0.29408738]  mse_f: 0.37578991055488586   total loss: [0.6698773]\n",
            "mse_b ====== [0.277723908]\n",
            "It: 2502, Time: 0.02\n",
            "mse_b  [0.2777239]  mse_f: 0.29317647218704224   total loss: [0.5709004]\n",
            "mse_b ====== [0.294142365]\n",
            "It: 2503, Time: 0.02\n",
            "mse_b  [0.29414237]  mse_f: 0.29480594396591187   total loss: [0.5889483]\n",
            "mse_b ====== [0.300634533]\n",
            "It: 2504, Time: 0.03\n",
            "mse_b  [0.30063453]  mse_f: 0.27416670322418213   total loss: [0.5748012]\n",
            "mse_b ====== [0.279656112]\n",
            "It: 2505, Time: 0.02\n",
            "mse_b  [0.2796561]  mse_f: 0.3403611183166504   total loss: [0.62001723]\n",
            "mse_b ====== [0.28300041]\n",
            "It: 2506, Time: 0.02\n",
            "mse_b  [0.2830004]  mse_f: 0.3030228912830353   total loss: [0.58602333]\n",
            "mse_b ====== [0.28267]\n",
            "It: 2507, Time: 0.03\n",
            "mse_b  [0.28267]  mse_f: 0.23963642120361328   total loss: [0.52230644]\n",
            "mse_b ====== [0.286077917]\n",
            "It: 2508, Time: 0.03\n",
            "mse_b  [0.28607792]  mse_f: 0.37934476137161255   total loss: [0.6654227]\n",
            "mse_b ====== [0.285256147]\n",
            "It: 2509, Time: 0.02\n",
            "mse_b  [0.28525615]  mse_f: 0.24570971727371216   total loss: [0.53096586]\n",
            "mse_b ====== [0.288151056]\n",
            "It: 2510, Time: 0.02\n",
            "mse_b  [0.28815106]  mse_f: 0.27113938331604004   total loss: [0.5592904]\n",
            "mse_b ====== [0.285689354]\n",
            "It: 2511, Time: 0.02\n",
            "mse_b  [0.28568935]  mse_f: 0.3303494155406952   total loss: [0.6160388]\n",
            "mse_b ====== [0.29820472]\n",
            "It: 2512, Time: 0.02\n",
            "mse_b  [0.29820472]  mse_f: 0.25248944759368896   total loss: [0.55069417]\n",
            "mse_b ====== [0.294507205]\n",
            "It: 2513, Time: 0.02\n",
            "mse_b  [0.2945072]  mse_f: 0.28610146045684814   total loss: [0.58060867]\n",
            "mse_b ====== [0.277949929]\n",
            "It: 2514, Time: 0.02\n",
            "mse_b  [0.27794993]  mse_f: 0.24596917629241943   total loss: [0.5239191]\n",
            "mse_b ====== [0.300525844]\n",
            "It: 2515, Time: 0.02\n",
            "mse_b  [0.30052584]  mse_f: 0.3132222294807434   total loss: [0.6137481]\n",
            "mse_b ====== [0.287086844]\n",
            "It: 2516, Time: 0.02\n",
            "mse_b  [0.28708684]  mse_f: 0.271100252866745   total loss: [0.5581871]\n",
            "mse_b ====== [0.279433489]\n",
            "It: 2517, Time: 0.02\n",
            "mse_b  [0.2794335]  mse_f: 0.22499418258666992   total loss: [0.5044277]\n",
            "mse_b ====== [0.312430352]\n",
            "It: 2518, Time: 0.02\n",
            "mse_b  [0.31243035]  mse_f: 0.32897689938545227   total loss: [0.64140725]\n",
            "mse_b ====== [0.275794119]\n",
            "It: 2519, Time: 0.02\n",
            "mse_b  [0.27579412]  mse_f: 0.23502960801124573   total loss: [0.5108237]\n",
            "mse_b ====== [0.276571214]\n",
            "It: 2520, Time: 0.02\n",
            "mse_b  [0.2765712]  mse_f: 0.2839939594268799   total loss: [0.5605652]\n",
            "mse_b ====== [0.28199169]\n",
            "It: 2521, Time: 0.02\n",
            "mse_b  [0.2819917]  mse_f: 0.28637266159057617   total loss: [0.5683644]\n",
            "mse_b ====== [0.26315555]\n",
            "It: 2522, Time: 0.03\n",
            "mse_b  [0.26315555]  mse_f: 0.28569743037223816   total loss: [0.548853]\n",
            "mse_b ====== [0.27083528]\n",
            "It: 2523, Time: 0.02\n",
            "mse_b  [0.27083528]  mse_f: 0.29295235872268677   total loss: [0.56378764]\n",
            "mse_b ====== [0.267159373]\n",
            "It: 2524, Time: 0.02\n",
            "mse_b  [0.26715937]  mse_f: 0.22093909978866577   total loss: [0.48809847]\n",
            "mse_b ====== [0.261439323]\n",
            "It: 2525, Time: 0.02\n",
            "mse_b  [0.26143932]  mse_f: 0.33920350670814514   total loss: [0.6006428]\n",
            "mse_b ====== [0.262564778]\n",
            "It: 2526, Time: 0.02\n",
            "mse_b  [0.26256478]  mse_f: 0.2551209330558777   total loss: [0.5176857]\n",
            "mse_b ====== [0.271095037]\n",
            "It: 2527, Time: 0.02\n",
            "mse_b  [0.27109504]  mse_f: 0.23738993704319   total loss: [0.50848496]\n",
            "mse_b ====== [0.265117317]\n",
            "It: 2528, Time: 0.03\n",
            "mse_b  [0.26511732]  mse_f: 0.31458890438079834   total loss: [0.5797062]\n",
            "mse_b ====== [0.260539412]\n",
            "It: 2529, Time: 0.02\n",
            "mse_b  [0.2605394]  mse_f: 0.25793877243995667   total loss: [0.51847816]\n",
            "mse_b ====== [0.258387268]\n",
            "It: 2530, Time: 0.03\n",
            "mse_b  [0.25838727]  mse_f: 0.2808097004890442   total loss: [0.53919697]\n",
            "mse_b ====== [0.259620965]\n",
            "It: 2531, Time: 0.02\n",
            "mse_b  [0.25962096]  mse_f: 0.2391710877418518   total loss: [0.49879205]\n",
            "mse_b ====== [0.286170214]\n",
            "It: 2532, Time: 0.02\n",
            "mse_b  [0.2861702]  mse_f: 0.30022966861724854   total loss: [0.5863999]\n",
            "mse_b ====== [0.25966233]\n",
            "It: 2533, Time: 0.02\n",
            "mse_b  [0.25966233]  mse_f: 0.2155306041240692   total loss: [0.47519293]\n",
            "mse_b ====== [0.268184304]\n",
            "It: 2534, Time: 0.02\n",
            "mse_b  [0.2681843]  mse_f: 0.24571825563907623   total loss: [0.51390254]\n",
            "mse_b ====== [0.288877517]\n",
            "It: 2535, Time: 0.03\n",
            "mse_b  [0.28887752]  mse_f: 0.27734893560409546   total loss: [0.5662265]\n",
            "mse_b ====== [0.262224436]\n",
            "It: 2536, Time: 0.02\n",
            "mse_b  [0.26222444]  mse_f: 0.22758440673351288   total loss: [0.48980886]\n",
            "mse_b ====== [0.260604739]\n",
            "It: 2537, Time: 0.03\n",
            "mse_b  [0.26060474]  mse_f: 0.25241801142692566   total loss: [0.5130228]\n",
            "mse_b ====== [0.262582034]\n",
            "It: 2538, Time: 0.02\n",
            "mse_b  [0.26258203]  mse_f: 0.25553807616233826   total loss: [0.5181201]\n",
            "mse_b ====== [0.252745211]\n",
            "It: 2539, Time: 0.02\n",
            "mse_b  [0.2527452]  mse_f: 0.28045910596847534   total loss: [0.5332043]\n",
            "mse_b ====== [0.252758265]\n",
            "It: 2540, Time: 0.02\n",
            "mse_b  [0.25275826]  mse_f: 0.2304459810256958   total loss: [0.48320425]\n",
            "mse_b ====== [0.2490554]\n",
            "It: 2541, Time: 0.03\n",
            "mse_b  [0.2490554]  mse_f: 0.2466452717781067   total loss: [0.49570066]\n",
            "mse_b ====== [0.25048542]\n",
            "It: 2542, Time: 0.02\n",
            "mse_b  [0.25048542]  mse_f: 0.3043886423110962   total loss: [0.55487406]\n",
            "mse_b ====== [0.247624531]\n",
            "It: 2543, Time: 0.03\n",
            "mse_b  [0.24762453]  mse_f: 0.2104458212852478   total loss: [0.45807034]\n",
            "mse_b ====== [0.245461166]\n",
            "It: 2544, Time: 0.02\n",
            "mse_b  [0.24546117]  mse_f: 0.26131367683410645   total loss: [0.50677484]\n",
            "mse_b ====== [0.248013765]\n",
            "It: 2545, Time: 0.02\n",
            "mse_b  [0.24801376]  mse_f: 0.2730310261249542   total loss: [0.5210448]\n",
            "mse_b ====== [0.258547485]\n",
            "It: 2546, Time: 0.02\n",
            "mse_b  [0.25854748]  mse_f: 0.23354750871658325   total loss: [0.492095]\n",
            "mse_b ====== [0.242242247]\n",
            "It: 2547, Time: 0.02\n",
            "mse_b  [0.24224225]  mse_f: 0.22796787321567535   total loss: [0.47021013]\n",
            "mse_b ====== [0.245001301]\n",
            "It: 2548, Time: 0.02\n",
            "mse_b  [0.2450013]  mse_f: 0.26083073019981384   total loss: [0.505832]\n",
            "mse_b ====== [0.249369815]\n",
            "It: 2549, Time: 0.02\n",
            "mse_b  [0.24936981]  mse_f: 0.2690056562423706   total loss: [0.51837546]\n",
            "mse_b ====== [0.240351558]\n",
            "It: 2550, Time: 0.02\n",
            "mse_b  [0.24035156]  mse_f: 0.19453056156635284   total loss: [0.4348821]\n",
            "mse_b ====== [0.256688774]\n",
            "It: 2551, Time: 0.02\n",
            "mse_b  [0.25668877]  mse_f: 0.2696087658405304   total loss: [0.52629757]\n",
            "mse_b ====== [0.24107568]\n",
            "It: 2552, Time: 0.02\n",
            "mse_b  [0.24107568]  mse_f: 0.23951765894889832   total loss: [0.48059332]\n",
            "mse_b ====== [0.239938572]\n",
            "It: 2553, Time: 0.02\n",
            "mse_b  [0.23993857]  mse_f: 0.23749521374702454   total loss: [0.4774338]\n",
            "mse_b ====== [0.247354805]\n",
            "It: 2554, Time: 0.02\n",
            "mse_b  [0.2473548]  mse_f: 0.22564010322093964   total loss: [0.47299492]\n",
            "mse_b ====== [0.237326249]\n",
            "It: 2555, Time: 0.02\n",
            "mse_b  [0.23732625]  mse_f: 0.2622221112251282   total loss: [0.49954838]\n",
            "mse_b ====== [0.235251933]\n",
            "It: 2556, Time: 0.03\n",
            "mse_b  [0.23525193]  mse_f: 0.23772943019866943   total loss: [0.47298136]\n",
            "mse_b ====== [0.23441714]\n",
            "It: 2557, Time: 0.02\n",
            "mse_b  [0.23441714]  mse_f: 0.19757047295570374   total loss: [0.4319876]\n",
            "mse_b ====== [0.233875439]\n",
            "It: 2558, Time: 0.02\n",
            "mse_b  [0.23387544]  mse_f: 0.2832980155944824   total loss: [0.51717347]\n",
            "mse_b ====== [0.236447]\n",
            "It: 2559, Time: 0.03\n",
            "mse_b  [0.236447]  mse_f: 0.20731490850448608   total loss: [0.44376191]\n",
            "mse_b ====== [0.239748776]\n",
            "It: 2560, Time: 0.02\n",
            "mse_b  [0.23974878]  mse_f: 0.2253420501947403   total loss: [0.4650908]\n",
            "mse_b ====== [0.230582714]\n",
            "It: 2561, Time: 0.02\n",
            "mse_b  [0.23058271]  mse_f: 0.24223458766937256   total loss: [0.4728173]\n",
            "mse_b ====== [0.235517263]\n",
            "It: 2562, Time: 0.02\n",
            "mse_b  [0.23551726]  mse_f: 0.25780773162841797   total loss: [0.493325]\n",
            "mse_b ====== [0.230186969]\n",
            "It: 2563, Time: 0.03\n",
            "mse_b  [0.23018697]  mse_f: 0.19889292120933533   total loss: [0.4290799]\n",
            "mse_b ====== [0.247957468]\n",
            "It: 2564, Time: 0.02\n",
            "mse_b  [0.24795747]  mse_f: 0.22687192261219025   total loss: [0.47482938]\n",
            "mse_b ====== [0.239194348]\n",
            "It: 2565, Time: 0.02\n",
            "mse_b  [0.23919435]  mse_f: 0.23251155018806458   total loss: [0.4717059]\n",
            "mse_b ====== [0.22464487]\n",
            "It: 2566, Time: 0.02\n",
            "mse_b  [0.22464487]  mse_f: 0.20381051301956177   total loss: [0.42845538]\n",
            "mse_b ====== [0.227974281]\n",
            "It: 2567, Time: 0.02\n",
            "mse_b  [0.22797428]  mse_f: 0.2260705679655075   total loss: [0.45404485]\n",
            "mse_b ====== [0.224867046]\n",
            "It: 2568, Time: 0.02\n",
            "mse_b  [0.22486705]  mse_f: 0.2387446165084839   total loss: [0.46361166]\n",
            "mse_b ====== [0.225381866]\n",
            "It: 2569, Time: 0.02\n",
            "mse_b  [0.22538187]  mse_f: 0.22892414033412933   total loss: [0.454306]\n",
            "mse_b ====== [0.222545713]\n",
            "It: 2570, Time: 0.03\n",
            "mse_b  [0.22254571]  mse_f: 0.19694243371486664   total loss: [0.41948813]\n",
            "mse_b ====== [0.220041]\n",
            "It: 2571, Time: 0.02\n",
            "mse_b  [0.220041]  mse_f: 0.27522146701812744   total loss: [0.49526247]\n",
            "mse_b ====== [0.221129045]\n",
            "It: 2572, Time: 0.02\n",
            "mse_b  [0.22112904]  mse_f: 0.20606780052185059   total loss: [0.42719686]\n",
            "mse_b ====== [0.221855015]\n",
            "It: 2573, Time: 0.02\n",
            "mse_b  [0.22185501]  mse_f: 0.21309514343738556   total loss: [0.43495017]\n",
            "mse_b ====== [0.215066671]\n",
            "It: 2574, Time: 0.02\n",
            "mse_b  [0.21506667]  mse_f: 0.22800587117671967   total loss: [0.44307256]\n",
            "mse_b ====== [0.214508757]\n",
            "It: 2575, Time: 0.02\n",
            "mse_b  [0.21450876]  mse_f: 0.2463165819644928   total loss: [0.46082532]\n",
            "mse_b ====== [0.211180031]\n",
            "It: 2576, Time: 0.02\n",
            "mse_b  [0.21118003]  mse_f: 0.18616604804992676   total loss: [0.39734608]\n",
            "mse_b ====== [0.231333524]\n",
            "It: 2577, Time: 0.03\n",
            "mse_b  [0.23133352]  mse_f: 0.23416398465633392   total loss: [0.4654975]\n",
            "mse_b ====== [0.218395144]\n",
            "It: 2578, Time: 0.03\n",
            "mse_b  [0.21839514]  mse_f: 0.21551451086997986   total loss: [0.43390965]\n",
            "mse_b ====== [0.207756072]\n",
            "It: 2579, Time: 0.02\n",
            "mse_b  [0.20775607]  mse_f: 0.21819350123405457   total loss: [0.42594957]\n",
            "mse_b ====== [0.208280236]\n",
            "It: 2580, Time: 0.03\n",
            "mse_b  [0.20828024]  mse_f: 0.207503080368042   total loss: [0.41578332]\n",
            "mse_b ====== [0.212867856]\n",
            "It: 2581, Time: 0.02\n",
            "mse_b  [0.21286786]  mse_f: 0.2566870152950287   total loss: [0.46955487]\n",
            "mse_b ====== [0.208752081]\n",
            "It: 2582, Time: 0.02\n",
            "mse_b  [0.20875208]  mse_f: 0.18371906876564026   total loss: [0.39247113]\n",
            "mse_b ====== [0.208767086]\n",
            "It: 2583, Time: 0.03\n",
            "mse_b  [0.20876709]  mse_f: 0.2223331481218338   total loss: [0.43110025]\n",
            "mse_b ====== [0.212061927]\n",
            "It: 2584, Time: 0.02\n",
            "mse_b  [0.21206193]  mse_f: 0.21477556228637695   total loss: [0.4268375]\n",
            "mse_b ====== [0.215376347]\n",
            "It: 2585, Time: 0.02\n",
            "mse_b  [0.21537635]  mse_f: 0.21220971643924713   total loss: [0.42758608]\n",
            "mse_b ====== [0.210238844]\n",
            "It: 2586, Time: 0.02\n",
            "mse_b  [0.21023884]  mse_f: 0.178885817527771   total loss: [0.38912466]\n",
            "mse_b ====== [0.222408205]\n",
            "It: 2587, Time: 0.02\n",
            "mse_b  [0.2224082]  mse_f: 0.23271600902080536   total loss: [0.4551242]\n",
            "mse_b ====== [0.211490065]\n",
            "It: 2588, Time: 0.02\n",
            "mse_b  [0.21149006]  mse_f: 0.18826964497566223   total loss: [0.3997597]\n",
            "mse_b ====== [0.210481018]\n",
            "It: 2589, Time: 0.02\n",
            "mse_b  [0.21048102]  mse_f: 0.2025780975818634   total loss: [0.41305912]\n",
            "mse_b ====== [0.205620587]\n",
            "It: 2590, Time: 0.02\n",
            "mse_b  [0.20562059]  mse_f: 0.206487238407135   total loss: [0.41210783]\n",
            "mse_b ====== [0.202014834]\n",
            "It: 2591, Time: 0.03\n",
            "mse_b  [0.20201483]  mse_f: 0.22922198474407196   total loss: [0.4312368]\n",
            "mse_b ====== [0.201250643]\n",
            "It: 2592, Time: 0.02\n",
            "mse_b  [0.20125064]  mse_f: 0.17071375250816345   total loss: [0.3719644]\n",
            "mse_b ====== [0.202327669]\n",
            "It: 2593, Time: 0.02\n",
            "mse_b  [0.20232767]  mse_f: 0.23222514986991882   total loss: [0.43455282]\n",
            "mse_b ====== [0.195728958]\n",
            "It: 2594, Time: 0.02\n",
            "mse_b  [0.19572896]  mse_f: 0.2175157070159912   total loss: [0.41324466]\n",
            "mse_b ====== [0.192317843]\n",
            "It: 2595, Time: 0.03\n",
            "mse_b  [0.19231784]  mse_f: 0.19685375690460205   total loss: [0.3891716]\n",
            "mse_b ====== [0.206283107]\n",
            "It: 2596, Time: 0.02\n",
            "mse_b  [0.2062831]  mse_f: 0.20210960507392883   total loss: [0.40839273]\n",
            "mse_b ====== [0.197246253]\n",
            "It: 2597, Time: 0.03\n",
            "mse_b  [0.19724625]  mse_f: 0.217737078666687   total loss: [0.41498333]\n",
            "mse_b ====== [0.188517556]\n",
            "It: 2598, Time: 0.02\n",
            "mse_b  [0.18851756]  mse_f: 0.18439418077468872   total loss: [0.37291175]\n",
            "mse_b ====== [0.189002275]\n",
            "It: 2599, Time: 0.02\n",
            "mse_b  [0.18900228]  mse_f: 0.21386438608169556   total loss: [0.40286666]\n",
            "mse_b ====== [0.188238636]\n",
            "It: 2600, Time: 0.02\n",
            "mse_b  [0.18823864]  mse_f: 0.21278759837150574   total loss: [0.40102625]\n",
            "mse_b ====== [0.186741352]\n",
            "It: 2601, Time: 0.02\n",
            "mse_b  [0.18674135]  mse_f: 0.1985921859741211   total loss: [0.38533354]\n",
            "mse_b ====== [0.189564049]\n",
            "It: 2602, Time: 0.02\n",
            "mse_b  [0.18956405]  mse_f: 0.1924050748348236   total loss: [0.38196912]\n",
            "mse_b ====== [0.190088183]\n",
            "It: 2603, Time: 0.03\n",
            "mse_b  [0.19008818]  mse_f: 0.2220066785812378   total loss: [0.41209486]\n",
            "mse_b ====== [0.185404986]\n",
            "It: 2604, Time: 0.02\n",
            "mse_b  [0.18540499]  mse_f: 0.19794300198554993   total loss: [0.383348]\n",
            "mse_b ====== [0.185134739]\n",
            "It: 2605, Time: 0.02\n",
            "mse_b  [0.18513474]  mse_f: 0.1840958595275879   total loss: [0.3692306]\n",
            "mse_b ====== [0.184172541]\n",
            "It: 2606, Time: 0.02\n",
            "mse_b  [0.18417254]  mse_f: 0.2243005484342575   total loss: [0.40847307]\n",
            "mse_b ====== [0.185433567]\n",
            "It: 2607, Time: 0.02\n",
            "mse_b  [0.18543357]  mse_f: 0.1849338859319687   total loss: [0.37036747]\n",
            "mse_b ====== [0.184977651]\n",
            "It: 2608, Time: 0.02\n",
            "mse_b  [0.18497765]  mse_f: 0.18242375552654266   total loss: [0.36740142]\n",
            "mse_b ====== [0.19087784]\n",
            "It: 2609, Time: 0.02\n",
            "mse_b  [0.19087784]  mse_f: 0.21364760398864746   total loss: [0.40452546]\n",
            "mse_b ====== [0.183519125]\n",
            "It: 2610, Time: 0.02\n",
            "mse_b  [0.18351912]  mse_f: 0.19961529970169067   total loss: [0.38313442]\n",
            "mse_b ====== [0.184318]\n",
            "It: 2611, Time: 0.02\n",
            "mse_b  [0.184318]  mse_f: 0.16795913875102997   total loss: [0.35227716]\n",
            "mse_b ====== [0.185727865]\n",
            "It: 2612, Time: 0.02\n",
            "mse_b  [0.18572786]  mse_f: 0.22458302974700928   total loss: [0.4103109]\n",
            "mse_b ====== [0.184082538]\n",
            "It: 2613, Time: 0.02\n",
            "mse_b  [0.18408254]  mse_f: 0.1850322037935257   total loss: [0.36911476]\n",
            "mse_b ====== [0.181321651]\n",
            "It: 2614, Time: 0.03\n",
            "mse_b  [0.18132165]  mse_f: 0.18229997158050537   total loss: [0.36362162]\n",
            "mse_b ====== [0.183955729]\n",
            "It: 2615, Time: 0.03\n",
            "mse_b  [0.18395573]  mse_f: 0.2049589604139328   total loss: [0.3889147]\n",
            "mse_b ====== [0.177861869]\n",
            "It: 2616, Time: 0.02\n",
            "mse_b  [0.17786187]  mse_f: 0.19859007000923157   total loss: [0.37645194]\n",
            "mse_b ====== [0.175947413]\n",
            "It: 2617, Time: 0.02\n",
            "mse_b  [0.17594741]  mse_f: 0.16487807035446167   total loss: [0.3408255]\n",
            "mse_b ====== [0.174995661]\n",
            "It: 2618, Time: 0.02\n",
            "mse_b  [0.17499566]  mse_f: 0.2136453092098236   total loss: [0.38864097]\n",
            "mse_b ====== [0.17524901]\n",
            "It: 2619, Time: 0.02\n",
            "mse_b  [0.17524901]  mse_f: 0.19497798383235931   total loss: [0.37022698]\n",
            "mse_b ====== [0.173715577]\n",
            "It: 2620, Time: 0.02\n",
            "mse_b  [0.17371558]  mse_f: 0.17049038410186768   total loss: [0.34420598]\n",
            "mse_b ====== [0.178216472]\n",
            "It: 2621, Time: 0.02\n",
            "mse_b  [0.17821647]  mse_f: 0.1985606700181961   total loss: [0.37677714]\n",
            "mse_b ====== [0.169371203]\n",
            "It: 2622, Time: 0.02\n",
            "mse_b  [0.1693712]  mse_f: 0.21708610653877258   total loss: [0.38645732]\n",
            "mse_b ====== [0.172788411]\n",
            "It: 2623, Time: 0.02\n",
            "mse_b  [0.17278841]  mse_f: 0.16435649991035461   total loss: [0.3371449]\n",
            "mse_b ====== [0.169959247]\n",
            "It: 2624, Time: 0.02\n",
            "mse_b  [0.16995925]  mse_f: 0.18684038519859314   total loss: [0.35679963]\n",
            "mse_b ====== [0.165931717]\n",
            "It: 2625, Time: 0.02\n",
            "mse_b  [0.16593172]  mse_f: 0.2232569009065628   total loss: [0.38918862]\n",
            "mse_b ====== [0.17171979]\n",
            "It: 2626, Time: 0.03\n",
            "mse_b  [0.17171979]  mse_f: 0.17317435145378113   total loss: [0.34489414]\n",
            "mse_b ====== [0.168267071]\n",
            "It: 2627, Time: 0.02\n",
            "mse_b  [0.16826707]  mse_f: 0.18857574462890625   total loss: [0.35684282]\n",
            "mse_b ====== [0.170330375]\n",
            "It: 2628, Time: 0.02\n",
            "mse_b  [0.17033038]  mse_f: 0.1850702166557312   total loss: [0.3554006]\n",
            "mse_b ====== [0.16875045]\n",
            "It: 2629, Time: 0.02\n",
            "mse_b  [0.16875045]  mse_f: 0.17258891463279724   total loss: [0.34133935]\n",
            "mse_b ====== [0.16619727]\n",
            "It: 2630, Time: 0.03\n",
            "mse_b  [0.16619727]  mse_f: 0.18586227297782898   total loss: [0.35205954]\n",
            "mse_b ====== [0.166855305]\n",
            "It: 2631, Time: 0.02\n",
            "mse_b  [0.1668553]  mse_f: 0.2023971527814865   total loss: [0.36925244]\n",
            "mse_b ====== [0.163930729]\n",
            "It: 2632, Time: 0.02\n",
            "mse_b  [0.16393073]  mse_f: 0.1586618721485138   total loss: [0.32259262]\n",
            "mse_b ====== [0.162924916]\n",
            "It: 2633, Time: 0.02\n",
            "mse_b  [0.16292492]  mse_f: 0.17488928139209747   total loss: [0.3378142]\n",
            "mse_b ====== [0.165334255]\n",
            "It: 2634, Time: 0.02\n",
            "mse_b  [0.16533425]  mse_f: 0.1909370720386505   total loss: [0.35627133]\n",
            "mse_b ====== [0.163241327]\n",
            "It: 2635, Time: 0.03\n",
            "mse_b  [0.16324133]  mse_f: 0.1847955584526062   total loss: [0.3480369]\n",
            "mse_b ====== [0.162692651]\n",
            "It: 2636, Time: 0.03\n",
            "mse_b  [0.16269265]  mse_f: 0.17143936455249786   total loss: [0.33413202]\n",
            "mse_b ====== [0.164171129]\n",
            "It: 2637, Time: 0.03\n",
            "mse_b  [0.16417113]  mse_f: 0.1865081489086151   total loss: [0.35067928]\n",
            "mse_b ====== [0.163852841]\n",
            "It: 2638, Time: 0.02\n",
            "mse_b  [0.16385284]  mse_f: 0.16232417523860931   total loss: [0.326177]\n",
            "mse_b ====== [0.164371014]\n",
            "It: 2639, Time: 0.03\n",
            "mse_b  [0.16437101]  mse_f: 0.17562632262706757   total loss: [0.33999735]\n",
            "mse_b ====== [0.161511287]\n",
            "It: 2640, Time: 0.02\n",
            "mse_b  [0.16151129]  mse_f: 0.21474981307983398   total loss: [0.37626112]\n",
            "mse_b ====== [0.163047448]\n",
            "It: 2641, Time: 0.02\n",
            "mse_b  [0.16304745]  mse_f: 0.1752263754606247   total loss: [0.33827382]\n",
            "mse_b ====== [0.160095021]\n",
            "It: 2642, Time: 0.02\n",
            "mse_b  [0.16009502]  mse_f: 0.16676321625709534   total loss: [0.32685822]\n",
            "mse_b ====== [0.159850419]\n",
            "It: 2643, Time: 0.02\n",
            "mse_b  [0.15985042]  mse_f: 0.17730925977230072   total loss: [0.3371597]\n",
            "mse_b ====== [0.158455655]\n",
            "It: 2644, Time: 0.02\n",
            "mse_b  [0.15845565]  mse_f: 0.16188198328018188   total loss: [0.32033765]\n",
            "mse_b ====== [0.157992244]\n",
            "It: 2645, Time: 0.02\n",
            "mse_b  [0.15799224]  mse_f: 0.1539401262998581   total loss: [0.31193238]\n",
            "mse_b ====== [0.165696174]\n",
            "It: 2646, Time: 0.02\n",
            "mse_b  [0.16569617]  mse_f: 0.19326657056808472   total loss: [0.35896274]\n",
            "mse_b ====== [0.157557145]\n",
            "It: 2647, Time: 0.03\n",
            "mse_b  [0.15755714]  mse_f: 0.26752468943595886   total loss: [0.42508185]\n",
            "mse_b ====== [0.171591565]\n",
            "It: 2648, Time: 0.02\n",
            "mse_b  [0.17159157]  mse_f: 0.37879666686058044   total loss: [0.5503882]\n",
            "mse_b ====== [0.251432359]\n",
            "It: 2649, Time: 0.02\n",
            "mse_b  [0.25143236]  mse_f: 0.8611964583396912   total loss: [1.1126288]\n",
            "mse_b ====== [0.200174972]\n",
            "It: 2650, Time: 0.02\n",
            "mse_b  [0.20017497]  mse_f: 0.2653740644454956   total loss: [0.46554905]\n",
            "mse_b ====== [0.204269871]\n",
            "It: 2651, Time: 0.03\n",
            "mse_b  [0.20426987]  mse_f: 1.6992456912994385   total loss: [1.9035156]\n",
            "mse_b ====== [0.91127938]\n",
            "It: 2652, Time: 0.02\n",
            "mse_b  [0.9112794]  mse_f: 1.9126392602920532   total loss: [2.8239186]\n",
            "mse_b ====== [1.13752031]\n",
            "It: 2653, Time: 0.02\n",
            "mse_b  [1.1375203]  mse_f: 2.061140537261963   total loss: [3.1986609]\n",
            "mse_b ====== [0.252144694]\n",
            "It: 2654, Time: 0.02\n",
            "mse_b  [0.2521447]  mse_f: 0.5446182489395142   total loss: [0.79676294]\n",
            "mse_b ====== [0.439836979]\n",
            "It: 2655, Time: 0.02\n",
            "mse_b  [0.43983698]  mse_f: 5.422083377838135   total loss: [5.8619204]\n",
            "mse_b ====== [0.870810449]\n",
            "It: 2656, Time: 0.02\n",
            "mse_b  [0.87081045]  mse_f: 1.426900863647461   total loss: [2.2977114]\n",
            "mse_b ====== [3.06283259]\n",
            "It: 2657, Time: 0.02\n",
            "mse_b  [3.0628326]  mse_f: 3.859478712081909   total loss: [6.9223113]\n",
            "mse_b ====== [3.65966082]\n",
            "It: 2658, Time: 0.02\n",
            "mse_b  [3.6596608]  mse_f: 2.9665842056274414   total loss: [6.626245]\n",
            "mse_b ====== [2.57754207]\n",
            "It: 2659, Time: 0.02\n",
            "mse_b  [2.577542]  mse_f: 1.4635838270187378   total loss: [4.041126]\n",
            "mse_b ====== [1.17135501]\n",
            "It: 2660, Time: 0.02\n",
            "mse_b  [1.171355]  mse_f: 2.295828342437744   total loss: [3.4671834]\n",
            "mse_b ====== [0.607641041]\n",
            "It: 2661, Time: 0.02\n",
            "mse_b  [0.60764104]  mse_f: 2.544520378112793   total loss: [3.1521614]\n",
            "mse_b ====== [0.794589579]\n",
            "It: 2662, Time: 0.02\n",
            "mse_b  [0.7945896]  mse_f: 2.3224294185638428   total loss: [3.117019]\n",
            "mse_b ====== [1.20119107]\n",
            "It: 2663, Time: 0.02\n",
            "mse_b  [1.2011911]  mse_f: 2.8149328231811523   total loss: [4.016124]\n",
            "mse_b ====== [1.59237194]\n",
            "It: 2664, Time: 0.02\n",
            "mse_b  [1.592372]  mse_f: 3.73427152633667   total loss: [5.3266435]\n",
            "mse_b ====== [1.91525364]\n",
            "It: 2665, Time: 0.02\n",
            "mse_b  [1.9152536]  mse_f: 2.980544328689575   total loss: [4.8957977]\n",
            "mse_b ====== [2.12059]\n",
            "It: 2666, Time: 0.02\n",
            "mse_b  [2.12059]  mse_f: 3.2545082569122314   total loss: [5.375098]\n",
            "mse_b ====== [2.14590812]\n",
            "It: 2667, Time: 0.02\n",
            "mse_b  [2.145908]  mse_f: 3.220505714416504   total loss: [5.366414]\n",
            "mse_b ====== [1.99356973]\n",
            "It: 2668, Time: 0.03\n",
            "mse_b  [1.9935697]  mse_f: 2.194685459136963   total loss: [4.1882553]\n",
            "mse_b ====== [1.75782132]\n",
            "It: 2669, Time: 0.02\n",
            "mse_b  [1.7578213]  mse_f: 1.292560338973999   total loss: [3.0503817]\n",
            "mse_b ====== [1.58324599]\n",
            "It: 2670, Time: 0.02\n",
            "mse_b  [1.583246]  mse_f: 2.1959609985351562   total loss: [3.779207]\n",
            "mse_b ====== [1.52991056]\n",
            "It: 2671, Time: 0.02\n",
            "mse_b  [1.5299106]  mse_f: 2.3915677070617676   total loss: [3.9214783]\n",
            "mse_b ====== [1.57002151]\n",
            "It: 2672, Time: 0.02\n",
            "mse_b  [1.5700215]  mse_f: 1.525133728981018   total loss: [3.0951552]\n",
            "mse_b ====== [1.66500401]\n",
            "It: 2673, Time: 0.02\n",
            "mse_b  [1.665004]  mse_f: 1.4130792617797852   total loss: [3.0780833]\n",
            "mse_b ====== [1.79404831]\n",
            "It: 2674, Time: 0.02\n",
            "mse_b  [1.7940483]  mse_f: 2.332613945007324   total loss: [4.1266623]\n",
            "mse_b ====== [1.95053947]\n",
            "It: 2675, Time: 0.02\n",
            "mse_b  [1.9505395]  mse_f: 2.367279529571533   total loss: [4.317819]\n",
            "mse_b ====== [2.09151363]\n",
            "It: 2676, Time: 0.02\n",
            "mse_b  [2.0915136]  mse_f: 1.4213310480117798   total loss: [3.5128446]\n",
            "mse_b ====== [2.12677622]\n",
            "It: 2677, Time: 0.02\n",
            "mse_b  [2.1267762]  mse_f: 1.833950161933899   total loss: [3.9607263]\n",
            "mse_b ====== [1.98476136]\n",
            "It: 2678, Time: 0.03\n",
            "mse_b  [1.9847614]  mse_f: 2.665607452392578   total loss: [4.6503687]\n",
            "mse_b ====== [1.71060693]\n",
            "It: 2679, Time: 0.02\n",
            "mse_b  [1.7106069]  mse_f: 2.606285572052002   total loss: [4.3168926]\n",
            "mse_b ====== [1.48159206]\n",
            "It: 2680, Time: 0.02\n",
            "mse_b  [1.481592]  mse_f: 2.272515296936035   total loss: [3.7541075]\n",
            "mse_b ====== [1.41367459]\n",
            "It: 2681, Time: 0.02\n",
            "mse_b  [1.4136746]  mse_f: 2.9865407943725586   total loss: [4.400215]\n",
            "mse_b ====== [1.44479346]\n",
            "It: 2682, Time: 0.02\n",
            "mse_b  [1.4447935]  mse_f: 2.613801956176758   total loss: [4.0585957]\n",
            "mse_b ====== [1.50562119]\n",
            "It: 2683, Time: 0.03\n",
            "mse_b  [1.5056212]  mse_f: 1.9366494417190552   total loss: [3.4422708]\n",
            "mse_b ====== [1.58245385]\n",
            "It: 2684, Time: 0.02\n",
            "mse_b  [1.5824538]  mse_f: 2.2412734031677246   total loss: [3.8237271]\n",
            "mse_b ====== [1.67130947]\n",
            "It: 2685, Time: 0.02\n",
            "mse_b  [1.6713095]  mse_f: 2.8425612449645996   total loss: [4.5138707]\n",
            "mse_b ====== [1.8012]\n",
            "It: 2686, Time: 0.02\n",
            "mse_b  [1.8012]  mse_f: 1.729458212852478   total loss: [3.5306582]\n",
            "mse_b ====== [2.00618696]\n",
            "It: 2687, Time: 0.03\n",
            "mse_b  [2.006187]  mse_f: 1.1279340982437134   total loss: [3.134121]\n",
            "mse_b ====== [2.20623398]\n",
            "It: 2688, Time: 0.03\n",
            "mse_b  [2.206234]  mse_f: 1.5182480812072754   total loss: [3.724482]\n",
            "mse_b ====== [2.2583282]\n",
            "It: 2689, Time: 0.03\n",
            "mse_b  [2.2583282]  mse_f: 1.7218033075332642   total loss: [3.9801316]\n",
            "mse_b ====== [2.13274]\n",
            "It: 2690, Time: 0.02\n",
            "mse_b  [2.13274]  mse_f: 0.9987925887107849   total loss: [3.1315327]\n",
            "mse_b ====== [1.96975112]\n",
            "It: 2691, Time: 0.02\n",
            "mse_b  [1.9697511]  mse_f: 0.9881700873374939   total loss: [2.9579213]\n",
            "mse_b ====== [1.90559804]\n",
            "It: 2692, Time: 0.03\n",
            "mse_b  [1.905598]  mse_f: 1.536750316619873   total loss: [3.4423485]\n",
            "mse_b ====== [1.96547818]\n",
            "It: 2693, Time: 0.02\n",
            "mse_b  [1.9654782]  mse_f: 1.6803706884384155   total loss: [3.6458488]\n",
            "mse_b ====== [2.09693694]\n",
            "It: 2694, Time: 0.03\n",
            "mse_b  [2.096937]  mse_f: 1.314234972000122   total loss: [3.411172]\n",
            "mse_b ====== [2.21242237]\n",
            "It: 2695, Time: 0.02\n",
            "mse_b  [2.2124224]  mse_f: 1.3400540351867676   total loss: [3.5524764]\n",
            "mse_b ====== [2.23947859]\n",
            "It: 2696, Time: 0.02\n",
            "mse_b  [2.2394786]  mse_f: 1.737898588180542   total loss: [3.9773772]\n",
            "mse_b ====== [2.17324281]\n",
            "It: 2697, Time: 0.02\n",
            "mse_b  [2.1732428]  mse_f: 1.5438005924224854   total loss: [3.7170434]\n",
            "mse_b ====== [2.07083225]\n",
            "It: 2698, Time: 0.02\n",
            "mse_b  [2.0708323]  mse_f: 1.3682397603988647   total loss: [3.4390721]\n",
            "mse_b ====== [1.99546552]\n",
            "It: 2699, Time: 0.02\n",
            "mse_b  [1.9954655]  mse_f: 1.5438367128372192   total loss: [3.5393023]\n",
            "mse_b ====== [1.96483266]\n",
            "It: 2700, Time: 0.02\n",
            "mse_b  [1.9648327]  mse_f: 2.118278741836548   total loss: [4.0831113]\n",
            "mse_b ====== [1.96053684]\n",
            "It: 2701, Time: 0.03\n",
            "mse_b  [1.9605368]  mse_f: 1.9008781909942627   total loss: [3.861415]\n",
            "mse_b ====== [1.97259009]\n",
            "It: 2702, Time: 0.02\n",
            "mse_b  [1.9725901]  mse_f: 1.4816871881484985   total loss: [3.4542773]\n",
            "mse_b ====== [1.99184716]\n",
            "It: 2703, Time: 0.03\n",
            "mse_b  [1.9918472]  mse_f: 1.5822572708129883   total loss: [3.5741043]\n",
            "mse_b ====== [2.01013756]\n",
            "It: 2704, Time: 0.03\n",
            "mse_b  [2.0101376]  mse_f: 2.1701278686523438   total loss: [4.1802654]\n",
            "mse_b ====== [2.03526068]\n",
            "It: 2705, Time: 0.03\n",
            "mse_b  [2.0352607]  mse_f: 2.2905893325805664   total loss: [4.32585]\n",
            "mse_b ====== [2.07499909]\n",
            "It: 2706, Time: 0.02\n",
            "mse_b  [2.074999]  mse_f: 1.6494816541671753   total loss: [3.7244806]\n",
            "mse_b ====== [2.12420678]\n",
            "It: 2707, Time: 0.03\n",
            "mse_b  [2.1242068]  mse_f: 1.525705099105835   total loss: [3.6499119]\n",
            "mse_b ====== [2.16301107]\n",
            "It: 2708, Time: 0.02\n",
            "mse_b  [2.163011]  mse_f: 1.8763238191604614   total loss: [4.039335]\n",
            "mse_b ====== [2.16059875]\n",
            "It: 2709, Time: 0.02\n",
            "mse_b  [2.1605988]  mse_f: 2.12644100189209   total loss: [4.2870398]\n",
            "mse_b ====== [2.10592985]\n",
            "It: 2710, Time: 0.02\n",
            "mse_b  [2.1059299]  mse_f: 1.6015102863311768   total loss: [3.7074401]\n",
            "mse_b ====== [2.03173971]\n",
            "It: 2711, Time: 0.02\n",
            "mse_b  [2.0317397]  mse_f: 1.3028590679168701   total loss: [3.3345988]\n",
            "mse_b ====== [1.97005892]\n",
            "It: 2712, Time: 0.02\n",
            "mse_b  [1.9700589]  mse_f: 1.466874361038208   total loss: [3.4369333]\n",
            "mse_b ====== [1.92789412]\n",
            "It: 2713, Time: 0.03\n",
            "mse_b  [1.9278941]  mse_f: 1.5221282243728638   total loss: [3.4500222]\n",
            "mse_b ====== [1.90467691]\n",
            "It: 2714, Time: 0.02\n",
            "mse_b  [1.9046769]  mse_f: 1.1815874576568604   total loss: [3.0862644]\n",
            "mse_b ====== [1.89381862]\n",
            "It: 2715, Time: 0.02\n",
            "mse_b  [1.8938186]  mse_f: 0.96915602684021   total loss: [2.8629746]\n",
            "mse_b ====== [1.8913269]\n",
            "It: 2716, Time: 0.02\n",
            "mse_b  [1.8913269]  mse_f: 1.2831172943115234   total loss: [3.1744442]\n",
            "mse_b ====== [1.90939164]\n",
            "It: 2717, Time: 0.02\n",
            "mse_b  [1.9093916]  mse_f: 1.2490410804748535   total loss: [3.1584327]\n",
            "mse_b ====== [1.96522093]\n",
            "It: 2718, Time: 0.02\n",
            "mse_b  [1.9652209]  mse_f: 1.009014368057251   total loss: [2.9742353]\n",
            "mse_b ====== [2.05398417]\n",
            "It: 2719, Time: 0.02\n",
            "mse_b  [2.0539842]  mse_f: 0.8833107948303223   total loss: [2.937295]\n",
            "mse_b ====== [2.13541508]\n",
            "It: 2720, Time: 0.03\n",
            "mse_b  [2.135415]  mse_f: 1.3169779777526855   total loss: [3.452393]\n",
            "mse_b ====== [2.1522789]\n",
            "It: 2721, Time: 0.02\n",
            "mse_b  [2.152279]  mse_f: 1.367148995399475   total loss: [3.5194278]\n",
            "mse_b ====== [2.10203862]\n",
            "It: 2722, Time: 0.02\n",
            "mse_b  [2.1020386]  mse_f: 1.0209290981292725   total loss: [3.1229677]\n",
            "mse_b ====== [2.03715181]\n",
            "It: 2723, Time: 0.03\n",
            "mse_b  [2.0371518]  mse_f: 0.7781936526298523   total loss: [2.8153455]\n",
            "mse_b ====== [2.00091839]\n",
            "It: 2724, Time: 0.02\n",
            "mse_b  [2.0009184]  mse_f: 1.2230021953582764   total loss: [3.2239206]\n",
            "mse_b ====== [1.98802006]\n",
            "It: 2725, Time: 0.02\n",
            "mse_b  [1.9880201]  mse_f: 1.718940258026123   total loss: [3.7069602]\n",
            "mse_b ====== [1.97085798]\n",
            "It: 2726, Time: 0.03\n",
            "mse_b  [1.970858]  mse_f: 1.4908267259597778   total loss: [3.4616847]\n",
            "mse_b ====== [1.95222116]\n",
            "It: 2727, Time: 0.02\n",
            "mse_b  [1.9522212]  mse_f: 1.1928303241729736   total loss: [3.1450515]\n",
            "mse_b ====== [1.95179331]\n",
            "It: 2728, Time: 0.02\n",
            "mse_b  [1.9517933]  mse_f: 1.2942709922790527   total loss: [3.2460642]\n",
            "mse_b ====== [1.95911741]\n",
            "It: 2729, Time: 0.02\n",
            "mse_b  [1.9591174]  mse_f: 1.70844566822052   total loss: [3.667563]\n",
            "mse_b ====== [1.94485831]\n",
            "It: 2730, Time: 0.02\n",
            "mse_b  [1.9448583]  mse_f: 1.5936384201049805   total loss: [3.5384967]\n",
            "mse_b ====== [1.91025615]\n",
            "It: 2731, Time: 0.03\n",
            "mse_b  [1.9102561]  mse_f: 1.4470534324645996   total loss: [3.3573096]\n",
            "mse_b ====== [1.8751359]\n",
            "It: 2732, Time: 0.02\n",
            "mse_b  [1.8751359]  mse_f: 1.5593056678771973   total loss: [3.4344416]\n",
            "mse_b ====== [1.85088038]\n",
            "It: 2733, Time: 0.02\n",
            "mse_b  [1.8508804]  mse_f: 1.7662795782089233   total loss: [3.6171598]\n",
            "mse_b ====== [1.83621347]\n",
            "It: 2734, Time: 0.02\n",
            "mse_b  [1.8362135]  mse_f: 1.5393397808074951   total loss: [3.3755531]\n",
            "mse_b ====== [1.82983983]\n",
            "It: 2735, Time: 0.02\n",
            "mse_b  [1.8298398]  mse_f: 1.2570827007293701   total loss: [3.0869226]\n",
            "mse_b ====== [1.83433044]\n",
            "It: 2736, Time: 0.02\n",
            "mse_b  [1.8343304]  mse_f: 1.4999661445617676   total loss: [3.3342967]\n",
            "mse_b ====== [1.84049439]\n",
            "It: 2737, Time: 0.02\n",
            "mse_b  [1.8404944]  mse_f: 1.6135666370391846   total loss: [3.454061]\n",
            "mse_b ====== [1.83049297]\n",
            "It: 2738, Time: 0.03\n",
            "mse_b  [1.830493]  mse_f: 1.284522294998169   total loss: [3.1150153]\n",
            "mse_b ====== [1.81565666]\n",
            "It: 2739, Time: 0.02\n",
            "mse_b  [1.8156567]  mse_f: 0.8234995603561401   total loss: [2.6391563]\n",
            "mse_b ====== [1.82496786]\n",
            "It: 2740, Time: 0.02\n",
            "mse_b  [1.8249679]  mse_f: 1.095383644104004   total loss: [2.9203515]\n",
            "mse_b ====== [1.8498826]\n",
            "It: 2741, Time: 0.02\n",
            "mse_b  [1.8498826]  mse_f: 1.3786945343017578   total loss: [3.2285771]\n",
            "mse_b ====== [1.86806202]\n",
            "It: 2742, Time: 0.02\n",
            "mse_b  [1.868062]  mse_f: 1.2344022989273071   total loss: [3.1024642]\n",
            "mse_b ====== [1.88022065]\n",
            "It: 2743, Time: 0.02\n",
            "mse_b  [1.8802207]  mse_f: 0.7829630374908447   total loss: [2.6631837]\n",
            "mse_b ====== [1.89892948]\n",
            "It: 2744, Time: 0.02\n",
            "mse_b  [1.8989295]  mse_f: 0.884620189666748   total loss: [2.7835498]\n",
            "mse_b ====== [1.92790627]\n",
            "It: 2745, Time: 0.02\n",
            "mse_b  [1.9279063]  mse_f: 1.1053340435028076   total loss: [3.0332403]\n",
            "mse_b ====== [1.96186686]\n",
            "It: 2746, Time: 0.02\n",
            "mse_b  [1.9618669]  mse_f: 1.0451164245605469   total loss: [3.0069833]\n",
            "mse_b ====== [1.99228883]\n",
            "It: 2747, Time: 0.02\n",
            "mse_b  [1.9922888]  mse_f: 0.746218204498291   total loss: [2.738507]\n",
            "mse_b ====== [2.01335239]\n",
            "It: 2748, Time: 0.02\n",
            "mse_b  [2.0133524]  mse_f: 0.782580554485321   total loss: [2.795933]\n",
            "mse_b ====== [2.01369381]\n",
            "It: 2749, Time: 0.02\n",
            "mse_b  [2.0136938]  mse_f: 1.0461194515228271   total loss: [3.0598133]\n",
            "mse_b ====== [1.99053538]\n",
            "It: 2750, Time: 0.02\n",
            "mse_b  [1.9905354]  mse_f: 1.0368967056274414   total loss: [3.027432]\n",
            "mse_b ====== [1.96440578]\n",
            "It: 2751, Time: 0.02\n",
            "mse_b  [1.9644058]  mse_f: 0.9088960289955139   total loss: [2.8733017]\n",
            "mse_b ====== [1.95251703]\n",
            "It: 2752, Time: 0.02\n",
            "mse_b  [1.952517]  mse_f: 0.9257510900497437   total loss: [2.8782682]\n",
            "mse_b ====== [1.9514308]\n",
            "It: 2753, Time: 0.02\n",
            "mse_b  [1.9514308]  mse_f: 1.2259612083435059   total loss: [3.177392]\n",
            "mse_b ====== [1.94977403]\n",
            "It: 2754, Time: 0.02\n",
            "mse_b  [1.949774]  mse_f: 1.1873416900634766   total loss: [3.1371157]\n",
            "mse_b ====== [1.94759846]\n",
            "It: 2755, Time: 0.02\n",
            "mse_b  [1.9475985]  mse_f: 0.9665189981460571   total loss: [2.9141173]\n",
            "mse_b ====== [1.95276332]\n",
            "It: 2756, Time: 0.03\n",
            "mse_b  [1.9527633]  mse_f: 0.8430963754653931   total loss: [2.7958598]\n",
            "mse_b ====== [1.96501958]\n",
            "It: 2757, Time: 0.02\n",
            "mse_b  [1.9650196]  mse_f: 1.1139185428619385   total loss: [3.078938]\n",
            "mse_b ====== [1.97389746]\n",
            "It: 2758, Time: 0.02\n",
            "mse_b  [1.9738975]  mse_f: 1.3038411140441895   total loss: [3.2777386]\n",
            "mse_b ====== [1.98633528]\n",
            "It: 2759, Time: 0.03\n",
            "mse_b  [1.9863353]  mse_f: 1.0746653079986572   total loss: [3.0610006]\n",
            "mse_b ====== [2.0235393]\n",
            "It: 2760, Time: 0.02\n",
            "mse_b  [2.0235393]  mse_f: 0.8509143590927124   total loss: [2.8744535]\n",
            "mse_b ====== [2.08436513]\n",
            "It: 2761, Time: 0.03\n",
            "mse_b  [2.0843651]  mse_f: 0.857518196105957   total loss: [2.9418833]\n",
            "mse_b ====== [2.14449096]\n",
            "It: 2762, Time: 0.02\n",
            "mse_b  [2.144491]  mse_f: 1.0974993705749512   total loss: [3.2419903]\n",
            "mse_b ====== [2.17652297]\n",
            "It: 2763, Time: 0.02\n",
            "mse_b  [2.176523]  mse_f: 0.9988529682159424   total loss: [3.175376]\n",
            "mse_b ====== [2.16444921]\n",
            "It: 2764, Time: 0.02\n",
            "mse_b  [2.1644492]  mse_f: 0.786910891532898   total loss: [2.9513602]\n",
            "mse_b ====== [2.11452913]\n",
            "It: 2765, Time: 0.02\n",
            "mse_b  [2.1145291]  mse_f: 0.6994643211364746   total loss: [2.8139935]\n",
            "mse_b ====== [2.05496573]\n",
            "It: 2766, Time: 0.03\n",
            "mse_b  [2.0549657]  mse_f: 0.909382700920105   total loss: [2.9643483]\n",
            "mse_b ====== [2.00954676]\n",
            "It: 2767, Time: 0.02\n",
            "mse_b  [2.0095468]  mse_f: 1.0675058364868164   total loss: [3.0770526]\n",
            "mse_b ====== [1.98484612]\n",
            "It: 2768, Time: 0.02\n",
            "mse_b  [1.9848461]  mse_f: 0.9110619425773621   total loss: [2.895908]\n",
            "mse_b ====== [1.97424197]\n",
            "It: 2769, Time: 0.02\n",
            "mse_b  [1.974242]  mse_f: 0.774168848991394   total loss: [2.7484107]\n",
            "mse_b ====== [1.96426225]\n",
            "It: 2770, Time: 0.03\n",
            "mse_b  [1.9642622]  mse_f: 0.7583199739456177   total loss: [2.7225823]\n",
            "mse_b ====== [1.94984818]\n",
            "It: 2771, Time: 0.02\n",
            "mse_b  [1.9498482]  mse_f: 0.8875691890716553   total loss: [2.8374174]\n",
            "mse_b ====== [1.93330705]\n",
            "It: 2772, Time: 0.02\n",
            "mse_b  [1.933307]  mse_f: 0.7476845979690552   total loss: [2.6809916]\n",
            "mse_b ====== [1.91691339]\n",
            "It: 2773, Time: 0.02\n",
            "mse_b  [1.9169134]  mse_f: 0.6342015266418457   total loss: [2.551115]\n",
            "mse_b ====== [1.90685368]\n",
            "It: 2774, Time: 0.02\n",
            "mse_b  [1.9068537]  mse_f: 0.678153395652771   total loss: [2.5850072]\n",
            "mse_b ====== [1.90960467]\n",
            "It: 2775, Time: 0.02\n",
            "mse_b  [1.9096047]  mse_f: 0.8798247575759888   total loss: [2.7894294]\n",
            "mse_b ====== [1.91572785]\n",
            "It: 2776, Time: 0.02\n",
            "mse_b  [1.9157279]  mse_f: 0.8411718010902405   total loss: [2.7568996]\n",
            "mse_b ====== [1.91469097]\n",
            "It: 2777, Time: 0.02\n",
            "mse_b  [1.914691]  mse_f: 0.5991888046264648   total loss: [2.5138798]\n",
            "mse_b ====== [1.90896416]\n",
            "It: 2778, Time: 0.02\n",
            "mse_b  [1.9089642]  mse_f: 0.5993914604187012   total loss: [2.5083556]\n",
            "mse_b ====== [1.90198565]\n",
            "It: 2779, Time: 0.02\n",
            "mse_b  [1.9019856]  mse_f: 0.824228048324585   total loss: [2.7262137]\n",
            "mse_b ====== [1.89469886]\n",
            "It: 2780, Time: 0.02\n",
            "mse_b  [1.8946989]  mse_f: 0.998460590839386   total loss: [2.8931594]\n",
            "mse_b ====== [1.8865453]\n",
            "It: 2781, Time: 0.02\n",
            "mse_b  [1.8865453]  mse_f: 0.7596083879470825   total loss: [2.6461537]\n",
            "mse_b ====== [1.87609816]\n",
            "It: 2782, Time: 0.02\n",
            "mse_b  [1.8760982]  mse_f: 0.6241127252578735   total loss: [2.5002108]\n",
            "mse_b ====== [1.85990739]\n",
            "It: 2783, Time: 0.03\n",
            "mse_b  [1.8599074]  mse_f: 0.7553958892822266   total loss: [2.6153033]\n",
            "mse_b ====== [1.83568931]\n",
            "It: 2784, Time: 0.02\n",
            "mse_b  [1.8356893]  mse_f: 1.0117309093475342   total loss: [2.8474202]\n",
            "mse_b ====== [1.80760241]\n",
            "It: 2785, Time: 0.02\n",
            "mse_b  [1.8076024]  mse_f: 0.9919386506080627   total loss: [2.799541]\n",
            "mse_b ====== [1.79078329]\n",
            "It: 2786, Time: 0.02\n",
            "mse_b  [1.7907833]  mse_f: 0.9060689210891724   total loss: [2.6968522]\n",
            "mse_b ====== [1.79098427]\n",
            "It: 2787, Time: 0.02\n",
            "mse_b  [1.7909843]  mse_f: 0.9914803504943848   total loss: [2.7824645]\n",
            "mse_b ====== [1.79716527]\n",
            "It: 2788, Time: 0.02\n",
            "mse_b  [1.7971653]  mse_f: 1.0328534841537476   total loss: [2.8300188]\n",
            "mse_b ====== [1.80141687]\n",
            "It: 2789, Time: 0.02\n",
            "mse_b  [1.8014169]  mse_f: 0.9222235679626465   total loss: [2.7236404]\n",
            "mse_b ====== [1.8032515]\n",
            "It: 2790, Time: 0.02\n",
            "mse_b  [1.8032515]  mse_f: 0.786647379398346   total loss: [2.5898988]\n",
            "mse_b ====== [1.80332673]\n",
            "It: 2791, Time: 0.02\n",
            "mse_b  [1.8033267]  mse_f: 0.9595068693161011   total loss: [2.7628336]\n",
            "mse_b ====== [1.80065453]\n",
            "It: 2792, Time: 0.02\n",
            "mse_b  [1.8006545]  mse_f: 1.0310702323913574   total loss: [2.8317246]\n",
            "mse_b ====== [1.79946244]\n",
            "It: 2793, Time: 0.02\n",
            "mse_b  [1.7994624]  mse_f: 0.872342586517334   total loss: [2.671805]\n",
            "mse_b ====== [1.81096685]\n",
            "It: 2794, Time: 0.02\n",
            "mse_b  [1.8109668]  mse_f: 0.6032437086105347   total loss: [2.4142106]\n",
            "mse_b ====== [1.83863866]\n",
            "It: 2795, Time: 0.03\n",
            "mse_b  [1.8386387]  mse_f: 0.6733390092849731   total loss: [2.5119777]\n",
            "mse_b ====== [1.86485946]\n",
            "It: 2796, Time: 0.03\n",
            "mse_b  [1.8648595]  mse_f: 0.8111321926116943   total loss: [2.6759915]\n",
            "mse_b ====== [1.87458205]\n",
            "It: 2797, Time: 0.02\n",
            "mse_b  [1.874582]  mse_f: 0.7692406177520752   total loss: [2.6438227]\n",
            "mse_b ====== [1.87218094]\n",
            "It: 2798, Time: 0.03\n",
            "mse_b  [1.8721809]  mse_f: 0.548703670501709   total loss: [2.4208846]\n",
            "mse_b ====== [1.87078786]\n",
            "It: 2799, Time: 0.02\n",
            "mse_b  [1.8707879]  mse_f: 0.4770490527153015   total loss: [2.347837]\n",
            "mse_b ====== [1.8781]\n",
            "It: 2800, Time: 0.02\n",
            "mse_b  [1.8781]  mse_f: 0.6225996613502502   total loss: [2.5006998]\n",
            "mse_b ====== [1.89085841]\n",
            "It: 2801, Time: 0.02\n",
            "mse_b  [1.8908584]  mse_f: 0.6768561005592346   total loss: [2.5677145]\n",
            "mse_b ====== [1.90170479]\n",
            "It: 2802, Time: 0.02\n",
            "mse_b  [1.9017048]  mse_f: 0.6344877481460571   total loss: [2.5361924]\n",
            "mse_b ====== [1.90942383]\n",
            "It: 2803, Time: 0.03\n",
            "mse_b  [1.9094238]  mse_f: 0.5657851696014404   total loss: [2.475209]\n",
            "mse_b ====== [1.91675246]\n",
            "It: 2804, Time: 0.02\n",
            "mse_b  [1.9167525]  mse_f: 0.6620253324508667   total loss: [2.5787778]\n",
            "mse_b ====== [1.9216584]\n",
            "It: 2805, Time: 0.02\n",
            "mse_b  [1.9216584]  mse_f: 0.6652929186820984   total loss: [2.5869513]\n",
            "mse_b ====== [1.92226863]\n",
            "It: 2806, Time: 0.02\n",
            "mse_b  [1.9222686]  mse_f: 0.5627899169921875   total loss: [2.4850585]\n",
            "mse_b ====== [1.91953194]\n",
            "It: 2807, Time: 0.02\n",
            "mse_b  [1.919532]  mse_f: 0.476039320230484   total loss: [2.3955712]\n",
            "mse_b ====== [1.91458607]\n",
            "It: 2808, Time: 0.02\n",
            "mse_b  [1.9145861]  mse_f: 0.5481187105178833   total loss: [2.4627047]\n",
            "mse_b ====== [1.90799332]\n",
            "It: 2809, Time: 0.02\n",
            "mse_b  [1.9079933]  mse_f: 0.7009308934211731   total loss: [2.6089242]\n",
            "mse_b ====== [1.89904785]\n",
            "It: 2810, Time: 0.02\n",
            "mse_b  [1.8990479]  mse_f: 0.6349472999572754   total loss: [2.5339952]\n",
            "mse_b ====== [1.88729]\n",
            "It: 2811, Time: 0.02\n",
            "mse_b  [1.88729]  mse_f: 0.4957895278930664   total loss: [2.3830795]\n",
            "mse_b ====== [1.87546384]\n",
            "It: 2812, Time: 0.02\n",
            "mse_b  [1.8754638]  mse_f: 0.4433358311653137   total loss: [2.3187997]\n",
            "mse_b ====== [1.86771142]\n",
            "It: 2813, Time: 0.02\n",
            "mse_b  [1.8677114]  mse_f: 0.61219322681427   total loss: [2.4799047]\n",
            "mse_b ====== [1.86343336]\n",
            "It: 2814, Time: 0.02\n",
            "mse_b  [1.8634334]  mse_f: 0.7260442972183228   total loss: [2.5894775]\n",
            "mse_b ====== [1.86169481]\n",
            "It: 2815, Time: 0.02\n",
            "mse_b  [1.8616948]  mse_f: 0.6320854425430298   total loss: [2.4937801]\n",
            "mse_b ====== [1.86455202]\n",
            "It: 2816, Time: 0.03\n",
            "mse_b  [1.864552]  mse_f: 0.5127788186073303   total loss: [2.3773308]\n",
            "mse_b ====== [1.87178898]\n",
            "It: 2817, Time: 0.02\n",
            "mse_b  [1.871789]  mse_f: 0.5221888422966003   total loss: [2.3939779]\n",
            "mse_b ====== [1.88019931]\n",
            "It: 2818, Time: 0.02\n",
            "mse_b  [1.8801993]  mse_f: 0.6809927225112915   total loss: [2.561192]\n",
            "mse_b ====== [1.88581514]\n",
            "It: 2819, Time: 0.03\n",
            "mse_b  [1.8858151]  mse_f: 0.6869441270828247   total loss: [2.5727592]\n",
            "mse_b ====== [1.8871243]\n",
            "It: 2820, Time: 0.02\n",
            "mse_b  [1.8871243]  mse_f: 0.6288022994995117   total loss: [2.5159266]\n",
            "mse_b ====== [1.88593686]\n",
            "It: 2821, Time: 0.02\n",
            "mse_b  [1.8859369]  mse_f: 0.6041524410247803   total loss: [2.4900894]\n",
            "mse_b ====== [1.88315618]\n",
            "It: 2822, Time: 0.02\n",
            "mse_b  [1.8831562]  mse_f: 0.674908459186554   total loss: [2.5580647]\n",
            "mse_b ====== [1.87521601]\n",
            "It: 2823, Time: 0.02\n",
            "mse_b  [1.875216]  mse_f: 0.6749756336212158   total loss: [2.5501916]\n",
            "mse_b ====== [1.86351597]\n",
            "It: 2824, Time: 0.02\n",
            "mse_b  [1.863516]  mse_f: 0.5616728067398071   total loss: [2.4251888]\n",
            "mse_b ====== [1.85464025]\n",
            "It: 2825, Time: 0.03\n",
            "mse_b  [1.8546402]  mse_f: 0.5395269393920898   total loss: [2.3941672]\n",
            "mse_b ====== [1.84768057]\n",
            "It: 2826, Time: 0.02\n",
            "mse_b  [1.8476806]  mse_f: 0.6078150272369385   total loss: [2.4554956]\n",
            "mse_b ====== [1.83757663]\n",
            "It: 2827, Time: 0.03\n",
            "mse_b  [1.8375766]  mse_f: 0.6793582439422607   total loss: [2.5169349]\n",
            "mse_b ====== [1.82589114]\n",
            "It: 2828, Time: 0.02\n",
            "mse_b  [1.8258911]  mse_f: 0.5545082092285156   total loss: [2.3803992]\n",
            "mse_b ====== [1.81952]\n",
            "It: 2829, Time: 0.02\n",
            "mse_b  [1.81952]  mse_f: 0.4554426074028015   total loss: [2.2749627]\n",
            "mse_b ====== [1.81993246]\n",
            "It: 2830, Time: 0.02\n",
            "mse_b  [1.8199325]  mse_f: 0.49348509311676025   total loss: [2.3134174]\n",
            "mse_b ====== [1.82042217]\n",
            "It: 2831, Time: 0.03\n",
            "mse_b  [1.8204222]  mse_f: 0.6040999889373779   total loss: [2.4245222]\n",
            "mse_b ====== [1.81277156]\n",
            "It: 2832, Time: 0.03\n",
            "mse_b  [1.8127716]  mse_f: 0.5913178324699402   total loss: [2.4040895]\n",
            "mse_b ====== [1.79735243]\n",
            "It: 2833, Time: 0.03\n",
            "mse_b  [1.7973524]  mse_f: 0.436195433139801   total loss: [2.233548]\n",
            "mse_b ====== [1.78182888]\n",
            "It: 2834, Time: 0.02\n",
            "mse_b  [1.7818289]  mse_f: 0.3965526819229126   total loss: [2.1783814]\n",
            "mse_b ====== [1.76954925]\n",
            "It: 2835, Time: 0.03\n",
            "mse_b  [1.7695493]  mse_f: 0.4708734452724457   total loss: [2.2404227]\n",
            "mse_b ====== [1.75827432]\n",
            "It: 2836, Time: 0.02\n",
            "mse_b  [1.7582743]  mse_f: 0.5643260478973389   total loss: [2.3226004]\n",
            "mse_b ====== [1.74601853]\n",
            "It: 2837, Time: 0.03\n",
            "mse_b  [1.7460185]  mse_f: 0.48819172382354736   total loss: [2.2342103]\n",
            "mse_b ====== [1.73536134]\n",
            "It: 2838, Time: 0.03\n",
            "mse_b  [1.7353613]  mse_f: 0.4220670461654663   total loss: [2.1574283]\n",
            "mse_b ====== [1.7290082]\n",
            "It: 2839, Time: 0.02\n",
            "mse_b  [1.7290082]  mse_f: 0.4678792953491211   total loss: [2.1968875]\n",
            "mse_b ====== [1.72437203]\n",
            "It: 2840, Time: 0.03\n",
            "mse_b  [1.724372]  mse_f: 0.5362704992294312   total loss: [2.2606425]\n",
            "mse_b ====== [1.71541739]\n",
            "It: 2841, Time: 0.02\n",
            "mse_b  [1.7154174]  mse_f: 0.5323734283447266   total loss: [2.2477908]\n",
            "mse_b ====== [1.70078468]\n",
            "It: 2842, Time: 0.03\n",
            "mse_b  [1.7007847]  mse_f: 0.4778847396373749   total loss: [2.1786695]\n",
            "mse_b ====== [1.68563986]\n",
            "It: 2843, Time: 0.03\n",
            "mse_b  [1.6856399]  mse_f: 0.575323760509491   total loss: [2.2609637]\n",
            "mse_b ====== [1.67502582]\n",
            "It: 2844, Time: 0.02\n",
            "mse_b  [1.6750258]  mse_f: 0.6785649061203003   total loss: [2.3535907]\n",
            "mse_b ====== [1.67204]\n",
            "It: 2845, Time: 0.02\n",
            "mse_b  [1.67204]  mse_f: 0.68595951795578   total loss: [2.3579996]\n",
            "mse_b ====== [1.67614555]\n",
            "It: 2846, Time: 0.02\n",
            "mse_b  [1.6761456]  mse_f: 0.5766653418540955   total loss: [2.252811]\n",
            "mse_b ====== [1.68416858]\n",
            "It: 2847, Time: 0.02\n",
            "mse_b  [1.6841686]  mse_f: 0.5979640483856201   total loss: [2.2821326]\n",
            "mse_b ====== [1.69199395]\n",
            "It: 2848, Time: 0.02\n",
            "mse_b  [1.691994]  mse_f: 0.7171760201454163   total loss: [2.40917]\n",
            "mse_b ====== [1.69537902]\n",
            "It: 2849, Time: 0.02\n",
            "mse_b  [1.695379]  mse_f: 0.7217386364936829   total loss: [2.4171176]\n",
            "mse_b ====== [1.69348502]\n",
            "It: 2850, Time: 0.02\n",
            "mse_b  [1.693485]  mse_f: 0.5886344909667969   total loss: [2.2821195]\n",
            "mse_b ====== [1.69135273]\n",
            "It: 2851, Time: 0.02\n",
            "mse_b  [1.6913527]  mse_f: 0.48399582505226135   total loss: [2.1753485]\n",
            "mse_b ====== [1.69191575]\n",
            "It: 2852, Time: 0.03\n",
            "mse_b  [1.6919158]  mse_f: 0.5574222207069397   total loss: [2.249338]\n",
            "mse_b ====== [1.69138765]\n",
            "It: 2853, Time: 0.02\n",
            "mse_b  [1.6913877]  mse_f: 0.5899295806884766   total loss: [2.2813172]\n",
            "mse_b ====== [1.68881631]\n",
            "It: 2854, Time: 0.02\n",
            "mse_b  [1.6888163]  mse_f: 0.5416197776794434   total loss: [2.230436]\n",
            "mse_b ====== [1.68604863]\n",
            "It: 2855, Time: 0.02\n",
            "mse_b  [1.6860486]  mse_f: 0.45813798904418945   total loss: [2.1441865]\n",
            "mse_b ====== [1.68238425]\n",
            "It: 2856, Time: 0.02\n",
            "mse_b  [1.6823843]  mse_f: 0.5017056465148926   total loss: [2.18409]\n",
            "mse_b ====== [1.67662024]\n",
            "It: 2857, Time: 0.02\n",
            "mse_b  [1.6766202]  mse_f: 0.5378692150115967   total loss: [2.2144895]\n",
            "mse_b ====== [1.67191672]\n",
            "It: 2858, Time: 0.03\n",
            "mse_b  [1.6719167]  mse_f: 0.49712812900543213   total loss: [2.169045]\n",
            "mse_b ====== [1.67343426]\n",
            "It: 2859, Time: 0.02\n",
            "mse_b  [1.6734343]  mse_f: 0.4503844082355499   total loss: [2.1238186]\n",
            "mse_b ====== [1.68160701]\n",
            "It: 2860, Time: 0.02\n",
            "mse_b  [1.681607]  mse_f: 0.47784143686294556   total loss: [2.1594484]\n",
            "mse_b ====== [1.68804455]\n",
            "It: 2861, Time: 0.02\n",
            "mse_b  [1.6880445]  mse_f: 0.5435468554496765   total loss: [2.2315915]\n",
            "mse_b ====== [1.68501544]\n",
            "It: 2862, Time: 0.02\n",
            "mse_b  [1.6850154]  mse_f: 0.47313055396080017   total loss: [2.158146]\n",
            "mse_b ====== [1.67778385]\n",
            "It: 2863, Time: 0.02\n",
            "mse_b  [1.6777838]  mse_f: 0.39133283495903015   total loss: [2.0691166]\n",
            "mse_b ====== [1.67529702]\n",
            "It: 2864, Time: 0.02\n",
            "mse_b  [1.675297]  mse_f: 0.3905196189880371   total loss: [2.0658166]\n",
            "mse_b ====== [1.67646194]\n",
            "It: 2865, Time: 0.02\n",
            "mse_b  [1.6764619]  mse_f: 0.5057294368743896   total loss: [2.1821914]\n",
            "mse_b ====== [1.6728754]\n",
            "It: 2866, Time: 0.02\n",
            "mse_b  [1.6728754]  mse_f: 0.5328869223594666   total loss: [2.2057624]\n",
            "mse_b ====== [1.66164923]\n",
            "It: 2867, Time: 0.02\n",
            "mse_b  [1.6616492]  mse_f: 0.44780445098876953   total loss: [2.1094537]\n",
            "mse_b ====== [1.64813113]\n",
            "It: 2868, Time: 0.02\n",
            "mse_b  [1.6481311]  mse_f: 0.3925711512565613   total loss: [2.0407023]\n",
            "mse_b ====== [1.63803339]\n",
            "It: 2869, Time: 0.02\n",
            "mse_b  [1.6380334]  mse_f: 0.43805190920829773   total loss: [2.0760853]\n",
            "mse_b ====== [1.6317929]\n",
            "It: 2870, Time: 0.02\n",
            "mse_b  [1.6317929]  mse_f: 0.5273181796073914   total loss: [2.159111]\n",
            "mse_b ====== [1.6285131]\n",
            "It: 2871, Time: 0.02\n",
            "mse_b  [1.6285131]  mse_f: 0.5048633813858032   total loss: [2.1333766]\n",
            "mse_b ====== [1.63023806]\n",
            "It: 2872, Time: 0.02\n",
            "mse_b  [1.630238]  mse_f: 0.4828236699104309   total loss: [2.1130617]\n",
            "mse_b ====== [1.63738608]\n",
            "It: 2873, Time: 0.03\n",
            "mse_b  [1.6373861]  mse_f: 0.4874081611633301   total loss: [2.1247942]\n",
            "mse_b ====== [1.64761066]\n",
            "It: 2874, Time: 0.02\n",
            "mse_b  [1.6476107]  mse_f: 0.520987868309021   total loss: [2.1685987]\n",
            "mse_b ====== [1.65645051]\n",
            "It: 2875, Time: 0.02\n",
            "mse_b  [1.6564505]  mse_f: 0.47205668687820435   total loss: [2.1285071]\n",
            "mse_b ====== [1.65940571]\n",
            "It: 2876, Time: 0.02\n",
            "mse_b  [1.6594057]  mse_f: 0.4197084307670593   total loss: [2.0791142]\n",
            "mse_b ====== [1.65582883]\n",
            "It: 2877, Time: 0.02\n",
            "mse_b  [1.6558288]  mse_f: 0.45473945140838623   total loss: [2.1105683]\n",
            "mse_b ====== [1.64849448]\n",
            "It: 2878, Time: 0.02\n",
            "mse_b  [1.6484945]  mse_f: 0.5178805589675903   total loss: [2.1663752]\n",
            "mse_b ====== [1.63925505]\n",
            "It: 2879, Time: 0.02\n",
            "mse_b  [1.639255]  mse_f: 0.5185476541519165   total loss: [2.1578026]\n",
            "mse_b ====== [1.6303091]\n",
            "It: 2880, Time: 0.02\n",
            "mse_b  [1.6303091]  mse_f: 0.41826167702674866   total loss: [2.0485709]\n",
            "mse_b ====== [1.62557149]\n",
            "It: 2881, Time: 0.02\n",
            "mse_b  [1.6255715]  mse_f: 0.4038977324962616   total loss: [2.0294693]\n",
            "mse_b ====== [1.62541366]\n",
            "It: 2882, Time: 0.02\n",
            "mse_b  [1.6254137]  mse_f: 0.46989792585372925   total loss: [2.0953116]\n",
            "mse_b ====== [1.62771797]\n",
            "It: 2883, Time: 0.03\n",
            "mse_b  [1.627718]  mse_f: 0.5420863032341003   total loss: [2.1698043]\n",
            "mse_b ====== [1.63024843]\n",
            "It: 2884, Time: 0.02\n",
            "mse_b  [1.6302484]  mse_f: 0.488772451877594   total loss: [2.119021]\n",
            "mse_b ====== [1.63007951]\n",
            "It: 2885, Time: 0.02\n",
            "mse_b  [1.6300795]  mse_f: 0.40251702070236206   total loss: [2.0325966]\n",
            "mse_b ====== [1.62450552]\n",
            "It: 2886, Time: 0.02\n",
            "mse_b  [1.6245055]  mse_f: 0.3969167470932007   total loss: [2.0214224]\n",
            "mse_b ====== [1.61332822]\n",
            "It: 2887, Time: 0.03\n",
            "mse_b  [1.6133282]  mse_f: 0.4350590705871582   total loss: [2.0483873]\n",
            "mse_b ====== [1.59848571]\n",
            "It: 2888, Time: 0.02\n",
            "mse_b  [1.5984857]  mse_f: 0.4545864164829254   total loss: [2.0530722]\n",
            "mse_b ====== [1.58379912]\n",
            "It: 2889, Time: 0.02\n",
            "mse_b  [1.5837991]  mse_f: 0.41352641582489014   total loss: [1.9973255]\n",
            "mse_b ====== [1.57292783]\n",
            "It: 2890, Time: 0.02\n",
            "mse_b  [1.5729278]  mse_f: 0.43031907081604004   total loss: [2.0032468]\n",
            "mse_b ====== [1.56378865]\n",
            "It: 2891, Time: 0.02\n",
            "mse_b  [1.5637887]  mse_f: 0.4554925858974457   total loss: [2.0192811]\n",
            "mse_b ====== [1.5525198]\n",
            "It: 2892, Time: 0.02\n",
            "mse_b  [1.5525198]  mse_f: 0.46055829524993896   total loss: [2.0130782]\n",
            "mse_b ====== [1.54004276]\n",
            "It: 2893, Time: 0.02\n",
            "mse_b  [1.5400428]  mse_f: 0.41909825801849365   total loss: [1.959141]\n",
            "mse_b ====== [1.53065681]\n",
            "It: 2894, Time: 0.02\n",
            "mse_b  [1.5306568]  mse_f: 0.43722331523895264   total loss: [1.9678801]\n",
            "mse_b ====== [1.52592421]\n",
            "It: 2895, Time: 0.03\n",
            "mse_b  [1.5259242]  mse_f: 0.5167474150657654   total loss: [2.0426717]\n",
            "mse_b ====== [1.52460146]\n",
            "It: 2896, Time: 0.02\n",
            "mse_b  [1.5246015]  mse_f: 0.5333794355392456   total loss: [2.057981]\n",
            "mse_b ====== [1.52686143]\n",
            "It: 2897, Time: 0.02\n",
            "mse_b  [1.5268614]  mse_f: 0.46859192848205566   total loss: [1.9954534]\n",
            "mse_b ====== [1.53411329]\n",
            "It: 2898, Time: 0.02\n",
            "mse_b  [1.5341133]  mse_f: 0.405930757522583   total loss: [1.940044]\n",
            "mse_b ====== [1.54341865]\n",
            "It: 2899, Time: 0.02\n",
            "mse_b  [1.5434186]  mse_f: 0.4622635841369629   total loss: [2.0056822]\n",
            "mse_b ====== [1.5466553]\n",
            "It: 2900, Time: 0.02\n",
            "mse_b  [1.5466553]  mse_f: 0.5046741962432861   total loss: [2.0513296]\n",
            "mse_b ====== [1.53962278]\n",
            "It: 2901, Time: 0.02\n",
            "mse_b  [1.5396228]  mse_f: 0.48852789402008057   total loss: [2.0281506]\n",
            "mse_b ====== [1.52644694]\n",
            "It: 2902, Time: 0.03\n",
            "mse_b  [1.5264469]  mse_f: 0.42951542139053345   total loss: [1.9559624]\n",
            "mse_b ====== [1.51604056]\n",
            "It: 2903, Time: 0.03\n",
            "mse_b  [1.5160406]  mse_f: 0.4539254307746887   total loss: [1.9699659]\n",
            "mse_b ====== [1.51271439]\n",
            "It: 2904, Time: 0.02\n",
            "mse_b  [1.5127144]  mse_f: 0.48962825536727905   total loss: [2.0023427]\n",
            "mse_b ====== [1.51228678]\n",
            "It: 2905, Time: 0.02\n",
            "mse_b  [1.5122868]  mse_f: 0.4815155267715454   total loss: [1.9938023]\n",
            "mse_b ====== [1.50945091]\n",
            "It: 2906, Time: 0.03\n",
            "mse_b  [1.5094509]  mse_f: 0.45868122577667236   total loss: [1.9681321]\n",
            "mse_b ====== [1.50468051]\n",
            "It: 2907, Time: 0.02\n",
            "mse_b  [1.5046805]  mse_f: 0.4866328835487366   total loss: [1.9913135]\n",
            "mse_b ====== [1.5002718]\n",
            "It: 2908, Time: 0.03\n",
            "mse_b  [1.5002718]  mse_f: 0.5349569916725159   total loss: [2.0352287]\n",
            "mse_b ====== [1.49665773]\n",
            "It: 2909, Time: 0.02\n",
            "mse_b  [1.4966577]  mse_f: 0.5003791451454163   total loss: [1.9970369]\n",
            "mse_b ====== [1.49365425]\n",
            "It: 2910, Time: 0.02\n",
            "mse_b  [1.4936543]  mse_f: 0.453277587890625   total loss: [1.9469318]\n",
            "mse_b ====== [1.49027765]\n",
            "It: 2911, Time: 0.02\n",
            "mse_b  [1.4902776]  mse_f: 0.4476986527442932   total loss: [1.9379764]\n",
            "mse_b ====== [1.48617887]\n",
            "It: 2912, Time: 0.02\n",
            "mse_b  [1.4861789]  mse_f: 0.49973657727241516   total loss: [1.9859154]\n",
            "mse_b ====== [1.48139691]\n",
            "It: 2913, Time: 0.02\n",
            "mse_b  [1.4813969]  mse_f: 0.4715615510940552   total loss: [1.9529585]\n",
            "mse_b ====== [1.47594404]\n",
            "It: 2914, Time: 0.02\n",
            "mse_b  [1.475944]  mse_f: 0.4018188416957855   total loss: [1.8777629]\n",
            "mse_b ====== [1.47106671]\n",
            "It: 2915, Time: 0.03\n",
            "mse_b  [1.4710667]  mse_f: 0.37166765332221985   total loss: [1.8427343]\n",
            "mse_b ====== [1.46866941]\n",
            "It: 2916, Time: 0.02\n",
            "mse_b  [1.4686694]  mse_f: 0.41782668232917786   total loss: [1.8864961]\n",
            "mse_b ====== [1.46723986]\n",
            "It: 2917, Time: 0.02\n",
            "mse_b  [1.4672399]  mse_f: 0.4426835775375366   total loss: [1.9099234]\n",
            "mse_b ====== [1.46411538]\n",
            "It: 2918, Time: 0.02\n",
            "mse_b  [1.4641154]  mse_f: 0.39772361516952515   total loss: [1.861839]\n",
            "mse_b ====== [1.45985901]\n",
            "It: 2919, Time: 0.02\n",
            "mse_b  [1.459859]  mse_f: 0.3735491633415222   total loss: [1.8334081]\n",
            "mse_b ====== [1.45659387]\n",
            "It: 2920, Time: 0.02\n",
            "mse_b  [1.4565939]  mse_f: 0.3907013535499573   total loss: [1.8472953]\n",
            "mse_b ====== [1.4563514]\n",
            "It: 2921, Time: 0.02\n",
            "mse_b  [1.4563514]  mse_f: 0.41861283779144287   total loss: [1.8749642]\n",
            "mse_b ====== [1.4606775]\n",
            "It: 2922, Time: 0.02\n",
            "mse_b  [1.4606775]  mse_f: 0.3735678195953369   total loss: [1.8342453]\n",
            "mse_b ====== [1.46915603]\n",
            "It: 2923, Time: 0.02\n",
            "mse_b  [1.469156]  mse_f: 0.34556353092193604   total loss: [1.8147196]\n",
            "mse_b ====== [1.47712731]\n",
            "It: 2924, Time: 0.02\n",
            "mse_b  [1.4771273]  mse_f: 0.3588833212852478   total loss: [1.8360107]\n",
            "mse_b ====== [1.47848666]\n",
            "It: 2925, Time: 0.02\n",
            "mse_b  [1.4784867]  mse_f: 0.3930503726005554   total loss: [1.871537]\n",
            "mse_b ====== [1.47186267]\n",
            "It: 2926, Time: 0.03\n",
            "mse_b  [1.4718627]  mse_f: 0.3834550678730011   total loss: [1.8553177]\n",
            "mse_b ====== [1.46350491]\n",
            "It: 2927, Time: 0.03\n",
            "mse_b  [1.4635049]  mse_f: 0.36879485845565796   total loss: [1.8322997]\n",
            "mse_b ====== [1.46032596]\n",
            "It: 2928, Time: 0.02\n",
            "mse_b  [1.460326]  mse_f: 0.4120965003967285   total loss: [1.8724225]\n",
            "mse_b ====== [1.46223927]\n",
            "It: 2929, Time: 0.02\n",
            "mse_b  [1.4622393]  mse_f: 0.4520941376686096   total loss: [1.9143333]\n",
            "mse_b ====== [1.46442354]\n",
            "It: 2930, Time: 0.02\n",
            "mse_b  [1.4644235]  mse_f: 0.4429576098918915   total loss: [1.9073812]\n",
            "mse_b ====== [1.46375418]\n",
            "It: 2931, Time: 0.02\n",
            "mse_b  [1.4637542]  mse_f: 0.3848416805267334   total loss: [1.8485959]\n",
            "mse_b ====== [1.46108294]\n",
            "It: 2932, Time: 0.02\n",
            "mse_b  [1.4610829]  mse_f: 0.39211249351501465   total loss: [1.8531954]\n",
            "mse_b ====== [1.45774579]\n",
            "It: 2933, Time: 0.02\n",
            "mse_b  [1.4577458]  mse_f: 0.43107300996780396   total loss: [1.8888187]\n",
            "mse_b ====== [1.45357013]\n",
            "It: 2934, Time: 0.03\n",
            "mse_b  [1.4535701]  mse_f: 0.4363155961036682   total loss: [1.8898857]\n",
            "mse_b ====== [1.44881177]\n",
            "It: 2935, Time: 0.02\n",
            "mse_b  [1.4488118]  mse_f: 0.3846299946308136   total loss: [1.8334417]\n",
            "mse_b ====== [1.44500375]\n",
            "It: 2936, Time: 0.02\n",
            "mse_b  [1.4450037]  mse_f: 0.3508411943912506   total loss: [1.7958449]\n",
            "mse_b ====== [1.44120121]\n",
            "It: 2937, Time: 0.02\n",
            "mse_b  [1.4412012]  mse_f: 0.37663519382476807   total loss: [1.8178364]\n",
            "mse_b ====== [1.43424869]\n",
            "It: 2938, Time: 0.02\n",
            "mse_b  [1.4342487]  mse_f: 0.386288583278656   total loss: [1.8205373]\n",
            "mse_b ====== [1.42448092]\n",
            "It: 2939, Time: 0.03\n",
            "mse_b  [1.4244809]  mse_f: 0.37150540947914124   total loss: [1.7959863]\n",
            "mse_b ====== [1.41555965]\n",
            "It: 2940, Time: 0.02\n",
            "mse_b  [1.4155596]  mse_f: 0.3520738482475281   total loss: [1.7676334]\n",
            "mse_b ====== [1.40999901]\n",
            "It: 2941, Time: 0.02\n",
            "mse_b  [1.409999]  mse_f: 0.3809729814529419   total loss: [1.790972]\n",
            "mse_b ====== [1.40695429]\n",
            "It: 2942, Time: 0.02\n",
            "mse_b  [1.4069543]  mse_f: 0.3808520436286926   total loss: [1.7878063]\n",
            "mse_b ====== [1.40451074]\n",
            "It: 2943, Time: 0.02\n",
            "mse_b  [1.4045107]  mse_f: 0.346676766872406   total loss: [1.7511876]\n",
            "mse_b ====== [1.40241683]\n",
            "It: 2944, Time: 0.02\n",
            "mse_b  [1.4024168]  mse_f: 0.3224002718925476   total loss: [1.724817]\n",
            "mse_b ====== [1.40172791]\n",
            "It: 2945, Time: 0.02\n",
            "mse_b  [1.4017279]  mse_f: 0.3490995764732361   total loss: [1.7508276]\n",
            "mse_b ====== [1.40201592]\n",
            "It: 2946, Time: 0.02\n",
            "mse_b  [1.4020159]  mse_f: 0.3661165237426758   total loss: [1.7681324]\n",
            "mse_b ====== [1.40288889]\n",
            "It: 2947, Time: 0.02\n",
            "mse_b  [1.4028889]  mse_f: 0.3142733871936798   total loss: [1.7171623]\n",
            "mse_b ====== [1.40579486]\n",
            "It: 2948, Time: 0.02\n",
            "mse_b  [1.4057949]  mse_f: 0.26876533031463623   total loss: [1.6745602]\n",
            "mse_b ====== [1.41031075]\n",
            "It: 2949, Time: 0.02\n",
            "mse_b  [1.4103107]  mse_f: 0.27851560711860657   total loss: [1.6888263]\n",
            "mse_b ====== [1.41303754]\n",
            "It: 2950, Time: 0.02\n",
            "mse_b  [1.4130375]  mse_f: 0.3325655460357666   total loss: [1.7456031]\n",
            "mse_b ====== [1.41168737]\n",
            "It: 2951, Time: 0.02\n",
            "mse_b  [1.4116874]  mse_f: 0.33134424686431885   total loss: [1.7430316]\n",
            "mse_b ====== [1.40813112]\n",
            "It: 2952, Time: 0.03\n",
            "mse_b  [1.4081311]  mse_f: 0.3123824894428253   total loss: [1.7205136]\n",
            "mse_b ====== [1.4049834]\n",
            "It: 2953, Time: 0.03\n",
            "mse_b  [1.4049834]  mse_f: 0.31052935123443604   total loss: [1.7155128]\n",
            "mse_b ====== [1.40144372]\n",
            "It: 2954, Time: 0.02\n",
            "mse_b  [1.4014437]  mse_f: 0.3354249596595764   total loss: [1.7368686]\n",
            "mse_b ====== [1.3950932]\n",
            "It: 2955, Time: 0.02\n",
            "mse_b  [1.3950932]  mse_f: 0.32943278551101685   total loss: [1.7245259]\n",
            "mse_b ====== [1.38725853]\n",
            "It: 2956, Time: 0.03\n",
            "mse_b  [1.3872585]  mse_f: 0.31148213148117065   total loss: [1.6987407]\n",
            "mse_b ====== [1.38087022]\n",
            "It: 2957, Time: 0.02\n",
            "mse_b  [1.3808702]  mse_f: 0.3282797038555145   total loss: [1.70915]\n",
            "mse_b ====== [1.37534392]\n",
            "It: 2958, Time: 0.02\n",
            "mse_b  [1.3753439]  mse_f: 0.3491800129413605   total loss: [1.7245239]\n",
            "mse_b ====== [1.36860192]\n",
            "It: 2959, Time: 0.02\n",
            "mse_b  [1.3686019]  mse_f: 0.3403145670890808   total loss: [1.7089164]\n",
            "mse_b ====== [1.36153793]\n",
            "It: 2960, Time: 0.02\n",
            "mse_b  [1.3615379]  mse_f: 0.3088873028755188   total loss: [1.6704252]\n",
            "mse_b ====== [1.35727465]\n",
            "It: 2961, Time: 0.02\n",
            "mse_b  [1.3572747]  mse_f: 0.3400554955005646   total loss: [1.6973301]\n",
            "mse_b ====== [1.35568738]\n",
            "It: 2962, Time: 0.02\n",
            "mse_b  [1.3556874]  mse_f: 0.3860068917274475   total loss: [1.7416942]\n",
            "mse_b ====== [1.35271072]\n",
            "It: 2963, Time: 0.02\n",
            "mse_b  [1.3527107]  mse_f: 0.3929280936717987   total loss: [1.7456388]\n",
            "mse_b ====== [1.34658873]\n",
            "It: 2964, Time: 0.02\n",
            "mse_b  [1.3465887]  mse_f: 0.3430675268173218   total loss: [1.6896563]\n",
            "mse_b ====== [1.34078288]\n",
            "It: 2965, Time: 0.02\n",
            "mse_b  [1.3407829]  mse_f: 0.32872503995895386   total loss: [1.669508]\n",
            "mse_b ====== [1.33762467]\n",
            "It: 2966, Time: 0.02\n",
            "mse_b  [1.3376247]  mse_f: 0.34673452377319336   total loss: [1.6843592]\n",
            "mse_b ====== [1.3358885]\n",
            "It: 2967, Time: 0.03\n",
            "mse_b  [1.3358885]  mse_f: 0.3475542664527893   total loss: [1.6834428]\n",
            "mse_b ====== [1.33473814]\n",
            "It: 2968, Time: 0.02\n",
            "mse_b  [1.3347381]  mse_f: 0.31679877638816833   total loss: [1.651537]\n",
            "mse_b ====== [1.33500338]\n",
            "It: 2969, Time: 0.02\n",
            "mse_b  [1.3350034]  mse_f: 0.29799121618270874   total loss: [1.6329947]\n",
            "mse_b ====== [1.33651268]\n",
            "It: 2970, Time: 0.02\n",
            "mse_b  [1.3365127]  mse_f: 0.30787304043769836   total loss: [1.6443857]\n",
            "mse_b ====== [1.33705044]\n",
            "It: 2971, Time: 0.02\n",
            "mse_b  [1.3370504]  mse_f: 0.29162296652793884   total loss: [1.6286734]\n",
            "mse_b ====== [1.33539438]\n",
            "It: 2972, Time: 0.02\n",
            "mse_b  [1.3353944]  mse_f: 0.2704642117023468   total loss: [1.6058586]\n",
            "mse_b ====== [1.33306515]\n",
            "It: 2973, Time: 0.02\n",
            "mse_b  [1.3330652]  mse_f: 0.27053409814834595   total loss: [1.6035993]\n",
            "mse_b ====== [1.33152843]\n",
            "It: 2974, Time: 0.02\n",
            "mse_b  [1.3315284]  mse_f: 0.3073144853115082   total loss: [1.6388429]\n",
            "mse_b ====== [1.32944202]\n",
            "It: 2975, Time: 0.03\n",
            "mse_b  [1.329442]  mse_f: 0.30346956849098206   total loss: [1.6329116]\n",
            "mse_b ====== [1.32499039]\n",
            "It: 2976, Time: 0.03\n",
            "mse_b  [1.3249904]  mse_f: 0.2727909982204437   total loss: [1.5977814]\n",
            "mse_b ====== [1.31812191]\n",
            "It: 2977, Time: 0.02\n",
            "mse_b  [1.3181219]  mse_f: 0.2635098099708557   total loss: [1.5816317]\n",
            "mse_b ====== [1.31057048]\n",
            "It: 2978, Time: 0.02\n",
            "mse_b  [1.3105705]  mse_f: 0.30011695623397827   total loss: [1.6106875]\n",
            "mse_b ====== [1.30428171]\n",
            "It: 2979, Time: 0.02\n",
            "mse_b  [1.3042817]  mse_f: 0.3201864957809448   total loss: [1.6244682]\n",
            "mse_b ====== [1.29980028]\n",
            "It: 2980, Time: 0.02\n",
            "mse_b  [1.2998003]  mse_f: 0.2927091121673584   total loss: [1.5925094]\n",
            "mse_b ====== [1.29613221]\n",
            "It: 2981, Time: 0.02\n",
            "mse_b  [1.2961322]  mse_f: 0.2768690288066864   total loss: [1.5730013]\n",
            "mse_b ====== [1.2915014]\n",
            "It: 2982, Time: 0.02\n",
            "mse_b  [1.2915014]  mse_f: 0.29211121797561646   total loss: [1.5836127]\n",
            "mse_b ====== [1.28454304]\n",
            "It: 2983, Time: 0.02\n",
            "mse_b  [1.284543]  mse_f: 0.31668591499328613   total loss: [1.601229]\n",
            "mse_b ====== [1.276263]\n",
            "It: 2984, Time: 0.02\n",
            "mse_b  [1.276263]  mse_f: 0.30537670850753784   total loss: [1.5816398]\n",
            "mse_b ====== [1.26966178]\n",
            "It: 2985, Time: 0.02\n",
            "mse_b  [1.2696618]  mse_f: 0.30968108773231506   total loss: [1.5793428]\n",
            "mse_b ====== [1.2655915]\n",
            "It: 2986, Time: 0.03\n",
            "mse_b  [1.2655915]  mse_f: 0.3286248445510864   total loss: [1.5942163]\n",
            "mse_b ====== [1.26263356]\n",
            "It: 2987, Time: 0.03\n",
            "mse_b  [1.2626336]  mse_f: 0.33922243118286133   total loss: [1.601856]\n",
            "mse_b ====== [1.25979722]\n",
            "It: 2988, Time: 0.02\n",
            "mse_b  [1.2597972]  mse_f: 0.312420129776001   total loss: [1.5722173]\n",
            "mse_b ====== [1.25700605]\n",
            "It: 2989, Time: 0.02\n",
            "mse_b  [1.257006]  mse_f: 0.3021428883075714   total loss: [1.5591489]\n",
            "mse_b ====== [1.25412726]\n",
            "It: 2990, Time: 0.02\n",
            "mse_b  [1.2541273]  mse_f: 0.3241325616836548   total loss: [1.5782598]\n",
            "mse_b ====== [1.25153327]\n",
            "It: 2991, Time: 0.02\n",
            "mse_b  [1.2515333]  mse_f: 0.3310248553752899   total loss: [1.5825582]\n",
            "mse_b ====== [1.25024986]\n",
            "It: 2992, Time: 0.02\n",
            "mse_b  [1.2502499]  mse_f: 0.3061791658401489   total loss: [1.556429]\n",
            "mse_b ====== [1.25038552]\n",
            "It: 2993, Time: 0.02\n",
            "mse_b  [1.2503855]  mse_f: 0.2872498631477356   total loss: [1.5376353]\n",
            "mse_b ====== [1.24927437]\n",
            "It: 2994, Time: 0.02\n",
            "mse_b  [1.2492744]  mse_f: 0.3142954111099243   total loss: [1.5635698]\n",
            "mse_b ====== [1.24344575]\n",
            "It: 2995, Time: 0.02\n",
            "mse_b  [1.2434458]  mse_f: 0.3265702724456787   total loss: [1.570016]\n",
            "mse_b ====== [1.23427331]\n",
            "It: 2996, Time: 0.03\n",
            "mse_b  [1.2342733]  mse_f: 0.30959832668304443   total loss: [1.5438716]\n",
            "mse_b ====== [1.22745776]\n",
            "It: 2997, Time: 0.03\n",
            "mse_b  [1.2274578]  mse_f: 0.2854393720626831   total loss: [1.5128971]\n",
            "mse_b ====== [1.22596288]\n",
            "It: 2998, Time: 0.02\n",
            "mse_b  [1.2259629]  mse_f: 0.29802829027175903   total loss: [1.5239911]\n",
            "mse_b ====== [1.22628915]\n",
            "It: 2999, Time: 0.03\n",
            "mse_b  [1.2262892]  mse_f: 0.30110692977905273   total loss: [1.5273961]\n",
            "mse_b ====== [1.22353768]\n",
            "It: 3000, Time: 0.03\n",
            "mse_b  [1.2235377]  mse_f: 0.2848343849182129   total loss: [1.5083721]\n",
            "mse_b ====== [1.21706152]\n",
            "It: 3001, Time: 0.03\n",
            "mse_b  [1.2170615]  mse_f: 0.2778378129005432   total loss: [1.4948993]\n",
            "mse_b ====== [1.20965433]\n",
            "It: 3002, Time: 0.03\n",
            "mse_b  [1.2096543]  mse_f: 0.29823577404022217   total loss: [1.5078901]\n",
            "mse_b ====== [1.20353782]\n",
            "It: 3003, Time: 0.03\n",
            "mse_b  [1.2035378]  mse_f: 0.3077307641506195   total loss: [1.5112686]\n",
            "mse_b ====== [1.19939625]\n",
            "It: 3004, Time: 0.02\n",
            "mse_b  [1.1993963]  mse_f: 0.2881835103034973   total loss: [1.4875798]\n",
            "mse_b ====== [1.19656289]\n",
            "It: 3005, Time: 0.03\n",
            "mse_b  [1.1965629]  mse_f: 0.2900439202785492   total loss: [1.4866068]\n",
            "mse_b ====== [1.19295418]\n",
            "It: 3006, Time: 0.02\n",
            "mse_b  [1.1929542]  mse_f: 0.31160154938697815   total loss: [1.5045557]\n",
            "mse_b ====== [1.18714941]\n",
            "It: 3007, Time: 0.02\n",
            "mse_b  [1.1871494]  mse_f: 0.3190552592277527   total loss: [1.5062046]\n",
            "mse_b ====== [1.18005478]\n",
            "It: 3008, Time: 0.02\n",
            "mse_b  [1.1800548]  mse_f: 0.28953495621681213   total loss: [1.4695897]\n",
            "mse_b ====== [1.17440772]\n",
            "It: 3009, Time: 0.03\n",
            "mse_b  [1.1744077]  mse_f: 0.29002803564071655   total loss: [1.4644358]\n",
            "mse_b ====== [1.17169726]\n",
            "It: 3010, Time: 0.03\n",
            "mse_b  [1.1716973]  mse_f: 0.3180278539657593   total loss: [1.4897251]\n",
            "mse_b ====== [1.17016149]\n",
            "It: 3011, Time: 0.03\n",
            "mse_b  [1.1701615]  mse_f: 0.33762601017951965   total loss: [1.5077875]\n",
            "mse_b ====== [1.1675179]\n",
            "It: 3012, Time: 0.03\n",
            "mse_b  [1.1675179]  mse_f: 0.3197999894618988   total loss: [1.4873179]\n",
            "mse_b ====== [1.16470516]\n",
            "It: 3013, Time: 0.03\n",
            "mse_b  [1.1647052]  mse_f: 0.312563419342041   total loss: [1.4772686]\n",
            "mse_b ====== [1.163486]\n",
            "It: 3014, Time: 0.03\n",
            "mse_b  [1.163486]  mse_f: 0.32021966576576233   total loss: [1.4837056]\n",
            "mse_b ====== [1.16375375]\n",
            "It: 3015, Time: 0.03\n",
            "mse_b  [1.1637537]  mse_f: 0.31380534172058105   total loss: [1.4775591]\n",
            "mse_b ====== [1.1644156]\n",
            "It: 3016, Time: 0.02\n",
            "mse_b  [1.1644156]  mse_f: 0.2936297655105591   total loss: [1.4580454]\n",
            "mse_b ====== [1.16403282]\n",
            "It: 3017, Time: 0.03\n",
            "mse_b  [1.1640328]  mse_f: 0.2902333736419678   total loss: [1.4542662]\n",
            "mse_b ====== [1.16124141]\n",
            "It: 3018, Time: 0.03\n",
            "mse_b  [1.1612414]  mse_f: 0.3036847710609436   total loss: [1.4649262]\n",
            "mse_b ====== [1.15694988]\n",
            "It: 3019, Time: 0.03\n",
            "mse_b  [1.1569499]  mse_f: 0.2877139449119568   total loss: [1.4446638]\n",
            "mse_b ====== [1.1548413]\n",
            "It: 3020, Time: 0.03\n",
            "mse_b  [1.1548413]  mse_f: 0.2658396363258362   total loss: [1.420681]\n",
            "mse_b ====== [1.15700769]\n",
            "It: 3021, Time: 0.02\n",
            "mse_b  [1.1570077]  mse_f: 0.2670384645462036   total loss: [1.4240462]\n",
            "mse_b ====== [1.16034925]\n",
            "It: 3022, Time: 0.03\n",
            "mse_b  [1.1603492]  mse_f: 0.2904321849346161   total loss: [1.4507815]\n",
            "mse_b ====== [1.15927386]\n",
            "It: 3023, Time: 0.03\n",
            "mse_b  [1.1592739]  mse_f: 0.27948498725891113   total loss: [1.4387589]\n",
            "mse_b ====== [1.15221882]\n",
            "It: 3024, Time: 0.03\n",
            "mse_b  [1.1522188]  mse_f: 0.2598293125629425   total loss: [1.4120481]\n",
            "mse_b ====== [1.14285529]\n",
            "It: 3025, Time: 0.02\n",
            "mse_b  [1.1428553]  mse_f: 0.2612292766571045   total loss: [1.4040846]\n",
            "mse_b ====== [1.13548374]\n",
            "It: 3026, Time: 0.02\n",
            "mse_b  [1.1354837]  mse_f: 0.2812637686729431   total loss: [1.4167476]\n",
            "mse_b ====== [1.13091063]\n",
            "It: 3027, Time: 0.02\n",
            "mse_b  [1.1309106]  mse_f: 0.2757982313632965   total loss: [1.4067088]\n",
            "mse_b ====== [1.1277833]\n",
            "It: 3028, Time: 0.02\n",
            "mse_b  [1.1277833]  mse_f: 0.2632366418838501   total loss: [1.3910199]\n",
            "mse_b ====== [1.12545109]\n",
            "It: 3029, Time: 0.03\n",
            "mse_b  [1.1254511]  mse_f: 0.26863721013069153   total loss: [1.3940883]\n",
            "mse_b ====== [1.12401271]\n",
            "It: 3030, Time: 0.02\n",
            "mse_b  [1.1240127]  mse_f: 0.27461329102516174   total loss: [1.398626]\n",
            "mse_b ====== [1.12384033]\n",
            "It: 3031, Time: 0.03\n",
            "mse_b  [1.1238403]  mse_f: 0.2619996666908264   total loss: [1.3858399]\n",
            "mse_b ====== [1.12558866]\n",
            "It: 3032, Time: 0.02\n",
            "mse_b  [1.1255887]  mse_f: 0.24945737421512604   total loss: [1.375046]\n",
            "mse_b ====== [1.12833571]\n",
            "It: 3033, Time: 0.02\n",
            "mse_b  [1.1283357]  mse_f: 0.2650597393512726   total loss: [1.3933954]\n",
            "mse_b ====== [1.12900925]\n",
            "It: 3034, Time: 0.02\n",
            "mse_b  [1.1290092]  mse_f: 0.27151310443878174   total loss: [1.4005224]\n",
            "mse_b ====== [1.12598908]\n",
            "It: 3035, Time: 0.02\n",
            "mse_b  [1.1259891]  mse_f: 0.25756341218948364   total loss: [1.3835526]\n",
            "mse_b ====== [1.12083626]\n",
            "It: 3036, Time: 0.02\n",
            "mse_b  [1.1208363]  mse_f: 0.2419860064983368   total loss: [1.3628223]\n",
            "mse_b ====== [1.11601222]\n",
            "It: 3037, Time: 0.02\n",
            "mse_b  [1.1160122]  mse_f: 0.2584501802921295   total loss: [1.3744624]\n",
            "mse_b ====== [1.11224234]\n",
            "It: 3038, Time: 0.03\n",
            "mse_b  [1.1122423]  mse_f: 0.26622411608695984   total loss: [1.3784665]\n",
            "mse_b ====== [1.10862494]\n",
            "It: 3039, Time: 0.02\n",
            "mse_b  [1.1086249]  mse_f: 0.2535971999168396   total loss: [1.3622222]\n",
            "mse_b ====== [1.10441792]\n",
            "It: 3040, Time: 0.02\n",
            "mse_b  [1.1044179]  mse_f: 0.24235203862190247   total loss: [1.3467699]\n",
            "mse_b ====== [1.10032797]\n",
            "It: 3041, Time: 0.02\n",
            "mse_b  [1.100328]  mse_f: 0.25701677799224854   total loss: [1.3573447]\n",
            "mse_b ====== [1.09789217]\n",
            "It: 3042, Time: 0.02\n",
            "mse_b  [1.0978922]  mse_f: 0.26487118005752563   total loss: [1.3627634]\n",
            "mse_b ====== [1.09852886]\n",
            "It: 3043, Time: 0.03\n",
            "mse_b  [1.0985289]  mse_f: 0.25001901388168335   total loss: [1.3485479]\n",
            "mse_b ====== [1.1014986]\n",
            "It: 3044, Time: 0.02\n",
            "mse_b  [1.1014986]  mse_f: 0.24082309007644653   total loss: [1.3423216]\n",
            "mse_b ====== [1.10265326]\n",
            "It: 3045, Time: 0.02\n",
            "mse_b  [1.1026533]  mse_f: 0.2438579797744751   total loss: [1.3465112]\n",
            "mse_b ====== [1.09829903]\n",
            "It: 3046, Time: 0.02\n",
            "mse_b  [1.098299]  mse_f: 0.24148133397102356   total loss: [1.3397803]\n",
            "mse_b ====== [1.09007037]\n",
            "It: 3047, Time: 0.02\n",
            "mse_b  [1.0900704]  mse_f: 0.2256179004907608   total loss: [1.3156883]\n",
            "mse_b ====== [1.08336568]\n",
            "It: 3048, Time: 0.03\n",
            "mse_b  [1.0833657]  mse_f: 0.23205797374248505   total loss: [1.3154236]\n",
            "mse_b ====== [1.08076358]\n",
            "It: 3049, Time: 0.02\n",
            "mse_b  [1.0807636]  mse_f: 0.2435493767261505   total loss: [1.3243129]\n",
            "mse_b ====== [1.07997298]\n",
            "It: 3050, Time: 0.02\n",
            "mse_b  [1.079973]  mse_f: 0.23877552151679993   total loss: [1.3187485]\n",
            "mse_b ====== [1.07812476]\n",
            "It: 3051, Time: 0.02\n",
            "mse_b  [1.0781248]  mse_f: 0.2190377414226532   total loss: [1.2971625]\n",
            "mse_b ====== [1.07485867]\n",
            "It: 3052, Time: 0.02\n",
            "mse_b  [1.0748587]  mse_f: 0.228314608335495   total loss: [1.3031733]\n",
            "mse_b ====== [1.07085657]\n",
            "It: 3053, Time: 0.02\n",
            "mse_b  [1.0708566]  mse_f: 0.24637159705162048   total loss: [1.3172282]\n",
            "mse_b ====== [1.0666765]\n",
            "It: 3054, Time: 0.02\n",
            "mse_b  [1.0666765]  mse_f: 0.24387460947036743   total loss: [1.3105512]\n",
            "mse_b ====== [1.06254232]\n",
            "It: 3055, Time: 0.02\n",
            "mse_b  [1.0625423]  mse_f: 0.22569730877876282   total loss: [1.2882396]\n",
            "mse_b ====== [1.05796289]\n",
            "It: 3056, Time: 0.02\n",
            "mse_b  [1.0579629]  mse_f: 0.22548528015613556   total loss: [1.2834482]\n",
            "mse_b ====== [1.05225325]\n",
            "It: 3057, Time: 0.02\n",
            "mse_b  [1.0522532]  mse_f: 0.2328852266073227   total loss: [1.2851385]\n",
            "mse_b ====== [1.04601026]\n",
            "It: 3058, Time: 0.02\n",
            "mse_b  [1.0460103]  mse_f: 0.2269304394721985   total loss: [1.2729406]\n",
            "mse_b ====== [1.04119349]\n",
            "It: 3059, Time: 0.02\n",
            "mse_b  [1.0411935]  mse_f: 0.22634756565093994   total loss: [1.267541]\n",
            "mse_b ====== [1.03894258]\n",
            "It: 3060, Time: 0.02\n",
            "mse_b  [1.0389426]  mse_f: 0.23730558156967163   total loss: [1.2762482]\n",
            "mse_b ====== [1.03791189]\n",
            "It: 3061, Time: 0.03\n",
            "mse_b  [1.0379119]  mse_f: 0.24102959036827087   total loss: [1.2789415]\n",
            "mse_b ====== [1.03581572]\n",
            "It: 3062, Time: 0.02\n",
            "mse_b  [1.0358157]  mse_f: 0.22367644309997559   total loss: [1.2594922]\n",
            "mse_b ====== [1.03193188]\n",
            "It: 3063, Time: 0.02\n",
            "mse_b  [1.0319319]  mse_f: 0.2255087047815323   total loss: [1.2574406]\n",
            "mse_b ====== [1.02730298]\n",
            "It: 3064, Time: 0.02\n",
            "mse_b  [1.027303]  mse_f: 0.24236546456813812   total loss: [1.2696685]\n",
            "mse_b ====== [1.0236063]\n",
            "It: 3065, Time: 0.02\n",
            "mse_b  [1.0236063]  mse_f: 0.2470933496952057   total loss: [1.2706996]\n",
            "mse_b ====== [1.02126718]\n",
            "It: 3066, Time: 0.02\n",
            "mse_b  [1.0212672]  mse_f: 0.22605007886886597   total loss: [1.2473173]\n",
            "mse_b ====== [1.0187788]\n",
            "It: 3067, Time: 0.03\n",
            "mse_b  [1.0187788]  mse_f: 0.22282402217388153   total loss: [1.2416028]\n",
            "mse_b ====== [1.01468313]\n",
            "It: 3068, Time: 0.02\n",
            "mse_b  [1.0146831]  mse_f: 0.23388823866844177   total loss: [1.2485714]\n",
            "mse_b ====== [1.00998104]\n",
            "It: 3069, Time: 0.02\n",
            "mse_b  [1.009981]  mse_f: 0.23789362609386444   total loss: [1.2478746]\n",
            "mse_b ====== [1.0068351]\n",
            "It: 3070, Time: 0.03\n",
            "mse_b  [1.0068351]  mse_f: 0.22919893264770508   total loss: [1.236034]\n",
            "mse_b ====== [1.00496101]\n",
            "It: 3071, Time: 0.02\n",
            "mse_b  [1.004961]  mse_f: 0.2319667637348175   total loss: [1.2369277]\n",
            "mse_b ====== [1.0012393]\n",
            "It: 3072, Time: 0.02\n",
            "mse_b  [1.0012393]  mse_f: 0.23549367487430573   total loss: [1.236733]\n",
            "mse_b ====== [0.994138062]\n",
            "It: 3073, Time: 0.03\n",
            "mse_b  [0.99413806]  mse_f: 0.22732572257518768   total loss: [1.2214638]\n",
            "mse_b ====== [0.985817194]\n",
            "It: 3074, Time: 0.03\n",
            "mse_b  [0.9858172]  mse_f: 0.22597286105155945   total loss: [1.2117901]\n",
            "mse_b ====== [0.978729367]\n",
            "It: 3075, Time: 0.03\n",
            "mse_b  [0.97872937]  mse_f: 0.24051886796951294   total loss: [1.2192483]\n",
            "mse_b ====== [0.973164499]\n",
            "It: 3076, Time: 0.02\n",
            "mse_b  [0.9731645]  mse_f: 0.24846641719341278   total loss: [1.2216309]\n",
            "mse_b ====== [0.968627632]\n",
            "It: 3077, Time: 0.02\n",
            "mse_b  [0.96862763]  mse_f: 0.23376122117042542   total loss: [1.2023889]\n",
            "mse_b ====== [0.96508944]\n",
            "It: 3078, Time: 0.03\n",
            "mse_b  [0.96508944]  mse_f: 0.2300363928079605   total loss: [1.1951258]\n",
            "mse_b ====== [0.962189138]\n",
            "It: 3079, Time: 0.02\n",
            "mse_b  [0.96218914]  mse_f: 0.2404840886592865   total loss: [1.2026732]\n",
            "mse_b ====== [0.959333658]\n",
            "It: 3080, Time: 0.03\n",
            "mse_b  [0.95933366]  mse_f: 0.24493464827537537   total loss: [1.2042683]\n",
            "mse_b ====== [0.956970155]\n",
            "It: 3081, Time: 0.02\n",
            "mse_b  [0.95697016]  mse_f: 0.23003821074962616   total loss: [1.1870084]\n",
            "mse_b ====== [0.955826521]\n",
            "It: 3082, Time: 0.02\n",
            "mse_b  [0.9558265]  mse_f: 0.22807830572128296   total loss: [1.1839049]\n",
            "mse_b ====== [0.954621136]\n",
            "It: 3083, Time: 0.02\n",
            "mse_b  [0.95462114]  mse_f: 0.23160426318645477   total loss: [1.1862254]\n",
            "mse_b ====== [0.951190889]\n",
            "It: 3084, Time: 0.02\n",
            "mse_b  [0.9511909]  mse_f: 0.2282160222530365   total loss: [1.1794069]\n",
            "mse_b ====== [0.945762038]\n",
            "It: 3085, Time: 0.02\n",
            "mse_b  [0.94576204]  mse_f: 0.21909309923648834   total loss: [1.1648551]\n",
            "mse_b ====== [0.940901041]\n",
            "It: 3086, Time: 0.03\n",
            "mse_b  [0.94090104]  mse_f: 0.2262364774942398   total loss: [1.1671375]\n",
            "mse_b ====== [0.937816381]\n",
            "It: 3087, Time: 0.02\n",
            "mse_b  [0.9378164]  mse_f: 0.2288409173488617   total loss: [1.1666573]\n",
            "mse_b ====== [0.93515873]\n",
            "It: 3088, Time: 0.02\n",
            "mse_b  [0.9351587]  mse_f: 0.21881422400474548   total loss: [1.153973]\n",
            "mse_b ====== [0.931524932]\n",
            "It: 3089, Time: 0.02\n",
            "mse_b  [0.93152493]  mse_f: 0.2156786173582077   total loss: [1.1472036]\n",
            "mse_b ====== [0.926823556]\n",
            "It: 3090, Time: 0.02\n",
            "mse_b  [0.92682356]  mse_f: 0.23017579317092896   total loss: [1.1569993]\n",
            "mse_b ====== [0.921467543]\n",
            "It: 3091, Time: 0.02\n",
            "mse_b  [0.92146754]  mse_f: 0.23618468642234802   total loss: [1.1576523]\n",
            "mse_b ====== [0.915939]\n",
            "It: 3092, Time: 0.03\n",
            "mse_b  [0.915939]  mse_f: 0.22530607879161835   total loss: [1.141245]\n",
            "mse_b ====== [0.910453081]\n",
            "It: 3093, Time: 0.02\n",
            "mse_b  [0.9104531]  mse_f: 0.2267155945301056   total loss: [1.1371686]\n",
            "mse_b ====== [0.904861748]\n",
            "It: 3094, Time: 0.02\n",
            "mse_b  [0.90486175]  mse_f: 0.2394198775291443   total loss: [1.1442816]\n",
            "mse_b ====== [0.899917603]\n",
            "It: 3095, Time: 0.02\n",
            "mse_b  [0.8999176]  mse_f: 0.24201813340187073   total loss: [1.1419357]\n",
            "mse_b ====== [0.896934]\n",
            "It: 3096, Time: 0.02\n",
            "mse_b  [0.896934]  mse_f: 0.2298964560031891   total loss: [1.1268305]\n",
            "mse_b ====== [0.895653]\n",
            "It: 3097, Time: 0.02\n",
            "mse_b  [0.895653]  mse_f: 0.23101913928985596   total loss: [1.1266721]\n",
            "mse_b ====== [0.894140124]\n",
            "It: 3098, Time: 0.02\n",
            "mse_b  [0.8941401]  mse_f: 0.23317058384418488   total loss: [1.1273108]\n",
            "mse_b ====== [0.891285837]\n",
            "It: 3099, Time: 0.02\n",
            "mse_b  [0.89128584]  mse_f: 0.22864645719528198   total loss: [1.1199323]\n",
            "mse_b ====== [0.887825787]\n",
            "It: 3100, Time: 0.02\n",
            "mse_b  [0.8878258]  mse_f: 0.22429034113883972   total loss: [1.1121161]\n",
            "mse_b ====== [0.884652257]\n",
            "It: 3101, Time: 0.02\n",
            "mse_b  [0.88465226]  mse_f: 0.23249384760856628   total loss: [1.1171461]\n",
            "mse_b ====== [0.881573856]\n",
            "It: 3102, Time: 0.02\n",
            "mse_b  [0.88157386]  mse_f: 0.2262825220823288   total loss: [1.1078564]\n",
            "mse_b ====== [0.878525]\n",
            "It: 3103, Time: 0.03\n",
            "mse_b  [0.878525]  mse_f: 0.21007011830806732   total loss: [1.0885952]\n",
            "mse_b ====== [0.875747144]\n",
            "It: 3104, Time: 0.02\n",
            "mse_b  [0.87574714]  mse_f: 0.20695699751377106   total loss: [1.0827042]\n",
            "mse_b ====== [0.872462511]\n",
            "It: 3105, Time: 0.02\n",
            "mse_b  [0.8724625]  mse_f: 0.21966128051280975   total loss: [1.0921237]\n",
            "mse_b ====== [0.867814541]\n",
            "It: 3106, Time: 0.02\n",
            "mse_b  [0.86781454]  mse_f: 0.22002525627613068   total loss: [1.0878398]\n",
            "mse_b ====== [0.863159895]\n",
            "It: 3107, Time: 0.02\n",
            "mse_b  [0.8631599]  mse_f: 0.21300166845321655   total loss: [1.0761616]\n",
            "mse_b ====== [0.86021173]\n",
            "It: 3108, Time: 0.02\n",
            "mse_b  [0.86021173]  mse_f: 0.21678867936134338   total loss: [1.0770004]\n",
            "mse_b ====== [0.857738316]\n",
            "It: 3109, Time: 0.02\n",
            "mse_b  [0.8577383]  mse_f: 0.22146648168563843   total loss: [1.0792048]\n",
            "mse_b ====== [0.853235722]\n",
            "It: 3110, Time: 0.03\n",
            "mse_b  [0.8532357]  mse_f: 0.2168758511543274   total loss: [1.0701115]\n",
            "mse_b ====== [0.846803248]\n",
            "It: 3111, Time: 0.02\n",
            "mse_b  [0.84680325]  mse_f: 0.21519672870635986   total loss: [1.062]\n",
            "mse_b ====== [0.840514]\n",
            "It: 3112, Time: 0.02\n",
            "mse_b  [0.840514]  mse_f: 0.22374185919761658   total loss: [1.0642558]\n",
            "mse_b ====== [0.835324764]\n",
            "It: 3113, Time: 0.02\n",
            "mse_b  [0.83532476]  mse_f: 0.21998938918113708   total loss: [1.0553142]\n",
            "mse_b ====== [0.830679238]\n",
            "It: 3114, Time: 0.03\n",
            "mse_b  [0.83067924]  mse_f: 0.21275895833969116   total loss: [1.0434382]\n",
            "mse_b ====== [0.826001167]\n",
            "It: 3115, Time: 0.02\n",
            "mse_b  [0.82600117]  mse_f: 0.21891304850578308   total loss: [1.0449142]\n",
            "mse_b ====== [0.82162261]\n",
            "It: 3116, Time: 0.02\n",
            "mse_b  [0.8216226]  mse_f: 0.2332739531993866   total loss: [1.0548966]\n",
            "mse_b ====== [0.817968547]\n",
            "It: 3117, Time: 0.03\n",
            "mse_b  [0.81796855]  mse_f: 0.22802433371543884   total loss: [1.0459929]\n",
            "mse_b ====== [0.814386547]\n",
            "It: 3118, Time: 0.02\n",
            "mse_b  [0.81438655]  mse_f: 0.2220298945903778   total loss: [1.0364164]\n",
            "mse_b ====== [0.809503734]\n",
            "It: 3119, Time: 0.02\n",
            "mse_b  [0.80950373]  mse_f: 0.2272004336118698   total loss: [1.0367042]\n",
            "mse_b ====== [0.803012192]\n",
            "It: 3120, Time: 0.02\n",
            "mse_b  [0.8030122]  mse_f: 0.23280069231987   total loss: [1.0358129]\n",
            "mse_b ====== [0.796238482]\n",
            "It: 3121, Time: 0.02\n",
            "mse_b  [0.7962385]  mse_f: 0.2251388430595398   total loss: [1.0213773]\n",
            "mse_b ====== [0.790587604]\n",
            "It: 3122, Time: 0.03\n",
            "mse_b  [0.7905876]  mse_f: 0.22600242495536804   total loss: [1.01659]\n",
            "mse_b ====== [0.78571856]\n",
            "It: 3123, Time: 0.02\n",
            "mse_b  [0.78571856]  mse_f: 0.23321686685085297   total loss: [1.0189354]\n",
            "mse_b ====== [0.780749261]\n",
            "It: 3124, Time: 0.02\n",
            "mse_b  [0.78074926]  mse_f: 0.2334727644920349   total loss: [1.014222]\n",
            "mse_b ====== [0.775980055]\n",
            "It: 3125, Time: 0.02\n",
            "mse_b  [0.77598006]  mse_f: 0.23087631165981293   total loss: [1.0068563]\n",
            "mse_b ====== [0.771914244]\n",
            "It: 3126, Time: 0.02\n",
            "mse_b  [0.77191424]  mse_f: 0.2386763095855713   total loss: [1.0105906]\n",
            "mse_b ====== [0.768041909]\n",
            "It: 3127, Time: 0.02\n",
            "mse_b  [0.7680419]  mse_f: 0.24111852049827576   total loss: [1.0091604]\n",
            "mse_b ====== [0.764077902]\n",
            "It: 3128, Time: 0.02\n",
            "mse_b  [0.7640779]  mse_f: 0.23078621923923492   total loss: [0.9948641]\n",
            "mse_b ====== [0.760397196]\n",
            "It: 3129, Time: 0.02\n",
            "mse_b  [0.7603972]  mse_f: 0.22873666882514954   total loss: [0.98913383]\n",
            "mse_b ====== [0.75650996]\n",
            "It: 3130, Time: 0.02\n",
            "mse_b  [0.75650996]  mse_f: 0.23643341660499573   total loss: [0.9929434]\n",
            "mse_b ====== [0.751655102]\n",
            "It: 3131, Time: 0.02\n",
            "mse_b  [0.7516551]  mse_f: 0.2352849841117859   total loss: [0.9869401]\n",
            "mse_b ====== [0.746741772]\n",
            "It: 3132, Time: 0.02\n",
            "mse_b  [0.7467418]  mse_f: 0.2265329360961914   total loss: [0.9732747]\n",
            "mse_b ====== [0.742930889]\n",
            "It: 3133, Time: 0.02\n",
            "mse_b  [0.7429309]  mse_f: 0.23111292719841003   total loss: [0.97404385]\n",
            "mse_b ====== [0.739096701]\n",
            "It: 3134, Time: 0.02\n",
            "mse_b  [0.7390967]  mse_f: 0.23633429408073425   total loss: [0.97543097]\n",
            "mse_b ====== [0.733571589]\n",
            "It: 3135, Time: 0.02\n",
            "mse_b  [0.7335716]  mse_f: 0.23500429093837738   total loss: [0.9685759]\n",
            "mse_b ====== [0.727094293]\n",
            "It: 3136, Time: 0.02\n",
            "mse_b  [0.7270943]  mse_f: 0.2353493720293045   total loss: [0.96244365]\n",
            "mse_b ====== [0.721802473]\n",
            "It: 3137, Time: 0.02\n",
            "mse_b  [0.7218025]  mse_f: 0.2417210340499878   total loss: [0.9635235]\n",
            "mse_b ====== [0.718373775]\n",
            "It: 3138, Time: 0.03\n",
            "mse_b  [0.7183738]  mse_f: 0.23457728326320648   total loss: [0.9529511]\n",
            "mse_b ====== [0.715798438]\n",
            "It: 3139, Time: 0.02\n",
            "mse_b  [0.71579844]  mse_f: 0.22842833399772644   total loss: [0.94422674]\n",
            "mse_b ====== [0.713096142]\n",
            "It: 3140, Time: 0.02\n",
            "mse_b  [0.71309614]  mse_f: 0.23438891768455505   total loss: [0.9474851]\n",
            "mse_b ====== [0.710307717]\n",
            "It: 3141, Time: 0.02\n",
            "mse_b  [0.7103077]  mse_f: 0.2406918853521347   total loss: [0.9509996]\n",
            "mse_b ====== [0.707898796]\n",
            "It: 3142, Time: 0.02\n",
            "mse_b  [0.7078988]  mse_f: 0.23106995224952698   total loss: [0.9389688]\n",
            "mse_b ====== [0.705611408]\n",
            "It: 3143, Time: 0.02\n",
            "mse_b  [0.7056114]  mse_f: 0.2285163253545761   total loss: [0.93412775]\n",
            "mse_b ====== [0.70230186]\n",
            "It: 3144, Time: 0.03\n",
            "mse_b  [0.70230186]  mse_f: 0.23540490865707397   total loss: [0.93770677]\n",
            "mse_b ====== [0.69786334]\n",
            "It: 3145, Time: 0.02\n",
            "mse_b  [0.69786334]  mse_f: 0.23785212635993958   total loss: [0.93571544]\n",
            "mse_b ====== [0.69372946]\n",
            "It: 3146, Time: 0.02\n",
            "mse_b  [0.69372946]  mse_f: 0.23387926816940308   total loss: [0.9276087]\n",
            "mse_b ====== [0.690514147]\n",
            "It: 3147, Time: 0.02\n",
            "mse_b  [0.69051415]  mse_f: 0.237865149974823   total loss: [0.9283793]\n",
            "mse_b ====== [0.687069893]\n",
            "It: 3148, Time: 0.02\n",
            "mse_b  [0.6870699]  mse_f: 0.2386801540851593   total loss: [0.92575]\n",
            "mse_b ====== [0.682626903]\n",
            "It: 3149, Time: 0.02\n",
            "mse_b  [0.6826269]  mse_f: 0.23331402242183685   total loss: [0.91594094]\n",
            "mse_b ====== [0.677725554]\n",
            "It: 3150, Time: 0.02\n",
            "mse_b  [0.67772555]  mse_f: 0.23509588837623596   total loss: [0.9128214]\n",
            "mse_b ====== [0.672761083]\n",
            "It: 3151, Time: 0.03\n",
            "mse_b  [0.6727611]  mse_f: 0.24236096441745758   total loss: [0.91512203]\n",
            "mse_b ====== [0.667505562]\n",
            "It: 3152, Time: 0.02\n",
            "mse_b  [0.66750556]  mse_f: 0.23884543776512146   total loss: [0.906351]\n",
            "mse_b ====== [0.662284613]\n",
            "It: 3153, Time: 0.02\n",
            "mse_b  [0.6622846]  mse_f: 0.23301756381988525   total loss: [0.8953022]\n",
            "mse_b ====== [0.657624662]\n",
            "It: 3154, Time: 0.03\n",
            "mse_b  [0.65762466]  mse_f: 0.2386671006679535   total loss: [0.89629173]\n",
            "mse_b ====== [0.653345287]\n",
            "It: 3155, Time: 0.02\n",
            "mse_b  [0.6533453]  mse_f: 0.24095885455608368   total loss: [0.89430416]\n",
            "mse_b ====== [0.649591327]\n",
            "It: 3156, Time: 0.02\n",
            "mse_b  [0.6495913]  mse_f: 0.23393277823925018   total loss: [0.8835241]\n",
            "mse_b ====== [0.646990299]\n",
            "It: 3157, Time: 0.03\n",
            "mse_b  [0.6469903]  mse_f: 0.2305893450975418   total loss: [0.8775796]\n",
            "mse_b ====== [0.644898891]\n",
            "It: 3158, Time: 0.03\n",
            "mse_b  [0.6448989]  mse_f: 0.2337028980255127   total loss: [0.8786018]\n",
            "mse_b ====== [0.641744494]\n",
            "It: 3159, Time: 0.02\n",
            "mse_b  [0.6417445]  mse_f: 0.22912538051605225   total loss: [0.8708699]\n",
            "mse_b ====== [0.637566805]\n",
            "It: 3160, Time: 0.02\n",
            "mse_b  [0.6375668]  mse_f: 0.22736337780952454   total loss: [0.86493015]\n",
            "mse_b ====== [0.633807659]\n",
            "It: 3161, Time: 0.02\n",
            "mse_b  [0.63380766]  mse_f: 0.23201093077659607   total loss: [0.8658186]\n",
            "mse_b ====== [0.631043911]\n",
            "It: 3162, Time: 0.02\n",
            "mse_b  [0.6310439]  mse_f: 0.2329503744840622   total loss: [0.8639943]\n",
            "mse_b ====== [0.628503323]\n",
            "It: 3163, Time: 0.03\n",
            "mse_b  [0.6285033]  mse_f: 0.22672024369239807   total loss: [0.85522354]\n",
            "mse_b ====== [0.625123501]\n",
            "It: 3164, Time: 0.03\n",
            "mse_b  [0.6251235]  mse_f: 0.23196056485176086   total loss: [0.85708404]\n",
            "mse_b ====== [0.620386481]\n",
            "It: 3165, Time: 0.02\n",
            "mse_b  [0.6203865]  mse_f: 0.2376861721277237   total loss: [0.85807264]\n",
            "mse_b ====== [0.614923298]\n",
            "It: 3166, Time: 0.03\n",
            "mse_b  [0.6149233]  mse_f: 0.23355501890182495   total loss: [0.8484783]\n",
            "mse_b ====== [0.609578848]\n",
            "It: 3167, Time: 0.02\n",
            "mse_b  [0.60957885]  mse_f: 0.22932380437850952   total loss: [0.83890265]\n",
            "mse_b ====== [0.604270518]\n",
            "It: 3168, Time: 0.02\n",
            "mse_b  [0.6042705]  mse_f: 0.23728187382221222   total loss: [0.8415524]\n",
            "mse_b ====== [0.599014759]\n",
            "It: 3169, Time: 0.02\n",
            "mse_b  [0.59901476]  mse_f: 0.23985281586647034   total loss: [0.83886755]\n",
            "mse_b ====== [0.594774544]\n",
            "It: 3170, Time: 0.03\n",
            "mse_b  [0.59477454]  mse_f: 0.23793470859527588   total loss: [0.83270925]\n",
            "mse_b ====== [0.591702163]\n",
            "It: 3171, Time: 0.02\n",
            "mse_b  [0.59170216]  mse_f: 0.23905156552791595   total loss: [0.83075374]\n",
            "mse_b ====== [0.588274062]\n",
            "It: 3172, Time: 0.02\n",
            "mse_b  [0.58827406]  mse_f: 0.23929128050804138   total loss: [0.8275653]\n",
            "mse_b ====== [0.583540082]\n",
            "It: 3173, Time: 0.02\n",
            "mse_b  [0.5835401]  mse_f: 0.2326112985610962   total loss: [0.8161514]\n",
            "mse_b ====== [0.578739524]\n",
            "It: 3174, Time: 0.02\n",
            "mse_b  [0.5787395]  mse_f: 0.23335573077201843   total loss: [0.8120953]\n",
            "mse_b ====== [0.575143695]\n",
            "It: 3175, Time: 0.02\n",
            "mse_b  [0.5751437]  mse_f: 0.23627078533172607   total loss: [0.8114145]\n",
            "mse_b ====== [0.57232511]\n",
            "It: 3176, Time: 0.02\n",
            "mse_b  [0.5723251]  mse_f: 0.22956961393356323   total loss: [0.8018947]\n",
            "mse_b ====== [0.569451332]\n",
            "It: 3177, Time: 0.03\n",
            "mse_b  [0.56945133]  mse_f: 0.22291530668735504   total loss: [0.7923666]\n",
            "mse_b ====== [0.566419125]\n",
            "It: 3178, Time: 0.02\n",
            "mse_b  [0.5664191]  mse_f: 0.22804927825927734   total loss: [0.7944684]\n",
            "mse_b ====== [0.563211441]\n",
            "It: 3179, Time: 0.02\n",
            "mse_b  [0.56321144]  mse_f: 0.23055465519428253   total loss: [0.7937661]\n",
            "mse_b ====== [0.559710264]\n",
            "It: 3180, Time: 0.02\n",
            "mse_b  [0.55971026]  mse_f: 0.22658011317253113   total loss: [0.7862904]\n",
            "mse_b ====== [0.555851758]\n",
            "It: 3181, Time: 0.02\n",
            "mse_b  [0.55585176]  mse_f: 0.2278970330953598   total loss: [0.7837488]\n",
            "mse_b ====== [0.551612854]\n",
            "It: 3182, Time: 0.02\n",
            "mse_b  [0.55161285]  mse_f: 0.22999006509780884   total loss: [0.7816029]\n",
            "mse_b ====== [0.547404826]\n",
            "It: 3183, Time: 0.02\n",
            "mse_b  [0.5474048]  mse_f: 0.22571294009685516   total loss: [0.7731178]\n",
            "mse_b ====== [0.543841243]\n",
            "It: 3184, Time: 0.02\n",
            "mse_b  [0.54384124]  mse_f: 0.2253606766462326   total loss: [0.76920193]\n",
            "mse_b ====== [0.540590346]\n",
            "It: 3185, Time: 0.03\n",
            "mse_b  [0.54059035]  mse_f: 0.23062574863433838   total loss: [0.7712161]\n",
            "mse_b ====== [0.536769509]\n",
            "It: 3186, Time: 0.03\n",
            "mse_b  [0.5367695]  mse_f: 0.2283351719379425   total loss: [0.76510465]\n",
            "mse_b ====== [0.532388806]\n",
            "It: 3187, Time: 0.03\n",
            "mse_b  [0.5323888]  mse_f: 0.2261858731508255   total loss: [0.75857466]\n",
            "mse_b ====== [0.527886391]\n",
            "It: 3188, Time: 0.02\n",
            "mse_b  [0.5278864]  mse_f: 0.23167547583580017   total loss: [0.7595619]\n",
            "mse_b ====== [0.523298383]\n",
            "It: 3189, Time: 0.02\n",
            "mse_b  [0.5232984]  mse_f: 0.23385632038116455   total loss: [0.7571547]\n",
            "mse_b ====== [0.518690825]\n",
            "It: 3190, Time: 0.03\n",
            "mse_b  [0.5186908]  mse_f: 0.22812527418136597   total loss: [0.7468161]\n",
            "mse_b ====== [0.514418364]\n",
            "It: 3191, Time: 0.02\n",
            "mse_b  [0.51441836]  mse_f: 0.2296219915151596   total loss: [0.74404037]\n",
            "mse_b ====== [0.510699391]\n",
            "It: 3192, Time: 0.03\n",
            "mse_b  [0.5106994]  mse_f: 0.23236419260501862   total loss: [0.74306357]\n",
            "mse_b ====== [0.507528901]\n",
            "It: 3193, Time: 0.02\n",
            "mse_b  [0.5075289]  mse_f: 0.22894826531410217   total loss: [0.73647714]\n",
            "mse_b ====== [0.504605711]\n",
            "It: 3194, Time: 0.02\n",
            "mse_b  [0.5046057]  mse_f: 0.226318821310997   total loss: [0.73092455]\n",
            "mse_b ====== [0.501052856]\n",
            "It: 3195, Time: 0.03\n",
            "mse_b  [0.50105286]  mse_f: 0.229017972946167   total loss: [0.7300708]\n",
            "mse_b ====== [0.496200383]\n",
            "It: 3196, Time: 0.02\n",
            "mse_b  [0.49620038]  mse_f: 0.2254742681980133   total loss: [0.7216747]\n",
            "mse_b ====== [0.490721017]\n",
            "It: 3197, Time: 0.02\n",
            "mse_b  [0.49072102]  mse_f: 0.22499653697013855   total loss: [0.71571755]\n",
            "mse_b ====== [0.485709]\n",
            "It: 3198, Time: 0.02\n",
            "mse_b  [0.485709]  mse_f: 0.23063261806964874   total loss: [0.7163416]\n",
            "mse_b ====== [0.481258273]\n",
            "It: 3199, Time: 0.03\n",
            "mse_b  [0.48125827]  mse_f: 0.23167158663272858   total loss: [0.71292984]\n",
            "mse_b ====== [0.47686246]\n",
            "It: 3200, Time: 0.02\n",
            "mse_b  [0.47686246]  mse_f: 0.225563645362854   total loss: [0.7024261]\n",
            "mse_b ====== [0.472382128]\n",
            "It: 3201, Time: 0.03\n",
            "mse_b  [0.47238213]  mse_f: 0.2279350310564041   total loss: [0.70031714]\n",
            "mse_b ====== [0.468034416]\n",
            "It: 3202, Time: 0.02\n",
            "mse_b  [0.46803442]  mse_f: 0.2305675745010376   total loss: [0.69860196]\n",
            "mse_b ====== [0.464205742]\n",
            "It: 3203, Time: 0.02\n",
            "mse_b  [0.46420574]  mse_f: 0.22741729021072388   total loss: [0.69162303]\n",
            "mse_b ====== [0.461075157]\n",
            "It: 3204, Time: 0.02\n",
            "mse_b  [0.46107516]  mse_f: 0.2254563271999359   total loss: [0.6865315]\n",
            "mse_b ====== [0.45830822]\n",
            "It: 3205, Time: 0.02\n",
            "mse_b  [0.45830822]  mse_f: 0.22783660888671875   total loss: [0.6861448]\n",
            "mse_b ====== [0.455699921]\n",
            "It: 3206, Time: 0.03\n",
            "mse_b  [0.45569992]  mse_f: 0.2243746519088745   total loss: [0.6800746]\n",
            "mse_b ====== [0.453525126]\n",
            "It: 3207, Time: 0.02\n",
            "mse_b  [0.45352513]  mse_f: 0.22396743297576904   total loss: [0.67749256]\n",
            "mse_b ====== [0.451366305]\n",
            "It: 3208, Time: 0.03\n",
            "mse_b  [0.4513663]  mse_f: 0.22756731510162354   total loss: [0.6789336]\n",
            "mse_b ====== [0.448080301]\n",
            "It: 3209, Time: 0.02\n",
            "mse_b  [0.4480803]  mse_f: 0.2254934012889862   total loss: [0.67357373]\n",
            "mse_b ====== [0.443659276]\n",
            "It: 3210, Time: 0.03\n",
            "mse_b  [0.44365928]  mse_f: 0.2195633500814438   total loss: [0.6632226]\n",
            "mse_b ====== [0.439388603]\n",
            "It: 3211, Time: 0.02\n",
            "mse_b  [0.4393886]  mse_f: 0.22210440039634705   total loss: [0.661493]\n",
            "mse_b ====== [0.435921878]\n",
            "It: 3212, Time: 0.02\n",
            "mse_b  [0.43592188]  mse_f: 0.2233545184135437   total loss: [0.65927637]\n",
            "mse_b ====== [0.432794213]\n",
            "It: 3213, Time: 0.02\n",
            "mse_b  [0.4327942]  mse_f: 0.22036820650100708   total loss: [0.6531624]\n",
            "mse_b ====== [0.429624468]\n",
            "It: 3214, Time: 0.02\n",
            "mse_b  [0.42962447]  mse_f: 0.22126752138137817   total loss: [0.650892]\n",
            "mse_b ====== [0.426640451]\n",
            "It: 3215, Time: 0.02\n",
            "mse_b  [0.42664045]  mse_f: 0.2235303521156311   total loss: [0.6501708]\n",
            "mse_b ====== [0.424028158]\n",
            "It: 3216, Time: 0.03\n",
            "mse_b  [0.42402816]  mse_f: 0.21912726759910583   total loss: [0.64315546]\n",
            "mse_b ====== [0.421263]\n",
            "It: 3217, Time: 0.02\n",
            "mse_b  [0.421263]  mse_f: 0.21801438927650452   total loss: [0.6392774]\n",
            "mse_b ====== [0.417569727]\n",
            "It: 3218, Time: 0.02\n",
            "mse_b  [0.41756973]  mse_f: 0.22063592076301575   total loss: [0.63820565]\n",
            "mse_b ====== [0.413431376]\n",
            "It: 3219, Time: 0.02\n",
            "mse_b  [0.41343138]  mse_f: 0.21796607971191406   total loss: [0.6313975]\n",
            "mse_b ====== [0.410260826]\n",
            "It: 3220, Time: 0.02\n",
            "mse_b  [0.41026083]  mse_f: 0.21506652235984802   total loss: [0.62532735]\n",
            "mse_b ====== [0.407751352]\n",
            "It: 3221, Time: 0.02\n",
            "mse_b  [0.40775135]  mse_f: 0.21863232553005219   total loss: [0.62638366]\n",
            "mse_b ====== [0.403969944]\n",
            "It: 3222, Time: 0.03\n",
            "mse_b  [0.40396994]  mse_f: 0.21913734078407288   total loss: [0.6231073]\n",
            "mse_b ====== [0.398737788]\n",
            "It: 3223, Time: 0.02\n",
            "mse_b  [0.3987378]  mse_f: 0.2173127830028534   total loss: [0.6160506]\n",
            "mse_b ====== [0.393701434]\n",
            "It: 3224, Time: 0.02\n",
            "mse_b  [0.39370143]  mse_f: 0.22056226432323456   total loss: [0.6142637]\n",
            "mse_b ====== [0.389469385]\n",
            "It: 3225, Time: 0.02\n",
            "mse_b  [0.3894694]  mse_f: 0.22169122099876404   total loss: [0.61116064]\n",
            "mse_b ====== [0.38548249]\n",
            "It: 3226, Time: 0.02\n",
            "mse_b  [0.3854825]  mse_f: 0.21852605044841766   total loss: [0.60400856]\n",
            "mse_b ====== [0.381667197]\n",
            "It: 3227, Time: 0.02\n",
            "mse_b  [0.3816672]  mse_f: 0.21958309412002563   total loss: [0.6012503]\n",
            "mse_b ====== [0.378294319]\n",
            "It: 3228, Time: 0.02\n",
            "mse_b  [0.37829432]  mse_f: 0.22097130119800568   total loss: [0.59926564]\n",
            "mse_b ====== [0.375157952]\n",
            "It: 3229, Time: 0.02\n",
            "mse_b  [0.37515795]  mse_f: 0.21745026111602783   total loss: [0.5926082]\n",
            "mse_b ====== [0.37174359]\n",
            "It: 3230, Time: 0.02\n",
            "mse_b  [0.3717436]  mse_f: 0.21884974837303162   total loss: [0.59059334]\n",
            "mse_b ====== [0.367854834]\n",
            "It: 3231, Time: 0.02\n",
            "mse_b  [0.36785483]  mse_f: 0.22313456237316132   total loss: [0.5909894]\n",
            "mse_b ====== [0.363885]\n",
            "It: 3232, Time: 0.02\n",
            "mse_b  [0.363885]  mse_f: 0.2210511714220047   total loss: [0.58493614]\n",
            "mse_b ====== [0.360227823]\n",
            "It: 3233, Time: 0.02\n",
            "mse_b  [0.36022782]  mse_f: 0.21903495490550995   total loss: [0.5792628]\n",
            "mse_b ====== [0.356773198]\n",
            "It: 3234, Time: 0.02\n",
            "mse_b  [0.3567732]  mse_f: 0.2228526622056961   total loss: [0.57962584]\n",
            "mse_b ====== [0.353316516]\n",
            "It: 3235, Time: 0.02\n",
            "mse_b  [0.35331652]  mse_f: 0.2227674126625061   total loss: [0.5760839]\n",
            "mse_b ====== [0.34996447]\n",
            "It: 3236, Time: 0.02\n",
            "mse_b  [0.34996447]  mse_f: 0.22089850902557373   total loss: [0.570863]\n",
            "mse_b ====== [0.346608907]\n",
            "It: 3237, Time: 0.02\n",
            "mse_b  [0.3466089]  mse_f: 0.22216761112213135   total loss: [0.5687765]\n",
            "mse_b ====== [0.342949629]\n",
            "It: 3238, Time: 0.02\n",
            "mse_b  [0.34294963]  mse_f: 0.22161799669265747   total loss: [0.5645676]\n",
            "mse_b ====== [0.339263529]\n",
            "It: 3239, Time: 0.02\n",
            "mse_b  [0.33926353]  mse_f: 0.21942859888076782   total loss: [0.5586921]\n",
            "mse_b ====== [0.336114913]\n",
            "It: 3240, Time: 0.02\n",
            "mse_b  [0.3361149]  mse_f: 0.2228868007659912   total loss: [0.5590017]\n",
            "mse_b ====== [0.333374172]\n",
            "It: 3241, Time: 0.03\n",
            "mse_b  [0.33337417]  mse_f: 0.2231694459915161   total loss: [0.5565436]\n",
            "mse_b ====== [0.330627412]\n",
            "It: 3242, Time: 0.03\n",
            "mse_b  [0.3306274]  mse_f: 0.21818006038665771   total loss: [0.5488075]\n",
            "mse_b ====== [0.327926874]\n",
            "It: 3243, Time: 0.02\n",
            "mse_b  [0.32792687]  mse_f: 0.217074453830719   total loss: [0.5450013]\n",
            "mse_b ====== [0.325297713]\n",
            "It: 3244, Time: 0.02\n",
            "mse_b  [0.3252977]  mse_f: 0.21878385543823242   total loss: [0.54408157]\n",
            "mse_b ====== [0.322593123]\n",
            "It: 3245, Time: 0.02\n",
            "mse_b  [0.32259312]  mse_f: 0.2156866490840912   total loss: [0.5382798]\n",
            "mse_b ====== [0.319915175]\n",
            "It: 3246, Time: 0.02\n",
            "mse_b  [0.31991518]  mse_f: 0.2150404155254364   total loss: [0.5349556]\n",
            "mse_b ====== [0.317338705]\n",
            "It: 3247, Time: 0.02\n",
            "mse_b  [0.3173387]  mse_f: 0.21578842401504517   total loss: [0.5331271]\n",
            "mse_b ====== [0.314896047]\n",
            "It: 3248, Time: 0.02\n",
            "mse_b  [0.31489605]  mse_f: 0.2130127251148224   total loss: [0.5279088]\n",
            "mse_b ====== [0.31275183]\n",
            "It: 3249, Time: 0.02\n",
            "mse_b  [0.31275183]  mse_f: 0.21132057905197144   total loss: [0.5240724]\n",
            "mse_b ====== [0.310759395]\n",
            "It: 3250, Time: 0.02\n",
            "mse_b  [0.3107594]  mse_f: 0.2132999300956726   total loss: [0.5240593]\n",
            "mse_b ====== [0.308531106]\n",
            "It: 3251, Time: 0.02\n",
            "mse_b  [0.3085311]  mse_f: 0.2098698914051056   total loss: [0.518401]\n",
            "mse_b ====== [0.306181401]\n",
            "It: 3252, Time: 0.03\n",
            "mse_b  [0.3061814]  mse_f: 0.20700064301490784   total loss: [0.51318204]\n",
            "mse_b ====== [0.303920209]\n",
            "It: 3253, Time: 0.02\n",
            "mse_b  [0.3039202]  mse_f: 0.20892173051834106   total loss: [0.51284194]\n",
            "mse_b ====== [0.30161798]\n",
            "It: 3254, Time: 0.02\n",
            "mse_b  [0.30161798]  mse_f: 0.20897284150123596   total loss: [0.5105908]\n",
            "mse_b ====== [0.299170792]\n",
            "It: 3255, Time: 0.02\n",
            "mse_b  [0.2991708]  mse_f: 0.2061961442232132   total loss: [0.5053669]\n",
            "mse_b ====== [0.29670918]\n",
            "It: 3256, Time: 0.02\n",
            "mse_b  [0.29670918]  mse_f: 0.2079344093799591   total loss: [0.50464356]\n",
            "mse_b ====== [0.294578105]\n",
            "It: 3257, Time: 0.02\n",
            "mse_b  [0.2945781]  mse_f: 0.2071399837732315   total loss: [0.5017181]\n",
            "mse_b ====== [0.293163627]\n",
            "It: 3258, Time: 0.04\n",
            "mse_b  [0.29316363]  mse_f: 0.20378105342388153   total loss: [0.49694467]\n",
            "mse_b ====== [0.291990787]\n",
            "It: 3259, Time: 0.02\n",
            "mse_b  [0.2919908]  mse_f: 0.20340080559253693   total loss: [0.4953916]\n",
            "mse_b ====== [0.289923579]\n",
            "It: 3260, Time: 0.02\n",
            "mse_b  [0.28992358]  mse_f: 0.20301854610443115   total loss: [0.49294212]\n",
            "mse_b ====== [0.286898375]\n",
            "It: 3261, Time: 0.02\n",
            "mse_b  [0.28689837]  mse_f: 0.19955407083034515   total loss: [0.48645246]\n",
            "mse_b ====== [0.284059763]\n",
            "It: 3262, Time: 0.02\n",
            "mse_b  [0.28405976]  mse_f: 0.20087674260139465   total loss: [0.4849365]\n",
            "mse_b ====== [0.281937301]\n",
            "It: 3263, Time: 0.02\n",
            "mse_b  [0.2819373]  mse_f: 0.2009229063987732   total loss: [0.4828602]\n",
            "mse_b ====== [0.280100048]\n",
            "It: 3264, Time: 0.02\n",
            "mse_b  [0.28010005]  mse_f: 0.1965843290090561   total loss: [0.4766844]\n",
            "mse_b ====== [0.278210074]\n",
            "It: 3265, Time: 0.03\n",
            "mse_b  [0.27821007]  mse_f: 0.19480767846107483   total loss: [0.47301775]\n",
            "mse_b ====== [0.27623263]\n",
            "It: 3266, Time: 0.02\n",
            "mse_b  [0.27623263]  mse_f: 0.19622790813446045   total loss: [0.47246054]\n",
            "mse_b ====== [0.274166733]\n",
            "It: 3267, Time: 0.02\n",
            "mse_b  [0.27416673]  mse_f: 0.19370876252651215   total loss: [0.46787548]\n",
            "mse_b ====== [0.272025168]\n",
            "It: 3268, Time: 0.02\n",
            "mse_b  [0.27202517]  mse_f: 0.19297638535499573   total loss: [0.46500155]\n",
            "mse_b ====== [0.269722402]\n",
            "It: 3269, Time: 0.02\n",
            "mse_b  [0.2697224]  mse_f: 0.19299009442329407   total loss: [0.4627125]\n",
            "mse_b ====== [0.267269582]\n",
            "It: 3270, Time: 0.02\n",
            "mse_b  [0.26726958]  mse_f: 0.19029608368873596   total loss: [0.45756567]\n",
            "mse_b ====== [0.264915287]\n",
            "It: 3271, Time: 0.02\n",
            "mse_b  [0.2649153]  mse_f: 0.1895487904548645   total loss: [0.45446408]\n",
            "mse_b ====== [0.262691915]\n",
            "It: 3272, Time: 0.03\n",
            "mse_b  [0.26269192]  mse_f: 0.1928158551454544   total loss: [0.45550776]\n",
            "mse_b ====== [0.260343492]\n",
            "It: 3273, Time: 0.02\n",
            "mse_b  [0.2603435]  mse_f: 0.19116920232772827   total loss: [0.4515127]\n",
            "mse_b ====== [0.257982194]\n",
            "It: 3274, Time: 0.02\n",
            "mse_b  [0.2579822]  mse_f: 0.18945422768592834   total loss: [0.44743642]\n",
            "mse_b ====== [0.255985886]\n",
            "It: 3275, Time: 0.02\n",
            "mse_b  [0.2559859]  mse_f: 0.1907036006450653   total loss: [0.4466895]\n",
            "mse_b ====== [0.254465193]\n",
            "It: 3276, Time: 0.02\n",
            "mse_b  [0.2544652]  mse_f: 0.18976879119873047   total loss: [0.44423398]\n",
            "mse_b ====== [0.253267914]\n",
            "It: 3277, Time: 0.02\n",
            "mse_b  [0.2532679]  mse_f: 0.18686315417289734   total loss: [0.44013107]\n",
            "mse_b ====== [0.252058774]\n",
            "It: 3278, Time: 0.02\n",
            "mse_b  [0.25205877]  mse_f: 0.18705704808235168   total loss: [0.43911582]\n",
            "mse_b ====== [0.250611722]\n",
            "It: 3279, Time: 0.02\n",
            "mse_b  [0.25061172]  mse_f: 0.1844310462474823   total loss: [0.43504277]\n",
            "mse_b ====== [0.249138564]\n",
            "It: 3280, Time: 0.02\n",
            "mse_b  [0.24913856]  mse_f: 0.18184572458267212   total loss: [0.4309843]\n",
            "mse_b ====== [0.24771072]\n",
            "It: 3281, Time: 0.02\n",
            "mse_b  [0.24771072]  mse_f: 0.18256622552871704   total loss: [0.43027693]\n",
            "mse_b ====== [0.246066391]\n",
            "It: 3282, Time: 0.03\n",
            "mse_b  [0.24606639]  mse_f: 0.18154463171958923   total loss: [0.42761102]\n",
            "mse_b ====== [0.244297355]\n",
            "It: 3283, Time: 0.02\n",
            "mse_b  [0.24429736]  mse_f: 0.17781275510787964   total loss: [0.4221101]\n",
            "mse_b ====== [0.242678702]\n",
            "It: 3284, Time: 0.02\n",
            "mse_b  [0.2426787]  mse_f: 0.17855152487754822   total loss: [0.42123023]\n",
            "mse_b ====== [0.240967795]\n",
            "It: 3285, Time: 0.02\n",
            "mse_b  [0.2409678]  mse_f: 0.17810042202472687   total loss: [0.41906822]\n",
            "mse_b ====== [0.23898609]\n",
            "It: 3286, Time: 0.02\n",
            "mse_b  [0.23898609]  mse_f: 0.17534616589546204   total loss: [0.41433227]\n",
            "mse_b ====== [0.237099618]\n",
            "It: 3287, Time: 0.02\n",
            "mse_b  [0.23709962]  mse_f: 0.1745588779449463   total loss: [0.4116585]\n",
            "mse_b ====== [0.2355147]\n",
            "It: 3288, Time: 0.02\n",
            "mse_b  [0.2355147]  mse_f: 0.17387902736663818   total loss: [0.40939373]\n",
            "mse_b ====== [0.234131947]\n",
            "It: 3289, Time: 0.02\n",
            "mse_b  [0.23413195]  mse_f: 0.17149820923805237   total loss: [0.40563017]\n",
            "mse_b ====== [0.232897982]\n",
            "It: 3290, Time: 0.03\n",
            "mse_b  [0.23289798]  mse_f: 0.17321108281612396   total loss: [0.40610906]\n",
            "mse_b ====== [0.23155424]\n",
            "It: 3291, Time: 0.02\n",
            "mse_b  [0.23155424]  mse_f: 0.17317581176757812   total loss: [0.40473005]\n",
            "mse_b ====== [0.229905516]\n",
            "It: 3292, Time: 0.03\n",
            "mse_b  [0.22990552]  mse_f: 0.16973480582237244   total loss: [0.39964032]\n",
            "mse_b ====== [0.228088126]\n",
            "It: 3293, Time: 0.03\n",
            "mse_b  [0.22808813]  mse_f: 0.16897450387477875   total loss: [0.39706263]\n",
            "mse_b ====== [0.226225182]\n",
            "It: 3294, Time: 0.02\n",
            "mse_b  [0.22622518]  mse_f: 0.1695612668991089   total loss: [0.39578646]\n",
            "mse_b ====== [0.224456608]\n",
            "It: 3295, Time: 0.02\n",
            "mse_b  [0.22445661]  mse_f: 0.16700419783592224   total loss: [0.3914608]\n",
            "mse_b ====== [0.222958088]\n",
            "It: 3296, Time: 0.02\n",
            "mse_b  [0.22295809]  mse_f: 0.16712430119514465   total loss: [0.3900824]\n",
            "mse_b ====== [0.2214275]\n",
            "It: 3297, Time: 0.02\n",
            "mse_b  [0.2214275]  mse_f: 0.1667812615633011   total loss: [0.38820875]\n",
            "mse_b ====== [0.219615877]\n",
            "It: 3298, Time: 0.02\n",
            "mse_b  [0.21961588]  mse_f: 0.1644553691148758   total loss: [0.38407123]\n",
            "mse_b ====== [0.217768043]\n",
            "It: 3299, Time: 0.03\n",
            "mse_b  [0.21776804]  mse_f: 0.1645609438419342   total loss: [0.382329]\n",
            "mse_b ====== [0.216014385]\n",
            "It: 3300, Time: 0.03\n",
            "mse_b  [0.21601439]  mse_f: 0.1649903953075409   total loss: [0.38100478]\n",
            "mse_b ====== [0.214327589]\n",
            "It: 3301, Time: 0.03\n",
            "mse_b  [0.21432759]  mse_f: 0.16221129894256592   total loss: [0.37653887]\n",
            "mse_b ====== [0.21264635]\n",
            "It: 3302, Time: 0.02\n",
            "mse_b  [0.21264635]  mse_f: 0.16276952624320984   total loss: [0.37541586]\n",
            "mse_b ====== [0.210703582]\n",
            "It: 3303, Time: 0.02\n",
            "mse_b  [0.21070358]  mse_f: 0.16325503587722778   total loss: [0.37395862]\n",
            "mse_b ====== [0.208674043]\n",
            "It: 3304, Time: 0.02\n",
            "mse_b  [0.20867404]  mse_f: 0.16167715191841125   total loss: [0.3703512]\n",
            "mse_b ====== [0.206882492]\n",
            "It: 3305, Time: 0.02\n",
            "mse_b  [0.20688249]  mse_f: 0.16226379573345184   total loss: [0.3691463]\n",
            "mse_b ====== [0.20510754]\n",
            "It: 3306, Time: 0.02\n",
            "mse_b  [0.20510754]  mse_f: 0.16268838942050934   total loss: [0.36779594]\n",
            "mse_b ====== [0.203671768]\n",
            "It: 3307, Time: 0.02\n",
            "mse_b  [0.20367177]  mse_f: 0.15982618927955627   total loss: [0.36349797]\n",
            "mse_b ====== [0.203241408]\n",
            "It: 3308, Time: 0.02\n",
            "mse_b  [0.20324141]  mse_f: 0.1593906283378601   total loss: [0.36263204]\n",
            "mse_b ====== [0.202970877]\n",
            "It: 3309, Time: 0.02\n",
            "mse_b  [0.20297088]  mse_f: 0.1579207330942154   total loss: [0.3608916]\n",
            "mse_b ====== [0.201584697]\n",
            "It: 3310, Time: 0.02\n",
            "mse_b  [0.2015847]  mse_f: 0.15528999269008636   total loss: [0.3568747]\n",
            "mse_b ====== [0.199575573]\n",
            "It: 3311, Time: 0.02\n",
            "mse_b  [0.19957557]  mse_f: 0.1556640863418579   total loss: [0.35523966]\n",
            "mse_b ====== [0.197920516]\n",
            "It: 3312, Time: 0.02\n",
            "mse_b  [0.19792052]  mse_f: 0.15461190044879913   total loss: [0.35253242]\n",
            "mse_b ====== [0.196644142]\n",
            "It: 3313, Time: 0.02\n",
            "mse_b  [0.19664414]  mse_f: 0.15180867910385132   total loss: [0.3484528]\n",
            "mse_b ====== [0.195397079]\n",
            "It: 3314, Time: 0.02\n",
            "mse_b  [0.19539708]  mse_f: 0.15305304527282715   total loss: [0.34845012]\n",
            "mse_b ====== [0.194161505]\n",
            "It: 3315, Time: 0.02\n",
            "mse_b  [0.1941615]  mse_f: 0.15196728706359863   total loss: [0.3461288]\n",
            "mse_b ====== [0.193223596]\n",
            "It: 3316, Time: 0.03\n",
            "mse_b  [0.1932236]  mse_f: 0.14933937788009644   total loss: [0.34256297]\n",
            "mse_b ====== [0.192274034]\n",
            "It: 3317, Time: 0.02\n",
            "mse_b  [0.19227403]  mse_f: 0.14968350529670715   total loss: [0.34195754]\n",
            "mse_b ====== [0.190900743]\n",
            "It: 3318, Time: 0.02\n",
            "mse_b  [0.19090074]  mse_f: 0.14902053773403168   total loss: [0.3399213]\n",
            "mse_b ====== [0.189560503]\n",
            "It: 3319, Time: 0.02\n",
            "mse_b  [0.1895605]  mse_f: 0.1475793719291687   total loss: [0.33713987]\n",
            "mse_b ====== [0.188342348]\n",
            "It: 3320, Time: 0.02\n",
            "mse_b  [0.18834235]  mse_f: 0.1478201001882553   total loss: [0.33616245]\n",
            "mse_b ====== [0.186627865]\n",
            "It: 3321, Time: 0.02\n",
            "mse_b  [0.18662786]  mse_f: 0.14574331045150757   total loss: [0.33237118]\n",
            "mse_b ====== [0.184697092]\n",
            "It: 3322, Time: 0.02\n",
            "mse_b  [0.18469709]  mse_f: 0.14661532640457153   total loss: [0.33131242]\n",
            "mse_b ====== [0.183071658]\n",
            "It: 3323, Time: 0.02\n",
            "mse_b  [0.18307166]  mse_f: 0.14820411801338196   total loss: [0.33127576]\n",
            "mse_b ====== [0.182032734]\n",
            "It: 3324, Time: 0.02\n",
            "mse_b  [0.18203273]  mse_f: 0.14530615508556366   total loss: [0.32733887]\n",
            "mse_b ====== [0.181654334]\n",
            "It: 3325, Time: 0.02\n",
            "mse_b  [0.18165433]  mse_f: 0.14397220313549042   total loss: [0.32562655]\n",
            "mse_b ====== [0.180621758]\n",
            "It: 3326, Time: 0.02\n",
            "mse_b  [0.18062176]  mse_f: 0.14351554214954376   total loss: [0.3241373]\n",
            "mse_b ====== [0.178543359]\n",
            "It: 3327, Time: 0.02\n",
            "mse_b  [0.17854336]  mse_f: 0.1420147866010666   total loss: [0.32055813]\n",
            "mse_b ====== [0.176839858]\n",
            "It: 3328, Time: 0.02\n",
            "mse_b  [0.17683986]  mse_f: 0.14332519471645355   total loss: [0.32016504]\n",
            "mse_b ====== [0.175546587]\n",
            "It: 3329, Time: 0.04\n",
            "mse_b  [0.17554659]  mse_f: 0.1416008621454239   total loss: [0.31714743]\n",
            "mse_b ====== [0.174222067]\n",
            "It: 3330, Time: 0.02\n",
            "mse_b  [0.17422207]  mse_f: 0.14024919271469116   total loss: [0.31447124]\n",
            "mse_b ====== [0.172949255]\n",
            "It: 3331, Time: 0.02\n",
            "mse_b  [0.17294925]  mse_f: 0.14121510088443756   total loss: [0.31416434]\n",
            "mse_b ====== [0.171669245]\n",
            "It: 3332, Time: 0.02\n",
            "mse_b  [0.17166924]  mse_f: 0.1387520283460617   total loss: [0.3104213]\n",
            "mse_b ====== [0.170510441]\n",
            "It: 3333, Time: 0.02\n",
            "mse_b  [0.17051044]  mse_f: 0.13869169354438782   total loss: [0.30920213]\n",
            "mse_b ====== [0.169385731]\n",
            "It: 3334, Time: 0.02\n",
            "mse_b  [0.16938573]  mse_f: 0.13875144720077515   total loss: [0.30813718]\n",
            "mse_b ====== [0.168436602]\n",
            "It: 3335, Time: 0.02\n",
            "mse_b  [0.1684366]  mse_f: 0.13622745871543884   total loss: [0.30466408]\n",
            "mse_b ====== [0.167433232]\n",
            "It: 3336, Time: 0.02\n",
            "mse_b  [0.16743323]  mse_f: 0.1367698311805725   total loss: [0.30420306]\n",
            "mse_b ====== [0.165500075]\n",
            "It: 3337, Time: 0.03\n",
            "mse_b  [0.16550007]  mse_f: 0.13580095767974854   total loss: [0.30130103]\n",
            "mse_b ====== [0.163425386]\n",
            "It: 3338, Time: 0.02\n",
            "mse_b  [0.16342539]  mse_f: 0.13645121455192566   total loss: [0.2998766]\n",
            "mse_b ====== [0.161869898]\n",
            "It: 3339, Time: 0.02\n",
            "mse_b  [0.1618699]  mse_f: 0.13644340634346008   total loss: [0.29831332]\n",
            "mse_b ====== [0.161007717]\n",
            "It: 3340, Time: 0.02\n",
            "mse_b  [0.16100772]  mse_f: 0.13350719213485718   total loss: [0.2945149]\n",
            "mse_b ====== [0.160621017]\n",
            "It: 3341, Time: 0.02\n",
            "mse_b  [0.16062102]  mse_f: 0.13448692858219147   total loss: [0.29510796]\n",
            "mse_b ====== [0.159007937]\n",
            "It: 3342, Time: 0.03\n",
            "mse_b  [0.15900794]  mse_f: 0.1327681541442871   total loss: [0.2917761]\n",
            "mse_b ====== [0.157092094]\n",
            "It: 3343, Time: 0.02\n",
            "mse_b  [0.1570921]  mse_f: 0.13310083746910095   total loss: [0.29019293]\n",
            "mse_b ====== [0.155945957]\n",
            "It: 3344, Time: 0.02\n",
            "mse_b  [0.15594596]  mse_f: 0.1318816840648651   total loss: [0.28782764]\n",
            "mse_b ====== [0.155403122]\n",
            "It: 3345, Time: 0.02\n",
            "mse_b  [0.15540312]  mse_f: 0.1295464187860489   total loss: [0.28494954]\n",
            "mse_b ====== [0.154658228]\n",
            "It: 3346, Time: 0.02\n",
            "mse_b  [0.15465823]  mse_f: 0.12978172302246094   total loss: [0.28443995]\n",
            "mse_b ====== [0.152821824]\n",
            "It: 3347, Time: 0.03\n",
            "mse_b  [0.15282182]  mse_f: 0.12769097089767456   total loss: [0.2805128]\n",
            "mse_b ====== [0.151277572]\n",
            "It: 3348, Time: 0.03\n",
            "mse_b  [0.15127757]  mse_f: 0.12935131788253784   total loss: [0.2806289]\n",
            "mse_b ====== [0.150458083]\n",
            "It: 3349, Time: 0.02\n",
            "mse_b  [0.15045808]  mse_f: 0.1278497278690338   total loss: [0.2783078]\n",
            "mse_b ====== [0.14983508]\n",
            "It: 3350, Time: 0.03\n",
            "mse_b  [0.14983508]  mse_f: 0.12768086791038513   total loss: [0.27751595]\n",
            "mse_b ====== [0.14796567]\n",
            "It: 3351, Time: 0.03\n",
            "mse_b  [0.14796567]  mse_f: 0.1272968053817749   total loss: [0.27526248]\n",
            "mse_b ====== [0.146038368]\n",
            "It: 3352, Time: 0.02\n",
            "mse_b  [0.14603837]  mse_f: 0.12724503874778748   total loss: [0.27328342]\n",
            "mse_b ====== [0.145065844]\n",
            "It: 3353, Time: 0.02\n",
            "mse_b  [0.14506584]  mse_f: 0.12694916129112244   total loss: [0.272015]\n",
            "mse_b ====== [0.144721881]\n",
            "It: 3354, Time: 0.02\n",
            "mse_b  [0.14472188]  mse_f: 0.12433575093746185   total loss: [0.26905763]\n",
            "mse_b ====== [0.144003049]\n",
            "It: 3355, Time: 0.03\n",
            "mse_b  [0.14400305]  mse_f: 0.12510791420936584   total loss: [0.26911098]\n",
            "mse_b ====== [0.142279327]\n",
            "It: 3356, Time: 0.02\n",
            "mse_b  [0.14227933]  mse_f: 0.12350799143314362   total loss: [0.2657873]\n",
            "mse_b ====== [0.140938401]\n",
            "It: 3357, Time: 0.03\n",
            "mse_b  [0.1409384]  mse_f: 0.12429600954055786   total loss: [0.2652344]\n",
            "mse_b ====== [0.140093669]\n",
            "It: 3358, Time: 0.02\n",
            "mse_b  [0.14009367]  mse_f: 0.12101253867149353   total loss: [0.2611062]\n",
            "mse_b ====== [0.139191434]\n",
            "It: 3359, Time: 0.02\n",
            "mse_b  [0.13919143]  mse_f: 0.12240679562091827   total loss: [0.26159823]\n",
            "mse_b ====== [0.137655228]\n",
            "It: 3360, Time: 0.02\n",
            "mse_b  [0.13765523]  mse_f: 0.1201377883553505   total loss: [0.257793]\n",
            "mse_b ====== [0.136782885]\n",
            "It: 3361, Time: 0.03\n",
            "mse_b  [0.13678288]  mse_f: 0.1205761656165123   total loss: [0.25735906]\n",
            "mse_b ====== [0.136768386]\n",
            "It: 3362, Time: 0.03\n",
            "mse_b  [0.13676839]  mse_f: 0.11685769259929657   total loss: [0.25362608]\n",
            "mse_b ====== [0.13629505]\n",
            "It: 3363, Time: 0.02\n",
            "mse_b  [0.13629505]  mse_f: 0.11789235472679138   total loss: [0.2541874]\n",
            "mse_b ====== [0.134526461]\n",
            "It: 3364, Time: 0.03\n",
            "mse_b  [0.13452646]  mse_f: 0.11638891696929932   total loss: [0.25091538]\n",
            "mse_b ====== [0.133504093]\n",
            "It: 3365, Time: 0.03\n",
            "mse_b  [0.1335041]  mse_f: 0.11746601015329361   total loss: [0.2509701]\n",
            "mse_b ====== [0.132882148]\n",
            "It: 3366, Time: 0.02\n",
            "mse_b  [0.13288215]  mse_f: 0.11537965387105942   total loss: [0.24826181]\n",
            "mse_b ====== [0.131629437]\n",
            "It: 3367, Time: 0.02\n",
            "mse_b  [0.13162944]  mse_f: 0.11661925166845322   total loss: [0.2482487]\n",
            "mse_b ====== [0.13052927]\n",
            "It: 3368, Time: 0.02\n",
            "mse_b  [0.13052927]  mse_f: 0.11473587900400162   total loss: [0.24526516]\n",
            "mse_b ====== [0.130160719]\n",
            "It: 3369, Time: 0.02\n",
            "mse_b  [0.13016072]  mse_f: 0.11514313519001007   total loss: [0.24530385]\n",
            "mse_b ====== [0.129395068]\n",
            "It: 3370, Time: 0.03\n",
            "mse_b  [0.12939507]  mse_f: 0.11267450451850891   total loss: [0.24206957]\n",
            "mse_b ====== [0.12795651]\n",
            "It: 3371, Time: 0.03\n",
            "mse_b  [0.12795651]  mse_f: 0.11403536796569824   total loss: [0.24199188]\n",
            "mse_b ====== [0.127168357]\n",
            "It: 3372, Time: 0.02\n",
            "mse_b  [0.12716836]  mse_f: 0.11144021153450012   total loss: [0.23860857]\n",
            "mse_b ====== [0.126522914]\n",
            "It: 3373, Time: 0.03\n",
            "mse_b  [0.12652291]  mse_f: 0.11328133940696716   total loss: [0.23980425]\n",
            "mse_b ====== [0.12522921]\n",
            "It: 3374, Time: 0.02\n",
            "mse_b  [0.12522921]  mse_f: 0.11113713681697845   total loss: [0.23636635]\n",
            "mse_b ====== [0.124545388]\n",
            "It: 3375, Time: 0.02\n",
            "mse_b  [0.12454539]  mse_f: 0.11147774755954742   total loss: [0.23602313]\n",
            "mse_b ====== [0.12405239]\n",
            "It: 3376, Time: 0.02\n",
            "mse_b  [0.12405239]  mse_f: 0.10922913253307343   total loss: [0.23328152]\n",
            "mse_b ====== [0.122686252]\n",
            "It: 3377, Time: 0.03\n",
            "mse_b  [0.12268625]  mse_f: 0.11093083024024963   total loss: [0.23361708]\n",
            "mse_b ====== [0.121817499]\n",
            "It: 3378, Time: 0.02\n",
            "mse_b  [0.1218175]  mse_f: 0.10849682986736298   total loss: [0.23031433]\n",
            "mse_b ====== [0.121004194]\n",
            "It: 3379, Time: 0.02\n",
            "mse_b  [0.12100419]  mse_f: 0.10957849770784378   total loss: [0.23058268]\n",
            "mse_b ====== [0.119782723]\n",
            "It: 3380, Time: 0.03\n",
            "mse_b  [0.11978272]  mse_f: 0.10844296216964722   total loss: [0.22822568]\n",
            "mse_b ====== [0.119343832]\n",
            "It: 3381, Time: 0.02\n",
            "mse_b  [0.11934383]  mse_f: 0.10833722352981567   total loss: [0.22768106]\n",
            "mse_b ====== [0.118482746]\n",
            "It: 3382, Time: 0.02\n",
            "mse_b  [0.11848275]  mse_f: 0.10680339485406876   total loss: [0.22528614]\n",
            "mse_b ====== [0.117355771]\n",
            "It: 3383, Time: 0.02\n",
            "mse_b  [0.11735577]  mse_f: 0.10835758596658707   total loss: [0.22571336]\n",
            "mse_b ====== [0.117162526]\n",
            "It: 3384, Time: 0.02\n",
            "mse_b  [0.11716253]  mse_f: 0.10659010708332062   total loss: [0.22375263]\n",
            "mse_b ====== [0.115666449]\n",
            "It: 3385, Time: 0.02\n",
            "mse_b  [0.11566645]  mse_f: 0.10742340981960297   total loss: [0.22308986]\n",
            "mse_b ====== [0.115026906]\n",
            "It: 3386, Time: 0.02\n",
            "mse_b  [0.11502691]  mse_f: 0.10627275705337524   total loss: [0.22129966]\n",
            "mse_b ====== [0.113815621]\n",
            "It: 3387, Time: 0.03\n",
            "mse_b  [0.11381562]  mse_f: 0.1067296713590622   total loss: [0.22054529]\n",
            "mse_b ====== [0.113149434]\n",
            "It: 3388, Time: 0.02\n",
            "mse_b  [0.11314943]  mse_f: 0.10538871586322784   total loss: [0.21853815]\n",
            "mse_b ====== [0.112621918]\n",
            "It: 3389, Time: 0.03\n",
            "mse_b  [0.11262192]  mse_f: 0.10503922402858734   total loss: [0.21766114]\n",
            "mse_b ====== [0.111054584]\n",
            "It: 3390, Time: 0.02\n",
            "mse_b  [0.11105458]  mse_f: 0.106292225420475   total loss: [0.21734682]\n",
            "mse_b ====== [0.111436285]\n",
            "It: 3391, Time: 0.02\n",
            "mse_b  [0.11143629]  mse_f: 0.10674101114273071   total loss: [0.21817729]\n",
            "mse_b ====== [0.108352736]\n",
            "It: 3392, Time: 0.03\n",
            "mse_b  [0.10835274]  mse_f: 0.11838813126087189   total loss: [0.22674087]\n",
            "mse_b ====== [0.113714576]\n",
            "It: 3393, Time: 0.02\n",
            "mse_b  [0.11371458]  mse_f: 0.1530558466911316   total loss: [0.26677042]\n",
            "mse_b ====== [0.111436807]\n",
            "It: 3394, Time: 0.03\n",
            "mse_b  [0.11143681]  mse_f: 0.4151216149330139   total loss: [0.5265584]\n",
            "mse_b ====== [0.131679043]\n",
            "It: 3395, Time: 0.02\n",
            "mse_b  [0.13167904]  mse_f: 0.7390817999839783   total loss: [0.87076086]\n",
            "mse_b ====== [0.320758492]\n",
            "It: 3396, Time: 0.02\n",
            "mse_b  [0.3207585]  mse_f: 1.811004638671875   total loss: [2.1317632]\n",
            "mse_b ====== [0.669032216]\n",
            "It: 3397, Time: 0.03\n",
            "mse_b  [0.6690322]  mse_f: 1.5126008987426758   total loss: [2.181633]\n",
            "mse_b ====== [0.446268767]\n",
            "It: 3398, Time: 0.02\n",
            "mse_b  [0.44626877]  mse_f: 0.39982619881629944   total loss: [0.84609497]\n",
            "mse_b ====== [0.189653814]\n",
            "It: 3399, Time: 0.02\n",
            "mse_b  [0.18965381]  mse_f: 2.4300377368927   total loss: [2.6196916]\n",
            "mse_b ====== [0.484306455]\n",
            "It: 3400, Time: 0.02\n",
            "mse_b  [0.48430645]  mse_f: 0.5019566416740417   total loss: [0.9862631]\n",
            "mse_b ====== [1.09899807]\n",
            "It: 3401, Time: 0.02\n",
            "mse_b  [1.0989981]  mse_f: 0.7901005744934082   total loss: [1.8890986]\n",
            "mse_b ====== [1.08285284]\n",
            "It: 3402, Time: 0.02\n",
            "mse_b  [1.0828528]  mse_f: 0.8909530639648438   total loss: [1.9738059]\n",
            "mse_b ====== [0.539428115]\n",
            "It: 3403, Time: 0.02\n",
            "mse_b  [0.5394281]  mse_f: 0.3792164921760559   total loss: [0.9186446]\n",
            "mse_b ====== [0.277021646]\n",
            "It: 3404, Time: 0.02\n",
            "mse_b  [0.27702165]  mse_f: 1.8296666145324707   total loss: [2.1066883]\n",
            "mse_b ====== [0.271466434]\n",
            "It: 3405, Time: 0.02\n",
            "mse_b  [0.27146643]  mse_f: 1.042952537536621   total loss: [1.314419]\n",
            "mse_b ====== [0.368323296]\n",
            "It: 3406, Time: 0.03\n",
            "mse_b  [0.3683233]  mse_f: 0.8885619640350342   total loss: [1.2568853]\n",
            "mse_b ====== [0.481235981]\n",
            "It: 3407, Time: 0.02\n",
            "mse_b  [0.48123598]  mse_f: 1.3660670518875122   total loss: [1.847303]\n",
            "mse_b ====== [0.486547]\n",
            "It: 3408, Time: 0.02\n",
            "mse_b  [0.486547]  mse_f: 0.9286007881164551   total loss: [1.4151478]\n",
            "mse_b ====== [0.407803595]\n",
            "It: 3409, Time: 0.02\n",
            "mse_b  [0.4078036]  mse_f: 0.7046801447868347   total loss: [1.1124837]\n",
            "mse_b ====== [0.352233648]\n",
            "It: 3410, Time: 0.02\n",
            "mse_b  [0.35223365]  mse_f: 1.5850337743759155   total loss: [1.9372674]\n",
            "mse_b ====== [0.341263562]\n",
            "It: 3411, Time: 0.03\n",
            "mse_b  [0.34126356]  mse_f: 1.0769493579864502   total loss: [1.4182129]\n",
            "mse_b ====== [0.359013855]\n",
            "It: 3412, Time: 0.02\n",
            "mse_b  [0.35901386]  mse_f: 0.8053212761878967   total loss: [1.1643351]\n",
            "mse_b ====== [0.359847516]\n",
            "It: 3413, Time: 0.02\n",
            "mse_b  [0.35984752]  mse_f: 1.3843939304351807   total loss: [1.7442415]\n",
            "mse_b ====== [0.297484577]\n",
            "It: 3414, Time: 0.02\n",
            "mse_b  [0.29748458]  mse_f: 1.1272575855255127   total loss: [1.4247422]\n",
            "mse_b ====== [0.236936927]\n",
            "It: 3415, Time: 0.02\n",
            "mse_b  [0.23693693]  mse_f: 0.6760485172271729   total loss: [0.91298544]\n",
            "mse_b ====== [0.250498384]\n",
            "It: 3416, Time: 0.02\n",
            "mse_b  [0.25049838]  mse_f: 1.4918136596679688   total loss: [1.7423121]\n",
            "mse_b ====== [0.248437077]\n",
            "It: 3417, Time: 0.02\n",
            "mse_b  [0.24843708]  mse_f: 0.7512248754501343   total loss: [0.9996619]\n",
            "mse_b ====== [0.287873983]\n",
            "It: 3418, Time: 0.02\n",
            "mse_b  [0.28787398]  mse_f: 0.8165701031684875   total loss: [1.104444]\n",
            "mse_b ====== [0.319373637]\n",
            "It: 3419, Time: 0.02\n",
            "mse_b  [0.31937364]  mse_f: 1.2375497817993164   total loss: [1.5569234]\n",
            "mse_b ====== [0.298155]\n",
            "It: 3420, Time: 0.02\n",
            "mse_b  [0.298155]  mse_f: 1.0019848346710205   total loss: [1.3001399]\n",
            "mse_b ====== [0.25815174]\n",
            "It: 3421, Time: 0.02\n",
            "mse_b  [0.25815174]  mse_f: 0.7028347849845886   total loss: [0.9609865]\n",
            "mse_b ====== [0.244637653]\n",
            "It: 3422, Time: 0.02\n",
            "mse_b  [0.24463765]  mse_f: 1.3150379657745361   total loss: [1.5596756]\n",
            "mse_b ====== [0.244632214]\n",
            "It: 3423, Time: 0.03\n",
            "mse_b  [0.24463221]  mse_f: 0.8344386219978333   total loss: [1.0790708]\n",
            "mse_b ====== [0.313080728]\n",
            "It: 3424, Time: 0.02\n",
            "mse_b  [0.31308073]  mse_f: 0.6814914345741272   total loss: [0.99457216]\n",
            "mse_b ====== [0.38191095]\n",
            "It: 3425, Time: 0.02\n",
            "mse_b  [0.38191095]  mse_f: 1.1441893577575684   total loss: [1.5261003]\n",
            "mse_b ====== [0.346414864]\n",
            "It: 3426, Time: 0.02\n",
            "mse_b  [0.34641486]  mse_f: 1.143322229385376   total loss: [1.489737]\n",
            "mse_b ====== [0.25314644]\n",
            "It: 3427, Time: 0.02\n",
            "mse_b  [0.25314644]  mse_f: 0.6418472528457642   total loss: [0.89499366]\n",
            "mse_b ====== [0.230071515]\n",
            "It: 3428, Time: 0.02\n",
            "mse_b  [0.23007151]  mse_f: 1.0161164999008179   total loss: [1.246188]\n",
            "mse_b ====== [0.248313606]\n",
            "It: 3429, Time: 0.03\n",
            "mse_b  [0.2483136]  mse_f: 0.9311420321464539   total loss: [1.1794556]\n",
            "mse_b ====== [0.234285027]\n",
            "It: 3430, Time: 0.02\n",
            "mse_b  [0.23428503]  mse_f: 0.5303661823272705   total loss: [0.7646512]\n",
            "mse_b ====== [0.239020571]\n",
            "It: 3431, Time: 0.02\n",
            "mse_b  [0.23902057]  mse_f: 1.022873044013977   total loss: [1.2618936]\n",
            "mse_b ====== [0.234655201]\n",
            "It: 3432, Time: 0.03\n",
            "mse_b  [0.2346552]  mse_f: 1.021773338317871   total loss: [1.2564285]\n",
            "mse_b ====== [0.222074091]\n",
            "It: 3433, Time: 0.03\n",
            "mse_b  [0.22207409]  mse_f: 0.5458439588546753   total loss: [0.76791805]\n",
            "mse_b ====== [0.222927853]\n",
            "It: 3434, Time: 0.02\n",
            "mse_b  [0.22292785]  mse_f: 0.8662874698638916   total loss: [1.0892153]\n",
            "mse_b ====== [0.223544404]\n",
            "It: 3435, Time: 0.02\n",
            "mse_b  [0.2235444]  mse_f: 1.0646461248397827   total loss: [1.2881905]\n",
            "mse_b ====== [0.212287232]\n",
            "It: 3436, Time: 0.02\n",
            "mse_b  [0.21228723]  mse_f: 0.6444492936134338   total loss: [0.85673654]\n",
            "mse_b ====== [0.223281354]\n",
            "It: 3437, Time: 0.02\n",
            "mse_b  [0.22328135]  mse_f: 0.8844390511512756   total loss: [1.1077204]\n",
            "mse_b ====== [0.235361576]\n",
            "It: 3438, Time: 0.03\n",
            "mse_b  [0.23536158]  mse_f: 1.1791579723358154   total loss: [1.4145195]\n",
            "mse_b ====== [0.222911835]\n",
            "It: 3439, Time: 0.02\n",
            "mse_b  [0.22291183]  mse_f: 0.6902197003364563   total loss: [0.91313154]\n",
            "mse_b ====== [0.209126]\n",
            "It: 3440, Time: 0.02\n",
            "mse_b  [0.209126]  mse_f: 0.7228233814239502   total loss: [0.9319494]\n",
            "mse_b ====== [0.209446788]\n",
            "It: 3441, Time: 0.02\n",
            "mse_b  [0.20944679]  mse_f: 1.1213222742080688   total loss: [1.3307691]\n",
            "mse_b ====== [0.211755425]\n",
            "It: 3442, Time: 0.02\n",
            "mse_b  [0.21175542]  mse_f: 0.778204619884491   total loss: [0.9899601]\n",
            "mse_b ====== [0.225192323]\n",
            "It: 3443, Time: 0.02\n",
            "mse_b  [0.22519232]  mse_f: 0.6032910943031311   total loss: [0.8284834]\n",
            "mse_b ====== [0.241621107]\n",
            "It: 3444, Time: 0.03\n",
            "mse_b  [0.2416211]  mse_f: 0.9394327402114868   total loss: [1.1810539]\n",
            "mse_b ====== [0.24807021]\n",
            "It: 3445, Time: 0.02\n",
            "mse_b  [0.24807021]  mse_f: 0.8107922077178955   total loss: [1.0588624]\n",
            "mse_b ====== [0.239558041]\n",
            "It: 3446, Time: 0.02\n",
            "mse_b  [0.23955804]  mse_f: 0.5119948387145996   total loss: [0.7515529]\n",
            "mse_b ====== [0.227433145]\n",
            "It: 3447, Time: 0.02\n",
            "mse_b  [0.22743315]  mse_f: 0.8460984230041504   total loss: [1.0735316]\n",
            "mse_b ====== [0.217407048]\n",
            "It: 3448, Time: 0.02\n",
            "mse_b  [0.21740705]  mse_f: 1.0208497047424316   total loss: [1.2382567]\n",
            "mse_b ====== [0.205956578]\n",
            "It: 3449, Time: 0.02\n",
            "mse_b  [0.20595658]  mse_f: 0.5432238578796387   total loss: [0.74918044]\n",
            "mse_b ====== [0.206807882]\n",
            "It: 3450, Time: 0.02\n",
            "mse_b  [0.20680788]  mse_f: 0.5494689345359802   total loss: [0.75627685]\n",
            "mse_b ====== [0.210584104]\n",
            "It: 3451, Time: 0.02\n",
            "mse_b  [0.2105841]  mse_f: 0.8423510193824768   total loss: [1.0529351]\n",
            "mse_b ====== [0.20485498]\n",
            "It: 3452, Time: 0.02\n",
            "mse_b  [0.20485498]  mse_f: 0.5899277329444885   total loss: [0.7947827]\n",
            "mse_b ====== [0.202928767]\n",
            "It: 3453, Time: 0.02\n",
            "mse_b  [0.20292877]  mse_f: 0.6323144435882568   total loss: [0.8352432]\n",
            "mse_b ====== [0.205104232]\n",
            "It: 3454, Time: 0.02\n",
            "mse_b  [0.20510423]  mse_f: 1.0855833292007446   total loss: [1.2906876]\n",
            "mse_b ====== [0.184267163]\n",
            "It: 3455, Time: 0.02\n",
            "mse_b  [0.18426716]  mse_f: 0.7976424694061279   total loss: [0.98190963]\n",
            "mse_b ====== [0.178119913]\n",
            "It: 3456, Time: 0.02\n",
            "mse_b  [0.17811991]  mse_f: 0.5687952041625977   total loss: [0.7469151]\n",
            "mse_b ====== [0.194087]\n",
            "It: 3457, Time: 0.02\n",
            "mse_b  [0.194087]  mse_f: 0.8167675137519836   total loss: [1.0108545]\n",
            "mse_b ====== [0.20812121]\n",
            "It: 3458, Time: 0.02\n",
            "mse_b  [0.20812121]  mse_f: 0.7069687247276306   total loss: [0.91508996]\n",
            "mse_b ====== [0.204429626]\n",
            "It: 3459, Time: 0.02\n",
            "mse_b  [0.20442963]  mse_f: 0.5131946802139282   total loss: [0.7176243]\n",
            "mse_b ====== [0.195327967]\n",
            "It: 3460, Time: 0.02\n",
            "mse_b  [0.19532797]  mse_f: 0.7902469635009766   total loss: [0.98557496]\n",
            "mse_b ====== [0.190033987]\n",
            "It: 3461, Time: 0.02\n",
            "mse_b  [0.19003399]  mse_f: 0.9471879601478577   total loss: [1.1372219]\n",
            "mse_b ====== [0.177694738]\n",
            "It: 3462, Time: 0.02\n",
            "mse_b  [0.17769474]  mse_f: 0.521415114402771   total loss: [0.69910985]\n",
            "mse_b ====== [0.165447414]\n",
            "It: 3463, Time: 0.02\n",
            "mse_b  [0.16544741]  mse_f: 0.5445691347122192   total loss: [0.71001655]\n",
            "mse_b ====== [0.162633806]\n",
            "It: 3464, Time: 0.03\n",
            "mse_b  [0.1626338]  mse_f: 0.7502903342247009   total loss: [0.9129242]\n",
            "mse_b ====== [0.176788554]\n",
            "It: 3465, Time: 0.03\n",
            "mse_b  [0.17678855]  mse_f: 0.493188738822937   total loss: [0.6699773]\n",
            "mse_b ====== [0.212431371]\n",
            "It: 3466, Time: 0.02\n",
            "mse_b  [0.21243137]  mse_f: 0.4940473437309265   total loss: [0.7064787]\n",
            "mse_b ====== [0.219904631]\n",
            "It: 3467, Time: 0.02\n",
            "mse_b  [0.21990463]  mse_f: 0.8306131362915039   total loss: [1.0505178]\n",
            "mse_b ====== [0.180093661]\n",
            "It: 3468, Time: 0.03\n",
            "mse_b  [0.18009366]  mse_f: 0.5254108309745789   total loss: [0.7055045]\n",
            "mse_b ====== [0.184746906]\n",
            "It: 3469, Time: 0.02\n",
            "mse_b  [0.1847469]  mse_f: 0.4175213873386383   total loss: [0.6022683]\n",
            "mse_b ====== [0.24543485]\n",
            "It: 3470, Time: 0.03\n",
            "mse_b  [0.24543485]  mse_f: 0.5177916288375854   total loss: [0.7632265]\n",
            "mse_b ====== [0.315409064]\n",
            "It: 3471, Time: 0.02\n",
            "mse_b  [0.31540906]  mse_f: 0.45231562852859497   total loss: [0.7677247]\n",
            "mse_b ====== [0.331938028]\n",
            "It: 3472, Time: 0.03\n",
            "mse_b  [0.33193803]  mse_f: 0.3442438244819641   total loss: [0.67618185]\n",
            "mse_b ====== [0.289397299]\n",
            "It: 3473, Time: 0.03\n",
            "mse_b  [0.2893973]  mse_f: 0.5609230995178223   total loss: [0.8503204]\n",
            "mse_b ====== [0.232058167]\n",
            "It: 3474, Time: 0.02\n",
            "mse_b  [0.23205817]  mse_f: 0.7090966105461121   total loss: [0.9411548]\n",
            "mse_b ====== [0.190293163]\n",
            "It: 3475, Time: 0.02\n",
            "mse_b  [0.19029316]  mse_f: 0.5035465955734253   total loss: [0.6938398]\n",
            "mse_b ====== [0.169618726]\n",
            "It: 3476, Time: 0.02\n",
            "mse_b  [0.16961873]  mse_f: 0.41702792048454285   total loss: [0.5866467]\n",
            "mse_b ====== [0.175906986]\n",
            "It: 3477, Time: 0.02\n",
            "mse_b  [0.17590699]  mse_f: 0.6207404732704163   total loss: [0.7966474]\n",
            "mse_b ====== [0.213728979]\n",
            "It: 3478, Time: 0.02\n",
            "mse_b  [0.21372898]  mse_f: 0.49712926149368286   total loss: [0.7108582]\n",
            "mse_b ====== [0.274453729]\n",
            "It: 3479, Time: 0.02\n",
            "mse_b  [0.27445373]  mse_f: 0.3571307361125946   total loss: [0.63158447]\n",
            "mse_b ====== [0.303321868]\n",
            "It: 3480, Time: 0.03\n",
            "mse_b  [0.30332187]  mse_f: 0.5997036695480347   total loss: [0.9030255]\n",
            "mse_b ====== [0.247984439]\n",
            "It: 3481, Time: 0.02\n",
            "mse_b  [0.24798444]  mse_f: 0.5622451901435852   total loss: [0.81022966]\n",
            "mse_b ====== [0.197125077]\n",
            "It: 3482, Time: 0.03\n",
            "mse_b  [0.19712508]  mse_f: 0.48456820845603943   total loss: [0.6816933]\n",
            "mse_b ====== [0.183469176]\n",
            "It: 3483, Time: 0.03\n",
            "mse_b  [0.18346918]  mse_f: 0.530655562877655   total loss: [0.71412474]\n",
            "mse_b ====== [0.195820034]\n",
            "It: 3484, Time: 0.02\n",
            "mse_b  [0.19582003]  mse_f: 0.40970808267593384   total loss: [0.6055281]\n",
            "mse_b ====== [0.215765402]\n",
            "It: 3485, Time: 0.02\n",
            "mse_b  [0.2157654]  mse_f: 0.3132210373878479   total loss: [0.52898645]\n",
            "mse_b ====== [0.230126292]\n",
            "It: 3486, Time: 0.02\n",
            "mse_b  [0.23012629]  mse_f: 0.4388170838356018   total loss: [0.6689434]\n",
            "mse_b ====== [0.23136121]\n",
            "It: 3487, Time: 0.02\n",
            "mse_b  [0.23136121]  mse_f: 0.6080727577209473   total loss: [0.83943397]\n",
            "mse_b ====== [0.211633652]\n",
            "It: 3488, Time: 0.02\n",
            "mse_b  [0.21163365]  mse_f: 0.47578293085098267   total loss: [0.68741655]\n",
            "mse_b ====== [0.175974146]\n",
            "It: 3489, Time: 0.02\n",
            "mse_b  [0.17597415]  mse_f: 0.4449806213378906   total loss: [0.62095475]\n",
            "mse_b ====== [0.141708717]\n",
            "It: 3490, Time: 0.03\n",
            "mse_b  [0.14170872]  mse_f: 0.5816953182220459   total loss: [0.72340405]\n",
            "mse_b ====== [0.12982896]\n",
            "It: 3491, Time: 0.02\n",
            "mse_b  [0.12982896]  mse_f: 0.6083357334136963   total loss: [0.73816466]\n",
            "mse_b ====== [0.149536267]\n",
            "It: 3492, Time: 0.02\n",
            "mse_b  [0.14953627]  mse_f: 0.4346875548362732   total loss: [0.5842238]\n",
            "mse_b ====== [0.191878706]\n",
            "It: 3493, Time: 0.02\n",
            "mse_b  [0.1918787]  mse_f: 0.4726812243461609   total loss: [0.66455996]\n",
            "mse_b ====== [0.217277184]\n",
            "It: 3494, Time: 0.03\n",
            "mse_b  [0.21727718]  mse_f: 0.509614109992981   total loss: [0.7268913]\n",
            "mse_b ====== [0.199931189]\n",
            "It: 3495, Time: 0.02\n",
            "mse_b  [0.19993119]  mse_f: 0.372039794921875   total loss: [0.571971]\n",
            "mse_b ====== [0.179757699]\n",
            "It: 3496, Time: 0.02\n",
            "mse_b  [0.1797577]  mse_f: 0.49542856216430664   total loss: [0.6751863]\n",
            "mse_b ====== [0.170620561]\n",
            "It: 3497, Time: 0.02\n",
            "mse_b  [0.17062056]  mse_f: 0.59918212890625   total loss: [0.7698027]\n",
            "mse_b ====== [0.16117774]\n",
            "It: 3498, Time: 0.02\n",
            "mse_b  [0.16117774]  mse_f: 0.41010963916778564   total loss: [0.5712874]\n",
            "mse_b ====== [0.149495929]\n",
            "It: 3499, Time: 0.02\n",
            "mse_b  [0.14949593]  mse_f: 0.38288116455078125   total loss: [0.5323771]\n",
            "mse_b ====== [0.143415198]\n",
            "It: 3500, Time: 0.02\n",
            "mse_b  [0.1434152]  mse_f: 0.5667785406112671   total loss: [0.71019375]\n",
            "mse_b ====== [0.1439659]\n",
            "It: 3501, Time: 0.02\n",
            "mse_b  [0.1439659]  mse_f: 0.5147899389266968   total loss: [0.65875584]\n",
            "mse_b ====== [0.147679031]\n",
            "It: 3502, Time: 0.03\n",
            "mse_b  [0.14767903]  mse_f: 0.3599402904510498   total loss: [0.5076193]\n",
            "mse_b ====== [0.155405074]\n",
            "It: 3503, Time: 0.03\n",
            "mse_b  [0.15540507]  mse_f: 0.4484415650367737   total loss: [0.60384667]\n",
            "mse_b ====== [0.163376629]\n",
            "It: 3504, Time: 0.03\n",
            "mse_b  [0.16337663]  mse_f: 0.5168080925941467   total loss: [0.6801847]\n",
            "mse_b ====== [0.162949]\n",
            "It: 3505, Time: 0.02\n",
            "mse_b  [0.162949]  mse_f: 0.34818553924560547   total loss: [0.5111345]\n",
            "mse_b ====== [0.155566022]\n",
            "It: 3506, Time: 0.02\n",
            "mse_b  [0.15556602]  mse_f: 0.31447890400886536   total loss: [0.4700449]\n",
            "mse_b ====== [0.15192087]\n",
            "It: 3507, Time: 0.02\n",
            "mse_b  [0.15192087]  mse_f: 0.4834897518157959   total loss: [0.6354106]\n",
            "mse_b ====== [0.156454906]\n",
            "It: 3508, Time: 0.03\n",
            "mse_b  [0.1564549]  mse_f: 0.4338100850582123   total loss: [0.590265]\n",
            "mse_b ====== [0.165029258]\n",
            "It: 3509, Time: 0.03\n",
            "mse_b  [0.16502926]  mse_f: 0.2934890687465668   total loss: [0.45851833]\n",
            "mse_b ====== [0.169920146]\n",
            "It: 3510, Time: 0.03\n",
            "mse_b  [0.16992015]  mse_f: 0.4492129385471344   total loss: [0.6191331]\n",
            "mse_b ====== [0.166849881]\n",
            "It: 3511, Time: 0.02\n",
            "mse_b  [0.16684988]  mse_f: 0.5295206904411316   total loss: [0.6963706]\n",
            "mse_b ====== [0.165462047]\n",
            "It: 3512, Time: 0.02\n",
            "mse_b  [0.16546205]  mse_f: 0.3656090497970581   total loss: [0.53107107]\n",
            "mse_b ====== [0.170921177]\n",
            "It: 3513, Time: 0.02\n",
            "mse_b  [0.17092118]  mse_f: 0.4036686420440674   total loss: [0.57458985]\n",
            "mse_b ====== [0.152806312]\n",
            "It: 3514, Time: 0.02\n",
            "mse_b  [0.15280631]  mse_f: 0.4202697277069092   total loss: [0.573076]\n",
            "mse_b ====== [0.129001826]\n",
            "It: 3515, Time: 0.02\n",
            "mse_b  [0.12900183]  mse_f: 0.2628909647464752   total loss: [0.3918928]\n",
            "mse_b ====== [0.140036479]\n",
            "It: 3516, Time: 0.02\n",
            "mse_b  [0.14003648]  mse_f: 0.3111080527305603   total loss: [0.45114452]\n",
            "mse_b ====== [0.185198933]\n",
            "It: 3517, Time: 0.02\n",
            "mse_b  [0.18519893]  mse_f: 0.35953909158706665   total loss: [0.54473805]\n",
            "mse_b ====== [0.239182681]\n",
            "It: 3518, Time: 0.02\n",
            "mse_b  [0.23918268]  mse_f: 0.357658714056015   total loss: [0.5968414]\n",
            "mse_b ====== [0.251228899]\n",
            "It: 3519, Time: 0.02\n",
            "mse_b  [0.2512289]  mse_f: 0.3344185948371887   total loss: [0.58564746]\n",
            "mse_b ====== [0.21213606]\n",
            "It: 3520, Time: 0.02\n",
            "mse_b  [0.21213606]  mse_f: 0.37346845865249634   total loss: [0.58560455]\n",
            "mse_b ====== [0.16634497]\n",
            "It: 3521, Time: 0.02\n",
            "mse_b  [0.16634497]  mse_f: 0.34218165278434753   total loss: [0.5085266]\n",
            "mse_b ====== [0.144678771]\n",
            "It: 3522, Time: 0.02\n",
            "mse_b  [0.14467877]  mse_f: 0.28595006465911865   total loss: [0.43062884]\n",
            "mse_b ====== [0.149312794]\n",
            "It: 3523, Time: 0.02\n",
            "mse_b  [0.1493128]  mse_f: 0.28936076164245605   total loss: [0.43867356]\n",
            "mse_b ====== [0.174760193]\n",
            "It: 3524, Time: 0.02\n",
            "mse_b  [0.1747602]  mse_f: 0.36451074481010437   total loss: [0.53927094]\n",
            "mse_b ====== [0.210513249]\n",
            "It: 3525, Time: 0.02\n",
            "mse_b  [0.21051325]  mse_f: 0.34902340173721313   total loss: [0.55953664]\n",
            "mse_b ====== [0.230357677]\n",
            "It: 3526, Time: 0.02\n",
            "mse_b  [0.23035768]  mse_f: 0.3353443443775177   total loss: [0.565702]\n",
            "mse_b ====== [0.216125935]\n",
            "It: 3527, Time: 0.03\n",
            "mse_b  [0.21612594]  mse_f: 0.2891692519187927   total loss: [0.50529516]\n",
            "mse_b ====== [0.192471534]\n",
            "It: 3528, Time: 0.02\n",
            "mse_b  [0.19247153]  mse_f: 0.2920535206794739   total loss: [0.48452505]\n",
            "mse_b ====== [0.180634692]\n",
            "It: 3529, Time: 0.02\n",
            "mse_b  [0.18063469]  mse_f: 0.2775709927082062   total loss: [0.4582057]\n",
            "mse_b ====== [0.184398234]\n",
            "It: 3530, Time: 0.03\n",
            "mse_b  [0.18439823]  mse_f: 0.2635493278503418   total loss: [0.44794756]\n",
            "mse_b ====== [0.194067672]\n",
            "It: 3531, Time: 0.03\n",
            "mse_b  [0.19406767]  mse_f: 0.2542228400707245   total loss: [0.44829053]\n",
            "mse_b ====== [0.193298936]\n",
            "It: 3532, Time: 0.02\n",
            "mse_b  [0.19329894]  mse_f: 0.2720486521720886   total loss: [0.4653476]\n",
            "mse_b ====== [0.180796757]\n",
            "It: 3533, Time: 0.02\n",
            "mse_b  [0.18079676]  mse_f: 0.28176796436309814   total loss: [0.4625647]\n",
            "mse_b ====== [0.173685014]\n",
            "It: 3534, Time: 0.02\n",
            "mse_b  [0.17368501]  mse_f: 0.2944735884666443   total loss: [0.4681586]\n",
            "mse_b ====== [0.181618929]\n",
            "It: 3535, Time: 0.02\n",
            "mse_b  [0.18161893]  mse_f: 0.3093399703502655   total loss: [0.4909589]\n",
            "mse_b ====== [0.193583384]\n",
            "It: 3536, Time: 0.02\n",
            "mse_b  [0.19358338]  mse_f: 0.32503825426101685   total loss: [0.5186216]\n",
            "mse_b ====== [0.195851207]\n",
            "It: 3537, Time: 0.02\n",
            "mse_b  [0.1958512]  mse_f: 0.3315852880477905   total loss: [0.5274365]\n",
            "mse_b ====== [0.19292888]\n",
            "It: 3538, Time: 0.03\n",
            "mse_b  [0.19292888]  mse_f: 0.2993658781051636   total loss: [0.49229476]\n",
            "mse_b ====== [0.198235631]\n",
            "It: 3539, Time: 0.02\n",
            "mse_b  [0.19823563]  mse_f: 0.2313011735677719   total loss: [0.42953682]\n",
            "mse_b ====== [0.208160162]\n",
            "It: 3540, Time: 0.03\n",
            "mse_b  [0.20816016]  mse_f: 0.18245145678520203   total loss: [0.39061162]\n",
            "mse_b ====== [0.205795094]\n",
            "It: 3541, Time: 0.02\n",
            "mse_b  [0.2057951]  mse_f: 0.20454049110412598   total loss: [0.4103356]\n",
            "mse_b ====== [0.192083865]\n",
            "It: 3542, Time: 0.02\n",
            "mse_b  [0.19208387]  mse_f: 0.26569366455078125   total loss: [0.45777753]\n",
            "mse_b ====== [0.179204673]\n",
            "It: 3543, Time: 0.02\n",
            "mse_b  [0.17920467]  mse_f: 0.32799023389816284   total loss: [0.5071949]\n",
            "mse_b ====== [0.172299027]\n",
            "It: 3544, Time: 0.02\n",
            "mse_b  [0.17229903]  mse_f: 0.32544273138046265   total loss: [0.49774176]\n",
            "mse_b ====== [0.170660779]\n",
            "It: 3545, Time: 0.03\n",
            "mse_b  [0.17066078]  mse_f: 0.2903384566307068   total loss: [0.46099925]\n",
            "mse_b ====== [0.171128839]\n",
            "It: 3546, Time: 0.02\n",
            "mse_b  [0.17112884]  mse_f: 0.24278834462165833   total loss: [0.41391718]\n",
            "mse_b ====== [0.16888234]\n",
            "It: 3547, Time: 0.02\n",
            "mse_b  [0.16888234]  mse_f: 0.23677115142345428   total loss: [0.40565348]\n",
            "mse_b ====== [0.160616204]\n",
            "It: 3548, Time: 0.03\n",
            "mse_b  [0.1606162]  mse_f: 0.2436276376247406   total loss: [0.40424383]\n",
            "mse_b ====== [0.150200009]\n",
            "It: 3549, Time: 0.02\n",
            "mse_b  [0.15020001]  mse_f: 0.28280526399612427   total loss: [0.43300527]\n",
            "mse_b ====== [0.144590691]\n",
            "It: 3550, Time: 0.03\n",
            "mse_b  [0.14459069]  mse_f: 0.30908361077308655   total loss: [0.45367432]\n",
            "mse_b ====== [0.146395266]\n",
            "It: 3551, Time: 0.02\n",
            "mse_b  [0.14639527]  mse_f: 0.3273688554763794   total loss: [0.47376412]\n",
            "mse_b ====== [0.151509017]\n",
            "It: 3552, Time: 0.02\n",
            "mse_b  [0.15150902]  mse_f: 0.30428817868232727   total loss: [0.4557972]\n",
            "mse_b ====== [0.151599213]\n",
            "It: 3553, Time: 0.03\n",
            "mse_b  [0.15159921]  mse_f: 0.29655659198760986   total loss: [0.44815582]\n",
            "mse_b ====== [0.141768664]\n",
            "It: 3554, Time: 0.03\n",
            "mse_b  [0.14176866]  mse_f: 0.26914888620376587   total loss: [0.41091755]\n",
            "mse_b ====== [0.131345168]\n",
            "It: 3555, Time: 0.03\n",
            "mse_b  [0.13134517]  mse_f: 0.2585594952106476   total loss: [0.38990468]\n",
            "mse_b ====== [0.132824391]\n",
            "It: 3556, Time: 0.02\n",
            "mse_b  [0.13282439]  mse_f: 0.2501273453235626   total loss: [0.38295174]\n",
            "mse_b ====== [0.146471754]\n",
            "It: 3557, Time: 0.02\n",
            "mse_b  [0.14647175]  mse_f: 0.2653178572654724   total loss: [0.4117896]\n",
            "mse_b ====== [0.160548553]\n",
            "It: 3558, Time: 0.03\n",
            "mse_b  [0.16054855]  mse_f: 0.24948088824748993   total loss: [0.41002944]\n",
            "mse_b ====== [0.163898543]\n",
            "It: 3559, Time: 0.02\n",
            "mse_b  [0.16389854]  mse_f: 0.2448863387107849   total loss: [0.40878487]\n",
            "mse_b ====== [0.155470878]\n",
            "It: 3560, Time: 0.02\n",
            "mse_b  [0.15547088]  mse_f: 0.23570629954338074   total loss: [0.39117718]\n",
            "mse_b ====== [0.142122895]\n",
            "It: 3561, Time: 0.02\n",
            "mse_b  [0.1421229]  mse_f: 0.25813862681388855   total loss: [0.40026152]\n",
            "mse_b ====== [0.129878834]\n",
            "It: 3562, Time: 0.03\n",
            "mse_b  [0.12987883]  mse_f: 0.26909345388412476   total loss: [0.39897227]\n",
            "mse_b ====== [0.123720512]\n",
            "It: 3563, Time: 0.03\n",
            "mse_b  [0.12372051]  mse_f: 0.2874613106250763   total loss: [0.4111818]\n",
            "mse_b ====== [0.126107514]\n",
            "It: 3564, Time: 0.03\n",
            "mse_b  [0.12610751]  mse_f: 0.2546157240867615   total loss: [0.38072324]\n",
            "mse_b ====== [0.13191539]\n",
            "It: 3565, Time: 0.02\n",
            "mse_b  [0.13191539]  mse_f: 0.22080592811107635   total loss: [0.35272133]\n",
            "mse_b ====== [0.133582279]\n",
            "It: 3566, Time: 0.02\n",
            "mse_b  [0.13358228]  mse_f: 0.208504319190979   total loss: [0.3420866]\n",
            "mse_b ====== [0.1311194]\n",
            "It: 3567, Time: 0.03\n",
            "mse_b  [0.1311194]  mse_f: 0.25765708088874817   total loss: [0.38877648]\n",
            "mse_b ====== [0.127766356]\n",
            "It: 3568, Time: 0.02\n",
            "mse_b  [0.12776636]  mse_f: 0.285030335187912   total loss: [0.41279668]\n",
            "mse_b ====== [0.124231339]\n",
            "It: 3569, Time: 0.02\n",
            "mse_b  [0.12423134]  mse_f: 0.2952958345413208   total loss: [0.41952717]\n",
            "mse_b ====== [0.119781978]\n",
            "It: 3570, Time: 0.02\n",
            "mse_b  [0.11978198]  mse_f: 0.2634909152984619   total loss: [0.3832729]\n",
            "mse_b ====== [0.115778059]\n",
            "It: 3571, Time: 0.02\n",
            "mse_b  [0.11577806]  mse_f: 0.23085807263851166   total loss: [0.34663612]\n",
            "mse_b ====== [0.114402436]\n",
            "It: 3572, Time: 0.02\n",
            "mse_b  [0.11440244]  mse_f: 0.19750916957855225   total loss: [0.3119116]\n",
            "mse_b ====== [0.116333254]\n",
            "It: 3573, Time: 0.03\n",
            "mse_b  [0.11633325]  mse_f: 0.22279459238052368   total loss: [0.33912784]\n",
            "mse_b ====== [0.119012728]\n",
            "It: 3574, Time: 0.02\n",
            "mse_b  [0.11901273]  mse_f: 0.2580130696296692   total loss: [0.37702578]\n",
            "mse_b ====== [0.118759282]\n",
            "It: 3575, Time: 0.03\n",
            "mse_b  [0.11875928]  mse_f: 0.28611981868743896   total loss: [0.4048791]\n",
            "mse_b ====== [0.115067437]\n",
            "It: 3576, Time: 0.02\n",
            "mse_b  [0.11506744]  mse_f: 0.2845841348171234   total loss: [0.3996516]\n",
            "mse_b ====== [0.110651888]\n",
            "It: 3577, Time: 0.02\n",
            "mse_b  [0.11065189]  mse_f: 0.27859365940093994   total loss: [0.38924554]\n",
            "mse_b ====== [0.107828155]\n",
            "It: 3578, Time: 0.03\n",
            "mse_b  [0.10782816]  mse_f: 0.23776878416538239   total loss: [0.34559694]\n",
            "mse_b ====== [0.107417509]\n",
            "It: 3579, Time: 0.03\n",
            "mse_b  [0.10741751]  mse_f: 0.21050001680850983   total loss: [0.31791753]\n",
            "mse_b ====== [0.109177917]\n",
            "It: 3580, Time: 0.02\n",
            "mse_b  [0.10917792]  mse_f: 0.2088094800710678   total loss: [0.31798738]\n",
            "mse_b ====== [0.112196922]\n",
            "It: 3581, Time: 0.02\n",
            "mse_b  [0.11219692]  mse_f: 0.2316390872001648   total loss: [0.343836]\n",
            "mse_b ====== [0.114354409]\n",
            "It: 3582, Time: 0.02\n",
            "mse_b  [0.11435441]  mse_f: 0.24726977944374084   total loss: [0.36162418]\n",
            "mse_b ====== [0.113741755]\n",
            "It: 3583, Time: 0.02\n",
            "mse_b  [0.11374176]  mse_f: 0.2621760964393616   total loss: [0.37591785]\n",
            "mse_b ====== [0.110685445]\n",
            "It: 3584, Time: 0.02\n",
            "mse_b  [0.11068545]  mse_f: 0.25105416774749756   total loss: [0.3617396]\n",
            "mse_b ====== [0.10699904]\n",
            "It: 3585, Time: 0.02\n",
            "mse_b  [0.10699904]  mse_f: 0.2228785753250122   total loss: [0.32987761]\n",
            "mse_b ====== [0.103991821]\n",
            "It: 3586, Time: 0.02\n",
            "mse_b  [0.10399182]  mse_f: 0.20675066113471985   total loss: [0.3107425]\n",
            "mse_b ====== [0.102951489]\n",
            "It: 3587, Time: 0.03\n",
            "mse_b  [0.10295149]  mse_f: 0.22548839449882507   total loss: [0.3284399]\n",
            "mse_b ====== [0.104671851]\n",
            "It: 3588, Time: 0.02\n",
            "mse_b  [0.10467185]  mse_f: 0.24301335215568542   total loss: [0.34768522]\n",
            "mse_b ====== [0.107608557]\n",
            "It: 3589, Time: 0.02\n",
            "mse_b  [0.10760856]  mse_f: 0.23828190565109253   total loss: [0.34589046]\n",
            "mse_b ====== [0.108422376]\n",
            "It: 3590, Time: 0.03\n",
            "mse_b  [0.10842238]  mse_f: 0.2231275737285614   total loss: [0.33154994]\n",
            "mse_b ====== [0.105947897]\n",
            "It: 3591, Time: 0.02\n",
            "mse_b  [0.1059479]  mse_f: 0.198451966047287   total loss: [0.30439985]\n",
            "mse_b ====== [0.102316849]\n",
            "It: 3592, Time: 0.03\n",
            "mse_b  [0.10231685]  mse_f: 0.1921064257621765   total loss: [0.29442328]\n",
            "mse_b ====== [0.0989471823]\n",
            "It: 3593, Time: 0.02\n",
            "mse_b  [0.09894718]  mse_f: 0.22497673332691193   total loss: [0.32392392]\n",
            "mse_b ====== [0.0950867385]\n",
            "It: 3594, Time: 0.02\n",
            "mse_b  [0.09508674]  mse_f: 0.2662864923477173   total loss: [0.36137325]\n",
            "mse_b ====== [0.0913820043]\n",
            "It: 3595, Time: 0.02\n",
            "mse_b  [0.091382]  mse_f: 0.2593907415866852   total loss: [0.35077274]\n",
            "mse_b ====== [0.0908410177]\n",
            "It: 3596, Time: 0.02\n",
            "mse_b  [0.09084102]  mse_f: 0.2218858301639557   total loss: [0.31272686]\n",
            "mse_b ====== [0.0942259654]\n",
            "It: 3597, Time: 0.02\n",
            "mse_b  [0.09422597]  mse_f: 0.1747601181268692   total loss: [0.26898608]\n",
            "mse_b ====== [0.098304905]\n",
            "It: 3598, Time: 0.03\n",
            "mse_b  [0.0983049]  mse_f: 0.16186968982219696   total loss: [0.2601746]\n",
            "mse_b ====== [0.0985880792]\n",
            "It: 3599, Time: 0.02\n",
            "mse_b  [0.09858808]  mse_f: 0.1922779381275177   total loss: [0.29086602]\n",
            "mse_b ====== [0.0950801224]\n",
            "It: 3600, Time: 0.02\n",
            "mse_b  [0.09508012]  mse_f: 0.23327244818210602   total loss: [0.32835257]\n",
            "mse_b ====== [0.09312924]\n",
            "It: 3601, Time: 0.02\n",
            "mse_b  [0.09312924]  mse_f: 0.2456589937210083   total loss: [0.33878824]\n",
            "mse_b ====== [0.0974986851]\n",
            "It: 3602, Time: 0.02\n",
            "mse_b  [0.09749869]  mse_f: 0.24374446272850037   total loss: [0.34124315]\n",
            "mse_b ====== [0.10750182]\n",
            "It: 3603, Time: 0.02\n",
            "mse_b  [0.10750182]  mse_f: 0.22104805707931519   total loss: [0.32854986]\n",
            "mse_b ====== [0.116592385]\n",
            "It: 3604, Time: 0.02\n",
            "mse_b  [0.11659238]  mse_f: 0.20063459873199463   total loss: [0.31722698]\n",
            "mse_b ====== [0.116483271]\n",
            "It: 3605, Time: 0.02\n",
            "mse_b  [0.11648327]  mse_f: 0.17649531364440918   total loss: [0.29297858]\n",
            "mse_b ====== [0.105069771]\n",
            "It: 3606, Time: 0.02\n",
            "mse_b  [0.10506977]  mse_f: 0.16936177015304565   total loss: [0.27443153]\n",
            "mse_b ====== [0.0888352916]\n",
            "It: 3607, Time: 0.03\n",
            "mse_b  [0.08883529]  mse_f: 0.16786819696426392   total loss: [0.2567035]\n",
            "mse_b ====== [0.0782307312]\n",
            "It: 3608, Time: 0.02\n",
            "mse_b  [0.07823073]  mse_f: 0.1913660168647766   total loss: [0.26959676]\n",
            "mse_b ====== [0.080384247]\n",
            "It: 3609, Time: 0.03\n",
            "mse_b  [0.08038425]  mse_f: 0.21134796738624573   total loss: [0.29173222]\n",
            "mse_b ====== [0.0935269]\n",
            "It: 3610, Time: 0.02\n",
            "mse_b  [0.0935269]  mse_f: 0.2263929843902588   total loss: [0.31991988]\n",
            "mse_b ====== [0.10807851]\n",
            "It: 3611, Time: 0.03\n",
            "mse_b  [0.10807851]  mse_f: 0.20940075814723969   total loss: [0.31747925]\n",
            "mse_b ====== [0.118170083]\n",
            "It: 3612, Time: 0.02\n",
            "mse_b  [0.11817008]  mse_f: 0.1897566169500351   total loss: [0.3079267]\n",
            "mse_b ====== [0.123786144]\n",
            "It: 3613, Time: 0.02\n",
            "mse_b  [0.12378614]  mse_f: 0.16678649187088013   total loss: [0.29057264]\n",
            "mse_b ====== [0.121103317]\n",
            "It: 3614, Time: 0.02\n",
            "mse_b  [0.12110332]  mse_f: 0.17162178456783295   total loss: [0.2927251]\n",
            "mse_b ====== [0.110386916]\n",
            "It: 3615, Time: 0.02\n",
            "mse_b  [0.11038692]  mse_f: 0.17711171507835388   total loss: [0.28749862]\n",
            "mse_b ====== [0.0999583378]\n",
            "It: 3616, Time: 0.02\n",
            "mse_b  [0.09995834]  mse_f: 0.18038125336170197   total loss: [0.2803396]\n",
            "mse_b ====== [0.0919999182]\n",
            "It: 3617, Time: 0.02\n",
            "mse_b  [0.09199992]  mse_f: 0.16645163297653198   total loss: [0.25845155]\n",
            "mse_b ====== [0.0874056295]\n",
            "It: 3618, Time: 0.03\n",
            "mse_b  [0.08740563]  mse_f: 0.16918912529945374   total loss: [0.25659475]\n",
            "mse_b ====== [0.0908113569]\n",
            "It: 3619, Time: 0.03\n",
            "mse_b  [0.09081136]  mse_f: 0.18411074578762054   total loss: [0.2749221]\n",
            "mse_b ====== [0.101933219]\n",
            "It: 3620, Time: 0.02\n",
            "mse_b  [0.10193322]  mse_f: 0.2135506123304367   total loss: [0.31548384]\n",
            "mse_b ====== [0.110957459]\n",
            "It: 3621, Time: 0.02\n",
            "mse_b  [0.11095746]  mse_f: 0.21164843440055847   total loss: [0.3226059]\n",
            "mse_b ====== [0.108683176]\n",
            "It: 3622, Time: 0.02\n",
            "mse_b  [0.10868318]  mse_f: 0.1807994842529297   total loss: [0.28948265]\n",
            "mse_b ====== [0.0965625942]\n",
            "It: 3623, Time: 0.02\n",
            "mse_b  [0.09656259]  mse_f: 0.13309499621391296   total loss: [0.22965759]\n",
            "mse_b ====== [0.0842820406]\n",
            "It: 3624, Time: 0.02\n",
            "mse_b  [0.08428204]  mse_f: 0.12648561596870422   total loss: [0.21076766]\n",
            "mse_b ====== [0.0805611834]\n",
            "It: 3625, Time: 0.02\n",
            "mse_b  [0.08056118]  mse_f: 0.14606717228889465   total loss: [0.22662836]\n",
            "mse_b ====== [0.0885759592]\n",
            "It: 3626, Time: 0.02\n",
            "mse_b  [0.08857596]  mse_f: 0.17747820913791656   total loss: [0.26605415]\n",
            "mse_b ====== [0.100132361]\n",
            "It: 3627, Time: 0.02\n",
            "mse_b  [0.10013236]  mse_f: 0.1889464557170868   total loss: [0.28907883]\n",
            "mse_b ====== [0.100549027]\n",
            "It: 3628, Time: 0.02\n",
            "mse_b  [0.10054903]  mse_f: 0.18364030122756958   total loss: [0.28418934]\n",
            "mse_b ====== [0.0985417366]\n",
            "It: 3629, Time: 0.03\n",
            "mse_b  [0.09854174]  mse_f: 0.16962718963623047   total loss: [0.26816893]\n",
            "mse_b ====== [0.100267164]\n",
            "It: 3630, Time: 0.02\n",
            "mse_b  [0.10026716]  mse_f: 0.158810555934906   total loss: [0.25907773]\n",
            "mse_b ====== [0.0988754928]\n",
            "It: 3631, Time: 0.03\n",
            "mse_b  [0.09887549]  mse_f: 0.15129706263542175   total loss: [0.25017256]\n",
            "mse_b ====== [0.0928872675]\n",
            "It: 3632, Time: 0.02\n",
            "mse_b  [0.09288727]  mse_f: 0.15749523043632507   total loss: [0.25038248]\n",
            "mse_b ====== [0.0902193636]\n",
            "It: 3633, Time: 0.02\n",
            "mse_b  [0.09021936]  mse_f: 0.13846786320209503   total loss: [0.22868723]\n",
            "mse_b ====== [0.0927215219]\n",
            "It: 3634, Time: 0.02\n",
            "mse_b  [0.09272152]  mse_f: 0.14601275324821472   total loss: [0.23873428]\n",
            "mse_b ====== [0.0966427848]\n",
            "It: 3635, Time: 0.03\n",
            "mse_b  [0.09664278]  mse_f: 0.14692217111587524   total loss: [0.24356496]\n",
            "mse_b ====== [0.100043088]\n",
            "It: 3636, Time: 0.02\n",
            "mse_b  [0.10004309]  mse_f: 0.1697222888469696   total loss: [0.26976538]\n",
            "mse_b ====== [0.102464899]\n",
            "It: 3637, Time: 0.02\n",
            "mse_b  [0.1024649]  mse_f: 0.1619524210691452   total loss: [0.26441732]\n",
            "mse_b ====== [0.102634348]\n",
            "It: 3638, Time: 0.02\n",
            "mse_b  [0.10263435]  mse_f: 0.1617499589920044   total loss: [0.2643843]\n",
            "mse_b ====== [0.0998110697]\n",
            "It: 3639, Time: 0.02\n",
            "mse_b  [0.09981107]  mse_f: 0.14625370502471924   total loss: [0.24606478]\n",
            "mse_b ====== [0.0960179567]\n",
            "It: 3640, Time: 0.02\n",
            "mse_b  [0.09601796]  mse_f: 0.16302406787872314   total loss: [0.25904202]\n",
            "mse_b ====== [0.0932398811]\n",
            "It: 3641, Time: 0.02\n",
            "mse_b  [0.09323988]  mse_f: 0.15363860130310059   total loss: [0.24687847]\n",
            "mse_b ====== [0.0925605372]\n",
            "It: 3642, Time: 0.03\n",
            "mse_b  [0.09256054]  mse_f: 0.15009693801403046   total loss: [0.24265748]\n",
            "mse_b ====== [0.0930474326]\n",
            "It: 3643, Time: 0.02\n",
            "mse_b  [0.09304743]  mse_f: 0.12803445756435394   total loss: [0.22108188]\n",
            "mse_b ====== [0.0916657522]\n",
            "It: 3644, Time: 0.02\n",
            "mse_b  [0.09166575]  mse_f: 0.13595864176750183   total loss: [0.22762439]\n",
            "mse_b ====== [0.0889878273]\n",
            "It: 3645, Time: 0.03\n",
            "mse_b  [0.08898783]  mse_f: 0.13965265452861786   total loss: [0.22864048]\n",
            "mse_b ====== [0.088088356]\n",
            "It: 3646, Time: 0.02\n",
            "mse_b  [0.08808836]  mse_f: 0.17324885725975037   total loss: [0.26133722]\n",
            "mse_b ====== [0.0894762948]\n",
            "It: 3647, Time: 0.02\n",
            "mse_b  [0.08947629]  mse_f: 0.16238364577293396   total loss: [0.25185993]\n",
            "mse_b ====== [0.0916300863]\n",
            "It: 3648, Time: 0.02\n",
            "mse_b  [0.09163009]  mse_f: 0.16230738162994385   total loss: [0.25393748]\n",
            "mse_b ====== [0.0903392211]\n",
            "It: 3649, Time: 0.02\n",
            "mse_b  [0.09033922]  mse_f: 0.12016616761684418   total loss: [0.2105054]\n",
            "mse_b ====== [0.0866406]\n",
            "It: 3650, Time: 0.02\n",
            "mse_b  [0.0866406]  mse_f: 0.1156538724899292   total loss: [0.20229447]\n",
            "mse_b ====== [0.0836814046]\n",
            "It: 3651, Time: 0.02\n",
            "mse_b  [0.0836814]  mse_f: 0.1122029721736908   total loss: [0.19588438]\n",
            "mse_b ====== [0.082168594]\n",
            "It: 3652, Time: 0.03\n",
            "mse_b  [0.08216859]  mse_f: 0.13987302780151367   total loss: [0.22204162]\n",
            "mse_b ====== [0.0806862935]\n",
            "It: 3653, Time: 0.02\n",
            "mse_b  [0.08068629]  mse_f: 0.14961190521717072   total loss: [0.23029819]\n",
            "mse_b ====== [0.0794852823]\n",
            "It: 3654, Time: 0.02\n",
            "mse_b  [0.07948528]  mse_f: 0.15206454694271088   total loss: [0.23154983]\n",
            "mse_b ====== [0.082144]\n",
            "It: 3655, Time: 0.02\n",
            "mse_b  [0.082144]  mse_f: 0.13150709867477417   total loss: [0.21365109]\n",
            "mse_b ====== [0.0873542726]\n",
            "It: 3656, Time: 0.02\n",
            "mse_b  [0.08735427]  mse_f: 0.13635319471359253   total loss: [0.22370747]\n",
            "mse_b ====== [0.0902294368]\n",
            "It: 3657, Time: 0.02\n",
            "mse_b  [0.09022944]  mse_f: 0.12918521463871002   total loss: [0.21941465]\n",
            "mse_b ====== [0.0887606293]\n",
            "It: 3658, Time: 0.03\n",
            "mse_b  [0.08876063]  mse_f: 0.13803353905677795   total loss: [0.22679417]\n",
            "mse_b ====== [0.083900556]\n",
            "It: 3659, Time: 0.02\n",
            "mse_b  [0.08390056]  mse_f: 0.12653899192810059   total loss: [0.21043955]\n",
            "mse_b ====== [0.0786134228]\n",
            "It: 3660, Time: 0.03\n",
            "mse_b  [0.07861342]  mse_f: 0.12801450490951538   total loss: [0.20662794]\n",
            "mse_b ====== [0.0756710693]\n",
            "It: 3661, Time: 0.02\n",
            "mse_b  [0.07567107]  mse_f: 0.11810524016618729   total loss: [0.19377631]\n",
            "mse_b ====== [0.0754211694]\n",
            "It: 3662, Time: 0.02\n",
            "mse_b  [0.07542117]  mse_f: 0.12618736922740936   total loss: [0.20160854]\n",
            "mse_b ====== [0.0760535449]\n",
            "It: 3663, Time: 0.02\n",
            "mse_b  [0.07605354]  mse_f: 0.1186082735657692   total loss: [0.19466183]\n",
            "mse_b ====== [0.0770470202]\n",
            "It: 3664, Time: 0.02\n",
            "mse_b  [0.07704702]  mse_f: 0.13188129663467407   total loss: [0.20892832]\n",
            "mse_b ====== [0.0779865]\n",
            "It: 3665, Time: 0.02\n",
            "mse_b  [0.0779865]  mse_f: 0.13507531583309174   total loss: [0.21306181]\n",
            "mse_b ====== [0.0769304857]\n",
            "It: 3666, Time: 0.02\n",
            "mse_b  [0.07693049]  mse_f: 0.1540108621120453   total loss: [0.23094136]\n",
            "mse_b ====== [0.0740069896]\n",
            "It: 3667, Time: 0.02\n",
            "mse_b  [0.07400699]  mse_f: 0.14878439903259277   total loss: [0.22279139]\n",
            "mse_b ====== [0.0721434504]\n",
            "It: 3668, Time: 0.02\n",
            "mse_b  [0.07214345]  mse_f: 0.14345762133598328   total loss: [0.21560107]\n",
            "mse_b ====== [0.0711362958]\n",
            "It: 3669, Time: 0.02\n",
            "mse_b  [0.0711363]  mse_f: 0.12072538584470749   total loss: [0.19186169]\n",
            "mse_b ====== [0.0705904812]\n",
            "It: 3670, Time: 0.02\n",
            "mse_b  [0.07059048]  mse_f: 0.11158744990825653   total loss: [0.18217793]\n",
            "mse_b ====== [0.0736400187]\n",
            "It: 3671, Time: 0.02\n",
            "mse_b  [0.07364002]  mse_f: 0.10557438433170319   total loss: [0.1792144]\n",
            "mse_b ====== [0.0806534216]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4a88ec6cdb8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m MSE_b1, MSE_f1, weightu, weightf, loss_saman, list_model_u = fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star1,xtt,ytt,xbb,ybb,xrr,yrr,xll1,yll1,xll2,yll2, \n\u001b[0m\u001b[1;32m    534\u001b[0m                                                                  \u001b[0mtf_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m                                                                  tf_iter2=1000, newton_iter1=5000,newton_iter2=15000)\n",
            "\u001b[0;32m<ipython-input-13-4a88ec6cdb8d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star, xtt, ytt, xbb, ybb, xrr, yrr, xll1, yll1, xll2, yll2, tf_iter, tf_iter2, newton_iter1, newton_iter2)\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mt_f_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_sz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_sz\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_sz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             loss_value, mse_b, mse_f, grads, grads_ub, grads_fu = grad(u_model, x_f_batch, y_f_batch,\n\u001b[0m\u001b[1;32m    447\u001b[0m                                                                        \u001b[0mxb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                                                                        \u001b[0mub_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_ub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "loss2=[]\n",
        "for i in range(len(loss_saman)):\n",
        "  loss2.append(loss_saman[i][0].numpy())\n",
        "print(\"Min: \", pd.Series(loss2).idxmin())\n",
        "print(\"Max: \", pd.Series(loss2).idxmax())\n",
        "print('loss_min',loss_saman[pd.Series(loss2).idxmin()])\n",
        "\n",
        "model_u=list_model_u[pd.Series(loss2).idxmin()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUO_e5nlIIJW",
        "outputId": "0804268c-cb21-4cd9-a39f-910302feb8cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min:  3671\n",
            "Max:  0\n",
            "loss_min tf.Tensor([0.1792144], shape=(1,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualization of concentration"
      ],
      "metadata": {
        "id": "3bOE3VAijCjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N=100\n",
        "x1 = (np.linspace(0, xmax, N+1)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, N+1)).flatten()[:, None]\n",
        "X1, Y1 = np.meshgrid(x1, y1)\n",
        "\n",
        "X_star1 = np.hstack((X1.flatten()[:, None], Y1.flatten()[:, None]))\n",
        "\n",
        "X_star = tf.convert_to_tensor(X_star1, dtype=tf.float32)\n",
        "#up, vp, pp = u_x_model(X_star[:, 0:1], X_star[:, 1:2])\n",
        "\n",
        "UU=model_u(tf.concat([X_star[:, 0:1], X_star[:, 1:2]],1))\n",
        "\n",
        "uuu2=tf.reshape(UU,shape=[tf.shape(UU).numpy()[0]])\n",
        "U = uuu2.numpy().reshape(N+1,N+1)\n",
        "\n",
        "x1 = (np.linspace(0, xmax, N+1)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, N+1)).flatten()[:, None]\n",
        "X1, Y1 = np.meshgrid(x1, y1)\n",
        "fig, ax = plt.subplots(figsize=(4, 1))\n",
        "ax.contourf(X1,Y1,U,80,cmap='rainbow')\n",
        "\n",
        "x1 = (np.linspace(0, xmax, N+1)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, N+1)).flatten()[:, None]\n",
        "X1, Y1 = np.meshgrid(x1, y1)\n",
        "fig, ax = plt.subplots(figsize=(4, 1))\n",
        "ax.contourf(X1,Y1,U,60,vmax=1.0,vmin=0,cmap='rainbow')\n",
        "plt.axis('scaled')\n",
        "plt.title('CFD')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "k0firwVjHujK",
        "outputId": "536da3dd-d671-4ac6-8704-d6555840a815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'CFD')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x72 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x72 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#concentration for electrowetting\n",
        "\n",
        "UU=model_u(tf.concat([X_star[:, 0:1], X_star[:, 1:2]],1))\n",
        "uuu2=tf.reshape(UU,shape=[tf.shape(UU).numpy()[0]])\n",
        "U = uuu2.numpy().reshape(N+1,N+1)\n",
        "plt.figure(figsize=(6,1))\n",
        "plt. contourf(X1, Y1, U,60,vmin=0, cmap='rainbow');\n",
        "#plt.colorbar();\n",
        "plt.axis('scaled')"
      ],
      "metadata": {
        "id": "eDH5PbcnorCq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "c3f9f972-d9a0-4f1a-cb8d-f2c8a1ec3f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 6.0, 0.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x72 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u_ref,v_ref"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VebtbXuea33P",
        "outputId": "118dc77e-0e1c-4f20-e747-09257bdbb1c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(10000, 1), dtype=float32, numpy=\n",
              " array([[1.1425333 ],\n",
              "        [1.1266822 ],\n",
              "        [0.14481366],\n",
              "        ...,\n",
              "        [1.1369725 ],\n",
              "        [0.5432961 ],\n",
              "        [0.55365133]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(10000, 1), dtype=float32, numpy=\n",
              " array([[ 0.20608276],\n",
              "        [-0.06612173],\n",
              "        [-0.00921361],\n",
              "        ...,\n",
              "        [ 0.05409827],\n",
              "        [-0.01267004],\n",
              "        [ 0.0223068 ]], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running PINNs for solving temperature"
      ],
      "metadata": {
        "id": "ESzLzOsfjHqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Solving Tem for electrowetting flow\n",
        "#NEW NS with importing fi and sai\n",
        "#NEW NS\n",
        "#########\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DTYPE='float32'\n",
        "tf.keras.backend.set_floatx(DTYPE)\n",
        "# Set number of data points\n",
        "#N_0 = 100\n",
        "#N_b = 100\n",
        "#N_r = 5000\n",
        "\n",
        "N_b = Nu1\n",
        "N_r = N_f\n",
        "\n",
        "\n",
        "# Set boundary\n",
        "xmin = 0.\n",
        "xmax = 6\n",
        "ymin = 0.\n",
        "ymax = 1.\n",
        "# Set constants\n",
        "pi = tf.constant(np.pi, dtype=DTYPE)\n",
        "viscosity = .01/pi\n",
        "\n",
        "# Define initial condition\n",
        "def fun_u_0(x):\n",
        "    return -tf.sin(pi * x)\n",
        "\n",
        "# Define boundary condition\n",
        "def fun_u_b(x, y):\n",
        "    n = x.shape[0]\n",
        "    return tf.zeros((n,1), dtype=DTYPE)\n",
        "\n",
        "# Define residual of the PDE\n",
        "def Nsx(x, y, u, v, u_x, u_y, v_x, v_y, u_xx, u_yy, v_xx, v_yy, p_x, p_y):\n",
        "    return -u*u_x-v*u_y-p_x+(1.0/1.0)*(u_xx+u_yy)\n",
        "#    return -p_x+(1.0/1.0)*(u_xx+u_yy)\n",
        "\n",
        "\n",
        "def Nsy(x, y, u, v, u_x, u_y, v_x, v_y, u_xx, u_yy, v_xx, v_yy, p_x, p_y):\n",
        "    return -u*v_x-v*v_y-p_y+(1.0/1.0)*(v_xx+v_yy)\n",
        "#    return -p_y+(1.0/1.0)*(v_xx+v_yy)\n",
        "\n",
        "def Cont(u, v, u_x, u_y, v_x, v_y):\n",
        "    return u_x+v_y\n",
        "\n",
        "# Lower bounds\n",
        "lb = tf.constant([xmin, ymin], dtype=DTYPE)\n",
        "# Upper bounds\n",
        "ub = tf.constant([xmax, ymax], dtype=DTYPE)\n",
        "\n",
        "# Set random seed for reproducible results\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# Draw uniform sample points for initial boundary data\n",
        "x_0 = lb[0] + (ub[0] - lb[0]) * tf.keras.backend.random_bernoulli((N_b,1), 0.5, dtype=DTYPE)\n",
        "y_0 = tf.random.uniform((N_b,1), lb[1], ub[1], dtype=DTYPE)\n",
        "X_0 = tf.concat([x_0, y_0], axis=1)\n",
        "\n",
        "# Evaluate intitial condition at x_0\n",
        "u_0 = fun_u_0(x_0)\n",
        "\n",
        "# Boundary data\n",
        "x_b = tf.random.uniform((N_b,1), lb[0], ub[0], dtype=DTYPE)\n",
        "y_b = lb[1] + (ub[1] - lb[1]) * tf.keras.backend.random_bernoulli((N_b,1), 0.5, dtype=DTYPE)\n",
        "X_b = tf.concat([x_b, y_b], axis=1)\n",
        "\n",
        "# Evaluate boundary condition at (t_b,x_b)\n",
        "u_b = fun_u_b(x_b, y_b)\n",
        "\n",
        "\n",
        "# Draw uniformly sampled collocation points\n",
        "x_r = tf.random.uniform((N_r,1), lb[0], ub[0], dtype=DTYPE)\n",
        "y_r = tf.random.uniform((N_r,1), lb[1], ub[1], dtype=DTYPE)\n",
        "X_r = tf.concat([x_r, y_r], axis=1)\n",
        "\n",
        "# Collect boundary and inital data in lists\n",
        "X_data = [X_0, X_b]\n",
        "u_data = [u_0, u_b]\n",
        "\n",
        "\n",
        "##################\n",
        "#################\n",
        "lo=np.zeros((N_b,1))\n",
        "#top\n",
        "xx=[]\n",
        "yy=[]\n",
        "for i in range(N_b):\n",
        "      if X_data[1][i,1].numpy() == 1.0:\n",
        "        xx.append(X_data[1][i,0].numpy())\n",
        "        yy.append(X_data[1][i,1].numpy())\n",
        "xtt=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "ytt=tf.zeros((len(yy),1), dtype=DTYPE)\n",
        "xtt=tf.add(xtt[:,0],xx)\n",
        "ytt=tf.add(ytt[:,0],yy)\n",
        "utt=tf.ones((len(xx),1), dtype=DTYPE)\n",
        "vtt=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "\n",
        "#bottom\n",
        "xx=[]\n",
        "yy=[]\n",
        "for i in range(N_b):\n",
        "      if X_data[1][i,1].numpy() == 0.0:\n",
        "        xx.append(X_data[1][i,0].numpy())\n",
        "        yy.append(X_data[1][i,1].numpy())\n",
        "xbb=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "ybb=tf.zeros((len(yy),1), dtype=DTYPE)\n",
        "xbb=tf.add(xbb[:,0],xx)\n",
        "ybb=tf.add(ybb[:,0],yy)\n",
        "ubb=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "vbb=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "#left1\n",
        "xx=[]\n",
        "yy=[]\n",
        "Nb_1= 50\n",
        "xll1= tf.zeros((Nb_1,1), dtype=DTYPE)\n",
        "yll1 =tf.random.uniform((Nb_1,1),0, 0.5, dtype=DTYPE)\n",
        "#xll1=tf.add(xll1[:,0],xx)\n",
        "#yll1=tf.add(yll1[:,0],yy)\n",
        "ull1=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "vll1=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "#left2\n",
        "xx=[]\n",
        "yy=[]\n",
        "Nb_2= 50\n",
        "xll2= tf.zeros((Nb_2,1), dtype=DTYPE)\n",
        "yll2= tf.random.uniform((Nb_2,1),0.5, 1.0, dtype=DTYPE)\n",
        "#xll2=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "#yll2=tf.zeros((len(yy),1), dtype=DTYPE)\n",
        "#xll2=tf.add(xll2[:,0],xx)\n",
        "#yll2=tf.add(yll2[:,0],yy)\n",
        "ull2=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "vll2=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "#right\n",
        "\n",
        "xx=[]\n",
        "yy=[]\n",
        "for i in range(N_b):\n",
        "      if X_data[0][i,0].numpy() == xmax:\n",
        "        xx.append(X_data[0][i,0].numpy())\n",
        "        yy.append(X_data[0][i,1].numpy())\n",
        "xrr=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "yrr=tf.zeros((len(yy),1), dtype=DTYPE)\n",
        "xrr=tf.add(xrr[:,0],xx)\n",
        "yrr=tf.add(yrr[:,0],yy)\n",
        "urr=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "vrr=tf.zeros((len(xx),1), dtype=DTYPE)\n",
        "\n",
        "xtt=tf.reshape(xtt,shape=[tf.shape(xtt).numpy()[0],1])\n",
        "ytt=tf.reshape(ytt,shape=[tf.shape(ytt).numpy()[0],1])\n",
        "\n",
        "xbb=tf.reshape(xbb,shape=[tf.shape(xbb).numpy()[0],1])\n",
        "ybb=tf.reshape(ybb,shape=[tf.shape(ybb).numpy()[0],1])\n",
        "\n",
        "xrr=tf.reshape(xrr,shape=[tf.shape(xrr).numpy()[0],1])\n",
        "yrr=tf.reshape(yrr,shape=[tf.shape(yrr).numpy()[0],1])\n",
        "\n",
        "\n",
        "xll1=tf.reshape(xll1,shape=[tf.shape(xll1).numpy()[0],1])\n",
        "yll1=tf.reshape(yll1,shape=[tf.shape(yll1).numpy()[0],1])\n",
        "xll2=tf.reshape(xll2,shape=[tf.shape(xll2).numpy()[0],1])\n",
        "yll2=tf.reshape(yll2,shape=[tf.shape(yll2).numpy()[0],1])\n",
        "\n",
        "xbound=tf.concat([xtt,xbb,xrr,xll1,xll2],0)\n",
        "ybound=tf.concat([ytt,ybb,yrr,yll1,yll2],0)\n",
        "plt.scatter(xbound,ybound)\n",
        "\n",
        "ubound=tf.concat([utt,ubb,urr,ull1,ull2],0)\n",
        "vbound=tf.concat([vtt,vbb,vrr,vll1,vll2],0)\n",
        "\n",
        "xb=xbound\n",
        "yb=ybound\n",
        "\n",
        "ub=ubound\n",
        "vb=vbound\n",
        "\n",
        "##################\n",
        "#xcor1=x_f[0 * N_f:(0 * N_f + N_f), ]\n",
        "#ycor1=y_f[0 * N_f:(0 * N_f + N_f), ]\n",
        "#sai_ref = model_sai(tf.concat([xcor1,ycor1],1))\n",
        "\n",
        "#fi_ref = model_fi(tf.concat([xcor1,ycor1],1))\n",
        "##################\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Oct 24 13:02:32 2021\n",
        "\n",
        "@author: SAMAN\n",
        "\"\"\"\n",
        "# import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import scipy.io\n",
        "import math\n",
        "import matplotlib.gridspec as gridspec\n",
        "#from plotting import newfig\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras import layers, activations\n",
        "from scipy.interpolate import griddata\n",
        "#from eager_lbfgs import lbfgs, Struct\n",
        "from pyDOE import lhs\n",
        "\n",
        "\n",
        "gh = tf.constant(0.01, dtype=tf.float32)\n",
        "gg = tf.constant(9.8, dtype=tf.float32)\n",
        "weight_ub = tf.Variable([5.0], dtype=tf.float32)  # weight_ub = tf.Variable([8.0], dtype=tf.float32)\n",
        "weight_fu = tf.Variable([1.0], dtype=tf.float32)  # weight_fu = tf.Variable([1.0], dtype=tf.float32)\n",
        "layer_sizes = [2, 20, 20, 20, 20, 20, 20, 20, 3]\n",
        "sizes_w = []\n",
        "sizes_b = []\n",
        "loss_saman=[]\n",
        "list_model_u=[]\n",
        "for i, width in enumerate(layer_sizes):\n",
        "    if i != 1:\n",
        "        sizes_w.append(int(width * layer_sizes[1]))\n",
        "        sizes_b.append(int(width if i != 0 else layer_sizes[1]))\n",
        "\n",
        "\n",
        "\n",
        "# L-BFGS weight getting and setting from https://github.com/pierremtb/PINNs-TF2.0\n",
        "\n",
        "def set_weights(model, w, sizes_w, sizes_b):  # 重新设置参数\n",
        "\n",
        "    for i, layer in enumerate(model.layers[1:len(sizes_w) + 1]):\n",
        "        start_weights = sum(sizes_w[:i]) + sum(sizes_b[:i])\n",
        "        end_weights = sum(sizes_w[:i + 1]) + sum(sizes_b[:i])\n",
        "        weights = w[start_weights:end_weights]\n",
        "        w_div = int(sizes_w[i] / sizes_b[i])\n",
        "        weights = tf.reshape(weights, [w_div, sizes_b[i]])\n",
        "        biases = w[end_weights:end_weights + sizes_b[i]]\n",
        "        weights_biases = [weights, biases]\n",
        "        layer.set_weights(weights_biases)\n",
        "\n",
        "\n",
        "def get_weights(model):\n",
        "    w = []\n",
        "    for layer in model.layers[1:len(sizes_w) + 1]:\n",
        "        weights_biases = layer.get_weights()\n",
        "        weights = weights_biases[0].flatten()\n",
        "        biases = weights_biases[1]\n",
        "        w.extend(weights)\n",
        "        w.extend(biases)\n",
        "    w = tf.convert_to_tensor(w)\n",
        "    return w\n",
        "\n",
        "def xavier_init(layer_sizes):\n",
        "    in_dim = layer_sizes[0]\n",
        "    out_dim = layer_sizes[1]\n",
        "    xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "    return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "\n",
        "def neural_net(layer_sizes):\n",
        "\n",
        "    input_tensor = keras.Input(shape=(layer_sizes[0],))\n",
        "\n",
        "    hide_layer_list = []\n",
        "    flag = True\n",
        "    for width in layer_sizes[1:-1]:\n",
        "        if flag:\n",
        "            x = layers.Dense(\n",
        "                width, activation=tf.nn.tanh,\n",
        "                kernel_initializer=\"glorot_normal\")(input_tensor)\n",
        "            flag = False\n",
        "        else:\n",
        "            x = layers.Dense(\n",
        "                width, activation=tf.nn.tanh,\n",
        "                kernel_initializer=\"glorot_normal\")(x)\n",
        "    output_tensor = layers.Dense(layer_sizes[-1], activation=None,kernel_initializer=\"glorot_normal\")(x)\n",
        "    print(\"xxxxxxxxxxxxxx\")\n",
        "    output0 = output_tensor[:, 0:1]\n",
        "\n",
        "\n",
        "    model_output = keras.models.Model(input_tensor, [output0])\n",
        "\n",
        "    return model_output\n",
        "\n",
        "# initialize the NN\n",
        "u_model = neural_net(layer_sizes)\n",
        "# view the NN\n",
        "u_model.summary()\n",
        "\n",
        "def gh1(pp):\n",
        "  return pp\n",
        "\n",
        "\n",
        "# define the loss\n",
        "def loss(x_f_batch, y_f_batch, xb, yb, ub, vb, weight_ub,weight_fu,x_top,y_top,x_bottom,y_bottom,x_right,y_right,x_left1,y_left1,x_left2,y_left2):\n",
        "\n",
        "    f_c_pred = f_model(x_f_batch, y_f_batch)\n",
        "\n",
        "\n",
        "    c_pred = u_model(tf.concat([xb, yb], 1))\n",
        "\n",
        "    #mse_b = 100*weight_ub*(tf.reduce_sum(tf.square(u_pred - ub)) + tf.reduce_sum(tf.square(v_pred - vb)))\n",
        "    #mse_b = 1*weight_ub*(tf.reduce_sum(tf.square(u_pred - ub)) + tf.reduce_sum(tf.square(v_pred - vb)))\n",
        "    loss_2 = loss_bd(x_top,y_top,x_bottom,y_bottom,x_right,y_right,x_left1,y_left1,x_left2,y_left2)\n",
        "    #mse_b = loss_2*weight_ub#+ mse_b\n",
        "\n",
        "    #mse_f = weight_fu*(tf.reduce_sum(tf.square(f_u_pred)) + tf.reduce_sum(tf.square(f_v_pred)) + tf.reduce_sum(tf.square(div_pred)))\n",
        "    loss3=tf.reduce_sum(tf.square(f_c_pred))\n",
        "\n",
        "    #weight_ub = tf.cond(loss3>loss_2,lambda:[loss_2/loss3],lambda:[1.0])\n",
        "    #weight_fu = tf.cond(loss_2>loss3, lambda:[1.0],lambda:[loss3/loss_2])\n",
        "\n",
        "\n",
        "\n",
        "    #weight_ub.assign([1.0])\n",
        "    #weight_fu.assign([1.0])\n",
        "\n",
        "    mse_b = loss_2 * weight_ub\n",
        "    mse_f = loss3\n",
        "\n",
        "    tf.print('mse_b ======',mse_b)\n",
        "\n",
        "    #tf.print('reduce_max',tf.reduce_max(f_u_pred))\n",
        "    #tf.print('min or max',tf.math.minimum(f_u_pred))\n",
        "    return mse_b + mse_f, mse_b, mse_f\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def loss_bd(xtop,ytop,xbottom,ybottom,xright,yright,xleft1,yleft1,xleft2,yleft2):\n",
        "  ctop = u_model(tf.concat([xtop, ytop],1))\n",
        "  cbottom = u_model(tf.concat([xbottom, ybottom],1))\n",
        "  cright = u_model(tf.concat([xright, yright],1))\n",
        "  cleft1 = u_model(tf.concat([xleft1, yleft1],1))\n",
        "  cleft2 = u_model(tf.concat([xleft2, yleft2],1))\n",
        "\n",
        "\n",
        "\n",
        "  loss_bd = tf.reduce_sum(tf.square(ctop-1))+tf.reduce_sum(tf.square(cbottom-1)) \\\n",
        "  + tf.reduce_sum(tf.square(tf.gradients(cright,xright)[0]))+tf.reduce_sum(tf.square(cleft1-0.0)) +tf.reduce_sum(tf.square(cleft2-0.0)) \\\n",
        "\n",
        "  return loss_bd\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def f_model(x, y):\n",
        "    c = u_model(tf.concat([x, y],1))\n",
        "\n",
        "    c_x = tf.gradients(c, x)[0]\n",
        "    c_y = tf.gradients(c, y)[0]\n",
        "    c_xx = tf.gradients(c_x, x)[0]\n",
        "    c_yy = tf.gradients(c_y, y)[0]\n",
        "\n",
        "\n",
        "\n",
        "    Reynolds=tf.constant(20.0, dtype=tf.float32)\n",
        "    Schimit= tf.constant(9.4, dtype=tf.float32)\n",
        "\n",
        "    #(1.0/Re)*(double(Lx)/double(Ly))*omega_h*omega_h*sai[i][j]*(fi[i+1][j]-fi[i-1][j])/double(2*dx);//inja kamel shavad\n",
        "    f_c = u_ref*c_x + v_ref*c_y  -(1.0/Reynolds)*(1.0/Schimit)*(c_xx + c_yy)\n",
        "\n",
        "    return f_c\n",
        "\n",
        "@tf.function\n",
        "def u_x_model(x, y):\n",
        "    c= u_model(tf.concat([x, y], 1))\n",
        "    return c\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def grad(u_model, x_f_batch, y_f_batch, xb_batch, yb_batch, ub_batch, vb_batch, weight_ub,\n",
        "         weight_fu,x_top,y_top,x_bottom,y_bottom,x_right,y_right,x_left1,y_left1,x_left2,y_left2):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "        loss_value, mse_b, mse_f = loss(x_f_batch, y_f_batch,  xb_batch, yb_batch, ub_batch,\n",
        "                                        vb_batch, weight_ub, weight_fu,x_top,y_top,x_bottom,y_bottom,x_right,y_right,x_left1,y_left1,x_left2,y_left2)\n",
        "        grads = tape.gradient(loss_value, u_model.trainable_variables)\n",
        "\n",
        "        grads_ub = tape.gradient(loss_value, weight_ub)\n",
        "\n",
        "        grads_fu = tape.gradient(loss_value, weight_fu)\n",
        "\n",
        "    return loss_value, mse_b, mse_f, grads, grads_ub, grads_fu\n",
        "\n",
        "\n",
        "def fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star,xtt,ytt,xbb,ybb,xrr,yrr,xll1,yll1,xll2,yll2, tf_iter, tf_iter2,\n",
        "        newton_iter1, newton_iter2):\n",
        "\n",
        "    batch_sz = N_f\n",
        "    n_batches = N_f // batch_sz\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    tf_optimizer = tf.keras.optimizers.Adam(lr=0.005, beta_1=.99) #tf_optimizer = tf.keras.optimizers.Adam(lr=0.005, beta_1=.99)\n",
        "    tf_optimizer_weights = tf.keras.optimizers.Adam(lr=0.003, beta_1=.99)\n",
        "    tf_optimizer_u = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
        "\n",
        "    tf.print(f\"weight_ub: {weight_ub}  weight_fu: {weight_fu}\")\n",
        "    print(\"starting Adam training\")\n",
        "\n",
        "    a = np.random.rand(1000)\n",
        "    loss_history = list(a)\n",
        "    MSE_b0 = list(a)\n",
        "    MSE_f0 = list(a)\n",
        "\n",
        "\n",
        "    MSE_b1 = []\n",
        "    MSE_f1 = []\n",
        "\n",
        "    weightu = []\n",
        "    weightf = []\n",
        "    # For mini-batch (if used)\n",
        "    for epoch in range(tf_iter):\n",
        "        for i in range(n_batches):\n",
        "            xb_batch = xb\n",
        "            yb_batch = yb\n",
        "\n",
        "            ub_batch = ub\n",
        "            vb_batch = vb\n",
        "\n",
        "            x_top=xtt\n",
        "            y_top=ytt\n",
        "\n",
        "            x_bottom=xbb\n",
        "            y_bottom=ybb\n",
        "\n",
        "            x_right=xrr\n",
        "            y_right=yrr\n",
        "\n",
        "            x_left1=xll1\n",
        "            y_left1=yll1\n",
        "\n",
        "            x_left2=xll2\n",
        "            y_left2=yll2\n",
        "\n",
        "            x_f_batch = x_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "            y_f_batch = y_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "            t_f_batch = t_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "\n",
        "            loss_value, mse_b, mse_f, grads, grads_ub, grads_fu = grad(u_model, x_f_batch, y_f_batch,\n",
        "                                                                       xb_batch, yb_batch,\n",
        "                                                                       ub_batch, vb_batch, weight_ub,\n",
        "                                                                       weight_fu,\n",
        "                                                                       x_top,y_top,x_bottom,y_bottom,\n",
        "                                                                       x_right,y_right,x_left1,y_left1,x_left2,y_left2)\n",
        "\n",
        "            tf_optimizer.apply_gradients(zip(grads, u_model.trainable_variables))\n",
        "            MSE_b0.append(mse_b)\n",
        "            MSE_f0.append(mse_f)\n",
        "\n",
        "            loss_history.append(loss_value)\n",
        "            loss_saman.append(loss_value)\n",
        "            list_model_u.append(u_model)\n",
        "#            if loss_history[-1] < loss_history[-2] and loss_history[-2] < loss_history[-3] and loss_history[-1] < \\\n",
        "#                    loss_history[-10]:\n",
        "#                tf_optimizer_weights.apply_gradients(zip([-grads_fu], [weight_fu]))\n",
        "#                tf_optimizer_u.apply_gradients(zip([-grads_ub], [weight_ub]))\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('It: %d, Time: %.2f' % (epoch, elapsed))\n",
        "            tf.print(f\"mse_b  {mse_b}  mse_f: {mse_f}   total loss: {loss_value}\")\n",
        "\n",
        "            wu = weight_ub.numpy()\n",
        "            wf = weight_fu.numpy()\n",
        "\n",
        "            MSE_b1.append(mse_b)\n",
        "            MSE_f1.append(mse_f)\n",
        "\n",
        "            weightu.append(wu)\n",
        "            weightf.append(wf)\n",
        "\n",
        "            start_time = time.time()\n",
        "    tf.print(f\"weight_ub: {weight_ub}  weight_fu: {weight_fu}\")\n",
        "    c_pred= predict(X_star)\n",
        "    tf.print('epoch',epoch,'loss',loss_value)\n",
        "    #error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
        "    #print('Error u: %e' % (error_u))\n",
        "    #error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
        "    #print('Error v: %e' % (error_v))\n",
        "    #print(\"Starting L-BFGS training\")\n",
        "\n",
        "    '''\n",
        "    loss_and_flat_grad = get_loss_and_flat_grad(x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch,\n",
        "                                                vb_batch, weight_ub, weight_fu)\n",
        "\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "          get_weights(u_model),\n",
        "          Struct(), maxIter=newton_iter1, learningRate=0.8)\n",
        "\n",
        "    u_pred, v_pred, p_pred = predict(X_star)\n",
        "    error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
        "    print('Error u: %e' % (error_u))\n",
        "    error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
        "    print('Error v: %e' % (error_v))\n",
        "\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "          get_weights(u_model),\n",
        "          Struct(), maxIter=newton_iter2, learningRate=0.8)\n",
        "    '''\n",
        "    return MSE_b1, MSE_f1,  weightu, weightf,loss_saman\n",
        "\n",
        "# L-BFGS implementation from https://github.com/pierremtb/PINNs-TF2.0\n",
        "def get_loss_and_flat_grad(x_f_batch, y_f_batch , xb_batch, yb_batch,\n",
        "                           ub_batch, vb_batch,weight_ub, weight_fu):\n",
        "    def loss_and_flat_grad(w):\n",
        "        with tf.GradientTape() as tape:\n",
        "            set_weights(u_model, w, sizes_w, sizes_b)\n",
        "            loss_value, _, _ = loss(x_f_batch, y_f_batch, xb_batch, yb_batch, ub_batch, vb_batch, weight_ub, weight_fu)\n",
        "        grad = tape.gradient(loss_value, u_model.trainable_variables)\n",
        "        grad_flat = []\n",
        "        for g in grad:\n",
        "            grad_flat.append(tf.reshape(g, [-1]))\n",
        "        grad_flat = tf.concat(grad_flat, 0)\n",
        "        # print(loss_value, grad_flat)\n",
        "        return loss_value, grad_flat\n",
        "\n",
        "    return loss_and_flat_grad\n",
        "\n",
        "\n",
        "def predict(X_star):\n",
        "    X_star = tf.convert_to_tensor(X_star, dtype=tf.float32)\n",
        "    c_star = u_x_model(X_star[:, 0:1], X_star[:, 1:2])\n",
        "    return c_star.numpy()\n",
        "\n",
        "start_time = time.time()\n",
        "MSE_b1, MSE_f1, weightu, weightf, loss_saman, list_model_u = fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star1,xtt,ytt,xbb,ybb,xrr,yrr,xll1,yll1,xll2,yll2,\n",
        "                                                                 tf_iter=300000,\n",
        "                                                                 tf_iter2=1000, newton_iter1=5000,newton_iter2=15000)\n",
        "\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.4f' % (elapsed))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ryZdo4RJZhR2",
        "outputId": "58f296c5-3c9d-44c4-feee-8009a95e68cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xxxxxxxxxxxxxx\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 2)]               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 20)                60        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3)                 63        \n",
            "                                                                 \n",
            " tf.__operators__.getitem (S  (None, 1)                0         \n",
            " licingOpLambda)                                                 \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,643\n",
            "Trainable params: 2,643\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "weight_ub: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([5.], dtype=float32)>  weight_fu: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>\n",
            "starting Adam training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "It: 37188, Time: 0.02\n",
            "mse_b  [0.7444893]  mse_f: 2.7204155921936035   total loss: [3.4649048]\n",
            "mse_b ====== [0.291953623]\n",
            "It: 37189, Time: 0.02\n",
            "mse_b  [0.29195362]  mse_f: 2.2519025802612305   total loss: [2.5438561]\n",
            "mse_b ====== [0.300016284]\n",
            "It: 37190, Time: 0.02\n",
            "mse_b  [0.30001628]  mse_f: 1.5224885940551758   total loss: [1.8225049]\n",
            "mse_b ====== [0.251965672]\n",
            "It: 37191, Time: 0.02\n",
            "mse_b  [0.25196567]  mse_f: 3.557494640350342   total loss: [3.8094604]\n",
            "mse_b ====== [0.525062442]\n",
            "It: 37192, Time: 0.02\n",
            "mse_b  [0.52506244]  mse_f: 1.917737603187561   total loss: [2.4428]\n",
            "mse_b ====== [0.557005227]\n",
            "It: 37193, Time: 0.03\n",
            "mse_b  [0.5570052]  mse_f: 2.1800177097320557   total loss: [2.7370229]\n",
            "mse_b ====== [0.245094806]\n",
            "It: 37194, Time: 0.02\n",
            "mse_b  [0.2450948]  mse_f: 2.907198905944824   total loss: [3.1522937]\n",
            "mse_b ====== [0.31448409]\n",
            "It: 37195, Time: 0.02\n",
            "mse_b  [0.3144841]  mse_f: 0.9589430093765259   total loss: [1.2734271]\n",
            "mse_b ====== [0.369519949]\n",
            "It: 37196, Time: 0.03\n",
            "mse_b  [0.36951995]  mse_f: 2.658886432647705   total loss: [3.0284064]\n",
            "mse_b ====== [0.570665121]\n",
            "It: 37197, Time: 0.02\n",
            "mse_b  [0.5706651]  mse_f: 1.3147947788238525   total loss: [1.8854599]\n",
            "mse_b ====== [0.59453392]\n",
            "It: 37198, Time: 0.02\n",
            "mse_b  [0.5945339]  mse_f: 2.1589739322662354   total loss: [2.7535079]\n",
            "mse_b ====== [0.28108269]\n",
            "It: 37199, Time: 0.02\n",
            "mse_b  [0.2810827]  mse_f: 2.681968927383423   total loss: [2.9630516]\n",
            "mse_b ====== [0.330042571]\n",
            "It: 37200, Time: 0.03\n",
            "mse_b  [0.33004257]  mse_f: 1.241687536239624   total loss: [1.5717301]\n",
            "mse_b ====== [0.473689288]\n",
            "It: 37201, Time: 0.02\n",
            "mse_b  [0.4736893]  mse_f: 2.7267231941223145   total loss: [3.2004125]\n",
            "mse_b ====== [0.389828742]\n",
            "It: 37202, Time: 0.03\n",
            "mse_b  [0.38982874]  mse_f: 0.9977577328681946   total loss: [1.3875865]\n",
            "mse_b ====== [0.488679528]\n",
            "It: 37203, Time: 0.02\n",
            "mse_b  [0.48867953]  mse_f: 2.5702381134033203   total loss: [3.0589175]\n",
            "mse_b ====== [0.386290491]\n",
            "It: 37204, Time: 0.02\n",
            "mse_b  [0.3862905]  mse_f: 1.1677337884902954   total loss: [1.5540242]\n",
            "mse_b ====== [0.694352448]\n",
            "It: 37205, Time: 0.02\n",
            "mse_b  [0.69435245]  mse_f: 2.2170794010162354   total loss: [2.9114318]\n",
            "mse_b ====== [0.302652657]\n",
            "It: 37206, Time: 0.02\n",
            "mse_b  [0.30265266]  mse_f: 1.4608441591262817   total loss: [1.7634969]\n",
            "mse_b ====== [0.610860884]\n",
            "It: 37207, Time: 0.03\n",
            "mse_b  [0.6108609]  mse_f: 2.060019016265869   total loss: [2.6708798]\n",
            "mse_b ====== [0.32217291]\n",
            "It: 37208, Time: 0.03\n",
            "mse_b  [0.3221729]  mse_f: 1.7671189308166504   total loss: [2.0892918]\n",
            "mse_b ====== [0.487727344]\n",
            "It: 37209, Time: 0.02\n",
            "mse_b  [0.48772734]  mse_f: 1.6045470237731934   total loss: [2.0922744]\n",
            "mse_b ====== [0.362001628]\n",
            "It: 37210, Time: 0.02\n",
            "mse_b  [0.36200163]  mse_f: 1.9742696285247803   total loss: [2.3362713]\n",
            "mse_b ====== [0.396349907]\n",
            "It: 37211, Time: 0.02\n",
            "mse_b  [0.3963499]  mse_f: 1.233636498451233   total loss: [1.6299864]\n",
            "mse_b ====== [0.33533144]\n",
            "It: 37212, Time: 0.03\n",
            "mse_b  [0.33533144]  mse_f: 2.587479591369629   total loss: [2.922811]\n",
            "mse_b ====== [0.278038114]\n",
            "It: 37213, Time: 0.02\n",
            "mse_b  [0.2780381]  mse_f: 1.174954891204834   total loss: [1.452993]\n",
            "mse_b ====== [0.343969643]\n",
            "It: 37214, Time: 0.02\n",
            "mse_b  [0.34396964]  mse_f: 2.316437244415283   total loss: [2.6604068]\n",
            "mse_b ====== [0.236206561]\n",
            "It: 37215, Time: 0.02\n",
            "mse_b  [0.23620656]  mse_f: 0.9692931175231934   total loss: [1.2054996]\n",
            "mse_b ====== [0.299658626]\n",
            "It: 37216, Time: 0.02\n",
            "mse_b  [0.29965863]  mse_f: 2.378443956375122   total loss: [2.6781025]\n",
            "mse_b ====== [0.252491891]\n",
            "It: 37217, Time: 0.02\n",
            "mse_b  [0.2524919]  mse_f: 1.0523126125335693   total loss: [1.3048046]\n",
            "mse_b ====== [0.337515742]\n",
            "It: 37218, Time: 0.02\n",
            "mse_b  [0.33751574]  mse_f: 2.2959814071655273   total loss: [2.6334972]\n",
            "mse_b ====== [0.295849264]\n",
            "It: 37219, Time: 0.02\n",
            "mse_b  [0.29584926]  mse_f: 1.4968377351760864   total loss: [1.7926869]\n",
            "mse_b ====== [0.193922147]\n",
            "It: 37220, Time: 0.02\n",
            "mse_b  [0.19392215]  mse_f: 1.970412015914917   total loss: [2.164334]\n",
            "mse_b ====== [0.364538401]\n",
            "It: 37221, Time: 0.02\n",
            "mse_b  [0.3645384]  mse_f: 1.620088815689087   total loss: [1.9846272]\n",
            "mse_b ====== [0.32898429]\n",
            "It: 37222, Time: 0.02\n",
            "mse_b  [0.3289843]  mse_f: 1.6043111085891724   total loss: [1.9332954]\n",
            "mse_b ====== [0.591881]\n",
            "It: 37223, Time: 0.03\n",
            "mse_b  [0.591881]  mse_f: 1.508418321609497   total loss: [2.1002994]\n",
            "mse_b ====== [0.285195172]\n",
            "It: 37224, Time: 0.02\n",
            "mse_b  [0.28519517]  mse_f: 1.457318902015686   total loss: [1.7425141]\n",
            "mse_b ====== [0.650093853]\n",
            "It: 37225, Time: 0.02\n",
            "mse_b  [0.65009385]  mse_f: 1.3934569358825684   total loss: [2.0435507]\n",
            "mse_b ====== [0.317098141]\n",
            "It: 37226, Time: 0.02\n",
            "mse_b  [0.31709814]  mse_f: 1.050289273262024   total loss: [1.3673874]\n",
            "mse_b ====== [0.648469925]\n",
            "It: 37227, Time: 0.02\n",
            "mse_b  [0.6484699]  mse_f: 1.718735694885254   total loss: [2.3672056]\n",
            "mse_b ====== [0.362030864]\n",
            "It: 37228, Time: 0.02\n",
            "mse_b  [0.36203086]  mse_f: 1.2462997436523438   total loss: [1.6083306]\n",
            "mse_b ====== [0.483388]\n",
            "It: 37229, Time: 0.02\n",
            "mse_b  [0.483388]  mse_f: 1.7437419891357422   total loss: [2.22713]\n",
            "mse_b ====== [0.441066772]\n",
            "It: 37230, Time: 0.03\n",
            "mse_b  [0.44106677]  mse_f: 0.9137535691261292   total loss: [1.3548204]\n",
            "mse_b ====== [0.295722485]\n",
            "It: 37231, Time: 0.02\n",
            "mse_b  [0.29572248]  mse_f: 1.854378581047058   total loss: [2.1501012]\n",
            "mse_b ====== [0.417497784]\n",
            "It: 37232, Time: 0.02\n",
            "mse_b  [0.41749778]  mse_f: 1.1590781211853027   total loss: [1.5765759]\n",
            "mse_b ====== [0.292010844]\n",
            "It: 37233, Time: 0.02\n",
            "mse_b  [0.29201084]  mse_f: 2.0566608905792236   total loss: [2.3486717]\n",
            "mse_b ====== [0.276991785]\n",
            "It: 37234, Time: 0.02\n",
            "mse_b  [0.27699178]  mse_f: 1.199587106704712   total loss: [1.476579]\n",
            "mse_b ====== [0.282281399]\n",
            "It: 37235, Time: 0.02\n",
            "mse_b  [0.2822814]  mse_f: 1.792982816696167   total loss: [2.0752642]\n",
            "mse_b ====== [0.417583287]\n",
            "It: 37236, Time: 0.02\n",
            "mse_b  [0.4175833]  mse_f: 0.8733822107315063   total loss: [1.2909656]\n",
            "mse_b ====== [0.524006248]\n",
            "It: 37237, Time: 0.02\n",
            "mse_b  [0.52400625]  mse_f: 1.724971890449524   total loss: [2.2489781]\n",
            "mse_b ====== [0.271733522]\n",
            "It: 37238, Time: 0.02\n",
            "mse_b  [0.27173352]  mse_f: 1.1167722940444946   total loss: [1.3885058]\n",
            "mse_b ====== [0.234585315]\n",
            "It: 37239, Time: 0.02\n",
            "mse_b  [0.23458531]  mse_f: 1.898753046989441   total loss: [2.1333385]\n",
            "mse_b ====== [0.229406238]\n",
            "It: 37240, Time: 0.02\n",
            "mse_b  [0.22940624]  mse_f: 1.0181710720062256   total loss: [1.2475773]\n",
            "mse_b ====== [0.272364795]\n",
            "It: 37241, Time: 0.02\n",
            "mse_b  [0.2723648]  mse_f: 1.810019850730896   total loss: [2.0823846]\n",
            "mse_b ====== [0.423529]\n",
            "It: 37242, Time: 0.02\n",
            "mse_b  [0.423529]  mse_f: 1.0644891262054443   total loss: [1.4880182]\n",
            "mse_b ====== [0.456185341]\n",
            "It: 37243, Time: 0.02\n",
            "mse_b  [0.45618534]  mse_f: 1.6980876922607422   total loss: [2.154273]\n",
            "mse_b ====== [0.402552128]\n",
            "It: 37244, Time: 0.02\n",
            "mse_b  [0.40255213]  mse_f: 1.096651315689087   total loss: [1.4992034]\n",
            "mse_b ====== [0.520587623]\n",
            "It: 37245, Time: 0.02\n",
            "mse_b  [0.5205876]  mse_f: 1.4533276557922363   total loss: [1.9739153]\n",
            "mse_b ====== [0.175936759]\n",
            "It: 37246, Time: 0.02\n",
            "mse_b  [0.17593676]  mse_f: 0.9969002604484558   total loss: [1.172837]\n",
            "mse_b ====== [0.480669]\n",
            "It: 37247, Time: 0.02\n",
            "mse_b  [0.480669]  mse_f: 1.4326337575912476   total loss: [1.9133028]\n",
            "mse_b ====== [0.200653076]\n",
            "It: 37248, Time: 0.02\n",
            "mse_b  [0.20065308]  mse_f: 0.9323246479034424   total loss: [1.1329777]\n",
            "mse_b ====== [0.442997813]\n",
            "It: 37249, Time: 0.03\n",
            "mse_b  [0.4429978]  mse_f: 1.4929548501968384   total loss: [1.9359527]\n",
            "mse_b ====== [0.292675078]\n",
            "It: 37250, Time: 0.02\n",
            "mse_b  [0.29267508]  mse_f: 0.9743303060531616   total loss: [1.2670054]\n",
            "mse_b ====== [0.611293435]\n",
            "It: 37251, Time: 0.02\n",
            "mse_b  [0.61129344]  mse_f: 1.4578449726104736   total loss: [2.0691385]\n",
            "mse_b ====== [0.405740976]\n",
            "It: 37252, Time: 0.02\n",
            "mse_b  [0.40574098]  mse_f: 1.0677189826965332   total loss: [1.47346]\n",
            "mse_b ====== [0.259401143]\n",
            "It: 37253, Time: 0.02\n",
            "mse_b  [0.25940114]  mse_f: 1.7511833906173706   total loss: [2.0105846]\n",
            "mse_b ====== [0.251778543]\n",
            "It: 37254, Time: 0.02\n",
            "mse_b  [0.25177854]  mse_f: 0.9905412197113037   total loss: [1.2423198]\n",
            "mse_b ====== [0.318974286]\n",
            "It: 37255, Time: 0.02\n",
            "mse_b  [0.3189743]  mse_f: 1.496133804321289   total loss: [1.8151081]\n",
            "mse_b ====== [0.329209805]\n",
            "It: 37256, Time: 0.02\n",
            "mse_b  [0.3292098]  mse_f: 0.9069225788116455   total loss: [1.2361324]\n",
            "mse_b ====== [0.484523416]\n",
            "It: 37257, Time: 0.02\n",
            "mse_b  [0.48452342]  mse_f: 1.2315495014190674   total loss: [1.7160729]\n",
            "mse_b ====== [0.29091537]\n",
            "It: 37258, Time: 0.02\n",
            "mse_b  [0.29091537]  mse_f: 0.9659766554832458   total loss: [1.256892]\n",
            "mse_b ====== [0.380033851]\n",
            "It: 37259, Time: 0.02\n",
            "mse_b  [0.38003385]  mse_f: 1.5234274864196777   total loss: [1.9034613]\n",
            "mse_b ====== [0.393770665]\n",
            "It: 37260, Time: 0.02\n",
            "mse_b  [0.39377066]  mse_f: 0.8403454422950745   total loss: [1.2341161]\n",
            "mse_b ====== [0.374599963]\n",
            "It: 37261, Time: 0.02\n",
            "mse_b  [0.37459996]  mse_f: 1.4107375144958496   total loss: [1.7853374]\n",
            "mse_b ====== [0.313318]\n",
            "It: 37262, Time: 0.02\n",
            "mse_b  [0.313318]  mse_f: 0.8124659061431885   total loss: [1.1257839]\n",
            "mse_b ====== [0.273856401]\n",
            "It: 37263, Time: 0.02\n",
            "mse_b  [0.2738564]  mse_f: 1.4903266429901123   total loss: [1.764183]\n",
            "mse_b ====== [0.125235483]\n",
            "It: 37264, Time: 0.02\n",
            "mse_b  [0.12523548]  mse_f: 1.0185061693191528   total loss: [1.1437416]\n",
            "mse_b ====== [0.25165087]\n",
            "It: 37265, Time: 0.02\n",
            "mse_b  [0.25165087]  mse_f: 1.5652294158935547   total loss: [1.8168802]\n",
            "mse_b ====== [0.291884869]\n",
            "It: 37266, Time: 0.02\n",
            "mse_b  [0.29188487]  mse_f: 0.883300244808197   total loss: [1.1751851]\n",
            "mse_b ====== [0.293001354]\n",
            "It: 37267, Time: 0.02\n",
            "mse_b  [0.29300135]  mse_f: 1.3215093612670898   total loss: [1.6145108]\n",
            "mse_b ====== [0.242767364]\n",
            "It: 37268, Time: 0.02\n",
            "mse_b  [0.24276736]  mse_f: 0.9646960496902466   total loss: [1.2074634]\n",
            "mse_b ====== [0.320355922]\n",
            "It: 37269, Time: 0.02\n",
            "mse_b  [0.32035592]  mse_f: 1.4204621315002441   total loss: [1.740818]\n",
            "mse_b ====== [0.310910702]\n",
            "It: 37270, Time: 0.02\n",
            "mse_b  [0.3109107]  mse_f: 0.8802922964096069   total loss: [1.191203]\n",
            "mse_b ====== [0.422688901]\n",
            "It: 37271, Time: 0.02\n",
            "mse_b  [0.4226889]  mse_f: 1.103384017944336   total loss: [1.526073]\n",
            "mse_b ====== [0.136594057]\n",
            "It: 37272, Time: 0.02\n",
            "mse_b  [0.13659406]  mse_f: 0.9471365213394165   total loss: [1.0837306]\n",
            "mse_b ====== [0.433940113]\n",
            "It: 37273, Time: 0.02\n",
            "mse_b  [0.4339401]  mse_f: 1.183996319770813   total loss: [1.6179364]\n",
            "mse_b ====== [0.334215015]\n",
            "It: 37274, Time: 0.03\n",
            "mse_b  [0.33421502]  mse_f: 0.9530802965164185   total loss: [1.2872953]\n",
            "mse_b ====== [0.384601384]\n",
            "It: 37275, Time: 0.02\n",
            "mse_b  [0.38460138]  mse_f: 1.2083797454833984   total loss: [1.5929811]\n",
            "mse_b ====== [0.383551717]\n",
            "It: 37276, Time: 0.03\n",
            "mse_b  [0.38355172]  mse_f: 0.760140597820282   total loss: [1.1436923]\n",
            "mse_b ====== [0.427661389]\n",
            "It: 37277, Time: 0.02\n",
            "mse_b  [0.4276614]  mse_f: 1.102267861366272   total loss: [1.5299293]\n",
            "mse_b ====== [0.324519455]\n",
            "It: 37278, Time: 0.03\n",
            "mse_b  [0.32451946]  mse_f: 1.005326747894287   total loss: [1.3298461]\n",
            "mse_b ====== [0.439268529]\n",
            "It: 37279, Time: 0.02\n",
            "mse_b  [0.43926853]  mse_f: 1.1561532020568848   total loss: [1.5954218]\n",
            "mse_b ====== [0.314594686]\n",
            "It: 37280, Time: 0.02\n",
            "mse_b  [0.3145947]  mse_f: 0.977580726146698   total loss: [1.2921754]\n",
            "mse_b ====== [0.274117529]\n",
            "It: 37281, Time: 0.02\n",
            "mse_b  [0.27411753]  mse_f: 0.9698400497436523   total loss: [1.2439575]\n",
            "mse_b ====== [0.193475157]\n",
            "It: 37282, Time: 0.02\n",
            "mse_b  [0.19347516]  mse_f: 0.8894289135932922   total loss: [1.0829041]\n",
            "mse_b ====== [0.236213744]\n",
            "It: 37283, Time: 0.02\n",
            "mse_b  [0.23621374]  mse_f: 1.1151481866836548   total loss: [1.351362]\n",
            "mse_b ====== [0.298380017]\n",
            "It: 37284, Time: 0.02\n",
            "mse_b  [0.29838002]  mse_f: 0.9845839738845825   total loss: [1.282964]\n",
            "mse_b ====== [0.272554755]\n",
            "It: 37285, Time: 0.03\n",
            "mse_b  [0.27255476]  mse_f: 1.0473605394363403   total loss: [1.3199153]\n",
            "mse_b ====== [0.416996151]\n",
            "It: 37286, Time: 0.02\n",
            "mse_b  [0.41699615]  mse_f: 0.8404356241226196   total loss: [1.2574317]\n",
            "mse_b ====== [0.260717809]\n",
            "It: 37287, Time: 0.02\n",
            "mse_b  [0.2607178]  mse_f: 1.0212056636810303   total loss: [1.2819235]\n",
            "mse_b ====== [0.287562639]\n",
            "It: 37288, Time: 0.02\n",
            "mse_b  [0.28756264]  mse_f: 1.0670067071914673   total loss: [1.3545693]\n",
            "mse_b ====== [0.23602508]\n",
            "It: 37289, Time: 0.02\n",
            "mse_b  [0.23602508]  mse_f: 1.099454641342163   total loss: [1.3354797]\n",
            "mse_b ====== [0.271162927]\n",
            "It: 37290, Time: 0.02\n",
            "mse_b  [0.27116293]  mse_f: 1.0133538246154785   total loss: [1.2845168]\n",
            "mse_b ====== [0.269466132]\n",
            "It: 37291, Time: 0.02\n",
            "mse_b  [0.26946613]  mse_f: 0.9229469895362854   total loss: [1.1924131]\n",
            "mse_b ====== [0.141700864]\n",
            "It: 37292, Time: 0.02\n",
            "mse_b  [0.14170086]  mse_f: 1.0367944240570068   total loss: [1.1784953]\n",
            "mse_b ====== [0.138670325]\n",
            "It: 37293, Time: 0.02\n",
            "mse_b  [0.13867033]  mse_f: 1.0391243696212769   total loss: [1.1777947]\n",
            "mse_b ====== [0.139806956]\n",
            "It: 37294, Time: 0.02\n",
            "mse_b  [0.13980696]  mse_f: 1.1042948961257935   total loss: [1.2441019]\n",
            "mse_b ====== [0.225533664]\n",
            "It: 37295, Time: 0.03\n",
            "mse_b  [0.22553366]  mse_f: 0.8836172223091125   total loss: [1.1091509]\n",
            "mse_b ====== [0.317852348]\n",
            "It: 37296, Time: 0.02\n",
            "mse_b  [0.31785235]  mse_f: 0.9828192591667175   total loss: [1.3006716]\n",
            "mse_b ====== [0.218037605]\n",
            "It: 37297, Time: 0.02\n",
            "mse_b  [0.2180376]  mse_f: 0.9154875874519348   total loss: [1.1335251]\n",
            "mse_b ====== [0.149018839]\n",
            "It: 37298, Time: 0.02\n",
            "mse_b  [0.14901884]  mse_f: 1.1233285665512085   total loss: [1.2723475]\n",
            "mse_b ====== [0.174875036]\n",
            "It: 37299, Time: 0.02\n",
            "mse_b  [0.17487504]  mse_f: 1.0268477201461792   total loss: [1.2017227]\n",
            "mse_b ====== [0.196756482]\n",
            "It: 37300, Time: 0.02\n",
            "mse_b  [0.19675648]  mse_f: 1.0174248218536377   total loss: [1.2141813]\n",
            "mse_b ====== [0.278479248]\n",
            "It: 37301, Time: 0.02\n",
            "mse_b  [0.27847925]  mse_f: 0.7918792366981506   total loss: [1.0703585]\n",
            "mse_b ====== [0.303957641]\n",
            "It: 37302, Time: 0.03\n",
            "mse_b  [0.30395764]  mse_f: 0.8792276382446289   total loss: [1.1831853]\n",
            "mse_b ====== [0.209169537]\n",
            "It: 37303, Time: 0.02\n",
            "mse_b  [0.20916954]  mse_f: 0.7453888654708862   total loss: [0.9545584]\n",
            "mse_b ====== [0.216566145]\n",
            "It: 37304, Time: 0.02\n",
            "mse_b  [0.21656615]  mse_f: 1.047554612159729   total loss: [1.2641208]\n",
            "mse_b ====== [0.211196363]\n",
            "It: 37305, Time: 0.02\n",
            "mse_b  [0.21119636]  mse_f: 0.9026747941970825   total loss: [1.1138711]\n",
            "mse_b ====== [0.278955042]\n",
            "It: 37306, Time: 0.02\n",
            "mse_b  [0.27895504]  mse_f: 0.9990489482879639   total loss: [1.2780039]\n",
            "mse_b ====== [0.380876064]\n",
            "It: 37307, Time: 0.02\n",
            "mse_b  [0.38087606]  mse_f: 0.717117190361023   total loss: [1.0979933]\n",
            "mse_b ====== [0.272128969]\n",
            "It: 37308, Time: 0.02\n",
            "mse_b  [0.27212897]  mse_f: 0.8954005837440491   total loss: [1.1675296]\n",
            "mse_b ====== [0.129614905]\n",
            "It: 37309, Time: 0.03\n",
            "mse_b  [0.1296149]  mse_f: 0.8757885694503784   total loss: [1.0054035]\n",
            "mse_b ====== [0.231923684]\n",
            "It: 37310, Time: 0.02\n",
            "mse_b  [0.23192368]  mse_f: 1.0306719541549683   total loss: [1.2625957]\n",
            "mse_b ====== [0.123873204]\n",
            "It: 37311, Time: 0.02\n",
            "mse_b  [0.1238732]  mse_f: 0.851321280002594   total loss: [0.97519445]\n",
            "mse_b ====== [0.214057848]\n",
            "It: 37312, Time: 0.02\n",
            "mse_b  [0.21405785]  mse_f: 1.0227198600769043   total loss: [1.2367777]\n",
            "mse_b ====== [0.132498369]\n",
            "It: 37313, Time: 0.02\n",
            "mse_b  [0.13249837]  mse_f: 0.7184619307518005   total loss: [0.8509603]\n",
            "mse_b ====== [0.186771572]\n",
            "It: 37314, Time: 0.02\n",
            "mse_b  [0.18677157]  mse_f: 1.0235860347747803   total loss: [1.2103577]\n",
            "mse_b ====== [0.16078873]\n",
            "It: 37315, Time: 0.02\n",
            "mse_b  [0.16078873]  mse_f: 0.8450990319252014   total loss: [1.0058877]\n",
            "mse_b ====== [0.263977617]\n",
            "It: 37316, Time: 0.02\n",
            "mse_b  [0.26397762]  mse_f: 0.9223730564117432   total loss: [1.1863507]\n",
            "mse_b ====== [0.219276637]\n",
            "It: 37317, Time: 0.03\n",
            "mse_b  [0.21927664]  mse_f: 0.760565459728241   total loss: [0.97984207]\n",
            "mse_b ====== [0.265556961]\n",
            "It: 37318, Time: 0.02\n",
            "mse_b  [0.26555696]  mse_f: 0.8354495763778687   total loss: [1.1010065]\n",
            "mse_b ====== [0.225410908]\n",
            "It: 37319, Time: 0.02\n",
            "mse_b  [0.22541091]  mse_f: 0.728854775428772   total loss: [0.9542657]\n",
            "mse_b ====== [0.181620404]\n",
            "It: 37320, Time: 0.02\n",
            "mse_b  [0.1816204]  mse_f: 1.0017987489700317   total loss: [1.1834191]\n",
            "mse_b ====== [0.192827761]\n",
            "It: 37321, Time: 0.03\n",
            "mse_b  [0.19282776]  mse_f: 0.86873459815979   total loss: [1.0615623]\n",
            "mse_b ====== [0.185238495]\n",
            "It: 37322, Time: 0.02\n",
            "mse_b  [0.1852385]  mse_f: 0.9324792623519897   total loss: [1.1177177]\n",
            "mse_b ====== [0.285585791]\n",
            "It: 37323, Time: 0.02\n",
            "mse_b  [0.2855858]  mse_f: 0.6501063108444214   total loss: [0.9356921]\n",
            "mse_b ====== [0.290085435]\n",
            "It: 37324, Time: 0.02\n",
            "mse_b  [0.29008543]  mse_f: 0.9155782461166382   total loss: [1.2056637]\n",
            "mse_b ====== [0.228964448]\n",
            "It: 37325, Time: 0.02\n",
            "mse_b  [0.22896445]  mse_f: 0.7386425137519836   total loss: [0.96760696]\n",
            "mse_b ====== [0.160203874]\n",
            "It: 37326, Time: 0.02\n",
            "mse_b  [0.16020387]  mse_f: 0.9799185991287231   total loss: [1.1401224]\n",
            "mse_b ====== [0.115375698]\n",
            "It: 37327, Time: 0.02\n",
            "mse_b  [0.1153757]  mse_f: 0.8163462281227112   total loss: [0.9317219]\n",
            "mse_b ====== [0.101914071]\n",
            "It: 37328, Time: 0.02\n",
            "mse_b  [0.10191407]  mse_f: 0.8636022806167603   total loss: [0.9655163]\n",
            "mse_b ====== [0.125088632]\n",
            "It: 37329, Time: 0.02\n",
            "mse_b  [0.12508863]  mse_f: 0.8017836809158325   total loss: [0.9268723]\n",
            "mse_b ====== [0.176719457]\n",
            "It: 37330, Time: 0.03\n",
            "mse_b  [0.17671946]  mse_f: 0.8771968483924866   total loss: [1.0539163]\n",
            "mse_b ====== [0.153135389]\n",
            "It: 37331, Time: 0.02\n",
            "mse_b  [0.15313539]  mse_f: 0.8249484300613403   total loss: [0.97808385]\n",
            "mse_b ====== [0.130377799]\n",
            "It: 37332, Time: 0.03\n",
            "mse_b  [0.1303778]  mse_f: 0.8398255705833435   total loss: [0.9702034]\n",
            "mse_b ====== [0.0930488259]\n",
            "It: 37333, Time: 0.02\n",
            "mse_b  [0.09304883]  mse_f: 0.9366753101348877   total loss: [1.0297241]\n",
            "mse_b ====== [0.197698459]\n",
            "It: 37334, Time: 0.02\n",
            "mse_b  [0.19769846]  mse_f: 0.8237568140029907   total loss: [1.0214553]\n",
            "mse_b ====== [0.26063782]\n",
            "It: 37335, Time: 0.02\n",
            "mse_b  [0.26063782]  mse_f: 0.8375136256217957   total loss: [1.0981514]\n",
            "mse_b ====== [0.219561324]\n",
            "It: 37336, Time: 0.02\n",
            "mse_b  [0.21956132]  mse_f: 0.7364892959594727   total loss: [0.95605063]\n",
            "mse_b ====== [0.127227709]\n",
            "It: 37337, Time: 0.02\n",
            "mse_b  [0.12722771]  mse_f: 0.7776855230331421   total loss: [0.90491325]\n",
            "mse_b ====== [0.186472967]\n",
            "It: 37338, Time: 0.02\n",
            "mse_b  [0.18647297]  mse_f: 0.7727345824241638   total loss: [0.95920753]\n",
            "mse_b ====== [0.18306233]\n",
            "It: 37339, Time: 0.02\n",
            "mse_b  [0.18306233]  mse_f: 0.755523681640625   total loss: [0.938586]\n",
            "mse_b ====== [0.270230711]\n",
            "It: 37340, Time: 0.02\n",
            "mse_b  [0.2702307]  mse_f: 0.7418947219848633   total loss: [1.0121255]\n",
            "mse_b ====== [0.249058634]\n",
            "It: 37341, Time: 0.02\n",
            "mse_b  [0.24905863]  mse_f: 0.7347818613052368   total loss: [0.98384047]\n",
            "mse_b ====== [0.132765085]\n",
            "It: 37342, Time: 0.02\n",
            "mse_b  [0.13276508]  mse_f: 0.7902010679244995   total loss: [0.9229661]\n",
            "mse_b ====== [0.131002769]\n",
            "It: 37343, Time: 0.02\n",
            "mse_b  [0.13100277]  mse_f: 0.9305438995361328   total loss: [1.0615467]\n",
            "mse_b ====== [0.166695595]\n",
            "It: 37344, Time: 0.02\n",
            "mse_b  [0.1666956]  mse_f: 0.7484757900238037   total loss: [0.9151714]\n",
            "mse_b ====== [0.251008391]\n",
            "It: 37345, Time: 0.02\n",
            "mse_b  [0.2510084]  mse_f: 0.8211694359779358   total loss: [1.0721779]\n",
            "mse_b ====== [0.133669809]\n",
            "It: 37346, Time: 0.02\n",
            "mse_b  [0.13366981]  mse_f: 0.6971046328544617   total loss: [0.8307744]\n",
            "mse_b ====== [0.187862948]\n",
            "It: 37347, Time: 0.02\n",
            "mse_b  [0.18786295]  mse_f: 0.8155895471572876   total loss: [1.0034525]\n",
            "mse_b ====== [0.123685814]\n",
            "It: 37348, Time: 0.03\n",
            "mse_b  [0.12368581]  mse_f: 0.6862061023712158   total loss: [0.80989194]\n",
            "mse_b ====== [0.185367495]\n",
            "It: 37349, Time: 0.02\n",
            "mse_b  [0.1853675]  mse_f: 0.7341972589492798   total loss: [0.9195647]\n",
            "mse_b ====== [0.197541177]\n",
            "It: 37350, Time: 0.03\n",
            "mse_b  [0.19754118]  mse_f: 0.6902406215667725   total loss: [0.8877818]\n",
            "mse_b ====== [0.263847381]\n",
            "It: 37351, Time: 0.02\n",
            "mse_b  [0.26384738]  mse_f: 0.7188572883605957   total loss: [0.98270464]\n",
            "mse_b ====== [0.173443481]\n",
            "It: 37352, Time: 0.02\n",
            "mse_b  [0.17344348]  mse_f: 0.7524738311767578   total loss: [0.9259173]\n",
            "mse_b ====== [0.16056034]\n",
            "It: 37353, Time: 0.02\n",
            "mse_b  [0.16056034]  mse_f: 0.8139709830284119   total loss: [0.9745313]\n",
            "mse_b ====== [0.156264484]\n",
            "It: 37354, Time: 0.03\n",
            "mse_b  [0.15626448]  mse_f: 0.7742384672164917   total loss: [0.93050295]\n",
            "mse_b ====== [0.16310522]\n",
            "It: 37355, Time: 0.02\n",
            "mse_b  [0.16310522]  mse_f: 0.7723281383514404   total loss: [0.9354334]\n",
            "mse_b ====== [0.194741488]\n",
            "It: 37356, Time: 0.03\n",
            "mse_b  [0.19474149]  mse_f: 0.734740674495697   total loss: [0.92948216]\n",
            "mse_b ====== [0.100762241]\n",
            "It: 37357, Time: 0.02\n",
            "mse_b  [0.10076224]  mse_f: 0.7649431228637695   total loss: [0.8657054]\n",
            "mse_b ====== [0.101409793]\n",
            "It: 37358, Time: 0.02\n",
            "mse_b  [0.10140979]  mse_f: 0.705856204032898   total loss: [0.807266]\n",
            "mse_b ====== [0.102870777]\n",
            "It: 37359, Time: 0.02\n",
            "mse_b  [0.10287078]  mse_f: 0.8025988936424255   total loss: [0.90546966]\n",
            "mse_b ====== [0.170743]\n",
            "It: 37360, Time: 0.02\n",
            "mse_b  [0.170743]  mse_f: 0.6770785450935364   total loss: [0.84782153]\n",
            "mse_b ====== [0.257284969]\n",
            "It: 37361, Time: 0.02\n",
            "mse_b  [0.25728497]  mse_f: 0.7757151126861572   total loss: [1.0330001]\n",
            "mse_b ====== [0.216100544]\n",
            "It: 37362, Time: 0.02\n",
            "mse_b  [0.21610054]  mse_f: 0.6383748054504395   total loss: [0.8544754]\n",
            "mse_b ====== [0.136133805]\n",
            "It: 37363, Time: 0.02\n",
            "mse_b  [0.1361338]  mse_f: 0.7311233282089233   total loss: [0.8672571]\n",
            "mse_b ====== [0.102642216]\n",
            "It: 37364, Time: 0.02\n",
            "mse_b  [0.10264222]  mse_f: 0.6507316827774048   total loss: [0.7533739]\n",
            "mse_b ====== [0.123291254]\n",
            "It: 37365, Time: 0.02\n",
            "mse_b  [0.12329125]  mse_f: 0.8028362989425659   total loss: [0.92612755]\n",
            "mse_b ====== [0.15822503]\n",
            "It: 37366, Time: 0.03\n",
            "mse_b  [0.15822503]  mse_f: 0.6631288528442383   total loss: [0.8213539]\n",
            "mse_b ====== [0.227374464]\n",
            "It: 37367, Time: 0.02\n",
            "mse_b  [0.22737446]  mse_f: 0.735629677772522   total loss: [0.9630041]\n",
            "mse_b ====== [0.147422582]\n",
            "It: 37368, Time: 0.02\n",
            "mse_b  [0.14742258]  mse_f: 0.6436448693275452   total loss: [0.7910675]\n",
            "mse_b ====== [0.105721198]\n",
            "It: 37369, Time: 0.02\n",
            "mse_b  [0.1057212]  mse_f: 0.7761001586914062   total loss: [0.88182133]\n",
            "mse_b ====== [0.114088461]\n",
            "It: 37370, Time: 0.02\n",
            "mse_b  [0.11408846]  mse_f: 0.7458561658859253   total loss: [0.85994464]\n",
            "mse_b ====== [0.146724328]\n",
            "It: 37371, Time: 0.02\n",
            "mse_b  [0.14672433]  mse_f: 0.7611043453216553   total loss: [0.9078287]\n",
            "mse_b ====== [0.218267754]\n",
            "It: 37372, Time: 0.02\n",
            "mse_b  [0.21826775]  mse_f: 0.6267809271812439   total loss: [0.84504867]\n",
            "mse_b ====== [0.158811525]\n",
            "It: 37373, Time: 0.02\n",
            "mse_b  [0.15881152]  mse_f: 0.63347327709198   total loss: [0.7922848]\n",
            "mse_b ====== [0.091658473]\n",
            "It: 37374, Time: 0.02\n",
            "mse_b  [0.09165847]  mse_f: 0.6585074663162231   total loss: [0.75016594]\n",
            "mse_b ====== [0.126412377]\n",
            "It: 37375, Time: 0.02\n",
            "mse_b  [0.12641238]  mse_f: 0.7628333568572998   total loss: [0.88924575]\n",
            "mse_b ====== [0.128199324]\n",
            "It: 37376, Time: 0.02\n",
            "mse_b  [0.12819932]  mse_f: 0.6993672251701355   total loss: [0.82756656]\n",
            "mse_b ====== [0.188157871]\n",
            "It: 37377, Time: 0.03\n",
            "mse_b  [0.18815787]  mse_f: 0.7212477922439575   total loss: [0.90940565]\n",
            "mse_b ====== [0.143346548]\n",
            "It: 37378, Time: 0.02\n",
            "mse_b  [0.14334655]  mse_f: 0.6241506338119507   total loss: [0.7674972]\n",
            "mse_b ====== [0.107056215]\n",
            "It: 37379, Time: 0.02\n",
            "mse_b  [0.10705622]  mse_f: 0.7442237734794617   total loss: [0.85128]\n",
            "mse_b ====== [0.102012791]\n",
            "It: 37380, Time: 0.02\n",
            "mse_b  [0.10201279]  mse_f: 0.7154108285903931   total loss: [0.81742364]\n",
            "mse_b ====== [0.134698361]\n",
            "It: 37381, Time: 0.02\n",
            "mse_b  [0.13469836]  mse_f: 0.7057962417602539   total loss: [0.84049463]\n",
            "mse_b ====== [0.157687545]\n",
            "It: 37382, Time: 0.02\n",
            "mse_b  [0.15768754]  mse_f: 0.5849473476409912   total loss: [0.7426349]\n",
            "mse_b ====== [0.159913272]\n",
            "It: 37383, Time: 0.02\n",
            "mse_b  [0.15991327]  mse_f: 0.6437911987304688   total loss: [0.8037045]\n",
            "mse_b ====== [0.0958236158]\n",
            "It: 37384, Time: 0.02\n",
            "mse_b  [0.09582362]  mse_f: 0.617243766784668   total loss: [0.7130674]\n",
            "mse_b ====== [0.158918723]\n",
            "It: 37385, Time: 0.03\n",
            "mse_b  [0.15891872]  mse_f: 0.7458950281143188   total loss: [0.90481377]\n",
            "mse_b ====== [0.155342326]\n",
            "It: 37386, Time: 0.03\n",
            "mse_b  [0.15534233]  mse_f: 0.6332706212997437   total loss: [0.78861296]\n",
            "mse_b ====== [0.304251134]\n",
            "It: 37387, Time: 0.02\n",
            "mse_b  [0.30425113]  mse_f: 0.6034727096557617   total loss: [0.90772384]\n",
            "mse_b ====== [0.23758772]\n",
            "It: 37388, Time: 0.02\n",
            "mse_b  [0.23758772]  mse_f: 0.5385095477104187   total loss: [0.7760973]\n",
            "mse_b ====== [0.203079596]\n",
            "It: 37389, Time: 0.02\n",
            "mse_b  [0.2030796]  mse_f: 0.6365358829498291   total loss: [0.83961546]\n",
            "mse_b ====== [0.115463696]\n",
            "It: 37390, Time: 0.02\n",
            "mse_b  [0.1154637]  mse_f: 0.6658415198326111   total loss: [0.7813052]\n",
            "mse_b ====== [0.103208244]\n",
            "It: 37391, Time: 0.02\n",
            "mse_b  [0.10320824]  mse_f: 0.7124669551849365   total loss: [0.8156752]\n",
            "mse_b ====== [0.0894354582]\n",
            "It: 37392, Time: 0.02\n",
            "mse_b  [0.08943546]  mse_f: 0.6302159428596497   total loss: [0.7196514]\n",
            "mse_b ====== [0.0800876692]\n",
            "It: 37393, Time: 0.02\n",
            "mse_b  [0.08008767]  mse_f: 0.7323464155197144   total loss: [0.8124341]\n",
            "mse_b ====== [0.0648667812]\n",
            "It: 37394, Time: 0.02\n",
            "mse_b  [0.06486678]  mse_f: 0.6811940670013428   total loss: [0.74606085]\n",
            "mse_b ====== [0.0670613274]\n",
            "It: 37395, Time: 0.02\n",
            "mse_b  [0.06706133]  mse_f: 0.7580220103263855   total loss: [0.8250833]\n",
            "mse_b ====== [0.0714000762]\n",
            "It: 37396, Time: 0.02\n",
            "mse_b  [0.07140008]  mse_f: 0.6617757081985474   total loss: [0.73317575]\n",
            "mse_b ====== [0.120737016]\n",
            "It: 37397, Time: 0.02\n",
            "mse_b  [0.12073702]  mse_f: 0.6400290727615356   total loss: [0.7607661]\n",
            "mse_b ====== [0.149362594]\n",
            "It: 37398, Time: 0.02\n",
            "mse_b  [0.1493626]  mse_f: 0.6028186082839966   total loss: [0.7521812]\n",
            "mse_b ====== [0.130667642]\n",
            "It: 37399, Time: 0.02\n",
            "mse_b  [0.13066764]  mse_f: 0.6629911661148071   total loss: [0.7936588]\n",
            "mse_b ====== [0.0992758572]\n",
            "It: 37400, Time: 0.02\n",
            "mse_b  [0.09927586]  mse_f: 0.6717149019241333   total loss: [0.7709907]\n",
            "mse_b ====== [0.120666884]\n",
            "It: 37401, Time: 0.02\n",
            "mse_b  [0.12066688]  mse_f: 0.6821915507316589   total loss: [0.8028584]\n",
            "mse_b ====== [0.0913203433]\n",
            "It: 37402, Time: 0.02\n",
            "mse_b  [0.09132034]  mse_f: 0.6294261813163757   total loss: [0.7207465]\n",
            "mse_b ====== [0.126146361]\n",
            "It: 37403, Time: 0.03\n",
            "mse_b  [0.12614636]  mse_f: 0.6615025401115417   total loss: [0.7876489]\n",
            "mse_b ====== [0.0902933776]\n",
            "It: 37404, Time: 0.02\n",
            "mse_b  [0.09029338]  mse_f: 0.6517419815063477   total loss: [0.7420354]\n",
            "mse_b ====== [0.115142383]\n",
            "It: 37405, Time: 0.03\n",
            "mse_b  [0.11514238]  mse_f: 0.6719069480895996   total loss: [0.78704935]\n",
            "mse_b ====== [0.114715472]\n",
            "It: 37406, Time: 0.02\n",
            "mse_b  [0.11471547]  mse_f: 0.6115739941596985   total loss: [0.72628945]\n",
            "mse_b ====== [0.150960118]\n",
            "It: 37407, Time: 0.02\n",
            "mse_b  [0.15096012]  mse_f: 0.6397064924240112   total loss: [0.7906666]\n",
            "mse_b ====== [0.120831326]\n",
            "It: 37408, Time: 0.02\n",
            "mse_b  [0.12083133]  mse_f: 0.5956871509552002   total loss: [0.71651846]\n",
            "mse_b ====== [0.0683646202]\n",
            "It: 37409, Time: 0.02\n",
            "mse_b  [0.06836462]  mse_f: 0.6768209934234619   total loss: [0.7451856]\n",
            "mse_b ====== [0.0813593119]\n",
            "It: 37410, Time: 0.02\n",
            "mse_b  [0.08135931]  mse_f: 0.6558576822280884   total loss: [0.737217]\n",
            "mse_b ====== [0.123563029]\n",
            "It: 37411, Time: 0.02\n",
            "mse_b  [0.12356303]  mse_f: 0.6167754530906677   total loss: [0.7403385]\n",
            "mse_b ====== [0.115799993]\n",
            "It: 37412, Time: 0.02\n",
            "mse_b  [0.11579999]  mse_f: 0.6060097217559814   total loss: [0.72180974]\n",
            "mse_b ====== [0.0762843937]\n",
            "It: 37413, Time: 0.02\n",
            "mse_b  [0.07628439]  mse_f: 0.626754879951477   total loss: [0.7030393]\n",
            "mse_b ====== [0.0513239]\n",
            "It: 37414, Time: 0.02\n",
            "mse_b  [0.0513239]  mse_f: 0.6744349002838135   total loss: [0.7257588]\n",
            "mse_b ====== [0.0727808923]\n",
            "It: 37415, Time: 0.02\n",
            "mse_b  [0.07278089]  mse_f: 0.6785057187080383   total loss: [0.7512866]\n",
            "mse_b ====== [0.122445896]\n",
            "It: 37416, Time: 0.02\n",
            "mse_b  [0.1224459]  mse_f: 0.6405833959579468   total loss: [0.7630293]\n",
            "mse_b ====== [0.123961404]\n",
            "It: 37417, Time: 0.02\n",
            "mse_b  [0.1239614]  mse_f: 0.6053448915481567   total loss: [0.7293063]\n",
            "mse_b ====== [0.0764427632]\n",
            "It: 37418, Time: 0.02\n",
            "mse_b  [0.07644276]  mse_f: 0.632874608039856   total loss: [0.7093174]\n",
            "mse_b ====== [0.0612206422]\n",
            "It: 37419, Time: 0.02\n",
            "mse_b  [0.06122064]  mse_f: 0.6527917385101318   total loss: [0.7140124]\n",
            "mse_b ====== [0.0725999326]\n",
            "It: 37420, Time: 0.03\n",
            "mse_b  [0.07259993]  mse_f: 0.6513190269470215   total loss: [0.723919]\n",
            "mse_b ====== [0.0930399373]\n",
            "It: 37421, Time: 0.02\n",
            "mse_b  [0.09303994]  mse_f: 0.6505464911460876   total loss: [0.7435864]\n",
            "mse_b ====== [0.101095773]\n",
            "It: 37422, Time: 0.02\n",
            "mse_b  [0.10109577]  mse_f: 0.6125249862670898   total loss: [0.7136208]\n",
            "mse_b ====== [0.064697057]\n",
            "It: 37423, Time: 0.02\n",
            "mse_b  [0.06469706]  mse_f: 0.6093566417694092   total loss: [0.67405367]\n",
            "mse_b ====== [0.0950816348]\n",
            "It: 37424, Time: 0.02\n",
            "mse_b  [0.09508163]  mse_f: 0.658659815788269   total loss: [0.75374144]\n",
            "mse_b ====== [0.106236994]\n",
            "It: 37425, Time: 0.03\n",
            "mse_b  [0.10623699]  mse_f: 0.6392584443092346   total loss: [0.74549544]\n",
            "mse_b ====== [0.111461245]\n",
            "It: 37426, Time: 0.02\n",
            "mse_b  [0.11146124]  mse_f: 0.6354650855064392   total loss: [0.7469263]\n",
            "mse_b ====== [0.054443758]\n",
            "It: 37427, Time: 0.02\n",
            "mse_b  [0.05444376]  mse_f: 0.5880945920944214   total loss: [0.64253837]\n",
            "mse_b ====== [0.0647442117]\n",
            "It: 37428, Time: 0.02\n",
            "mse_b  [0.06474421]  mse_f: 0.6134769916534424   total loss: [0.6782212]\n",
            "mse_b ====== [0.0834528]\n",
            "It: 37429, Time: 0.02\n",
            "mse_b  [0.0834528]  mse_f: 0.5621666312217712   total loss: [0.64561945]\n",
            "mse_b ====== [0.147429913]\n",
            "It: 37430, Time: 0.03\n",
            "mse_b  [0.14742991]  mse_f: 0.6017638444900513   total loss: [0.7491938]\n",
            "mse_b ====== [0.122970574]\n",
            "It: 37431, Time: 0.02\n",
            "mse_b  [0.12297057]  mse_f: 0.5627264976501465   total loss: [0.6856971]\n",
            "mse_b ====== [0.082353726]\n",
            "It: 37432, Time: 0.03\n",
            "mse_b  [0.08235373]  mse_f: 0.6484570503234863   total loss: [0.73081076]\n",
            "mse_b ====== [0.0817108825]\n",
            "It: 37433, Time: 0.02\n",
            "mse_b  [0.08171088]  mse_f: 0.6036560535430908   total loss: [0.6853669]\n",
            "mse_b ====== [0.12637718]\n",
            "It: 37434, Time: 0.02\n",
            "mse_b  [0.12637718]  mse_f: 0.6290309429168701   total loss: [0.7554081]\n",
            "mse_b ====== [0.102993406]\n",
            "It: 37435, Time: 0.02\n",
            "mse_b  [0.10299341]  mse_f: 0.6104363799095154   total loss: [0.7134298]\n",
            "mse_b ====== [0.0911570042]\n",
            "It: 37436, Time: 0.02\n",
            "mse_b  [0.091157]  mse_f: 0.6264578700065613   total loss: [0.7176149]\n",
            "mse_b ====== [0.0514431596]\n",
            "It: 37437, Time: 0.02\n",
            "mse_b  [0.05144316]  mse_f: 0.5586569309234619   total loss: [0.6101001]\n",
            "mse_b ====== [0.0946059674]\n",
            "It: 37438, Time: 0.02\n",
            "mse_b  [0.09460597]  mse_f: 0.5801976919174194   total loss: [0.6748037]\n",
            "mse_b ====== [0.0623738319]\n",
            "It: 37439, Time: 0.03\n",
            "mse_b  [0.06237383]  mse_f: 0.5976110696792603   total loss: [0.6599849]\n",
            "mse_b ====== [0.0743396729]\n",
            "It: 37440, Time: 0.02\n",
            "mse_b  [0.07433967]  mse_f: 0.6316584944725037   total loss: [0.7059982]\n",
            "mse_b ====== [0.0776452124]\n",
            "It: 37441, Time: 0.02\n",
            "mse_b  [0.07764521]  mse_f: 0.5884808301925659   total loss: [0.666126]\n",
            "mse_b ====== [0.0791402236]\n",
            "It: 37442, Time: 0.02\n",
            "mse_b  [0.07914022]  mse_f: 0.638395369052887   total loss: [0.7175356]\n",
            "mse_b ====== [0.0772988498]\n",
            "It: 37443, Time: 0.02\n",
            "mse_b  [0.07729885]  mse_f: 0.5995955467224121   total loss: [0.6768944]\n",
            "mse_b ====== [0.0659156144]\n",
            "It: 37444, Time: 0.02\n",
            "mse_b  [0.06591561]  mse_f: 0.6358441710472107   total loss: [0.7017598]\n",
            "mse_b ====== [0.0707896724]\n",
            "It: 37445, Time: 0.02\n",
            "mse_b  [0.07078967]  mse_f: 0.5711984038352966   total loss: [0.6419881]\n",
            "mse_b ====== [0.0677764341]\n",
            "It: 37446, Time: 0.02\n",
            "mse_b  [0.06777643]  mse_f: 0.5953376889228821   total loss: [0.66311413]\n",
            "mse_b ====== [0.0575147755]\n",
            "It: 37447, Time: 0.02\n",
            "mse_b  [0.05751478]  mse_f: 0.5986058712005615   total loss: [0.65612066]\n",
            "mse_b ====== [0.0671599954]\n",
            "It: 37448, Time: 0.03\n",
            "mse_b  [0.06716]  mse_f: 0.6427627205848694   total loss: [0.70992273]\n",
            "mse_b ====== [0.072425887]\n",
            "It: 37449, Time: 0.02\n",
            "mse_b  [0.07242589]  mse_f: 0.6110931634902954   total loss: [0.68351907]\n",
            "mse_b ====== [0.0795929134]\n",
            "It: 37450, Time: 0.02\n",
            "mse_b  [0.07959291]  mse_f: 0.5983160734176636   total loss: [0.677909]\n",
            "mse_b ====== [0.0570095517]\n",
            "It: 37451, Time: 0.02\n",
            "mse_b  [0.05700955]  mse_f: 0.590143620967865   total loss: [0.6471532]\n",
            "mse_b ====== [0.0969852358]\n",
            "It: 37452, Time: 0.02\n",
            "mse_b  [0.09698524]  mse_f: 0.5742766857147217   total loss: [0.6712619]\n",
            "mse_b ====== [0.0863399357]\n",
            "It: 37453, Time: 0.02\n",
            "mse_b  [0.08633994]  mse_f: 0.5943063497543335   total loss: [0.6806463]\n",
            "mse_b ====== [0.0542961359]\n",
            "It: 37454, Time: 0.02\n",
            "mse_b  [0.05429614]  mse_f: 0.5931971669197083   total loss: [0.6474933]\n",
            "mse_b ====== [0.0502170697]\n",
            "It: 37455, Time: 0.02\n",
            "mse_b  [0.05021707]  mse_f: 0.5946532487869263   total loss: [0.64487034]\n",
            "mse_b ====== [0.0575969815]\n",
            "It: 37456, Time: 0.03\n",
            "mse_b  [0.05759698]  mse_f: 0.5972700119018555   total loss: [0.654867]\n",
            "mse_b ====== [0.0823396742]\n",
            "It: 37457, Time: 0.02\n",
            "mse_b  [0.08233967]  mse_f: 0.603784441947937   total loss: [0.6861241]\n",
            "mse_b ====== [0.056465745]\n",
            "It: 37458, Time: 0.02\n",
            "mse_b  [0.05646574]  mse_f: 0.6089147329330444   total loss: [0.6653805]\n",
            "mse_b ====== [0.0656546503]\n",
            "It: 37459, Time: 0.03\n",
            "mse_b  [0.06565465]  mse_f: 0.618518054485321   total loss: [0.6841727]\n",
            "mse_b ====== [0.07211411]\n",
            "It: 37460, Time: 0.02\n",
            "mse_b  [0.07211411]  mse_f: 0.5526601076126099   total loss: [0.6247742]\n",
            "mse_b ====== [0.102645949]\n",
            "It: 37461, Time: 0.02\n",
            "mse_b  [0.10264595]  mse_f: 0.5575175881385803   total loss: [0.6601635]\n",
            "mse_b ====== [0.0697622076]\n",
            "It: 37462, Time: 0.02\n",
            "mse_b  [0.06976221]  mse_f: 0.5602223873138428   total loss: [0.6299846]\n",
            "mse_b ====== [0.0569785386]\n",
            "It: 37463, Time: 0.02\n",
            "mse_b  [0.05697854]  mse_f: 0.6017231345176697   total loss: [0.65870166]\n",
            "mse_b ====== [0.0685271323]\n",
            "It: 37464, Time: 0.02\n",
            "mse_b  [0.06852713]  mse_f: 0.5558912754058838   total loss: [0.6244184]\n",
            "mse_b ====== [0.0964311138]\n",
            "It: 37465, Time: 0.02\n",
            "mse_b  [0.09643111]  mse_f: 0.5781139135360718   total loss: [0.67454505]\n",
            "mse_b ====== [0.080428265]\n",
            "It: 37466, Time: 0.02\n",
            "mse_b  [0.08042827]  mse_f: 0.5719537734985352   total loss: [0.652382]\n",
            "mse_b ====== [0.0599512309]\n",
            "It: 37467, Time: 0.03\n",
            "mse_b  [0.05995123]  mse_f: 0.6254130005836487   total loss: [0.68536425]\n",
            "mse_b ====== [0.0498932637]\n",
            "It: 37468, Time: 0.02\n",
            "mse_b  [0.04989326]  mse_f: 0.5918654203414917   total loss: [0.6417587]\n",
            "mse_b ====== [0.0668821409]\n",
            "It: 37469, Time: 0.02\n",
            "mse_b  [0.06688214]  mse_f: 0.579531192779541   total loss: [0.6464133]\n",
            "mse_b ====== [0.0878673196]\n",
            "It: 37470, Time: 0.02\n",
            "mse_b  [0.08786732]  mse_f: 0.5296951532363892   total loss: [0.6175625]\n",
            "mse_b ====== [0.0612760372]\n",
            "It: 37471, Time: 0.02\n",
            "mse_b  [0.06127604]  mse_f: 0.553986668586731   total loss: [0.6152627]\n",
            "mse_b ====== [0.0434905961]\n",
            "It: 37472, Time: 0.03\n",
            "mse_b  [0.0434906]  mse_f: 0.574805498123169   total loss: [0.6182961]\n",
            "mse_b ====== [0.0765042752]\n",
            "It: 37473, Time: 0.02\n",
            "mse_b  [0.07650428]  mse_f: 0.578013002872467   total loss: [0.6545173]\n",
            "mse_b ====== [0.0703874379]\n",
            "It: 37474, Time: 0.02\n",
            "mse_b  [0.07038744]  mse_f: 0.5743093490600586   total loss: [0.6446968]\n",
            "mse_b ====== [0.0720215961]\n",
            "It: 37475, Time: 0.02\n",
            "mse_b  [0.0720216]  mse_f: 0.5606036186218262   total loss: [0.6326252]\n",
            "mse_b ====== [0.0666458085]\n",
            "It: 37476, Time: 0.02\n",
            "mse_b  [0.06664581]  mse_f: 0.561977207660675   total loss: [0.628623]\n",
            "mse_b ====== [0.0643477216]\n",
            "It: 37477, Time: 0.02\n",
            "mse_b  [0.06434772]  mse_f: 0.5541117787361145   total loss: [0.6184595]\n",
            "mse_b ====== [0.0833798125]\n",
            "It: 37478, Time: 0.02\n",
            "mse_b  [0.08337981]  mse_f: 0.5570761561393738   total loss: [0.64045596]\n",
            "mse_b ====== [0.0827221721]\n",
            "It: 37479, Time: 0.02\n",
            "mse_b  [0.08272217]  mse_f: 0.5489716529846191   total loss: [0.63169384]\n",
            "mse_b ====== [0.0567169711]\n",
            "It: 37480, Time: 0.02\n",
            "mse_b  [0.05671697]  mse_f: 0.5685442686080933   total loss: [0.62526125]\n",
            "mse_b ====== [0.0438791476]\n",
            "It: 37481, Time: 0.03\n",
            "mse_b  [0.04387915]  mse_f: 0.5545206665992737   total loss: [0.5983998]\n",
            "mse_b ====== [0.0629407316]\n",
            "It: 37482, Time: 0.03\n",
            "mse_b  [0.06294073]  mse_f: 0.5668387413024902   total loss: [0.62977946]\n",
            "mse_b ====== [0.0710634738]\n",
            "It: 37483, Time: 0.02\n",
            "mse_b  [0.07106347]  mse_f: 0.5552998185157776   total loss: [0.6263633]\n",
            "mse_b ====== [0.0763773248]\n",
            "It: 37484, Time: 0.02\n",
            "mse_b  [0.07637732]  mse_f: 0.5779368877410889   total loss: [0.6543142]\n",
            "mse_b ====== [0.0670539588]\n",
            "It: 37485, Time: 0.02\n",
            "mse_b  [0.06705396]  mse_f: 0.5344797372817993   total loss: [0.6015337]\n",
            "mse_b ====== [0.0670787394]\n",
            "It: 37486, Time: 0.02\n",
            "mse_b  [0.06707874]  mse_f: 0.5436609387397766   total loss: [0.6107397]\n",
            "mse_b ====== [0.0816786885]\n",
            "It: 37487, Time: 0.02\n",
            "mse_b  [0.08167869]  mse_f: 0.5279625654220581   total loss: [0.60964125]\n",
            "mse_b ====== [0.0567067973]\n",
            "It: 37488, Time: 0.03\n",
            "mse_b  [0.0567068]  mse_f: 0.5842248797416687   total loss: [0.64093167]\n",
            "mse_b ====== [0.0524934754]\n",
            "It: 37489, Time: 0.02\n",
            "mse_b  [0.05249348]  mse_f: 0.5582851767539978   total loss: [0.6107786]\n",
            "mse_b ====== [0.0549694709]\n",
            "It: 37490, Time: 0.02\n",
            "mse_b  [0.05496947]  mse_f: 0.5735973119735718   total loss: [0.6285668]\n",
            "mse_b ====== [0.0512069128]\n",
            "It: 37491, Time: 0.02\n",
            "mse_b  [0.05120691]  mse_f: 0.5486260056495667   total loss: [0.5998329]\n",
            "mse_b ====== [0.0500508621]\n",
            "It: 37492, Time: 0.02\n",
            "mse_b  [0.05005086]  mse_f: 0.5747479200363159   total loss: [0.6247988]\n",
            "mse_b ====== [0.0509961173]\n",
            "It: 37493, Time: 0.02\n",
            "mse_b  [0.05099612]  mse_f: 0.5583077073097229   total loss: [0.60930383]\n",
            "mse_b ====== [0.0657390356]\n",
            "It: 37494, Time: 0.02\n",
            "mse_b  [0.06573904]  mse_f: 0.5457417964935303   total loss: [0.61148083]\n",
            "mse_b ====== [0.0727127194]\n",
            "It: 37495, Time: 0.03\n",
            "mse_b  [0.07271272]  mse_f: 0.5218875408172607   total loss: [0.59460026]\n",
            "mse_b ====== [0.0501621701]\n",
            "It: 37496, Time: 0.02\n",
            "mse_b  [0.05016217]  mse_f: 0.5753271579742432   total loss: [0.62548935]\n",
            "mse_b ====== [0.0544514917]\n",
            "It: 37497, Time: 0.03\n",
            "mse_b  [0.05445149]  mse_f: 0.5608956217765808   total loss: [0.6153471]\n",
            "mse_b ====== [0.0817456394]\n",
            "It: 37498, Time: 0.02\n",
            "mse_b  [0.08174564]  mse_f: 0.5356360673904419   total loss: [0.6173817]\n",
            "mse_b ====== [0.0781080723]\n",
            "It: 37499, Time: 0.02\n",
            "mse_b  [0.07810807]  mse_f: 0.5268306732177734   total loss: [0.60493875]\n",
            "mse_b ====== [0.0556372106]\n",
            "It: 37500, Time: 0.02\n",
            "mse_b  [0.05563721]  mse_f: 0.5594131946563721   total loss: [0.61505044]\n",
            "mse_b ====== [0.0554045141]\n",
            "It: 37501, Time: 0.03\n",
            "mse_b  [0.05540451]  mse_f: 0.5616058707237244   total loss: [0.61701035]\n",
            "mse_b ====== [0.0681552663]\n",
            "It: 37502, Time: 0.02\n",
            "mse_b  [0.06815527]  mse_f: 0.5402564406394958   total loss: [0.6084117]\n",
            "mse_b ====== [0.0677992478]\n",
            "It: 37503, Time: 0.03\n",
            "mse_b  [0.06779925]  mse_f: 0.5120605230331421   total loss: [0.5798598]\n",
            "mse_b ====== [0.0636390448]\n",
            "It: 37504, Time: 0.02\n",
            "mse_b  [0.06363904]  mse_f: 0.5357910990715027   total loss: [0.59943014]\n",
            "mse_b ====== [0.0577544793]\n",
            "It: 37505, Time: 0.02\n",
            "mse_b  [0.05775448]  mse_f: 0.5453165769577026   total loss: [0.60307103]\n",
            "mse_b ====== [0.0812376365]\n",
            "It: 37506, Time: 0.02\n",
            "mse_b  [0.08123764]  mse_f: 0.5236032605171204   total loss: [0.6048409]\n",
            "mse_b ====== [0.0696558282]\n",
            "It: 37507, Time: 0.02\n",
            "mse_b  [0.06965583]  mse_f: 0.5340195894241333   total loss: [0.6036754]\n",
            "mse_b ====== [0.0456756167]\n",
            "It: 37508, Time: 0.02\n",
            "mse_b  [0.04567562]  mse_f: 0.5325162410736084   total loss: [0.5781919]\n",
            "mse_b ====== [0.045978725]\n",
            "It: 37509, Time: 0.03\n",
            "mse_b  [0.04597872]  mse_f: 0.5514702796936035   total loss: [0.597449]\n",
            "mse_b ====== [0.0595152751]\n",
            "It: 37510, Time: 0.02\n",
            "mse_b  [0.05951528]  mse_f: 0.5508136749267578   total loss: [0.610329]\n",
            "mse_b ====== [0.0461420268]\n",
            "It: 37511, Time: 0.02\n",
            "mse_b  [0.04614203]  mse_f: 0.5551071166992188   total loss: [0.60124916]\n",
            "mse_b ====== [0.0443055853]\n",
            "It: 37512, Time: 0.02\n",
            "mse_b  [0.04430559]  mse_f: 0.5432201623916626   total loss: [0.5875257]\n",
            "mse_b ====== [0.0547435209]\n",
            "It: 37513, Time: 0.02\n",
            "mse_b  [0.05474352]  mse_f: 0.5341765880584717   total loss: [0.5889201]\n",
            "mse_b ====== [0.0759263188]\n",
            "It: 37514, Time: 0.02\n",
            "mse_b  [0.07592632]  mse_f: 0.5226978063583374   total loss: [0.5986241]\n",
            "mse_b ====== [0.0736587346]\n",
            "It: 37515, Time: 0.02\n",
            "mse_b  [0.07365873]  mse_f: 0.5397595167160034   total loss: [0.6134182]\n",
            "mse_b ====== [0.0473284051]\n",
            "It: 37516, Time: 0.03\n",
            "mse_b  [0.04732841]  mse_f: 0.5388492345809937   total loss: [0.58617765]\n",
            "mse_b ====== [0.055231709]\n",
            "It: 37517, Time: 0.02\n",
            "mse_b  [0.05523171]  mse_f: 0.525154173374176   total loss: [0.58038586]\n",
            "mse_b ====== [0.0562143736]\n",
            "It: 37518, Time: 0.02\n",
            "mse_b  [0.05621437]  mse_f: 0.5214781165122986   total loss: [0.5776925]\n",
            "mse_b ====== [0.0398617312]\n",
            "It: 37519, Time: 0.02\n",
            "mse_b  [0.03986173]  mse_f: 0.5594552755355835   total loss: [0.599317]\n",
            "mse_b ====== [0.0365508199]\n",
            "It: 37520, Time: 0.02\n",
            "mse_b  [0.03655082]  mse_f: 0.5531004667282104   total loss: [0.5896513]\n",
            "mse_b ====== [0.0462654047]\n",
            "It: 37521, Time: 0.02\n",
            "mse_b  [0.0462654]  mse_f: 0.5518683195114136   total loss: [0.59813374]\n",
            "mse_b ====== [0.0556806289]\n",
            "It: 37522, Time: 0.02\n",
            "mse_b  [0.05568063]  mse_f: 0.5067981481552124   total loss: [0.5624788]\n",
            "mse_b ====== [0.0502497554]\n",
            "It: 37523, Time: 0.03\n",
            "mse_b  [0.05024976]  mse_f: 0.5432256460189819   total loss: [0.5934754]\n",
            "mse_b ====== [0.0561012551]\n",
            "It: 37524, Time: 0.02\n",
            "mse_b  [0.05610126]  mse_f: 0.5359077453613281   total loss: [0.592009]\n",
            "mse_b ====== [0.0654450282]\n",
            "It: 37525, Time: 0.02\n",
            "mse_b  [0.06544503]  mse_f: 0.5266817212104797   total loss: [0.5921267]\n",
            "mse_b ====== [0.0676885471]\n",
            "It: 37526, Time: 0.02\n",
            "mse_b  [0.06768855]  mse_f: 0.5172616243362427   total loss: [0.58495015]\n",
            "mse_b ====== [0.0530220196]\n",
            "It: 37527, Time: 0.02\n",
            "mse_b  [0.05302202]  mse_f: 0.5344460010528564   total loss: [0.587468]\n",
            "mse_b ====== [0.0475585088]\n",
            "It: 37528, Time: 0.02\n",
            "mse_b  [0.04755851]  mse_f: 0.5342222452163696   total loss: [0.58178073]\n",
            "mse_b ====== [0.052116394]\n",
            "It: 37529, Time: 0.02\n",
            "mse_b  [0.05211639]  mse_f: 0.5290758609771729   total loss: [0.58119226]\n",
            "mse_b ====== [0.0495967343]\n",
            "It: 37530, Time: 0.02\n",
            "mse_b  [0.04959673]  mse_f: 0.5385786890983582   total loss: [0.5881754]\n",
            "mse_b ====== [0.054569453]\n",
            "It: 37531, Time: 0.02\n",
            "mse_b  [0.05456945]  mse_f: 0.5316094160079956   total loss: [0.5861789]\n",
            "mse_b ====== [0.0516215637]\n",
            "It: 37532, Time: 0.02\n",
            "mse_b  [0.05162156]  mse_f: 0.5485684275627136   total loss: [0.60019]\n",
            "mse_b ====== [0.0538238101]\n",
            "It: 37533, Time: 0.02\n",
            "mse_b  [0.05382381]  mse_f: 0.5237058997154236   total loss: [0.5775297]\n",
            "mse_b ====== [0.0564322621]\n",
            "It: 37534, Time: 0.03\n",
            "mse_b  [0.05643226]  mse_f: 0.5240301489830017   total loss: [0.5804624]\n",
            "mse_b ====== [0.0465557911]\n",
            "It: 37535, Time: 0.02\n",
            "mse_b  [0.04655579]  mse_f: 0.5137108564376831   total loss: [0.5602667]\n",
            "mse_b ====== [0.0471896902]\n",
            "It: 37536, Time: 0.02\n",
            "mse_b  [0.04718969]  mse_f: 0.5277138352394104   total loss: [0.57490355]\n",
            "mse_b ====== [0.0490651]\n",
            "It: 37537, Time: 0.02\n",
            "mse_b  [0.0490651]  mse_f: 0.5161897540092468   total loss: [0.56525487]\n",
            "mse_b ====== [0.0600334927]\n",
            "It: 37538, Time: 0.03\n",
            "mse_b  [0.06003349]  mse_f: 0.5138026475906372   total loss: [0.57383615]\n",
            "mse_b ====== [0.0472586788]\n",
            "It: 37539, Time: 0.03\n",
            "mse_b  [0.04725868]  mse_f: 0.5043855905532837   total loss: [0.55164427]\n",
            "mse_b ====== [0.0620090514]\n",
            "It: 37540, Time: 0.02\n",
            "mse_b  [0.06200905]  mse_f: 0.5231540203094482   total loss: [0.58516306]\n",
            "mse_b ====== [0.0612679906]\n",
            "It: 37541, Time: 0.02\n",
            "mse_b  [0.06126799]  mse_f: 0.5191341638565063   total loss: [0.58040214]\n",
            "mse_b ====== [0.0712694824]\n",
            "It: 37542, Time: 0.02\n",
            "mse_b  [0.07126948]  mse_f: 0.5167770385742188   total loss: [0.58804655]\n",
            "mse_b ====== [0.0643268377]\n",
            "It: 37543, Time: 0.02\n",
            "mse_b  [0.06432684]  mse_f: 0.5058995485305786   total loss: [0.5702264]\n",
            "mse_b ====== [0.0584820844]\n",
            "It: 37544, Time: 0.02\n",
            "mse_b  [0.05848208]  mse_f: 0.5152876973152161   total loss: [0.5737698]\n",
            "mse_b ====== [0.0496850125]\n",
            "It: 37545, Time: 0.03\n",
            "mse_b  [0.04968501]  mse_f: 0.5089016556739807   total loss: [0.55858666]\n",
            "mse_b ====== [0.0589439273]\n",
            "It: 37546, Time: 0.02\n",
            "mse_b  [0.05894393]  mse_f: 0.5159571170806885   total loss: [0.57490104]\n",
            "mse_b ====== [0.0414948091]\n",
            "It: 37547, Time: 0.02\n",
            "mse_b  [0.04149481]  mse_f: 0.5176125764846802   total loss: [0.55910736]\n",
            "mse_b ====== [0.0329211466]\n",
            "It: 37548, Time: 0.02\n",
            "mse_b  [0.03292115]  mse_f: 0.5304706692695618   total loss: [0.5633918]\n",
            "mse_b ====== [0.0661223158]\n",
            "It: 37549, Time: 0.02\n",
            "mse_b  [0.06612232]  mse_f: 0.4960070848464966   total loss: [0.5621294]\n",
            "mse_b ====== [0.0605070665]\n",
            "It: 37550, Time: 0.02\n",
            "mse_b  [0.06050707]  mse_f: 0.512000322341919   total loss: [0.5725074]\n",
            "mse_b ====== [0.0380687602]\n",
            "It: 37551, Time: 0.02\n",
            "mse_b  [0.03806876]  mse_f: 0.5309632420539856   total loss: [0.569032]\n",
            "mse_b ====== [0.0401887335]\n",
            "It: 37552, Time: 0.03\n",
            "mse_b  [0.04018873]  mse_f: 0.5231753587722778   total loss: [0.5633641]\n",
            "mse_b ====== [0.0495040715]\n",
            "It: 37553, Time: 0.03\n",
            "mse_b  [0.04950407]  mse_f: 0.507030725479126   total loss: [0.55653477]\n",
            "mse_b ====== [0.0423023328]\n",
            "It: 37554, Time: 0.02\n",
            "mse_b  [0.04230233]  mse_f: 0.5171751976013184   total loss: [0.5594775]\n",
            "mse_b ====== [0.0386463851]\n",
            "It: 37555, Time: 0.02\n",
            "mse_b  [0.03864639]  mse_f: 0.5280358791351318   total loss: [0.5666823]\n",
            "mse_b ====== [0.0480915196]\n",
            "It: 37556, Time: 0.02\n",
            "mse_b  [0.04809152]  mse_f: 0.5157284736633301   total loss: [0.56382]\n",
            "mse_b ====== [0.0564949885]\n",
            "It: 37557, Time: 0.02\n",
            "mse_b  [0.05649499]  mse_f: 0.5226799249649048   total loss: [0.57917494]\n",
            "mse_b ====== [0.0458341055]\n",
            "It: 37558, Time: 0.02\n",
            "mse_b  [0.04583411]  mse_f: 0.5130112767219543   total loss: [0.5588454]\n",
            "mse_b ====== [0.0453521684]\n",
            "It: 37559, Time: 0.03\n",
            "mse_b  [0.04535217]  mse_f: 0.5198128819465637   total loss: [0.56516504]\n",
            "mse_b ====== [0.0600512028]\n",
            "It: 37560, Time: 0.02\n",
            "mse_b  [0.0600512]  mse_f: 0.49594172835350037   total loss: [0.55599296]\n",
            "mse_b ====== [0.052494254]\n",
            "It: 37561, Time: 0.02\n",
            "mse_b  [0.05249425]  mse_f: 0.5087313652038574   total loss: [0.5612256]\n",
            "mse_b ====== [0.0333669186]\n",
            "It: 37562, Time: 0.02\n",
            "mse_b  [0.03336692]  mse_f: 0.5181272029876709   total loss: [0.5514941]\n",
            "mse_b ====== [0.0436079502]\n",
            "It: 37563, Time: 0.02\n",
            "mse_b  [0.04360795]  mse_f: 0.5194903016090393   total loss: [0.56309825]\n",
            "mse_b ====== [0.0481493212]\n",
            "It: 37564, Time: 0.02\n",
            "mse_b  [0.04814932]  mse_f: 0.5024604797363281   total loss: [0.5506098]\n",
            "mse_b ====== [0.0535421]\n",
            "It: 37565, Time: 0.02\n",
            "mse_b  [0.0535421]  mse_f: 0.5098053216934204   total loss: [0.5633474]\n",
            "mse_b ====== [0.0387027971]\n",
            "It: 37566, Time: 0.02\n",
            "mse_b  [0.0387028]  mse_f: 0.5216277837753296   total loss: [0.56033057]\n",
            "mse_b ====== [0.0417902134]\n",
            "It: 37567, Time: 0.03\n",
            "mse_b  [0.04179021]  mse_f: 0.5170265436172485   total loss: [0.55881673]\n",
            "mse_b ====== [0.0410028]\n",
            "It: 37568, Time: 0.02\n",
            "mse_b  [0.0410028]  mse_f: 0.5044513940811157   total loss: [0.5454542]\n",
            "mse_b ====== [0.0371331386]\n",
            "It: 37569, Time: 0.02\n",
            "mse_b  [0.03713314]  mse_f: 0.5082587003707886   total loss: [0.54539186]\n",
            "mse_b ====== [0.0438102111]\n",
            "It: 37570, Time: 0.02\n",
            "mse_b  [0.04381021]  mse_f: 0.5025535821914673   total loss: [0.5463638]\n",
            "mse_b ====== [0.0462599844]\n",
            "It: 37571, Time: 0.02\n",
            "mse_b  [0.04625998]  mse_f: 0.5096224546432495   total loss: [0.55588245]\n",
            "mse_b ====== [0.0455971956]\n",
            "It: 37572, Time: 0.02\n",
            "mse_b  [0.0455972]  mse_f: 0.5127712488174438   total loss: [0.55836844]\n",
            "mse_b ====== [0.0452471823]\n",
            "It: 37573, Time: 0.02\n",
            "mse_b  [0.04524718]  mse_f: 0.5127249956130981   total loss: [0.5579722]\n",
            "mse_b ====== [0.0530692972]\n",
            "It: 37574, Time: 0.03\n",
            "mse_b  [0.0530693]  mse_f: 0.5062934160232544   total loss: [0.5593627]\n",
            "mse_b ====== [0.0401138067]\n",
            "It: 37575, Time: 0.03\n",
            "mse_b  [0.04011381]  mse_f: 0.5085728764533997   total loss: [0.5486867]\n",
            "mse_b ====== [0.0505064242]\n",
            "It: 37576, Time: 0.02\n",
            "mse_b  [0.05050642]  mse_f: 0.5022590756416321   total loss: [0.5527655]\n",
            "mse_b ====== [0.0482517481]\n",
            "It: 37577, Time: 0.02\n",
            "mse_b  [0.04825175]  mse_f: 0.4964487552642822   total loss: [0.5447005]\n",
            "mse_b ====== [0.0441352352]\n",
            "It: 37578, Time: 0.02\n",
            "mse_b  [0.04413524]  mse_f: 0.5010373592376709   total loss: [0.5451726]\n",
            "mse_b ====== [0.0399411619]\n",
            "It: 37579, Time: 0.02\n",
            "mse_b  [0.03994116]  mse_f: 0.5047622919082642   total loss: [0.5447035]\n",
            "mse_b ====== [0.0438250192]\n",
            "It: 37580, Time: 0.02\n",
            "mse_b  [0.04382502]  mse_f: 0.5064873695373535   total loss: [0.5503124]\n",
            "mse_b ====== [0.0397374146]\n",
            "It: 37581, Time: 0.02\n",
            "mse_b  [0.03973741]  mse_f: 0.4996812045574188   total loss: [0.53941864]\n",
            "mse_b ====== [0.0359855816]\n",
            "It: 37582, Time: 0.02\n",
            "mse_b  [0.03598558]  mse_f: 0.522345244884491   total loss: [0.55833083]\n",
            "mse_b ====== [0.0428334661]\n",
            "It: 37583, Time: 0.03\n",
            "mse_b  [0.04283347]  mse_f: 0.5043012499809265   total loss: [0.5471347]\n",
            "mse_b ====== [0.0466159731]\n",
            "It: 37584, Time: 0.03\n",
            "mse_b  [0.04661597]  mse_f: 0.5044234395027161   total loss: [0.5510394]\n",
            "mse_b ====== [0.0415367298]\n",
            "It: 37585, Time: 0.03\n",
            "mse_b  [0.04153673]  mse_f: 0.49611926078796387   total loss: [0.537656]\n",
            "mse_b ====== [0.0386705473]\n",
            "It: 37586, Time: 0.03\n",
            "mse_b  [0.03867055]  mse_f: 0.5084505081176758   total loss: [0.54712105]\n",
            "mse_b ====== [0.0516583771]\n",
            "It: 37587, Time: 0.03\n",
            "mse_b  [0.05165838]  mse_f: 0.49453988671302795   total loss: [0.54619825]\n",
            "mse_b ====== [0.0566728562]\n",
            "It: 37588, Time: 0.03\n",
            "mse_b  [0.05667286]  mse_f: 0.4995843172073364   total loss: [0.5562572]\n",
            "mse_b ====== [0.0457226783]\n",
            "It: 37589, Time: 0.03\n",
            "mse_b  [0.04572268]  mse_f: 0.49862563610076904   total loss: [0.5443483]\n",
            "mse_b ====== [0.0398217142]\n",
            "It: 37590, Time: 0.02\n",
            "mse_b  [0.03982171]  mse_f: 0.5049967765808105   total loss: [0.5448185]\n",
            "mse_b ====== [0.0450854041]\n",
            "It: 37591, Time: 0.02\n",
            "mse_b  [0.0450854]  mse_f: 0.49901390075683594   total loss: [0.54409933]\n",
            "mse_b ====== [0.0428522564]\n",
            "It: 37592, Time: 0.02\n",
            "mse_b  [0.04285226]  mse_f: 0.49867957830429077   total loss: [0.54153186]\n",
            "mse_b ====== [0.0308749154]\n",
            "It: 37593, Time: 0.02\n",
            "mse_b  [0.03087492]  mse_f: 0.512091338634491   total loss: [0.54296625]\n",
            "mse_b ====== [0.0378796607]\n",
            "It: 37594, Time: 0.02\n",
            "mse_b  [0.03787966]  mse_f: 0.49181923270225525   total loss: [0.5296989]\n",
            "mse_b ====== [0.0520943888]\n",
            "It: 37595, Time: 0.02\n",
            "mse_b  [0.05209439]  mse_f: 0.49685394763946533   total loss: [0.54894835]\n",
            "mse_b ====== [0.0417869538]\n",
            "It: 37596, Time: 0.02\n",
            "mse_b  [0.04178695]  mse_f: 0.5068052411079407   total loss: [0.5485922]\n",
            "mse_b ====== [0.0401017144]\n",
            "It: 37597, Time: 0.02\n",
            "mse_b  [0.04010171]  mse_f: 0.5103667974472046   total loss: [0.5504685]\n",
            "mse_b ====== [0.0474775508]\n",
            "It: 37598, Time: 0.02\n",
            "mse_b  [0.04747755]  mse_f: 0.48702472448349   total loss: [0.53450227]\n",
            "mse_b ====== [0.0437255651]\n",
            "It: 37599, Time: 0.02\n",
            "mse_b  [0.04372557]  mse_f: 0.4942052662372589   total loss: [0.53793085]\n",
            "mse_b ====== [0.0322393551]\n",
            "It: 37600, Time: 0.02\n",
            "mse_b  [0.03223936]  mse_f: 0.4986846446990967   total loss: [0.530924]\n",
            "mse_b ====== [0.049221091]\n",
            "It: 37601, Time: 0.02\n",
            "mse_b  [0.04922109]  mse_f: 0.4970797300338745   total loss: [0.5463008]\n",
            "mse_b ====== [0.0501903296]\n",
            "It: 37602, Time: 0.02\n",
            "mse_b  [0.05019033]  mse_f: 0.4827269911766052   total loss: [0.5329173]\n",
            "mse_b ====== [0.0422268026]\n",
            "It: 37603, Time: 0.02\n",
            "mse_b  [0.0422268]  mse_f: 0.4994392395019531   total loss: [0.54166603]\n",
            "mse_b ====== [0.0425576456]\n",
            "It: 37604, Time: 0.03\n",
            "mse_b  [0.04255765]  mse_f: 0.49584388732910156   total loss: [0.53840154]\n",
            "mse_b ====== [0.0521837734]\n",
            "It: 37605, Time: 0.03\n",
            "mse_b  [0.05218377]  mse_f: 0.4942203462123871   total loss: [0.5464041]\n",
            "mse_b ====== [0.0391517878]\n",
            "It: 37606, Time: 0.03\n",
            "mse_b  [0.03915179]  mse_f: 0.4971769154071808   total loss: [0.5363287]\n",
            "mse_b ====== [0.0374938175]\n",
            "It: 37607, Time: 0.02\n",
            "mse_b  [0.03749382]  mse_f: 0.497132807970047   total loss: [0.5346266]\n",
            "mse_b ====== [0.0456541032]\n",
            "It: 37608, Time: 0.03\n",
            "mse_b  [0.0456541]  mse_f: 0.48953965306282043   total loss: [0.53519374]\n",
            "mse_b ====== [0.040965952]\n",
            "It: 37609, Time: 0.03\n",
            "mse_b  [0.04096595]  mse_f: 0.49093371629714966   total loss: [0.5318997]\n",
            "mse_b ====== [0.0417873971]\n",
            "It: 37610, Time: 0.02\n",
            "mse_b  [0.0417874]  mse_f: 0.49438825249671936   total loss: [0.53617567]\n",
            "mse_b ====== [0.0397903807]\n",
            "It: 37611, Time: 0.02\n",
            "mse_b  [0.03979038]  mse_f: 0.4885800778865814   total loss: [0.52837044]\n",
            "mse_b ====== [0.0414351]\n",
            "It: 37612, Time: 0.02\n",
            "mse_b  [0.0414351]  mse_f: 0.5013163089752197   total loss: [0.54275143]\n",
            "mse_b ====== [0.0319625884]\n",
            "It: 37613, Time: 0.02\n",
            "mse_b  [0.03196259]  mse_f: 0.503196120262146   total loss: [0.5351587]\n",
            "mse_b ====== [0.0411063507]\n",
            "It: 37614, Time: 0.02\n",
            "mse_b  [0.04110635]  mse_f: 0.4961947500705719   total loss: [0.5373011]\n",
            "mse_b ====== [0.0414168797]\n",
            "It: 37615, Time: 0.02\n",
            "mse_b  [0.04141688]  mse_f: 0.4835054576396942   total loss: [0.5249223]\n",
            "mse_b ====== [0.0319581367]\n",
            "It: 37616, Time: 0.02\n",
            "mse_b  [0.03195814]  mse_f: 0.4970371127128601   total loss: [0.5289953]\n",
            "mse_b ====== [0.0366129577]\n",
            "It: 37617, Time: 0.03\n",
            "mse_b  [0.03661296]  mse_f: 0.49689456820487976   total loss: [0.5335075]\n",
            "mse_b ====== [0.0489744842]\n",
            "It: 37618, Time: 0.02\n",
            "mse_b  [0.04897448]  mse_f: 0.4937642216682434   total loss: [0.5427387]\n",
            "mse_b ====== [0.0424556062]\n",
            "It: 37619, Time: 0.03\n",
            "mse_b  [0.04245561]  mse_f: 0.484255313873291   total loss: [0.5267109]\n",
            "mse_b ====== [0.0457288027]\n",
            "It: 37620, Time: 0.03\n",
            "mse_b  [0.0457288]  mse_f: 0.4925793409347534   total loss: [0.53830814]\n",
            "mse_b ====== [0.0506288111]\n",
            "It: 37621, Time: 0.02\n",
            "mse_b  [0.05062881]  mse_f: 0.4794212579727173   total loss: [0.53005004]\n",
            "mse_b ====== [0.0447974019]\n",
            "It: 37622, Time: 0.02\n",
            "mse_b  [0.0447974]  mse_f: 0.48877593874931335   total loss: [0.5335733]\n",
            "mse_b ====== [0.0346025713]\n",
            "It: 37623, Time: 0.02\n",
            "mse_b  [0.03460257]  mse_f: 0.49107417464256287   total loss: [0.5256767]\n",
            "mse_b ====== [0.0351956822]\n",
            "It: 37624, Time: 0.02\n",
            "mse_b  [0.03519568]  mse_f: 0.4842955470085144   total loss: [0.51949126]\n",
            "mse_b ====== [0.0463422425]\n",
            "It: 37625, Time: 0.02\n",
            "mse_b  [0.04634224]  mse_f: 0.48034361004829407   total loss: [0.52668583]\n",
            "mse_b ====== [0.0398001783]\n",
            "It: 37626, Time: 0.02\n",
            "mse_b  [0.03980018]  mse_f: 0.49391672015190125   total loss: [0.5337169]\n",
            "mse_b ====== [0.0370728672]\n",
            "It: 37627, Time: 0.03\n",
            "mse_b  [0.03707287]  mse_f: 0.49822524189949036   total loss: [0.5352981]\n",
            "mse_b ====== [0.0466876253]\n",
            "It: 37628, Time: 0.03\n",
            "mse_b  [0.04668763]  mse_f: 0.48089587688446045   total loss: [0.5275835]\n",
            "mse_b ====== [0.0382160358]\n",
            "It: 37629, Time: 0.02\n",
            "mse_b  [0.03821604]  mse_f: 0.4907302260398865   total loss: [0.5289463]\n",
            "mse_b ====== [0.0343117192]\n",
            "It: 37630, Time: 0.02\n",
            "mse_b  [0.03431172]  mse_f: 0.4912406802177429   total loss: [0.5255524]\n",
            "mse_b ====== [0.0444756038]\n",
            "It: 37631, Time: 0.02\n",
            "mse_b  [0.0444756]  mse_f: 0.48578986525535583   total loss: [0.53026545]\n",
            "mse_b ====== [0.0421649441]\n",
            "It: 37632, Time: 0.02\n",
            "mse_b  [0.04216494]  mse_f: 0.47345876693725586   total loss: [0.5156237]\n",
            "mse_b ====== [0.0371479951]\n",
            "It: 37633, Time: 0.02\n",
            "mse_b  [0.037148]  mse_f: 0.49236443638801575   total loss: [0.5295124]\n",
            "mse_b ====== [0.0441172272]\n",
            "It: 37634, Time: 0.02\n",
            "mse_b  [0.04411723]  mse_f: 0.48080235719680786   total loss: [0.52491957]\n",
            "mse_b ====== [0.0440207794]\n",
            "It: 37635, Time: 0.02\n",
            "mse_b  [0.04402078]  mse_f: 0.48911964893341064   total loss: [0.5331404]\n",
            "mse_b ====== [0.0342068784]\n",
            "It: 37636, Time: 0.03\n",
            "mse_b  [0.03420688]  mse_f: 0.49543505907058716   total loss: [0.5296419]\n",
            "mse_b ====== [0.0357286111]\n",
            "It: 37637, Time: 0.02\n",
            "mse_b  [0.03572861]  mse_f: 0.49132630228996277   total loss: [0.5270549]\n",
            "mse_b ====== [0.0423659682]\n",
            "It: 37638, Time: 0.02\n",
            "mse_b  [0.04236597]  mse_f: 0.4810997545719147   total loss: [0.52346575]\n",
            "mse_b ====== [0.0342178494]\n",
            "It: 37639, Time: 0.02\n",
            "mse_b  [0.03421785]  mse_f: 0.49035871028900146   total loss: [0.52457654]\n",
            "mse_b ====== [0.0330057815]\n",
            "It: 37640, Time: 0.03\n",
            "mse_b  [0.03300578]  mse_f: 0.4867030084133148   total loss: [0.5197088]\n",
            "mse_b ====== [0.0460495353]\n",
            "It: 37641, Time: 0.02\n",
            "mse_b  [0.04604954]  mse_f: 0.474216103553772   total loss: [0.52026564]\n",
            "mse_b ====== [0.0403065532]\n",
            "It: 37642, Time: 0.02\n",
            "mse_b  [0.04030655]  mse_f: 0.48638129234313965   total loss: [0.52668786]\n",
            "mse_b ====== [0.0337884389]\n",
            "It: 37643, Time: 0.03\n",
            "mse_b  [0.03378844]  mse_f: 0.4908921718597412   total loss: [0.5246806]\n",
            "mse_b ====== [0.0409355]\n",
            "It: 37644, Time: 0.02\n",
            "mse_b  [0.0409355]  mse_f: 0.48207831382751465   total loss: [0.52301383]\n",
            "mse_b ====== [0.0434933677]\n",
            "It: 37645, Time: 0.02\n",
            "mse_b  [0.04349337]  mse_f: 0.47281453013420105   total loss: [0.5163079]\n",
            "mse_b ====== [0.0420685634]\n",
            "It: 37646, Time: 0.02\n",
            "mse_b  [0.04206856]  mse_f: 0.4833621382713318   total loss: [0.5254307]\n",
            "mse_b ====== [0.0467600338]\n",
            "It: 37647, Time: 0.03\n",
            "mse_b  [0.04676003]  mse_f: 0.47409671545028687   total loss: [0.52085674]\n",
            "mse_b ====== [0.0500071347]\n",
            "It: 37648, Time: 0.02\n",
            "mse_b  [0.05000713]  mse_f: 0.47682076692581177   total loss: [0.52682793]\n",
            "mse_b ====== [0.0391741246]\n",
            "It: 37649, Time: 0.02\n",
            "mse_b  [0.03917412]  mse_f: 0.47758644819259644   total loss: [0.5167606]\n",
            "mse_b ====== [0.041054748]\n",
            "It: 37650, Time: 0.02\n",
            "mse_b  [0.04105475]  mse_f: 0.4819002151489258   total loss: [0.52295494]\n",
            "mse_b ====== [0.0473755635]\n",
            "It: 37651, Time: 0.02\n",
            "mse_b  [0.04737556]  mse_f: 0.4754640460014343   total loss: [0.5228396]\n",
            "mse_b ====== [0.0454366244]\n",
            "It: 37652, Time: 0.02\n",
            "mse_b  [0.04543662]  mse_f: 0.4803292155265808   total loss: [0.52576584]\n",
            "mse_b ====== [0.0357241631]\n",
            "It: 37653, Time: 0.02\n",
            "mse_b  [0.03572416]  mse_f: 0.4812310039997101   total loss: [0.51695514]\n",
            "mse_b ====== [0.0363097377]\n",
            "It: 37654, Time: 0.03\n",
            "mse_b  [0.03630974]  mse_f: 0.4774419665336609   total loss: [0.5137517]\n",
            "mse_b ====== [0.0356315114]\n",
            "It: 37655, Time: 0.03\n",
            "mse_b  [0.03563151]  mse_f: 0.4815102815628052   total loss: [0.5171418]\n",
            "mse_b ====== [0.0330143832]\n",
            "It: 37656, Time: 0.02\n",
            "mse_b  [0.03301438]  mse_f: 0.4852592945098877   total loss: [0.51827365]\n",
            "mse_b ====== [0.0408893265]\n",
            "It: 37657, Time: 0.03\n",
            "mse_b  [0.04088933]  mse_f: 0.47951000928878784   total loss: [0.52039933]\n",
            "mse_b ====== [0.0430454314]\n",
            "It: 37658, Time: 0.02\n",
            "mse_b  [0.04304543]  mse_f: 0.47658732533454895   total loss: [0.51963276]\n",
            "mse_b ====== [0.0354744568]\n",
            "It: 37659, Time: 0.02\n",
            "mse_b  [0.03547446]  mse_f: 0.4855884313583374   total loss: [0.5210629]\n",
            "mse_b ====== [0.0358172581]\n",
            "It: 37660, Time: 0.02\n",
            "mse_b  [0.03581726]  mse_f: 0.4787938594818115   total loss: [0.5146111]\n",
            "mse_b ====== [0.0434133336]\n",
            "It: 37661, Time: 0.02\n",
            "mse_b  [0.04341333]  mse_f: 0.4767761528491974   total loss: [0.52018946]\n",
            "mse_b ====== [0.0349761099]\n",
            "It: 37662, Time: 0.02\n",
            "mse_b  [0.03497611]  mse_f: 0.4775005578994751   total loss: [0.5124767]\n",
            "mse_b ====== [0.0382946767]\n",
            "It: 37663, Time: 0.02\n",
            "mse_b  [0.03829468]  mse_f: 0.4816722273826599   total loss: [0.5199669]\n",
            "mse_b ====== [0.0458297133]\n",
            "It: 37664, Time: 0.02\n",
            "mse_b  [0.04582971]  mse_f: 0.4709692597389221   total loss: [0.516799]\n",
            "mse_b ====== [0.0413013399]\n",
            "It: 37665, Time: 0.02\n",
            "mse_b  [0.04130134]  mse_f: 0.47673410177230835   total loss: [0.5180354]\n",
            "mse_b ====== [0.0350641944]\n",
            "It: 37666, Time: 0.02\n",
            "mse_b  [0.03506419]  mse_f: 0.48168912529945374   total loss: [0.5167533]\n",
            "mse_b ====== [0.0452569909]\n",
            "It: 37667, Time: 0.02\n",
            "mse_b  [0.04525699]  mse_f: 0.4722789525985718   total loss: [0.5175359]\n",
            "mse_b ====== [0.0411607362]\n",
            "It: 37668, Time: 0.03\n",
            "mse_b  [0.04116074]  mse_f: 0.47451695799827576   total loss: [0.5156777]\n",
            "mse_b ====== [0.0331555642]\n",
            "It: 37669, Time: 0.03\n",
            "mse_b  [0.03315556]  mse_f: 0.4793024957180023   total loss: [0.5124581]\n",
            "mse_b ====== [0.0372020379]\n",
            "It: 37670, Time: 0.02\n",
            "mse_b  [0.03720204]  mse_f: 0.4757847189903259   total loss: [0.5129868]\n",
            "mse_b ====== [0.0377343632]\n",
            "It: 37671, Time: 0.02\n",
            "mse_b  [0.03773436]  mse_f: 0.47625118494033813   total loss: [0.5139856]\n",
            "mse_b ====== [0.0308167022]\n",
            "It: 37672, Time: 0.02\n",
            "mse_b  [0.0308167]  mse_f: 0.48767074942588806   total loss: [0.51848745]\n",
            "mse_b ====== [0.0337573215]\n",
            "It: 37673, Time: 0.02\n",
            "mse_b  [0.03375732]  mse_f: 0.47771063446998596   total loss: [0.51146793]\n",
            "mse_b ====== [0.0434225798]\n",
            "It: 37674, Time: 0.02\n",
            "mse_b  [0.04342258]  mse_f: 0.4724261164665222   total loss: [0.5158487]\n",
            "mse_b ====== [0.0324222408]\n",
            "It: 37675, Time: 0.02\n",
            "mse_b  [0.03242224]  mse_f: 0.47838470339775085   total loss: [0.5108069]\n",
            "mse_b ====== [0.0327646583]\n",
            "It: 37676, Time: 0.02\n",
            "mse_b  [0.03276466]  mse_f: 0.48321160674095154   total loss: [0.51597625]\n",
            "mse_b ====== [0.038822379]\n",
            "It: 37677, Time: 0.02\n",
            "mse_b  [0.03882238]  mse_f: 0.4740593731403351   total loss: [0.51288176]\n",
            "mse_b ====== [0.0358642973]\n",
            "It: 37678, Time: 0.02\n",
            "mse_b  [0.0358643]  mse_f: 0.4760937988758087   total loss: [0.5119581]\n",
            "mse_b ====== [0.0336480923]\n",
            "It: 37679, Time: 0.03\n",
            "mse_b  [0.03364809]  mse_f: 0.47852465510368347   total loss: [0.51217276]\n",
            "mse_b ====== [0.0396892466]\n",
            "It: 37680, Time: 0.03\n",
            "mse_b  [0.03968925]  mse_f: 0.4750325083732605   total loss: [0.51472175]\n",
            "mse_b ====== [0.0361661]\n",
            "It: 37681, Time: 0.03\n",
            "mse_b  [0.0361661]  mse_f: 0.4776434302330017   total loss: [0.51380956]\n",
            "mse_b ====== [0.0382783301]\n",
            "It: 37682, Time: 0.02\n",
            "mse_b  [0.03827833]  mse_f: 0.472956120967865   total loss: [0.51123446]\n",
            "mse_b ====== [0.0439892784]\n",
            "It: 37683, Time: 0.02\n",
            "mse_b  [0.04398928]  mse_f: 0.4681641459465027   total loss: [0.51215345]\n",
            "mse_b ====== [0.0367101207]\n",
            "It: 37684, Time: 0.02\n",
            "mse_b  [0.03671012]  mse_f: 0.4690436124801636   total loss: [0.50575376]\n",
            "mse_b ====== [0.0358756185]\n",
            "It: 37685, Time: 0.02\n",
            "mse_b  [0.03587562]  mse_f: 0.473550021648407   total loss: [0.50942564]\n",
            "mse_b ====== [0.0390674546]\n",
            "It: 37686, Time: 0.02\n",
            "mse_b  [0.03906745]  mse_f: 0.46907278895378113   total loss: [0.50814027]\n",
            "mse_b ====== [0.0387857854]\n",
            "It: 37687, Time: 0.02\n",
            "mse_b  [0.03878579]  mse_f: 0.4759647846221924   total loss: [0.5147506]\n",
            "mse_b ====== [0.0335057452]\n",
            "It: 37688, Time: 0.02\n",
            "mse_b  [0.03350575]  mse_f: 0.4787571132183075   total loss: [0.5122629]\n",
            "mse_b ====== [0.0427597687]\n",
            "It: 37689, Time: 0.02\n",
            "mse_b  [0.04275977]  mse_f: 0.46960651874542236   total loss: [0.5123663]\n",
            "mse_b ====== [0.0405001268]\n",
            "It: 37690, Time: 0.02\n",
            "mse_b  [0.04050013]  mse_f: 0.46730971336364746   total loss: [0.5078098]\n",
            "mse_b ====== [0.0342130736]\n",
            "It: 37691, Time: 0.02\n",
            "mse_b  [0.03421307]  mse_f: 0.47646647691726685   total loss: [0.51067954]\n",
            "mse_b ====== [0.0420105]\n",
            "It: 37692, Time: 0.02\n",
            "mse_b  [0.0420105]  mse_f: 0.4691063165664673   total loss: [0.5111168]\n",
            "mse_b ====== [0.0450769216]\n",
            "It: 37693, Time: 0.02\n",
            "mse_b  [0.04507692]  mse_f: 0.46275514364242554   total loss: [0.50783205]\n",
            "mse_b ====== [0.0354796611]\n",
            "It: 37694, Time: 0.02\n",
            "mse_b  [0.03547966]  mse_f: 0.47224313020706177   total loss: [0.5077228]\n",
            "mse_b ====== [0.0371886827]\n",
            "It: 37695, Time: 0.02\n",
            "mse_b  [0.03718868]  mse_f: 0.4699869453907013   total loss: [0.5071756]\n",
            "mse_b ====== [0.0412994064]\n",
            "It: 37696, Time: 0.02\n",
            "mse_b  [0.04129941]  mse_f: 0.4712919294834137   total loss: [0.51259136]\n",
            "mse_b ====== [0.0329765156]\n",
            "It: 37697, Time: 0.02\n",
            "mse_b  [0.03297652]  mse_f: 0.4748762845993042   total loss: [0.5078528]\n",
            "mse_b ====== [0.033554785]\n",
            "It: 37698, Time: 0.03\n",
            "mse_b  [0.03355478]  mse_f: 0.4737352728843689   total loss: [0.50729007]\n",
            "mse_b ====== [0.0374659374]\n",
            "It: 37699, Time: 0.02\n",
            "mse_b  [0.03746594]  mse_f: 0.46666038036346436   total loss: [0.5041263]\n",
            "mse_b ====== [0.0380348675]\n",
            "It: 37700, Time: 0.03\n",
            "mse_b  [0.03803487]  mse_f: 0.4699033498764038   total loss: [0.5079382]\n",
            "mse_b ====== [0.0325965472]\n",
            "It: 37701, Time: 0.03\n",
            "mse_b  [0.03259655]  mse_f: 0.4733436703681946   total loss: [0.5059402]\n",
            "mse_b ====== [0.0348301232]\n",
            "It: 37702, Time: 0.03\n",
            "mse_b  [0.03483012]  mse_f: 0.47254478931427   total loss: [0.5073749]\n",
            "mse_b ====== [0.0320821181]\n",
            "It: 37703, Time: 0.02\n",
            "mse_b  [0.03208212]  mse_f: 0.47444069385528564   total loss: [0.50652283]\n",
            "mse_b ====== [0.0350285694]\n",
            "It: 37704, Time: 0.03\n",
            "mse_b  [0.03502857]  mse_f: 0.472016841173172   total loss: [0.5070454]\n",
            "mse_b ====== [0.0401729271]\n",
            "It: 37705, Time: 0.02\n",
            "mse_b  [0.04017293]  mse_f: 0.46841514110565186   total loss: [0.5085881]\n",
            "mse_b ====== [0.0360443555]\n",
            "It: 37706, Time: 0.03\n",
            "mse_b  [0.03604436]  mse_f: 0.4663439393043518   total loss: [0.5023883]\n",
            "mse_b ====== [0.0389226489]\n",
            "It: 37707, Time: 0.02\n",
            "mse_b  [0.03892265]  mse_f: 0.46706581115722656   total loss: [0.5059885]\n",
            "mse_b ====== [0.0429814719]\n",
            "It: 37708, Time: 0.02\n",
            "mse_b  [0.04298147]  mse_f: 0.46209701895713806   total loss: [0.5050785]\n",
            "mse_b ====== [0.036130324]\n",
            "It: 37709, Time: 0.03\n",
            "mse_b  [0.03613032]  mse_f: 0.4705812633037567   total loss: [0.5067116]\n",
            "mse_b ====== [0.0363881066]\n",
            "It: 37710, Time: 0.02\n",
            "mse_b  [0.03638811]  mse_f: 0.46638333797454834   total loss: [0.50277144]\n",
            "mse_b ====== [0.0435622334]\n",
            "It: 37711, Time: 0.02\n",
            "mse_b  [0.04356223]  mse_f: 0.4630637764930725   total loss: [0.506626]\n",
            "mse_b ====== [0.034712594]\n",
            "It: 37712, Time: 0.02\n",
            "mse_b  [0.03471259]  mse_f: 0.4688957631587982   total loss: [0.50360835]\n",
            "mse_b ====== [0.032731723]\n",
            "It: 37713, Time: 0.02\n",
            "mse_b  [0.03273172]  mse_f: 0.473128080368042   total loss: [0.5058598]\n",
            "mse_b ====== [0.0387798287]\n",
            "It: 37714, Time: 0.03\n",
            "mse_b  [0.03877983]  mse_f: 0.46401554346084595   total loss: [0.5027954]\n",
            "mse_b ====== [0.0368092433]\n",
            "It: 37715, Time: 0.02\n",
            "mse_b  [0.03680924]  mse_f: 0.4634922742843628   total loss: [0.50030154]\n",
            "mse_b ====== [0.0313974656]\n",
            "It: 37716, Time: 0.02\n",
            "mse_b  [0.03139747]  mse_f: 0.4748154580593109   total loss: [0.50621295]\n",
            "mse_b ====== [0.0389863066]\n",
            "It: 37717, Time: 0.02\n",
            "mse_b  [0.03898631]  mse_f: 0.4645565450191498   total loss: [0.50354284]\n",
            "mse_b ====== [0.0389125794]\n",
            "It: 37718, Time: 0.02\n",
            "mse_b  [0.03891258]  mse_f: 0.465861439704895   total loss: [0.50477403]\n",
            "mse_b ====== [0.03188245]\n",
            "It: 37719, Time: 0.02\n",
            "mse_b  [0.03188245]  mse_f: 0.4705061912536621   total loss: [0.50238866]\n",
            "mse_b ====== [0.0362818502]\n",
            "It: 37720, Time: 0.02\n",
            "mse_b  [0.03628185]  mse_f: 0.4714607000350952   total loss: [0.5077425]\n",
            "mse_b ====== [0.0353927687]\n",
            "It: 37721, Time: 0.02\n",
            "mse_b  [0.03539277]  mse_f: 0.4677695333957672   total loss: [0.5031623]\n",
            "mse_b ====== [0.0337043256]\n",
            "It: 37722, Time: 0.02\n",
            "mse_b  [0.03370433]  mse_f: 0.46905848383903503   total loss: [0.5027628]\n",
            "mse_b ====== [0.0394782424]\n",
            "It: 37723, Time: 0.02\n",
            "mse_b  [0.03947824]  mse_f: 0.4605153799057007   total loss: [0.49999362]\n",
            "mse_b ====== [0.0368661135]\n",
            "It: 37724, Time: 0.03\n",
            "mse_b  [0.03686611]  mse_f: 0.4649971127510071   total loss: [0.50186324]\n",
            "mse_b ====== [0.0358859524]\n",
            "It: 37725, Time: 0.02\n",
            "mse_b  [0.03588595]  mse_f: 0.46843233704566956   total loss: [0.5043183]\n",
            "mse_b ====== [0.0430135876]\n",
            "It: 37726, Time: 0.02\n",
            "mse_b  [0.04301359]  mse_f: 0.45911574363708496   total loss: [0.5021293]\n",
            "mse_b ====== [0.0359118022]\n",
            "It: 37727, Time: 0.02\n",
            "mse_b  [0.0359118]  mse_f: 0.46461495757102966   total loss: [0.5005268]\n",
            "mse_b ====== [0.032028906]\n",
            "It: 37728, Time: 0.02\n",
            "mse_b  [0.03202891]  mse_f: 0.4679166376590729   total loss: [0.49994555]\n",
            "mse_b ====== [0.0408703797]\n",
            "It: 37729, Time: 0.02\n",
            "mse_b  [0.04087038]  mse_f: 0.46458733081817627   total loss: [0.5054577]\n",
            "mse_b ====== [0.0335962661]\n",
            "It: 37730, Time: 0.03\n",
            "mse_b  [0.03359627]  mse_f: 0.46606507897377014   total loss: [0.49966136]\n",
            "mse_b ====== [0.0312149245]\n",
            "It: 37731, Time: 0.02\n",
            "mse_b  [0.03121492]  mse_f: 0.4712892770767212   total loss: [0.5025042]\n",
            "mse_b ====== [0.0441519879]\n",
            "It: 37732, Time: 0.03\n",
            "mse_b  [0.04415199]  mse_f: 0.4574004113674164   total loss: [0.5015524]\n",
            "mse_b ====== [0.0364312455]\n",
            "It: 37733, Time: 0.02\n",
            "mse_b  [0.03643125]  mse_f: 0.46510809659957886   total loss: [0.50153935]\n",
            "mse_b ====== [0.0299239755]\n",
            "It: 37734, Time: 0.02\n",
            "mse_b  [0.02992398]  mse_f: 0.471041738986969   total loss: [0.5009657]\n",
            "mse_b ====== [0.0401728302]\n",
            "It: 37735, Time: 0.02\n",
            "mse_b  [0.04017283]  mse_f: 0.4607052206993103   total loss: [0.50087804]\n",
            "mse_b ====== [0.0345607437]\n",
            "It: 37736, Time: 0.02\n",
            "mse_b  [0.03456074]  mse_f: 0.46626541018486023   total loss: [0.5008262]\n",
            "mse_b ====== [0.0310383774]\n",
            "It: 37737, Time: 0.02\n",
            "mse_b  [0.03103838]  mse_f: 0.4698934853076935   total loss: [0.50093186]\n",
            "mse_b ====== [0.0394248776]\n",
            "It: 37738, Time: 0.02\n",
            "mse_b  [0.03942488]  mse_f: 0.46388477087020874   total loss: [0.50330967]\n",
            "mse_b ====== [0.0318677463]\n",
            "It: 37739, Time: 0.02\n",
            "mse_b  [0.03186775]  mse_f: 0.46609294414520264   total loss: [0.4979607]\n",
            "mse_b ====== [0.0340956785]\n",
            "It: 37740, Time: 0.03\n",
            "mse_b  [0.03409568]  mse_f: 0.4669717848300934   total loss: [0.50106746]\n",
            "mse_b ====== [0.042509973]\n",
            "It: 37741, Time: 0.02\n",
            "mse_b  [0.04250997]  mse_f: 0.45919790863990784   total loss: [0.5017079]\n",
            "mse_b ====== [0.0342472978]\n",
            "It: 37742, Time: 0.02\n",
            "mse_b  [0.0342473]  mse_f: 0.4663042426109314   total loss: [0.5005515]\n",
            "mse_b ====== [0.0348506719]\n",
            "It: 37743, Time: 0.02\n",
            "mse_b  [0.03485067]  mse_f: 0.4626316428184509   total loss: [0.4974823]\n",
            "mse_b ====== [0.042798]\n",
            "It: 37744, Time: 0.02\n",
            "mse_b  [0.042798]  mse_f: 0.4579956531524658   total loss: [0.50079364]\n",
            "mse_b ====== [0.034266986]\n",
            "It: 37745, Time: 0.02\n",
            "mse_b  [0.03426699]  mse_f: 0.4651520252227783   total loss: [0.499419]\n",
            "mse_b ====== [0.0329845063]\n",
            "It: 37746, Time: 0.02\n",
            "mse_b  [0.03298451]  mse_f: 0.46480247378349304   total loss: [0.49778697]\n",
            "mse_b ====== [0.0443615839]\n",
            "It: 37747, Time: 0.02\n",
            "mse_b  [0.04436158]  mse_f: 0.4570091664791107   total loss: [0.5013707]\n",
            "mse_b ====== [0.0323328301]\n",
            "It: 37748, Time: 0.03\n",
            "mse_b  [0.03233283]  mse_f: 0.4659023880958557   total loss: [0.49823523]\n",
            "mse_b ====== [0.0325554088]\n",
            "It: 37749, Time: 0.02\n",
            "mse_b  [0.03255541]  mse_f: 0.46834859251976013   total loss: [0.500904]\n",
            "mse_b ====== [0.0438412428]\n",
            "It: 37750, Time: 0.03\n",
            "mse_b  [0.04384124]  mse_f: 0.45573684573173523   total loss: [0.4995781]\n",
            "mse_b ====== [0.0347853452]\n",
            "It: 37751, Time: 0.02\n",
            "mse_b  [0.03478535]  mse_f: 0.46326756477355957   total loss: [0.4980529]\n",
            "mse_b ====== [0.0339944698]\n",
            "It: 37752, Time: 0.03\n",
            "mse_b  [0.03399447]  mse_f: 0.461824893951416   total loss: [0.49581936]\n",
            "mse_b ====== [0.0424217954]\n",
            "It: 37753, Time: 0.02\n",
            "mse_b  [0.0424218]  mse_f: 0.457979291677475   total loss: [0.5004011]\n",
            "mse_b ====== [0.0325957574]\n",
            "It: 37754, Time: 0.03\n",
            "mse_b  [0.03259576]  mse_f: 0.4657972455024719   total loss: [0.498393]\n",
            "mse_b ====== [0.0334253125]\n",
            "It: 37755, Time: 0.02\n",
            "mse_b  [0.03342531]  mse_f: 0.4625232517719269   total loss: [0.49594855]\n",
            "mse_b ====== [0.0391952842]\n",
            "It: 37756, Time: 0.02\n",
            "mse_b  [0.03919528]  mse_f: 0.45840710401535034   total loss: [0.4976024]\n",
            "mse_b ====== [0.0312521532]\n",
            "It: 37757, Time: 0.02\n",
            "mse_b  [0.03125215]  mse_f: 0.4657168388366699   total loss: [0.49696898]\n",
            "mse_b ====== [0.033963725]\n",
            "It: 37758, Time: 0.03\n",
            "mse_b  [0.03396372]  mse_f: 0.46334534883499146   total loss: [0.4973091]\n",
            "mse_b ====== [0.0358101279]\n",
            "It: 37759, Time: 0.02\n",
            "mse_b  [0.03581013]  mse_f: 0.4600021243095398   total loss: [0.49581224]\n",
            "mse_b ====== [0.0290544704]\n",
            "It: 37760, Time: 0.02\n",
            "mse_b  [0.02905447]  mse_f: 0.469364732503891   total loss: [0.4984192]\n",
            "mse_b ====== [0.0333857834]\n",
            "It: 37761, Time: 0.02\n",
            "mse_b  [0.03338578]  mse_f: 0.46070754528045654   total loss: [0.49409333]\n",
            "mse_b ====== [0.0379910469]\n",
            "It: 37762, Time: 0.03\n",
            "mse_b  [0.03799105]  mse_f: 0.45879027247428894   total loss: [0.49678132]\n",
            "mse_b ====== [0.0314518]\n",
            "It: 37763, Time: 0.03\n",
            "mse_b  [0.0314518]  mse_f: 0.46550309658050537   total loss: [0.4969549]\n",
            "mse_b ====== [0.0372144096]\n",
            "It: 37764, Time: 0.02\n",
            "mse_b  [0.03721441]  mse_f: 0.4589097201824188   total loss: [0.49612412]\n",
            "mse_b ====== [0.0375222228]\n",
            "It: 37765, Time: 0.02\n",
            "mse_b  [0.03752222]  mse_f: 0.4584137797355652   total loss: [0.495936]\n",
            "mse_b ====== [0.0326137096]\n",
            "It: 37766, Time: 0.02\n",
            "mse_b  [0.03261371]  mse_f: 0.4622945785522461   total loss: [0.49490827]\n",
            "mse_b ====== [0.0403112322]\n",
            "It: 37767, Time: 0.02\n",
            "mse_b  [0.04031123]  mse_f: 0.4560732841491699   total loss: [0.4963845]\n",
            "mse_b ====== [0.0350042731]\n",
            "It: 37768, Time: 0.02\n",
            "mse_b  [0.03500427]  mse_f: 0.4587400555610657   total loss: [0.4937443]\n",
            "mse_b ====== [0.0327154025]\n",
            "It: 37769, Time: 0.02\n",
            "mse_b  [0.0327154]  mse_f: 0.4643089771270752   total loss: [0.4970244]\n",
            "mse_b ====== [0.0401751]\n",
            "It: 37770, Time: 0.02\n",
            "mse_b  [0.0401751]  mse_f: 0.4548432528972626   total loss: [0.49501836]\n",
            "mse_b ====== [0.0315697566]\n",
            "It: 37771, Time: 0.03\n",
            "mse_b  [0.03156976]  mse_f: 0.4630085229873657   total loss: [0.49457827]\n",
            "mse_b ====== [0.0328811035]\n",
            "It: 37772, Time: 0.02\n",
            "mse_b  [0.0328811]  mse_f: 0.4616965055465698   total loss: [0.49457762]\n",
            "mse_b ====== [0.0399862416]\n",
            "It: 37773, Time: 0.02\n",
            "mse_b  [0.03998624]  mse_f: 0.45583590865135193   total loss: [0.49582216]\n",
            "mse_b ====== [0.0307850614]\n",
            "It: 37774, Time: 0.02\n",
            "mse_b  [0.03078506]  mse_f: 0.4638437032699585   total loss: [0.49462876]\n",
            "mse_b ====== [0.0328603]\n",
            "It: 37775, Time: 0.02\n",
            "mse_b  [0.0328603]  mse_f: 0.4590824842453003   total loss: [0.4919428]\n",
            "mse_b ====== [0.0372759365]\n",
            "It: 37776, Time: 0.02\n",
            "mse_b  [0.03727594]  mse_f: 0.4576287865638733   total loss: [0.49490473]\n",
            "mse_b ====== [0.0303973164]\n",
            "It: 37777, Time: 0.03\n",
            "mse_b  [0.03039732]  mse_f: 0.4636842608451843   total loss: [0.4940816]\n",
            "mse_b ====== [0.0340909064]\n",
            "It: 37778, Time: 0.02\n",
            "mse_b  [0.03409091]  mse_f: 0.4611789584159851   total loss: [0.49526986]\n",
            "mse_b ====== [0.0343945697]\n",
            "It: 37779, Time: 0.02\n",
            "mse_b  [0.03439457]  mse_f: 0.4584752917289734   total loss: [0.49286985]\n",
            "mse_b ====== [0.031210348]\n",
            "It: 37780, Time: 0.02\n",
            "mse_b  [0.03121035]  mse_f: 0.46361592411994934   total loss: [0.49482626]\n",
            "mse_b ====== [0.0378657766]\n",
            "It: 37781, Time: 0.02\n",
            "mse_b  [0.03786578]  mse_f: 0.4558510482311249   total loss: [0.49371684]\n",
            "mse_b ====== [0.03353443]\n",
            "It: 37782, Time: 0.02\n",
            "mse_b  [0.03353443]  mse_f: 0.4582732319831848   total loss: [0.49180767]\n",
            "mse_b ====== [0.0326653644]\n",
            "It: 37783, Time: 0.02\n",
            "mse_b  [0.03266536]  mse_f: 0.4602171778678894   total loss: [0.49288255]\n",
            "mse_b ====== [0.0393689871]\n",
            "It: 37784, Time: 0.02\n",
            "mse_b  [0.03936899]  mse_f: 0.4535006284713745   total loss: [0.49286962]\n",
            "mse_b ====== [0.0322982147]\n",
            "It: 37785, Time: 0.03\n",
            "mse_b  [0.03229821]  mse_f: 0.46036747097969055   total loss: [0.49266568]\n",
            "mse_b ====== [0.0313545205]\n",
            "It: 37786, Time: 0.03\n",
            "mse_b  [0.03135452]  mse_f: 0.4599359929561615   total loss: [0.4912905]\n",
            "mse_b ====== [0.0396781936]\n",
            "It: 37787, Time: 0.02\n",
            "mse_b  [0.03967819]  mse_f: 0.4545535445213318   total loss: [0.49423173]\n",
            "mse_b ====== [0.0322321877]\n",
            "It: 37788, Time: 0.02\n",
            "mse_b  [0.03223219]  mse_f: 0.4607941508293152   total loss: [0.49302635]\n",
            "mse_b ====== [0.0331146605]\n",
            "It: 37789, Time: 0.02\n",
            "mse_b  [0.03311466]  mse_f: 0.45893919467926025   total loss: [0.49205387]\n",
            "mse_b ====== [0.0383836739]\n",
            "It: 37790, Time: 0.02\n",
            "mse_b  [0.03838367]  mse_f: 0.45336148142814636   total loss: [0.49174514]\n",
            "mse_b ====== [0.0321535245]\n",
            "It: 37791, Time: 0.02\n",
            "mse_b  [0.03215352]  mse_f: 0.4603387713432312   total loss: [0.4924923]\n",
            "mse_b ====== [0.0366186053]\n",
            "It: 37792, Time: 0.02\n",
            "mse_b  [0.03661861]  mse_f: 0.455703467130661   total loss: [0.4923221]\n",
            "mse_b ====== [0.0361541659]\n",
            "It: 37793, Time: 0.03\n",
            "mse_b  [0.03615417]  mse_f: 0.45556825399398804   total loss: [0.4917224]\n",
            "mse_b ====== [0.0320921317]\n",
            "It: 37794, Time: 0.02\n",
            "mse_b  [0.03209213]  mse_f: 0.45985299348831177   total loss: [0.49194512]\n",
            "mse_b ====== [0.0367422588]\n",
            "It: 37795, Time: 0.02\n",
            "mse_b  [0.03674226]  mse_f: 0.453904390335083   total loss: [0.49064666]\n",
            "mse_b ====== [0.0323711149]\n",
            "It: 37796, Time: 0.02\n",
            "mse_b  [0.03237111]  mse_f: 0.4599776268005371   total loss: [0.49234873]\n",
            "mse_b ====== [0.0309834257]\n",
            "It: 37797, Time: 0.02\n",
            "mse_b  [0.03098343]  mse_f: 0.4603624939918518   total loss: [0.4913459]\n",
            "mse_b ====== [0.039269682]\n",
            "It: 37798, Time: 0.02\n",
            "mse_b  [0.03926968]  mse_f: 0.4520059823989868   total loss: [0.49127567]\n",
            "mse_b ====== [0.0316856094]\n",
            "It: 37799, Time: 0.02\n",
            "mse_b  [0.03168561]  mse_f: 0.45779040455818176   total loss: [0.48947603]\n",
            "mse_b ====== [0.0322042592]\n",
            "It: 37800, Time: 0.02\n",
            "mse_b  [0.03220426]  mse_f: 0.45867472887039185   total loss: [0.490879]\n",
            "mse_b ====== [0.0384835862]\n",
            "It: 37801, Time: 0.03\n",
            "mse_b  [0.03848359]  mse_f: 0.45419690012931824   total loss: [0.4926805]\n",
            "mse_b ====== [0.0310556181]\n",
            "It: 37802, Time: 0.02\n",
            "mse_b  [0.03105562]  mse_f: 0.4594515264034271   total loss: [0.49050716]\n",
            "mse_b ====== [0.0320710279]\n",
            "It: 37803, Time: 0.02\n",
            "mse_b  [0.03207103]  mse_f: 0.4581376910209656   total loss: [0.49020872]\n",
            "mse_b ====== [0.0352566503]\n",
            "It: 37804, Time: 0.02\n",
            "mse_b  [0.03525665]  mse_f: 0.4540551006793976   total loss: [0.48931175]\n",
            "mse_b ====== [0.0312552266]\n",
            "It: 37805, Time: 0.02\n",
            "mse_b  [0.03125523]  mse_f: 0.46031367778778076   total loss: [0.4915689]\n",
            "mse_b ====== [0.0354631357]\n",
            "It: 37806, Time: 0.02\n",
            "mse_b  [0.03546314]  mse_f: 0.45424357056617737   total loss: [0.4897067]\n",
            "mse_b ====== [0.0323898681]\n",
            "It: 37807, Time: 0.02\n",
            "mse_b  [0.03238987]  mse_f: 0.45734503865242004   total loss: [0.48973492]\n",
            "mse_b ====== [0.0323752202]\n",
            "It: 37808, Time: 0.02\n",
            "mse_b  [0.03237522]  mse_f: 0.458341121673584   total loss: [0.49071634]\n",
            "mse_b ====== [0.0398545042]\n",
            "It: 37809, Time: 0.03\n",
            "mse_b  [0.0398545]  mse_f: 0.45037585496902466   total loss: [0.49023035]\n",
            "mse_b ====== [0.0323294811]\n",
            "It: 37810, Time: 0.03\n",
            "mse_b  [0.03232948]  mse_f: 0.45680904388427734   total loss: [0.4891385]\n",
            "mse_b ====== [0.03401592]\n",
            "It: 37811, Time: 0.02\n",
            "mse_b  [0.03401592]  mse_f: 0.4538639485836029   total loss: [0.48787987]\n",
            "mse_b ====== [0.0391700305]\n",
            "It: 37812, Time: 0.02\n",
            "mse_b  [0.03917003]  mse_f: 0.4511471092700958   total loss: [0.49031714]\n",
            "mse_b ====== [0.0310869]\n",
            "It: 37813, Time: 0.02\n",
            "mse_b  [0.0310869]  mse_f: 0.4573059678077698   total loss: [0.48839286]\n",
            "mse_b ====== [0.0333458111]\n",
            "It: 37814, Time: 0.02\n",
            "mse_b  [0.03334581]  mse_f: 0.4548037350177765   total loss: [0.48814955]\n",
            "mse_b ====== [0.0336779952]\n",
            "It: 37815, Time: 0.02\n",
            "mse_b  [0.033678]  mse_f: 0.45447269082069397   total loss: [0.4881507]\n",
            "mse_b ====== [0.0296523273]\n",
            "It: 37816, Time: 0.02\n",
            "mse_b  [0.02965233]  mse_f: 0.4598649740219116   total loss: [0.4895173]\n",
            "mse_b ====== [0.0332813747]\n",
            "It: 37817, Time: 0.02\n",
            "mse_b  [0.03328137]  mse_f: 0.4565754532814026   total loss: [0.48985684]\n",
            "mse_b ====== [0.0310638156]\n",
            "It: 37818, Time: 0.02\n",
            "mse_b  [0.03106382]  mse_f: 0.4561100900173187   total loss: [0.48717391]\n",
            "mse_b ====== [0.032146953]\n",
            "It: 37819, Time: 0.02\n",
            "mse_b  [0.03214695]  mse_f: 0.456388920545578   total loss: [0.48853588]\n",
            "mse_b ====== [0.0354966484]\n",
            "It: 37820, Time: 0.02\n",
            "mse_b  [0.03549665]  mse_f: 0.453142374753952   total loss: [0.48863903]\n",
            "mse_b ====== [0.0304966774]\n",
            "It: 37821, Time: 0.02\n",
            "mse_b  [0.03049668]  mse_f: 0.458273708820343   total loss: [0.4887704]\n",
            "mse_b ====== [0.0331555828]\n",
            "It: 37822, Time: 0.03\n",
            "mse_b  [0.03315558]  mse_f: 0.4535715878009796   total loss: [0.48672718]\n",
            "mse_b ====== [0.0371676944]\n",
            "It: 37823, Time: 0.02\n",
            "mse_b  [0.03716769]  mse_f: 0.4503162205219269   total loss: [0.48748392]\n",
            "mse_b ====== [0.0313223489]\n",
            "It: 37824, Time: 0.03\n",
            "mse_b  [0.03132235]  mse_f: 0.45751944184303284   total loss: [0.4888418]\n",
            "mse_b ====== [0.0359866843]\n",
            "It: 37825, Time: 0.02\n",
            "mse_b  [0.03598668]  mse_f: 0.450914591550827   total loss: [0.48690128]\n",
            "mse_b ====== [0.0340997316]\n",
            "It: 37826, Time: 0.02\n",
            "mse_b  [0.03409973]  mse_f: 0.4525451362133026   total loss: [0.48664486]\n",
            "mse_b ====== [0.0311601814]\n",
            "It: 37827, Time: 0.02\n",
            "mse_b  [0.03116018]  mse_f: 0.455218106508255   total loss: [0.48637828]\n",
            "mse_b ====== [0.0360135213]\n",
            "It: 37828, Time: 0.02\n",
            "mse_b  [0.03601352]  mse_f: 0.45222872495651245   total loss: [0.48824224]\n",
            "mse_b ====== [0.032394398]\n",
            "It: 37829, Time: 0.02\n",
            "mse_b  [0.0323944]  mse_f: 0.455352783203125   total loss: [0.4877472]\n",
            "mse_b ====== [0.033340089]\n",
            "It: 37830, Time: 0.02\n",
            "mse_b  [0.03334009]  mse_f: 0.45345818996429443   total loss: [0.4867983]\n",
            "mse_b ====== [0.0340417959]\n",
            "It: 37831, Time: 0.02\n",
            "mse_b  [0.0340418]  mse_f: 0.45264357328414917   total loss: [0.48668537]\n",
            "mse_b ====== [0.0297111645]\n",
            "It: 37832, Time: 0.02\n",
            "mse_b  [0.02971116]  mse_f: 0.4570426940917969   total loss: [0.48675385]\n",
            "mse_b ====== [0.035326466]\n",
            "It: 37833, Time: 0.02\n",
            "mse_b  [0.03532647]  mse_f: 0.451416015625   total loss: [0.4867425]\n",
            "mse_b ====== [0.0314025283]\n",
            "It: 37834, Time: 0.02\n",
            "mse_b  [0.03140253]  mse_f: 0.4538743495941162   total loss: [0.48527688]\n",
            "mse_b ====== [0.0302575491]\n",
            "It: 37835, Time: 0.02\n",
            "mse_b  [0.03025755]  mse_f: 0.45632681250572205   total loss: [0.48658437]\n",
            "mse_b ====== [0.0358444825]\n",
            "It: 37836, Time: 0.02\n",
            "mse_b  [0.03584448]  mse_f: 0.4511028826236725   total loss: [0.48694736]\n",
            "mse_b ====== [0.0297713652]\n",
            "It: 37837, Time: 0.02\n",
            "mse_b  [0.02977137]  mse_f: 0.45677024126052856   total loss: [0.4865416]\n",
            "mse_b ====== [0.031668324]\n",
            "It: 37838, Time: 0.03\n",
            "mse_b  [0.03166832]  mse_f: 0.4538343548774719   total loss: [0.4855027]\n",
            "mse_b ====== [0.0327770934]\n",
            "It: 37839, Time: 0.02\n",
            "mse_b  [0.03277709]  mse_f: 0.453078031539917   total loss: [0.48585513]\n",
            "mse_b ====== [0.0295927078]\n",
            "It: 37840, Time: 0.02\n",
            "mse_b  [0.02959271]  mse_f: 0.45654720067977905   total loss: [0.4861399]\n",
            "mse_b ====== [0.0343607515]\n",
            "It: 37841, Time: 0.02\n",
            "mse_b  [0.03436075]  mse_f: 0.4508502781391144   total loss: [0.485211]\n",
            "mse_b ====== [0.0304305889]\n",
            "It: 37842, Time: 0.02\n",
            "mse_b  [0.03043059]  mse_f: 0.4549739360809326   total loss: [0.48540452]\n",
            "mse_b ====== [0.0312095694]\n",
            "It: 37843, Time: 0.02\n",
            "mse_b  [0.03120957]  mse_f: 0.45366817712783813   total loss: [0.48487774]\n",
            "mse_b ====== [0.0351271741]\n",
            "It: 37844, Time: 0.02\n",
            "mse_b  [0.03512717]  mse_f: 0.4514184594154358   total loss: [0.48654562]\n",
            "mse_b ====== [0.0291083343]\n",
            "It: 37845, Time: 0.02\n",
            "mse_b  [0.02910833]  mse_f: 0.45650407671928406   total loss: [0.48561242]\n",
            "mse_b ====== [0.0338343941]\n",
            "It: 37846, Time: 0.02\n",
            "mse_b  [0.03383439]  mse_f: 0.45054811239242554   total loss: [0.4843825]\n",
            "mse_b ====== [0.0338416807]\n",
            "It: 37847, Time: 0.02\n",
            "mse_b  [0.03384168]  mse_f: 0.4511067569255829   total loss: [0.48494843]\n",
            "mse_b ====== [0.0324276611]\n",
            "It: 37848, Time: 0.02\n",
            "mse_b  [0.03242766]  mse_f: 0.45217037200927734   total loss: [0.48459804]\n",
            "mse_b ====== [0.0371213593]\n",
            "It: 37849, Time: 0.02\n",
            "mse_b  [0.03712136]  mse_f: 0.44849592447280884   total loss: [0.48561728]\n",
            "mse_b ====== [0.0317909271]\n",
            "It: 37850, Time: 0.02\n",
            "mse_b  [0.03179093]  mse_f: 0.45251813530921936   total loss: [0.48430908]\n",
            "mse_b ====== [0.0333926119]\n",
            "It: 37851, Time: 0.02\n",
            "mse_b  [0.03339261]  mse_f: 0.4513290226459503   total loss: [0.48472163]\n",
            "mse_b ====== [0.0342277]\n",
            "It: 37852, Time: 0.02\n",
            "mse_b  [0.0342277]  mse_f: 0.45039647817611694   total loss: [0.48462418]\n",
            "mse_b ====== [0.0300395042]\n",
            "It: 37853, Time: 0.02\n",
            "mse_b  [0.0300395]  mse_f: 0.4540082514286041   total loss: [0.48404777]\n",
            "mse_b ====== [0.0349419601]\n",
            "It: 37854, Time: 0.02\n",
            "mse_b  [0.03494196]  mse_f: 0.4496423006057739   total loss: [0.48458427]\n",
            "mse_b ====== [0.0300165452]\n",
            "It: 37855, Time: 0.02\n",
            "mse_b  [0.03001655]  mse_f: 0.4529235363006592   total loss: [0.48294008]\n",
            "mse_b ====== [0.0306857452]\n",
            "It: 37856, Time: 0.03\n",
            "mse_b  [0.03068575]  mse_f: 0.45344871282577515   total loss: [0.48413447]\n",
            "mse_b ====== [0.0337668173]\n",
            "It: 37857, Time: 0.02\n",
            "mse_b  [0.03376682]  mse_f: 0.45033544301986694   total loss: [0.48410225]\n",
            "mse_b ====== [0.0303441081]\n",
            "It: 37858, Time: 0.03\n",
            "mse_b  [0.03034411]  mse_f: 0.453969806432724   total loss: [0.4843139]\n",
            "mse_b ====== [0.0339397341]\n",
            "It: 37859, Time: 0.02\n",
            "mse_b  [0.03393973]  mse_f: 0.45050451159477234   total loss: [0.48444426]\n",
            "mse_b ====== [0.0310564153]\n",
            "It: 37860, Time: 0.02\n",
            "mse_b  [0.03105642]  mse_f: 0.4517597556114197   total loss: [0.48281616]\n",
            "mse_b ====== [0.0300759096]\n",
            "It: 37861, Time: 0.02\n",
            "mse_b  [0.03007591]  mse_f: 0.4532042145729065   total loss: [0.48328012]\n",
            "mse_b ====== [0.0336923748]\n",
            "It: 37862, Time: 0.02\n",
            "mse_b  [0.03369237]  mse_f: 0.44931796193122864   total loss: [0.48301035]\n",
            "mse_b ====== [0.0285629909]\n",
            "It: 37863, Time: 0.02\n",
            "mse_b  [0.02856299]  mse_f: 0.45502233505249023   total loss: [0.48358533]\n",
            "mse_b ====== [0.0335514396]\n",
            "It: 37864, Time: 0.02\n",
            "mse_b  [0.03355144]  mse_f: 0.4494898319244385   total loss: [0.4830413]\n",
            "mse_b ====== [0.0324750841]\n",
            "It: 37865, Time: 0.02\n",
            "mse_b  [0.03247508]  mse_f: 0.45040738582611084   total loss: [0.48288247]\n",
            "mse_b ====== [0.0311278291]\n",
            "It: 37866, Time: 0.02\n",
            "mse_b  [0.03112783]  mse_f: 0.4528021812438965   total loss: [0.48393002]\n",
            "mse_b ====== [0.0355951339]\n",
            "It: 37867, Time: 0.02\n",
            "mse_b  [0.03559513]  mse_f: 0.44756177067756653   total loss: [0.48315692]\n",
            "mse_b ====== [0.0312391631]\n",
            "It: 37868, Time: 0.02\n",
            "mse_b  [0.03123916]  mse_f: 0.45162442326545715   total loss: [0.48286358]\n",
            "mse_b ====== [0.0335095786]\n",
            "It: 37869, Time: 0.02\n",
            "mse_b  [0.03350958]  mse_f: 0.4484681487083435   total loss: [0.48197773]\n",
            "mse_b ====== [0.0319486074]\n",
            "It: 37870, Time: 0.02\n",
            "mse_b  [0.03194861]  mse_f: 0.4504544138908386   total loss: [0.482403]\n",
            "mse_b ====== [0.030200012]\n",
            "It: 37871, Time: 0.02\n",
            "mse_b  [0.03020001]  mse_f: 0.45288562774658203   total loss: [0.48308563]\n",
            "mse_b ====== [0.0347994529]\n",
            "It: 37872, Time: 0.03\n",
            "mse_b  [0.03479945]  mse_f: 0.4478389024734497   total loss: [0.48263836]\n",
            "mse_b ====== [0.0288023818]\n",
            "It: 37873, Time: 0.02\n",
            "mse_b  [0.02880238]  mse_f: 0.4535401463508606   total loss: [0.48234254]\n",
            "mse_b ====== [0.031702809]\n",
            "It: 37874, Time: 0.02\n",
            "mse_b  [0.03170281]  mse_f: 0.4503238797187805   total loss: [0.4820267]\n",
            "mse_b ====== [0.0323884934]\n",
            "It: 37875, Time: 0.02\n",
            "mse_b  [0.03238849]  mse_f: 0.44983863830566406   total loss: [0.48222715]\n",
            "mse_b ====== [0.0288993]\n",
            "It: 37876, Time: 0.02\n",
            "mse_b  [0.0288993]  mse_f: 0.4529085159301758   total loss: [0.48180783]\n",
            "mse_b ====== [0.033514671]\n",
            "It: 37877, Time: 0.02\n",
            "mse_b  [0.03351467]  mse_f: 0.44863957166671753   total loss: [0.48215425]\n",
            "mse_b ====== [0.0302657746]\n",
            "It: 37878, Time: 0.03\n",
            "mse_b  [0.03026577]  mse_f: 0.4518433213233948   total loss: [0.4821091]\n",
            "mse_b ====== [0.0328749865]\n",
            "It: 37879, Time: 0.02\n",
            "mse_b  [0.03287499]  mse_f: 0.44871336221694946   total loss: [0.48158836]\n",
            "mse_b ====== [0.0320605971]\n",
            "It: 37880, Time: 0.03\n",
            "mse_b  [0.0320606]  mse_f: 0.4495711922645569   total loss: [0.4816318]\n",
            "mse_b ====== [0.0303994343]\n",
            "It: 37881, Time: 0.02\n",
            "mse_b  [0.03039943]  mse_f: 0.4509090185165405   total loss: [0.48130846]\n",
            "mse_b ====== [0.0359394103]\n",
            "It: 37882, Time: 0.02\n",
            "mse_b  [0.03593941]  mse_f: 0.4458017349243164   total loss: [0.48174113]\n",
            "mse_b ====== [0.030192215]\n",
            "It: 37883, Time: 0.02\n",
            "mse_b  [0.03019221]  mse_f: 0.4507003724575043   total loss: [0.4808926]\n",
            "mse_b ====== [0.0317578539]\n",
            "It: 37884, Time: 0.02\n",
            "mse_b  [0.03175785]  mse_f: 0.44895097613334656   total loss: [0.48070884]\n",
            "mse_b ====== [0.0325296856]\n",
            "It: 37885, Time: 0.02\n",
            "mse_b  [0.03252969]  mse_f: 0.44866782426834106   total loss: [0.4811975]\n",
            "mse_b ====== [0.0293824226]\n",
            "It: 37886, Time: 0.02\n",
            "mse_b  [0.02938242]  mse_f: 0.45212453603744507   total loss: [0.48150694]\n",
            "mse_b ====== [0.033382915]\n",
            "It: 37887, Time: 0.02\n",
            "mse_b  [0.03338291]  mse_f: 0.44838088750839233   total loss: [0.4817638]\n",
            "mse_b ====== [0.0301675685]\n",
            "It: 37888, Time: 0.02\n",
            "mse_b  [0.03016757]  mse_f: 0.45025262236595154   total loss: [0.4804202]\n",
            "mse_b ====== [0.0344565734]\n",
            "It: 37889, Time: 0.03\n",
            "mse_b  [0.03445657]  mse_f: 0.44626134634017944   total loss: [0.48071793]\n",
            "mse_b ====== [0.0317298956]\n",
            "It: 37890, Time: 0.03\n",
            "mse_b  [0.0317299]  mse_f: 0.44868096709251404   total loss: [0.48041087]\n",
            "mse_b ====== [0.0299009494]\n",
            "It: 37891, Time: 0.02\n",
            "mse_b  [0.02990095]  mse_f: 0.4501067101955414   total loss: [0.48000765]\n",
            "mse_b ====== [0.0348074883]\n",
            "It: 37892, Time: 0.02\n",
            "mse_b  [0.03480749]  mse_f: 0.4461512565612793   total loss: [0.48095876]\n",
            "mse_b ====== [0.0290350821]\n",
            "It: 37893, Time: 0.04\n",
            "mse_b  [0.02903508]  mse_f: 0.4516509175300598   total loss: [0.480686]\n",
            "mse_b ====== [0.0322438478]\n",
            "It: 37894, Time: 0.02\n",
            "mse_b  [0.03224385]  mse_f: 0.4486743211746216   total loss: [0.48091817]\n",
            "mse_b ====== [0.0302686915]\n",
            "It: 37895, Time: 0.02\n",
            "mse_b  [0.03026869]  mse_f: 0.44942542910575867   total loss: [0.47969413]\n",
            "mse_b ====== [0.0295988582]\n",
            "It: 37896, Time: 0.02\n",
            "mse_b  [0.02959886]  mse_f: 0.44964563846588135   total loss: [0.4792445]\n",
            "mse_b ====== [0.031876266]\n",
            "It: 37897, Time: 0.02\n",
            "mse_b  [0.03187627]  mse_f: 0.4485044479370117   total loss: [0.4803807]\n",
            "mse_b ====== [0.0288812257]\n",
            "It: 37898, Time: 0.02\n",
            "mse_b  [0.02888123]  mse_f: 0.4513780474662781   total loss: [0.48025927]\n",
            "mse_b ====== [0.0331004485]\n",
            "It: 37899, Time: 0.02\n",
            "mse_b  [0.03310045]  mse_f: 0.44704803824424744   total loss: [0.4801485]\n",
            "mse_b ====== [0.0292867459]\n",
            "It: 37900, Time: 0.02\n",
            "mse_b  [0.02928675]  mse_f: 0.4498361349105835   total loss: [0.47912288]\n",
            "mse_b ====== [0.0295028854]\n",
            "It: 37901, Time: 0.02\n",
            "mse_b  [0.02950289]  mse_f: 0.4499909579753876   total loss: [0.47949386]\n",
            "mse_b ====== [0.0336529352]\n",
            "It: 37902, Time: 0.02\n",
            "mse_b  [0.03365294]  mse_f: 0.4463489353656769   total loss: [0.48000187]\n",
            "mse_b ====== [0.0296549033]\n",
            "It: 37903, Time: 0.02\n",
            "mse_b  [0.0296549]  mse_f: 0.4495929479598999   total loss: [0.47924784]\n",
            "mse_b ====== [0.0336609185]\n",
            "It: 37904, Time: 0.02\n",
            "mse_b  [0.03366092]  mse_f: 0.4459094703197479   total loss: [0.4795704]\n",
            "mse_b ====== [0.0308289155]\n",
            "It: 37905, Time: 0.02\n",
            "mse_b  [0.03082892]  mse_f: 0.4481765925884247   total loss: [0.47900552]\n",
            "mse_b ====== [0.0323972628]\n",
            "It: 37906, Time: 0.03\n",
            "mse_b  [0.03239726]  mse_f: 0.4468144178390503   total loss: [0.4792117]\n",
            "mse_b ====== [0.0325485542]\n",
            "It: 37907, Time: 0.02\n",
            "mse_b  [0.03254855]  mse_f: 0.4468954801559448   total loss: [0.47944403]\n",
            "mse_b ====== [0.0296609253]\n",
            "It: 37908, Time: 0.02\n",
            "mse_b  [0.02966093]  mse_f: 0.44947803020477295   total loss: [0.47913897]\n",
            "mse_b ====== [0.0345267132]\n",
            "It: 37909, Time: 0.02\n",
            "mse_b  [0.03452671]  mse_f: 0.44501006603240967   total loss: [0.47953677]\n",
            "mse_b ====== [0.0282857493]\n",
            "It: 37910, Time: 0.02\n",
            "mse_b  [0.02828575]  mse_f: 0.4503653049468994   total loss: [0.47865105]\n",
            "mse_b ====== [0.0322075747]\n",
            "It: 37911, Time: 0.02\n",
            "mse_b  [0.03220757]  mse_f: 0.4461912214756012   total loss: [0.4783988]\n",
            "mse_b ====== [0.0302980077]\n",
            "It: 37912, Time: 0.02\n",
            "mse_b  [0.03029801]  mse_f: 0.44828516244888306   total loss: [0.47858316]\n",
            "mse_b ====== [0.0308033451]\n",
            "It: 37913, Time: 0.03\n",
            "mse_b  [0.03080335]  mse_f: 0.4479878544807434   total loss: [0.4787912]\n",
            "mse_b ====== [0.0320098437]\n",
            "It: 37914, Time: 0.02\n",
            "mse_b  [0.03200984]  mse_f: 0.4469398856163025   total loss: [0.47894973]\n",
            "mse_b ====== [0.0287318267]\n",
            "It: 37915, Time: 0.02\n",
            "mse_b  [0.02873183]  mse_f: 0.449869841337204   total loss: [0.47860166]\n",
            "mse_b ====== [0.0330351666]\n",
            "It: 37916, Time: 0.02\n",
            "mse_b  [0.03303517]  mse_f: 0.44590866565704346   total loss: [0.47894382]\n",
            "mse_b ====== [0.0283380374]\n",
            "It: 37917, Time: 0.02\n",
            "mse_b  [0.02833804]  mse_f: 0.4500189423561096   total loss: [0.478357]\n",
            "mse_b ====== [0.0310375206]\n",
            "It: 37918, Time: 0.02\n",
            "mse_b  [0.03103752]  mse_f: 0.44672977924346924   total loss: [0.4777673]\n",
            "mse_b ====== [0.0311672129]\n",
            "It: 37919, Time: 0.02\n",
            "mse_b  [0.03116721]  mse_f: 0.44657671451568604   total loss: [0.47774392]\n",
            "mse_b ====== [0.029427778]\n",
            "It: 37920, Time: 0.02\n",
            "mse_b  [0.02942778]  mse_f: 0.44846534729003906   total loss: [0.4778931]\n",
            "mse_b ====== [0.0331363268]\n",
            "It: 37921, Time: 0.03\n",
            "mse_b  [0.03313633]  mse_f: 0.4454757571220398   total loss: [0.4786121]\n",
            "mse_b ====== [0.0294898339]\n",
            "It: 37922, Time: 0.02\n",
            "mse_b  [0.02948983]  mse_f: 0.4486432671546936   total loss: [0.4781331]\n",
            "mse_b ====== [0.0342842042]\n",
            "It: 37923, Time: 0.02\n",
            "mse_b  [0.0342842]  mse_f: 0.44354042410850525   total loss: [0.47782463]\n",
            "mse_b ====== [0.0296392757]\n",
            "It: 37924, Time: 0.02\n",
            "mse_b  [0.02963928]  mse_f: 0.4481379985809326   total loss: [0.47777727]\n",
            "mse_b ====== [0.0322858877]\n",
            "It: 37925, Time: 0.02\n",
            "mse_b  [0.03228589]  mse_f: 0.4452016055583954   total loss: [0.4774875]\n",
            "mse_b ====== [0.031062346]\n",
            "It: 37926, Time: 0.02\n",
            "mse_b  [0.03106235]  mse_f: 0.446392297744751   total loss: [0.47745463]\n",
            "mse_b ====== [0.030004058]\n",
            "It: 37927, Time: 0.02\n",
            "mse_b  [0.03000406]  mse_f: 0.4471508264541626   total loss: [0.47715488]\n",
            "mse_b ====== [0.0318761319]\n",
            "It: 37928, Time: 0.02\n",
            "mse_b  [0.03187613]  mse_f: 0.4456951320171356   total loss: [0.47757125]\n",
            "mse_b ====== [0.0293003861]\n",
            "It: 37929, Time: 0.03\n",
            "mse_b  [0.02930039]  mse_f: 0.4481499493122101   total loss: [0.47745034]\n",
            "mse_b ====== [0.0323664695]\n",
            "It: 37930, Time: 0.02\n",
            "mse_b  [0.03236647]  mse_f: 0.4447644054889679   total loss: [0.4771309]\n",
            "mse_b ====== [0.0287064705]\n",
            "It: 37931, Time: 0.02\n",
            "mse_b  [0.02870647]  mse_f: 0.448422908782959   total loss: [0.47712937]\n",
            "mse_b ====== [0.0311860591]\n",
            "It: 37932, Time: 0.02\n",
            "mse_b  [0.03118606]  mse_f: 0.4453078508377075   total loss: [0.4764939]\n",
            "mse_b ====== [0.0300066192]\n",
            "It: 37933, Time: 0.02\n",
            "mse_b  [0.03000662]  mse_f: 0.4468415081501007   total loss: [0.47684813]\n",
            "mse_b ====== [0.0297614653]\n",
            "It: 37934, Time: 0.02\n",
            "mse_b  [0.02976147]  mse_f: 0.44711247086524963   total loss: [0.47687393]\n",
            "mse_b ====== [0.0318725482]\n",
            "It: 37935, Time: 0.02\n",
            "mse_b  [0.03187255]  mse_f: 0.4449189305305481   total loss: [0.47679147]\n",
            "mse_b ====== [0.0298694801]\n",
            "It: 37936, Time: 0.03\n",
            "mse_b  [0.02986948]  mse_f: 0.447206050157547   total loss: [0.47707552]\n",
            "mse_b ====== [0.0333298]\n",
            "It: 37937, Time: 0.02\n",
            "mse_b  [0.0333298]  mse_f: 0.44304072856903076   total loss: [0.4763705]\n",
            "mse_b ====== [0.0286843814]\n",
            "It: 37938, Time: 0.02\n",
            "mse_b  [0.02868438]  mse_f: 0.44796067476272583   total loss: [0.47664505]\n",
            "mse_b ====== [0.0331057943]\n",
            "It: 37939, Time: 0.02\n",
            "mse_b  [0.03310579]  mse_f: 0.44374826550483704   total loss: [0.47685406]\n",
            "mse_b ====== [0.0282700881]\n",
            "It: 37940, Time: 0.02\n",
            "mse_b  [0.02827009]  mse_f: 0.4481056332588196   total loss: [0.47637573]\n",
            "mse_b ====== [0.0314421244]\n",
            "It: 37941, Time: 0.02\n",
            "mse_b  [0.03144212]  mse_f: 0.4450634717941284   total loss: [0.4765056]\n",
            "mse_b ====== [0.0294006448]\n",
            "It: 37942, Time: 0.02\n",
            "mse_b  [0.02940064]  mse_f: 0.4467185437679291   total loss: [0.4761192]\n",
            "mse_b ====== [0.0316416025]\n",
            "It: 37943, Time: 0.02\n",
            "mse_b  [0.0316416]  mse_f: 0.4445180892944336   total loss: [0.4761597]\n",
            "mse_b ====== [0.0306668654]\n",
            "It: 37944, Time: 0.02\n",
            "mse_b  [0.03066687]  mse_f: 0.4453190565109253   total loss: [0.4759859]\n",
            "mse_b ====== [0.0303224493]\n",
            "It: 37945, Time: 0.02\n",
            "mse_b  [0.03032245]  mse_f: 0.4451223909854889   total loss: [0.47544485]\n",
            "mse_b ====== [0.0324948207]\n",
            "It: 37946, Time: 0.02\n",
            "mse_b  [0.03249482]  mse_f: 0.4433266520500183   total loss: [0.47582147]\n",
            "mse_b ====== [0.0299282353]\n",
            "It: 37947, Time: 0.03\n",
            "mse_b  [0.02992824]  mse_f: 0.44591933488845825   total loss: [0.47584757]\n",
            "mse_b ====== [0.0329046287]\n",
            "It: 37948, Time: 0.02\n",
            "mse_b  [0.03290463]  mse_f: 0.44307219982147217   total loss: [0.47597682]\n",
            "mse_b ====== [0.0288546644]\n",
            "It: 37949, Time: 0.02\n",
            "mse_b  [0.02885466]  mse_f: 0.44712695479393005   total loss: [0.47598162]\n",
            "mse_b ====== [0.0322469845]\n",
            "It: 37950, Time: 0.02\n",
            "mse_b  [0.03224698]  mse_f: 0.44335222244262695   total loss: [0.4755992]\n",
            "mse_b ====== [0.0280294772]\n",
            "It: 37951, Time: 0.02\n",
            "mse_b  [0.02802948]  mse_f: 0.4476395845413208   total loss: [0.47566906]\n",
            "mse_b ====== [0.0317140631]\n",
            "It: 37952, Time: 0.02\n",
            "mse_b  [0.03171406]  mse_f: 0.4435965418815613   total loss: [0.4753106]\n",
            "mse_b ====== [0.0295183454]\n",
            "It: 37953, Time: 0.02\n",
            "mse_b  [0.02951835]  mse_f: 0.4457511305809021   total loss: [0.47526947]\n",
            "mse_b ====== [0.0306327846]\n",
            "It: 37954, Time: 0.02\n",
            "mse_b  [0.03063278]  mse_f: 0.44483914971351624   total loss: [0.47547194]\n",
            "mse_b ====== [0.0296040401]\n",
            "It: 37955, Time: 0.02\n",
            "mse_b  [0.02960404]  mse_f: 0.4453856647014618   total loss: [0.4749897]\n",
            "mse_b ====== [0.0300161075]\n",
            "It: 37956, Time: 0.02\n",
            "mse_b  [0.03001611]  mse_f: 0.44493168592453003   total loss: [0.47494778]\n",
            "mse_b ====== [0.030881796]\n",
            "It: 37957, Time: 0.02\n",
            "mse_b  [0.0308818]  mse_f: 0.444033682346344   total loss: [0.47491547]\n",
            "mse_b ====== [0.0290991366]\n",
            "It: 37958, Time: 0.02\n",
            "mse_b  [0.02909914]  mse_f: 0.44568759202957153   total loss: [0.47478673]\n",
            "mse_b ====== [0.0326885507]\n",
            "It: 37959, Time: 0.03\n",
            "mse_b  [0.03268855]  mse_f: 0.4422096014022827   total loss: [0.47489816]\n",
            "mse_b ====== [0.0292911865]\n",
            "It: 37960, Time: 0.02\n",
            "mse_b  [0.02929119]  mse_f: 0.44562047719955444   total loss: [0.47491166]\n",
            "mse_b ====== [0.0337285101]\n",
            "It: 37961, Time: 0.02\n",
            "mse_b  [0.03372851]  mse_f: 0.4416695833206177   total loss: [0.4753981]\n",
            "mse_b ====== [0.0287128091]\n",
            "It: 37962, Time: 0.02\n",
            "mse_b  [0.02871281]  mse_f: 0.44698119163513184   total loss: [0.475694]\n",
            "mse_b ====== [0.0358547084]\n",
            "It: 37963, Time: 0.02\n",
            "mse_b  [0.03585471]  mse_f: 0.43999266624450684   total loss: [0.47584736]\n",
            "mse_b ====== [0.0274958275]\n",
            "It: 37964, Time: 0.02\n",
            "mse_b  [0.02749583]  mse_f: 0.44867146015167236   total loss: [0.4761673]\n",
            "mse_b ====== [0.0381823182]\n",
            "It: 37965, Time: 0.03\n",
            "mse_b  [0.03818232]  mse_f: 0.43922317028045654   total loss: [0.4774055]\n",
            "mse_b ====== [0.0276672058]\n",
            "It: 37966, Time: 0.02\n",
            "mse_b  [0.02766721]  mse_f: 0.45077183842658997   total loss: [0.47843903]\n",
            "mse_b ====== [0.0436563045]\n",
            "It: 37967, Time: 0.02\n",
            "mse_b  [0.0436563]  mse_f: 0.4371083974838257   total loss: [0.4807647]\n",
            "mse_b ====== [0.0277468152]\n",
            "It: 37968, Time: 0.02\n",
            "mse_b  [0.02774682]  mse_f: 0.45440366864204407   total loss: [0.4821505]\n",
            "mse_b ====== [0.0526783913]\n",
            "It: 37969, Time: 0.02\n",
            "mse_b  [0.05267839]  mse_f: 0.43500909209251404   total loss: [0.48768747]\n",
            "mse_b ====== [0.0301720649]\n",
            "It: 37970, Time: 0.02\n",
            "mse_b  [0.03017206]  mse_f: 0.45740097761154175   total loss: [0.48757303]\n",
            "mse_b ====== [0.0606732]\n",
            "It: 37971, Time: 0.02\n",
            "mse_b  [0.0606732]  mse_f: 0.4343547523021698   total loss: [0.49502796]\n",
            "mse_b ====== [0.0324264057]\n",
            "It: 37972, Time: 0.02\n",
            "mse_b  [0.03242641]  mse_f: 0.4590376615524292   total loss: [0.49146408]\n",
            "mse_b ====== [0.0646868423]\n",
            "It: 37973, Time: 0.02\n",
            "mse_b  [0.06468684]  mse_f: 0.4324496388435364   total loss: [0.49713647]\n",
            "mse_b ====== [0.0295485016]\n",
            "It: 37974, Time: 0.02\n",
            "mse_b  [0.0295485]  mse_f: 0.45793604850769043   total loss: [0.48748454]\n",
            "mse_b ====== [0.0477749817]\n",
            "It: 37975, Time: 0.03\n",
            "mse_b  [0.04777498]  mse_f: 0.4354512691497803   total loss: [0.48322624]\n",
            "mse_b ====== [0.027826909]\n",
            "It: 37976, Time: 0.03\n",
            "mse_b  [0.02782691]  mse_f: 0.4489019811153412   total loss: [0.4767289]\n",
            "mse_b ====== [0.0311044492]\n",
            "It: 37977, Time: 0.02\n",
            "mse_b  [0.03110445]  mse_f: 0.44268789887428284   total loss: [0.47379234]\n",
            "mse_b ====== [0.0353177227]\n",
            "It: 37978, Time: 0.02\n",
            "mse_b  [0.03531772]  mse_f: 0.4395841658115387   total loss: [0.47490188]\n",
            "mse_b ====== [0.0284574628]\n",
            "It: 37979, Time: 0.02\n",
            "mse_b  [0.02845746]  mse_f: 0.45134082436561584   total loss: [0.4797983]\n",
            "mse_b ====== [0.055872947]\n",
            "It: 37980, Time: 0.03\n",
            "mse_b  [0.05587295]  mse_f: 0.43281689286231995   total loss: [0.48868984]\n",
            "mse_b ====== [0.0329753458]\n",
            "It: 37981, Time: 0.02\n",
            "mse_b  [0.03297535]  mse_f: 0.45781248807907104   total loss: [0.49078783]\n",
            "mse_b ====== [0.0709875599]\n",
            "It: 37982, Time: 0.02\n",
            "mse_b  [0.07098756]  mse_f: 0.4291968047618866   total loss: [0.50018436]\n",
            "mse_b ====== [0.031545233]\n",
            "It: 37983, Time: 0.02\n",
            "mse_b  [0.03154523]  mse_f: 0.4573694169521332   total loss: [0.48891464]\n",
            "mse_b ====== [0.0492162071]\n",
            "It: 37984, Time: 0.02\n",
            "mse_b  [0.04921621]  mse_f: 0.43478167057037354   total loss: [0.48399788]\n",
            "mse_b ====== [0.0275816806]\n",
            "It: 37985, Time: 0.03\n",
            "mse_b  [0.02758168]  mse_f: 0.4479055404663086   total loss: [0.47548723]\n",
            "mse_b ====== [0.0302636344]\n",
            "It: 37986, Time: 0.04\n",
            "mse_b  [0.03026363]  mse_f: 0.4424964189529419   total loss: [0.47276005]\n",
            "mse_b ====== [0.0367110446]\n",
            "It: 37987, Time: 0.02\n",
            "mse_b  [0.03671104]  mse_f: 0.4390772879123688   total loss: [0.47578833]\n",
            "mse_b ====== [0.0289935805]\n",
            "It: 37988, Time: 0.02\n",
            "mse_b  [0.02899358]  mse_f: 0.45355284214019775   total loss: [0.48254642]\n",
            "mse_b ====== [0.0666354]\n",
            "It: 37989, Time: 0.02\n",
            "mse_b  [0.0666354]  mse_f: 0.43116313219070435   total loss: [0.49779853]\n",
            "mse_b ====== [0.0359543599]\n",
            "It: 37990, Time: 0.02\n",
            "mse_b  [0.03595436]  mse_f: 0.46205803751945496   total loss: [0.4980124]\n",
            "mse_b ====== [0.0828871503]\n",
            "It: 37991, Time: 0.02\n",
            "mse_b  [0.08288715]  mse_f: 0.42931467294692993   total loss: [0.51220185]\n",
            "mse_b ====== [0.0315508693]\n",
            "It: 37992, Time: 0.02\n",
            "mse_b  [0.03155087]  mse_f: 0.4602055847644806   total loss: [0.49175644]\n",
            "mse_b ====== [0.0436488688]\n",
            "It: 37993, Time: 0.02\n",
            "mse_b  [0.04364887]  mse_f: 0.43612927198410034   total loss: [0.47977814]\n",
            "mse_b ====== [0.028404912]\n",
            "It: 37994, Time: 0.02\n",
            "mse_b  [0.02840491]  mse_f: 0.444369375705719   total loss: [0.4727743]\n",
            "mse_b ====== [0.0272742845]\n",
            "It: 37995, Time: 0.02\n",
            "mse_b  [0.02727428]  mse_f: 0.4501548707485199   total loss: [0.47742915]\n",
            "mse_b ====== [0.0593649559]\n",
            "It: 37996, Time: 0.02\n",
            "mse_b  [0.05936496]  mse_f: 0.4345286190509796   total loss: [0.49389356]\n",
            "mse_b ====== [0.0398982279]\n",
            "It: 37997, Time: 0.02\n",
            "mse_b  [0.03989823]  mse_f: 0.4635320007801056   total loss: [0.50343025]\n",
            "mse_b ====== [0.107984826]\n",
            "It: 37998, Time: 0.02\n",
            "mse_b  [0.10798483]  mse_f: 0.42692041397094727   total loss: [0.53490525]\n",
            "mse_b ====== [0.0359644033]\n",
            "It: 37999, Time: 0.02\n",
            "mse_b  [0.0359644]  mse_f: 0.46317625045776367   total loss: [0.49914065]\n",
            "mse_b ====== [0.0463615134]\n",
            "It: 38000, Time: 0.02\n",
            "mse_b  [0.04636151]  mse_f: 0.4355889558792114   total loss: [0.48195046]\n",
            "mse_b ====== [0.0343948752]\n",
            "It: 38001, Time: 0.04\n",
            "mse_b  [0.03439488]  mse_f: 0.4396763741970062   total loss: [0.47407126]\n",
            "mse_b ====== [0.0311796851]\n",
            "It: 38002, Time: 0.03\n",
            "mse_b  [0.03117969]  mse_f: 0.4568710923194885   total loss: [0.4880508]\n",
            "mse_b ====== [0.0910074]\n",
            "It: 38003, Time: 0.02\n",
            "mse_b  [0.0910074]  mse_f: 0.4310902953147888   total loss: [0.5220977]\n",
            "mse_b ====== [0.0408061519]\n",
            "It: 38004, Time: 0.02\n",
            "mse_b  [0.04080615]  mse_f: 0.46511489152908325   total loss: [0.50592107]\n",
            "mse_b ====== [0.0695664734]\n",
            "It: 38005, Time: 0.03\n",
            "mse_b  [0.06956647]  mse_f: 0.4305269122123718   total loss: [0.5000934]\n",
            "mse_b ====== [0.0280082785]\n",
            "It: 38006, Time: 0.02\n",
            "mse_b  [0.02800828]  mse_f: 0.4483904540538788   total loss: [0.47639874]\n",
            "mse_b ====== [0.0276133809]\n",
            "It: 38007, Time: 0.03\n",
            "mse_b  [0.02761338]  mse_f: 0.452331006526947   total loss: [0.47994438]\n",
            "mse_b ====== [0.0762947]\n",
            "It: 38008, Time: 0.02\n",
            "mse_b  [0.0762947]  mse_f: 0.4279620051383972   total loss: [0.5042567]\n",
            "mse_b ====== [0.0338479728]\n",
            "It: 38009, Time: 0.03\n",
            "mse_b  [0.03384797]  mse_f: 0.46132588386535645   total loss: [0.49517387]\n",
            "mse_b ====== [0.0508734435]\n",
            "It: 38010, Time: 0.02\n",
            "mse_b  [0.05087344]  mse_f: 0.43533915281295776   total loss: [0.4862126]\n",
            "mse_b ====== [0.0288223438]\n",
            "It: 38011, Time: 0.02\n",
            "mse_b  [0.02882234]  mse_f: 0.4449731707572937   total loss: [0.4737955]\n",
            "mse_b ====== [0.0281587411]\n",
            "It: 38012, Time: 0.02\n",
            "mse_b  [0.02815874]  mse_f: 0.4504382014274597   total loss: [0.47859696]\n",
            "mse_b ====== [0.0672040209]\n",
            "It: 38013, Time: 0.02\n",
            "mse_b  [0.06720402]  mse_f: 0.43118977546691895   total loss: [0.4983938]\n",
            "mse_b ====== [0.0380096957]\n",
            "It: 38014, Time: 0.02\n",
            "mse_b  [0.0380097]  mse_f: 0.4630011320114136   total loss: [0.50101084]\n",
            "mse_b ====== [0.0851558298]\n",
            "It: 38015, Time: 0.02\n",
            "mse_b  [0.08515583]  mse_f: 0.4261799454689026   total loss: [0.5113358]\n",
            "mse_b ====== [0.028409062]\n",
            "It: 38016, Time: 0.03\n",
            "mse_b  [0.02840906]  mse_f: 0.45278412103652954   total loss: [0.48119318]\n",
            "mse_b ====== [0.0291589815]\n",
            "It: 38017, Time: 0.02\n",
            "mse_b  [0.02915898]  mse_f: 0.44342300295829773   total loss: [0.47258198]\n",
            "mse_b ====== [0.0544682331]\n",
            "It: 38018, Time: 0.02\n",
            "mse_b  [0.05446823]  mse_f: 0.4321427643299103   total loss: [0.486611]\n",
            "mse_b ====== [0.0400492474]\n",
            "It: 38019, Time: 0.02\n",
            "mse_b  [0.04004925]  mse_f: 0.4627678394317627   total loss: [0.5028171]\n",
            "mse_b ====== [0.104278862]\n",
            "It: 38020, Time: 0.03\n",
            "mse_b  [0.10427886]  mse_f: 0.4316924214363098   total loss: [0.5359713]\n",
            "mse_b ====== [0.0358241685]\n",
            "It: 38021, Time: 0.03\n",
            "mse_b  [0.03582417]  mse_f: 0.4602510929107666   total loss: [0.49607527]\n",
            "mse_b ====== [0.0419820659]\n",
            "It: 38022, Time: 0.02\n",
            "mse_b  [0.04198207]  mse_f: 0.435583233833313   total loss: [0.4775653]\n",
            "mse_b ====== [0.0369181298]\n",
            "It: 38023, Time: 0.03\n",
            "mse_b  [0.03691813]  mse_f: 0.4396563768386841   total loss: [0.4765745]\n",
            "mse_b ====== [0.0338178799]\n",
            "It: 38024, Time: 0.03\n",
            "mse_b  [0.03381788]  mse_f: 0.45861971378326416   total loss: [0.4924376]\n",
            "mse_b ====== [0.0980447903]\n",
            "It: 38025, Time: 0.02\n",
            "mse_b  [0.09804479]  mse_f: 0.4272269606590271   total loss: [0.5252718]\n",
            "mse_b ====== [0.0332306512]\n",
            "It: 38026, Time: 0.02\n",
            "mse_b  [0.03323065]  mse_f: 0.4590897858142853   total loss: [0.49232045]\n",
            "mse_b ====== [0.0365085341]\n",
            "It: 38027, Time: 0.02\n",
            "mse_b  [0.03650853]  mse_f: 0.4386427402496338   total loss: [0.47515127]\n",
            "mse_b ====== [0.0387511]\n",
            "It: 38028, Time: 0.02\n",
            "mse_b  [0.0387511]  mse_f: 0.43619391322135925   total loss: [0.474945]\n",
            "mse_b ====== [0.032297302]\n",
            "It: 38029, Time: 0.02\n",
            "mse_b  [0.0322973]  mse_f: 0.45816802978515625   total loss: [0.49046534]\n",
            "mse_b ====== [0.0904039]\n",
            "It: 38030, Time: 0.03\n",
            "mse_b  [0.0904039]  mse_f: 0.42701196670532227   total loss: [0.5174159]\n",
            "mse_b ====== [0.0314904936]\n",
            "It: 38031, Time: 0.02\n",
            "mse_b  [0.03149049]  mse_f: 0.457457035779953   total loss: [0.48894754]\n",
            "mse_b ====== [0.0333854072]\n",
            "It: 38032, Time: 0.02\n",
            "mse_b  [0.03338541]  mse_f: 0.43860191106796265   total loss: [0.4719873]\n",
            "mse_b ====== [0.0451651439]\n",
            "It: 38033, Time: 0.02\n",
            "mse_b  [0.04516514]  mse_f: 0.4338977336883545   total loss: [0.47906289]\n",
            "mse_b ====== [0.037607953]\n",
            "It: 38034, Time: 0.02\n",
            "mse_b  [0.03760795]  mse_f: 0.46019086241722107   total loss: [0.4977988]\n",
            "mse_b ====== [0.0966296643]\n",
            "It: 38035, Time: 0.02\n",
            "mse_b  [0.09662966]  mse_f: 0.4286002516746521   total loss: [0.52522993]\n",
            "mse_b ====== [0.0317096114]\n",
            "It: 38036, Time: 0.03\n",
            "mse_b  [0.03170961]  mse_f: 0.4545179605484009   total loss: [0.48622757]\n",
            "mse_b ====== [0.0311031528]\n",
            "It: 38037, Time: 0.03\n",
            "mse_b  [0.03110315]  mse_f: 0.44050437211990356   total loss: [0.47160754]\n",
            "mse_b ====== [0.0569310822]\n",
            "It: 38038, Time: 0.03\n",
            "mse_b  [0.05693108]  mse_f: 0.4293135404586792   total loss: [0.48624462]\n",
            "mse_b ====== [0.0361682139]\n",
            "It: 38039, Time: 0.02\n",
            "mse_b  [0.03616821]  mse_f: 0.46072810888290405   total loss: [0.49689633]\n",
            "mse_b ====== [0.0779331401]\n",
            "It: 38040, Time: 0.02\n",
            "mse_b  [0.07793314]  mse_f: 0.42807427048683167   total loss: [0.50600743]\n",
            "mse_b ====== [0.028882809]\n",
            "It: 38041, Time: 0.02\n",
            "mse_b  [0.02888281]  mse_f: 0.4495444893836975   total loss: [0.4784273]\n",
            "mse_b ====== [0.0279701874]\n",
            "It: 38042, Time: 0.02\n",
            "mse_b  [0.02797019]  mse_f: 0.44419044256210327   total loss: [0.47216064]\n",
            "mse_b ====== [0.0585851]\n",
            "It: 38043, Time: 0.02\n",
            "mse_b  [0.0585851]  mse_f: 0.43178093433380127   total loss: [0.49036604]\n",
            "mse_b ====== [0.0381755382]\n",
            "It: 38044, Time: 0.02\n",
            "mse_b  [0.03817554]  mse_f: 0.46261143684387207   total loss: [0.50078696]\n",
            "mse_b ====== [0.0872268081]\n",
            "It: 38045, Time: 0.02\n",
            "mse_b  [0.08722681]  mse_f: 0.4252721965312958   total loss: [0.512499]\n",
            "mse_b ====== [0.0276681595]\n",
            "It: 38046, Time: 0.02\n",
            "mse_b  [0.02766816]  mse_f: 0.45065027475357056   total loss: [0.47831842]\n",
            "mse_b ====== [0.027550336]\n",
            "It: 38047, Time: 0.03\n",
            "mse_b  [0.02755034]  mse_f: 0.4488348662853241   total loss: [0.4763852]\n",
            "mse_b ====== [0.0812432617]\n",
            "It: 38048, Time: 0.02\n",
            "mse_b  [0.08124326]  mse_f: 0.42710721492767334   total loss: [0.5083505]\n",
            "mse_b ====== [0.0399698019]\n",
            "It: 38049, Time: 0.02\n",
            "mse_b  [0.0399698]  mse_f: 0.46198734641075134   total loss: [0.5019572]\n",
            "mse_b ====== [0.0657144189]\n",
            "It: 38050, Time: 0.02\n",
            "mse_b  [0.06571442]  mse_f: 0.43211591243743896   total loss: [0.49783033]\n",
            "mse_b ====== [0.0271816719]\n",
            "It: 38051, Time: 0.02\n",
            "mse_b  [0.02718167]  mse_f: 0.4457123577594757   total loss: [0.47289404]\n",
            "mse_b ====== [0.0274685398]\n",
            "It: 38052, Time: 0.02\n",
            "mse_b  [0.02746854]  mse_f: 0.4485286474227905   total loss: [0.47599718]\n",
            "mse_b ====== [0.077586472]\n",
            "It: 38053, Time: 0.02\n",
            "mse_b  [0.07758647]  mse_f: 0.4257279634475708   total loss: [0.50331444]\n",
            "mse_b ====== [0.0329027399]\n",
            "It: 38054, Time: 0.02\n",
            "mse_b  [0.03290274]  mse_f: 0.45889145135879517   total loss: [0.4917942]\n",
            "mse_b ====== [0.0436078385]\n",
            "It: 38055, Time: 0.02\n",
            "mse_b  [0.04360784]  mse_f: 0.4325191378593445   total loss: [0.47612697]\n",
            "mse_b ====== [0.0346136875]\n",
            "It: 38056, Time: 0.02\n",
            "mse_b  [0.03461369]  mse_f: 0.4359240233898163   total loss: [0.47053772]\n",
            "mse_b ====== [0.0317527801]\n",
            "It: 38057, Time: 0.02\n",
            "mse_b  [0.03175278]  mse_f: 0.45533010363578796   total loss: [0.4870829]\n",
            "mse_b ====== [0.0866768658]\n",
            "It: 38058, Time: 0.02\n",
            "mse_b  [0.08667687]  mse_f: 0.42917436361312866   total loss: [0.51585126]\n",
            "mse_b ====== [0.0340972021]\n",
            "It: 38059, Time: 0.02\n",
            "mse_b  [0.0340972]  mse_f: 0.45685118436813354   total loss: [0.49094838]\n",
            "mse_b ====== [0.0430722386]\n",
            "It: 38060, Time: 0.02\n",
            "mse_b  [0.04307224]  mse_f: 0.43157559633255005   total loss: [0.47464782]\n",
            "mse_b ====== [0.0351117514]\n",
            "It: 38061, Time: 0.02\n",
            "mse_b  [0.03511175]  mse_f: 0.43638479709625244   total loss: [0.47149655]\n",
            "mse_b ====== [0.0300681107]\n",
            "It: 38062, Time: 0.02\n",
            "mse_b  [0.03006811]  mse_f: 0.4562545716762543   total loss: [0.48632267]\n",
            "mse_b ====== [0.0807261765]\n",
            "It: 38063, Time: 0.02\n",
            "mse_b  [0.08072618]  mse_f: 0.42494282126426697   total loss: [0.505669]\n",
            "mse_b ====== [0.0297844447]\n",
            "It: 38064, Time: 0.02\n",
            "mse_b  [0.02978444]  mse_f: 0.45008939504623413   total loss: [0.47987384]\n",
            "mse_b ====== [0.0285292]\n",
            "It: 38065, Time: 0.02\n",
            "mse_b  [0.0285292]  mse_f: 0.441669762134552   total loss: [0.47019896]\n",
            "mse_b ====== [0.0505149774]\n",
            "It: 38066, Time: 0.02\n",
            "mse_b  [0.05051498]  mse_f: 0.43250715732574463   total loss: [0.48302212]\n",
            "mse_b ====== [0.0356419384]\n",
            "It: 38067, Time: 0.02\n",
            "mse_b  [0.03564194]  mse_f: 0.4588458240032196   total loss: [0.49448776]\n",
            "mse_b ====== [0.0767677]\n",
            "It: 38068, Time: 0.02\n",
            "mse_b  [0.0767677]  mse_f: 0.42737266421318054   total loss: [0.5041404]\n",
            "mse_b ====== [0.0281892]\n",
            "It: 38069, Time: 0.02\n",
            "mse_b  [0.0281892]  mse_f: 0.4480396509170532   total loss: [0.47622886]\n",
            "mse_b ====== [0.0278458074]\n",
            "It: 38070, Time: 0.02\n",
            "mse_b  [0.02784581]  mse_f: 0.4460638761520386   total loss: [0.47390968]\n",
            "mse_b ====== [0.0715664253]\n",
            "It: 38071, Time: 0.03\n",
            "mse_b  [0.07156643]  mse_f: 0.42620259523391724   total loss: [0.49776903]\n",
            "mse_b ====== [0.0355274118]\n",
            "It: 38072, Time: 0.02\n",
            "mse_b  [0.03552741]  mse_f: 0.4585025906562805   total loss: [0.49403]\n",
            "mse_b ====== [0.059918128]\n",
            "It: 38073, Time: 0.03\n",
            "mse_b  [0.05991813]  mse_f: 0.42883312702178955   total loss: [0.48875126]\n",
            "mse_b ====== [0.027970776]\n",
            "It: 38074, Time: 0.02\n",
            "mse_b  [0.02797078]  mse_f: 0.4426525831222534   total loss: [0.47062337]\n",
            "mse_b ====== [0.0276407599]\n",
            "It: 38075, Time: 0.02\n",
            "mse_b  [0.02764076]  mse_f: 0.4459541440010071   total loss: [0.4735949]\n",
            "mse_b ====== [0.0699319243]\n",
            "It: 38076, Time: 0.02\n",
            "mse_b  [0.06993192]  mse_f: 0.4258911609649658   total loss: [0.4958231]\n",
            "mse_b ====== [0.0334187672]\n",
            "It: 38077, Time: 0.02\n",
            "mse_b  [0.03341877]  mse_f: 0.45706838369369507   total loss: [0.49048716]\n",
            "mse_b ====== [0.0528388508]\n",
            "It: 38078, Time: 0.03\n",
            "mse_b  [0.05283885]  mse_f: 0.4292435050010681   total loss: [0.48208237]\n",
            "mse_b ====== [0.0296028499]\n",
            "It: 38079, Time: 0.03\n",
            "mse_b  [0.02960285]  mse_f: 0.4391101598739624   total loss: [0.46871302]\n",
            "mse_b ====== [0.0279378183]\n",
            "It: 38080, Time: 0.02\n",
            "mse_b  [0.02793782]  mse_f: 0.4494474530220032   total loss: [0.47738528]\n",
            "mse_b ====== [0.0742688552]\n",
            "It: 38081, Time: 0.02\n",
            "mse_b  [0.07426886]  mse_f: 0.4282388687133789   total loss: [0.50250775]\n",
            "mse_b ====== [0.0343775414]\n",
            "It: 38082, Time: 0.02\n",
            "mse_b  [0.03437754]  mse_f: 0.4566715359687805   total loss: [0.49104908]\n",
            "mse_b ====== [0.050026767]\n",
            "It: 38083, Time: 0.02\n",
            "mse_b  [0.05002677]  mse_f: 0.4295172691345215   total loss: [0.47954404]\n",
            "mse_b ====== [0.0294855777]\n",
            "It: 38084, Time: 0.02\n",
            "mse_b  [0.02948558]  mse_f: 0.43947917222976685   total loss: [0.46896476]\n",
            "mse_b ====== [0.0280573387]\n",
            "It: 38085, Time: 0.02\n",
            "mse_b  [0.02805734]  mse_f: 0.4506950378417969   total loss: [0.47875237]\n",
            "mse_b ====== [0.0745163932]\n",
            "It: 38086, Time: 0.02\n",
            "mse_b  [0.07451639]  mse_f: 0.4244202971458435   total loss: [0.49893668]\n",
            "mse_b ====== [0.0303203464]\n",
            "It: 38087, Time: 0.02\n",
            "mse_b  [0.03032035]  mse_f: 0.45190945267677307   total loss: [0.4822298]\n",
            "mse_b ====== [0.0354512595]\n",
            "It: 38088, Time: 0.02\n",
            "mse_b  [0.03545126]  mse_f: 0.4346078634262085   total loss: [0.47005913]\n",
            "mse_b ====== [0.0373138748]\n",
            "It: 38089, Time: 0.02\n",
            "mse_b  [0.03731387]  mse_f: 0.4338381290435791   total loss: [0.471152]\n",
            "mse_b ====== [0.0306100957]\n",
            "It: 38090, Time: 0.02\n",
            "mse_b  [0.0306101]  mse_f: 0.45284974575042725   total loss: [0.48345983]\n",
            "mse_b ====== [0.0744279474]\n",
            "It: 38091, Time: 0.02\n",
            "mse_b  [0.07442795]  mse_f: 0.4259922504425049   total loss: [0.5004202]\n",
            "mse_b ====== [0.0314176157]\n",
            "It: 38092, Time: 0.02\n",
            "mse_b  [0.03141762]  mse_f: 0.44995954632759094   total loss: [0.48137715]\n",
            "mse_b ====== [0.0342531465]\n",
            "It: 38093, Time: 0.02\n",
            "mse_b  [0.03425315]  mse_f: 0.4349955916404724   total loss: [0.46924874]\n",
            "mse_b ====== [0.0413039029]\n",
            "It: 38094, Time: 0.02\n",
            "mse_b  [0.0413039]  mse_f: 0.4314187467098236   total loss: [0.47272265]\n",
            "mse_b ====== [0.031504862]\n",
            "It: 38095, Time: 0.03\n",
            "mse_b  [0.03150486]  mse_f: 0.4537757933139801   total loss: [0.48528066]\n",
            "mse_b ====== [0.0750981718]\n",
            "It: 38096, Time: 0.02\n",
            "mse_b  [0.07509817]  mse_f: 0.42632684111595154   total loss: [0.501425]\n",
            "mse_b ====== [0.0304645523]\n",
            "It: 38097, Time: 0.02\n",
            "mse_b  [0.03046455]  mse_f: 0.4495403468608856   total loss: [0.4800049]\n",
            "mse_b ====== [0.0323167741]\n",
            "It: 38098, Time: 0.02\n",
            "mse_b  [0.03231677]  mse_f: 0.4350965619087219   total loss: [0.46741334]\n",
            "mse_b ====== [0.0420438275]\n",
            "It: 38099, Time: 0.02\n",
            "mse_b  [0.04204383]  mse_f: 0.43062514066696167   total loss: [0.47266898]\n",
            "mse_b ====== [0.0304187052]\n",
            "It: 38100, Time: 0.02\n",
            "mse_b  [0.03041871]  mse_f: 0.45385560393333435   total loss: [0.4842743]\n",
            "mse_b ====== [0.0692898929]\n",
            "It: 38101, Time: 0.02\n",
            "mse_b  [0.06928989]  mse_f: 0.42518994212150574   total loss: [0.49447984]\n",
            "mse_b ====== [0.028311016]\n",
            "It: 38102, Time: 0.03\n",
            "mse_b  [0.02831102]  mse_f: 0.445632666349411   total loss: [0.47394368]\n",
            "mse_b ====== [0.0272926912]\n",
            "It: 38103, Time: 0.02\n",
            "mse_b  [0.02729269]  mse_f: 0.4405288100242615   total loss: [0.4678215]\n",
            "mse_b ====== [0.0532631762]\n",
            "It: 38104, Time: 0.02\n",
            "mse_b  [0.05326318]  mse_f: 0.42884936928749084   total loss: [0.48211256]\n",
            "mse_b ====== [0.0331151076]\n",
            "It: 38105, Time: 0.02\n",
            "mse_b  [0.03311511]  mse_f: 0.4559627175331116   total loss: [0.48907784]\n",
            "mse_b ====== [0.0654327273]\n",
            "It: 38106, Time: 0.02\n",
            "mse_b  [0.06543273]  mse_f: 0.4264746308326721   total loss: [0.49190736]\n",
            "mse_b ====== [0.0278729238]\n",
            "It: 38107, Time: 0.02\n",
            "mse_b  [0.02787292]  mse_f: 0.4431474804878235   total loss: [0.4710204]\n",
            "mse_b ====== [0.0279553384]\n",
            "It: 38108, Time: 0.02\n",
            "mse_b  [0.02795534]  mse_f: 0.44205188751220703   total loss: [0.47000724]\n",
            "mse_b ====== [0.0616575107]\n",
            "It: 38109, Time: 0.03\n",
            "mse_b  [0.06165751]  mse_f: 0.4259216785430908   total loss: [0.4875792]\n",
            "mse_b ====== [0.0337854326]\n",
            "It: 38110, Time: 0.03\n",
            "mse_b  [0.03378543]  mse_f: 0.453827440738678   total loss: [0.48761287]\n",
            "mse_b ====== [0.0587321073]\n",
            "It: 38111, Time: 0.02\n",
            "mse_b  [0.05873211]  mse_f: 0.4264605641365051   total loss: [0.48519266]\n",
            "mse_b ====== [0.027477555]\n",
            "It: 38112, Time: 0.02\n",
            "mse_b  [0.02747755]  mse_f: 0.4413013756275177   total loss: [0.46877894]\n",
            "mse_b ====== [0.027164286]\n",
            "It: 38113, Time: 0.02\n",
            "mse_b  [0.02716429]  mse_f: 0.4432644546031952   total loss: [0.47042874]\n",
            "mse_b ====== [0.0623398051]\n",
            "It: 38114, Time: 0.02\n",
            "mse_b  [0.06233981]  mse_f: 0.42483916878700256   total loss: [0.48717898]\n",
            "mse_b ====== [0.0322113931]\n",
            "It: 38115, Time: 0.02\n",
            "mse_b  [0.03221139]  mse_f: 0.45216482877731323   total loss: [0.48437622]\n",
            "mse_b ====== [0.0490209609]\n",
            "It: 38116, Time: 0.02\n",
            "mse_b  [0.04902096]  mse_f: 0.4282876253128052   total loss: [0.47730857]\n",
            "mse_b ====== [0.0298274644]\n",
            "It: 38117, Time: 0.02\n",
            "mse_b  [0.02982746]  mse_f: 0.43683451414108276   total loss: [0.466662]\n",
            "mse_b ====== [0.0276431702]\n",
            "It: 38118, Time: 0.02\n",
            "mse_b  [0.02764317]  mse_f: 0.4464528560638428   total loss: [0.47409603]\n",
            "mse_b ====== [0.0667194203]\n",
            "It: 38119, Time: 0.02\n",
            "mse_b  [0.06671942]  mse_f: 0.42715921998023987   total loss: [0.49387863]\n",
            "mse_b ====== [0.0338968635]\n",
            "It: 38120, Time: 0.02\n",
            "mse_b  [0.03389686]  mse_f: 0.45277875661849976   total loss: [0.48667562]\n",
            "mse_b ====== [0.0487964749]\n",
            "It: 38121, Time: 0.02\n",
            "mse_b  [0.04879647]  mse_f: 0.42821386456489563   total loss: [0.47701034]\n",
            "mse_b ====== [0.0281297974]\n",
            "It: 38122, Time: 0.02\n",
            "mse_b  [0.0281298]  mse_f: 0.43833494186401367   total loss: [0.46646473]\n",
            "mse_b ====== [0.0266140327]\n",
            "It: 38123, Time: 0.02\n",
            "mse_b  [0.02661403]  mse_f: 0.4465479254722595   total loss: [0.47316197]\n",
            "mse_b ====== [0.0657939464]\n",
            "It: 38124, Time: 0.02\n",
            "mse_b  [0.06579395]  mse_f: 0.423958420753479   total loss: [0.48975235]\n",
            "mse_b ====== [0.0314924382]\n",
            "It: 38125, Time: 0.02\n",
            "mse_b  [0.03149244]  mse_f: 0.4497692584991455   total loss: [0.4812617]\n",
            "mse_b ====== [0.0420330055]\n",
            "It: 38126, Time: 0.02\n",
            "mse_b  [0.04203301]  mse_f: 0.42914992570877075   total loss: [0.47118294]\n",
            "mse_b ====== [0.0300621167]\n",
            "It: 38127, Time: 0.02\n",
            "mse_b  [0.03006212]  mse_f: 0.43603140115737915   total loss: [0.4660935]\n",
            "mse_b ====== [0.0278338939]\n",
            "It: 38128, Time: 0.02\n",
            "mse_b  [0.02783389]  mse_f: 0.4450721740722656   total loss: [0.47290605]\n",
            "mse_b ====== [0.0634899586]\n",
            "It: 38129, Time: 0.02\n",
            "mse_b  [0.06348996]  mse_f: 0.42459553480148315   total loss: [0.4880855]\n",
            "mse_b ====== [0.0324494317]\n",
            "It: 38130, Time: 0.02\n",
            "mse_b  [0.03244943]  mse_f: 0.44923850893974304   total loss: [0.48168793]\n",
            "mse_b ====== [0.0456476137]\n",
            "It: 38131, Time: 0.02\n",
            "mse_b  [0.04564761]  mse_f: 0.42750418186187744   total loss: [0.4731518]\n",
            "mse_b ====== [0.0307960622]\n",
            "It: 38132, Time: 0.03\n",
            "mse_b  [0.03079606]  mse_f: 0.4355490207672119   total loss: [0.46634507]\n",
            "mse_b ====== [0.0279319864]\n",
            "It: 38133, Time: 0.02\n",
            "mse_b  [0.02793199]  mse_f: 0.4446149170398712   total loss: [0.4725469]\n",
            "mse_b ====== [0.0636818483]\n",
            "It: 38134, Time: 0.02\n",
            "mse_b  [0.06368185]  mse_f: 0.4252709448337555   total loss: [0.4889528]\n",
            "mse_b ====== [0.0323862545]\n",
            "It: 38135, Time: 0.02\n",
            "mse_b  [0.03238625]  mse_f: 0.4491012990474701   total loss: [0.48148754]\n",
            "mse_b ====== [0.0444631055]\n",
            "It: 38136, Time: 0.02\n",
            "mse_b  [0.04446311]  mse_f: 0.4285152852535248   total loss: [0.47297838]\n",
            "mse_b ====== [0.0290779583]\n",
            "It: 38137, Time: 0.02\n",
            "mse_b  [0.02907796]  mse_f: 0.4362215995788574   total loss: [0.46529955]\n",
            "mse_b ====== [0.0275432281]\n",
            "It: 38138, Time: 0.02\n",
            "mse_b  [0.02754323]  mse_f: 0.44257935881614685   total loss: [0.47012258]\n",
            "mse_b ====== [0.0598559454]\n",
            "It: 38139, Time: 0.02\n",
            "mse_b  [0.05985595]  mse_f: 0.4238079786300659   total loss: [0.48366392]\n",
            "mse_b ====== [0.0293012373]\n",
            "It: 38140, Time: 0.02\n",
            "mse_b  [0.02930124]  mse_f: 0.4476829171180725   total loss: [0.47698414]\n",
            "mse_b ====== [0.0398725234]\n",
            "It: 38141, Time: 0.02\n",
            "mse_b  [0.03987252]  mse_f: 0.4294298589229584   total loss: [0.4693024]\n",
            "mse_b ====== [0.0300015174]\n",
            "It: 38142, Time: 0.02\n",
            "mse_b  [0.03000152]  mse_f: 0.434665709733963   total loss: [0.46466723]\n",
            "mse_b ====== [0.0276684985]\n",
            "It: 38143, Time: 0.02\n",
            "mse_b  [0.0276685]  mse_f: 0.44280898571014404   total loss: [0.4704775]\n",
            "mse_b ====== [0.0568891838]\n",
            "It: 38144, Time: 0.02\n",
            "mse_b  [0.05688918]  mse_f: 0.4266282618045807   total loss: [0.48351744]\n",
            "mse_b ====== [0.0314237513]\n",
            "It: 38145, Time: 0.02\n",
            "mse_b  [0.03142375]  mse_f: 0.44973456859588623   total loss: [0.48115832]\n",
            "mse_b ====== [0.0549515188]\n",
            "It: 38146, Time: 0.02\n",
            "mse_b  [0.05495152]  mse_f: 0.42447203397750854   total loss: [0.47942355]\n",
            "mse_b ====== [0.0273106471]\n",
            "It: 38147, Time: 0.02\n",
            "mse_b  [0.02731065]  mse_f: 0.440137654542923   total loss: [0.4674483]\n",
            "mse_b ====== [0.0289084911]\n",
            "It: 38148, Time: 0.02\n",
            "mse_b  [0.02890849]  mse_f: 0.4368440508842468   total loss: [0.46575254]\n",
            "mse_b ====== [0.0495306402]\n",
            "It: 38149, Time: 0.02\n",
            "mse_b  [0.04953064]  mse_f: 0.42526453733444214   total loss: [0.47479516]\n",
            "mse_b ====== [0.0313130058]\n",
            "It: 38150, Time: 0.02\n",
            "mse_b  [0.03131301]  mse_f: 0.44961684942245483   total loss: [0.48092985]\n",
            "mse_b ====== [0.0639050901]\n",
            "It: 38151, Time: 0.02\n",
            "mse_b  [0.06390509]  mse_f: 0.42444029450416565   total loss: [0.48834538]\n",
            "mse_b ====== [0.0290575735]\n",
            "It: 38152, Time: 0.02\n",
            "mse_b  [0.02905757]  mse_f: 0.4445638060569763   total loss: [0.47362137]\n",
            "mse_b ====== [0.0351660401]\n",
            "It: 38153, Time: 0.02\n",
            "mse_b  [0.03516604]  mse_f: 0.4304552674293518   total loss: [0.4656213]\n",
            "mse_b ====== [0.03520675]\n",
            "It: 38154, Time: 0.03\n",
            "mse_b  [0.03520675]  mse_f: 0.4302781820297241   total loss: [0.46548492]\n",
            "mse_b ====== [0.0279135294]\n",
            "It: 38155, Time: 0.03\n",
            "mse_b  [0.02791353]  mse_f: 0.4454702138900757   total loss: [0.47338375]\n",
            "mse_b ====== [0.0601651333]\n",
            "It: 38156, Time: 0.02\n",
            "mse_b  [0.06016513]  mse_f: 0.42351508140563965   total loss: [0.48368022]\n",
            "mse_b ====== [0.030106876]\n",
            "It: 38157, Time: 0.02\n",
            "mse_b  [0.03010688]  mse_f: 0.4458944499492645   total loss: [0.47600132]\n",
            "mse_b ====== [0.0409026593]\n",
            "It: 38158, Time: 0.02\n",
            "mse_b  [0.04090266]  mse_f: 0.42811715602874756   total loss: [0.46901983]\n",
            "mse_b ====== [0.0293936096]\n",
            "It: 38159, Time: 0.02\n",
            "mse_b  [0.02939361]  mse_f: 0.4342351257801056   total loss: [0.46362874]\n",
            "mse_b ====== [0.0272501167]\n",
            "It: 38160, Time: 0.03\n",
            "mse_b  [0.02725012]  mse_f: 0.44022855162620544   total loss: [0.46747866]\n",
            "mse_b ====== [0.0537201092]\n",
            "It: 38161, Time: 0.02\n",
            "mse_b  [0.05372011]  mse_f: 0.4236154556274414   total loss: [0.47733557]\n",
            "mse_b ====== [0.0311598014]\n",
            "It: 38162, Time: 0.02\n",
            "mse_b  [0.0311598]  mse_f: 0.4461994469165802   total loss: [0.47735924]\n",
            "mse_b ====== [0.0506284311]\n",
            "It: 38163, Time: 0.02\n",
            "mse_b  [0.05062843]  mse_f: 0.4238949716091156   total loss: [0.4745234]\n",
            "mse_b ====== [0.0275547616]\n",
            "It: 38164, Time: 0.02\n",
            "mse_b  [0.02755476]  mse_f: 0.43780288100242615   total loss: [0.46535763]\n",
            "mse_b ====== [0.0282537397]\n",
            "It: 38165, Time: 0.02\n",
            "mse_b  [0.02825374]  mse_f: 0.43585240840911865   total loss: [0.46410614]\n",
            "mse_b ====== [0.0447590724]\n",
            "It: 38166, Time: 0.02\n",
            "mse_b  [0.04475907]  mse_f: 0.42641183733940125   total loss: [0.4711709]\n",
            "mse_b ====== [0.0313038267]\n",
            "It: 38167, Time: 0.02\n",
            "mse_b  [0.03130383]  mse_f: 0.44579601287841797   total loss: [0.47709984]\n",
            "mse_b ====== [0.060536705]\n",
            "It: 38168, Time: 0.02\n",
            "mse_b  [0.0605367]  mse_f: 0.42263472080230713   total loss: [0.48317143]\n",
            "mse_b ====== [0.0284150019]\n",
            "It: 38169, Time: 0.03\n",
            "mse_b  [0.028415]  mse_f: 0.4444301724433899   total loss: [0.47284517]\n",
            "mse_b ====== [0.036773622]\n",
            "It: 38170, Time: 0.02\n",
            "mse_b  [0.03677362]  mse_f: 0.4285919964313507   total loss: [0.46536562]\n",
            "mse_b ====== [0.0328829065]\n",
            "It: 38171, Time: 0.02\n",
            "mse_b  [0.03288291]  mse_f: 0.4305098056793213   total loss: [0.4633927]\n",
            "mse_b ====== [0.0280809812]\n",
            "It: 38172, Time: 0.02\n",
            "mse_b  [0.02808098]  mse_f: 0.4411442279815674   total loss: [0.4692252]\n",
            "mse_b ====== [0.0584524721]\n",
            "It: 38173, Time: 0.02\n",
            "mse_b  [0.05845247]  mse_f: 0.42357534170150757   total loss: [0.48202783]\n",
            "mse_b ====== [0.0317809209]\n",
            "It: 38174, Time: 0.02\n",
            "mse_b  [0.03178092]  mse_f: 0.4495461583137512   total loss: [0.4813271]\n",
            "mse_b ====== [0.059918426]\n",
            "It: 38175, Time: 0.02\n",
            "mse_b  [0.05991843]  mse_f: 0.4225786626338959   total loss: [0.4824971]\n",
            "mse_b ====== [0.0285549816]\n",
            "It: 38176, Time: 0.03\n",
            "mse_b  [0.02855498]  mse_f: 0.4412645697593689   total loss: [0.46981955]\n",
            "mse_b ====== [0.0325086527]\n",
            "It: 38177, Time: 0.02\n",
            "mse_b  [0.03250865]  mse_f: 0.4308299422264099   total loss: [0.46333858]\n",
            "mse_b ====== [0.036967814]\n",
            "It: 38178, Time: 0.02\n",
            "mse_b  [0.03696781]  mse_f: 0.4283196032047272   total loss: [0.46528742]\n",
            "mse_b ====== [0.0289677661]\n",
            "It: 38179, Time: 0.02\n",
            "mse_b  [0.02896777]  mse_f: 0.4436854422092438   total loss: [0.4726532]\n",
            "mse_b ====== [0.0622803867]\n",
            "It: 38180, Time: 0.02\n",
            "mse_b  [0.06228039]  mse_f: 0.42317995429039   total loss: [0.48546034]\n",
            "mse_b ====== [0.0328526311]\n",
            "It: 38181, Time: 0.02\n",
            "mse_b  [0.03285263]  mse_f: 0.4462378919124603   total loss: [0.4790905]\n",
            "mse_b ====== [0.0501344055]\n",
            "It: 38182, Time: 0.03\n",
            "mse_b  [0.05013441]  mse_f: 0.423722505569458   total loss: [0.47385693]\n",
            "mse_b ====== [0.0276658107]\n",
            "It: 38183, Time: 0.02\n",
            "mse_b  [0.02766581]  mse_f: 0.4367191791534424   total loss: [0.464385]\n",
            "mse_b ====== [0.0286306106]\n",
            "It: 38184, Time: 0.02\n",
            "mse_b  [0.02863061]  mse_f: 0.43461596965789795   total loss: [0.46324658]\n",
            "mse_b ====== [0.0475681201]\n",
            "It: 38185, Time: 0.02\n",
            "mse_b  [0.04756812]  mse_f: 0.42342764139175415   total loss: [0.47099575]\n",
            "mse_b ====== [0.0311266705]\n",
            "It: 38186, Time: 0.02\n",
            "mse_b  [0.03112667]  mse_f: 0.4447639584541321   total loss: [0.47589064]\n",
            "mse_b ====== [0.0615885295]\n",
            "It: 38187, Time: 0.02\n",
            "mse_b  [0.06158853]  mse_f: 0.4218001067638397   total loss: [0.48338863]\n",
            "mse_b ====== [0.0296219829]\n",
            "It: 38188, Time: 0.02\n",
            "mse_b  [0.02962198]  mse_f: 0.4444926083087921   total loss: [0.4741146]\n",
            "mse_b ====== [0.0424538553]\n",
            "It: 38189, Time: 0.02\n",
            "mse_b  [0.04245386]  mse_f: 0.4252569079399109   total loss: [0.46771076]\n",
            "mse_b ====== [0.0290109143]\n",
            "It: 38190, Time: 0.02\n",
            "mse_b  [0.02901091]  mse_f: 0.43332934379577637   total loss: [0.46234027]\n",
            "mse_b ====== [0.0280797333]\n",
            "It: 38191, Time: 0.02\n",
            "mse_b  [0.02807973]  mse_f: 0.43490543961524963   total loss: [0.46298516]\n",
            "mse_b ====== [0.045460742]\n",
            "It: 38192, Time: 0.03\n",
            "mse_b  [0.04546074]  mse_f: 0.42410987615585327   total loss: [0.4695706]\n",
            "mse_b ====== [0.0290553086]\n",
            "It: 38193, Time: 0.02\n",
            "mse_b  [0.02905531]  mse_f: 0.4437549114227295   total loss: [0.4728102]\n",
            "mse_b ====== [0.0553036183]\n",
            "It: 38194, Time: 0.02\n",
            "mse_b  [0.05530362]  mse_f: 0.4221699833869934   total loss: [0.47747362]\n",
            "mse_b ====== [0.0289335568]\n",
            "It: 38195, Time: 0.02\n",
            "mse_b  [0.02893356]  mse_f: 0.4412499666213989   total loss: [0.47018352]\n",
            "mse_b ====== [0.0381989069]\n",
            "It: 38196, Time: 0.02\n",
            "mse_b  [0.03819891]  mse_f: 0.42706501483917236   total loss: [0.46526393]\n",
            "mse_b ====== [0.0283784792]\n",
            "It: 38197, Time: 0.02\n",
            "mse_b  [0.02837848]  mse_f: 0.4332190155982971   total loss: [0.4615975]\n",
            "mse_b ====== [0.0280363224]\n",
            "It: 38198, Time: 0.02\n",
            "mse_b  [0.02803632]  mse_f: 0.43392929434776306   total loss: [0.46196562]\n",
            "mse_b ====== [0.0422336683]\n",
            "It: 38199, Time: 0.02\n",
            "mse_b  [0.04223367]  mse_f: 0.42466944456100464   total loss: [0.46690312]\n",
            "mse_b ====== [0.0288714562]\n",
            "It: 38200, Time: 0.02\n",
            "mse_b  [0.02887146]  mse_f: 0.4416390359401703   total loss: [0.47051048]\n",
            "mse_b ====== [0.0554578081]\n",
            "It: 38201, Time: 0.02\n",
            "mse_b  [0.05545781]  mse_f: 0.42119210958480835   total loss: [0.4766499]\n",
            "mse_b ====== [0.0296698492]\n",
            "It: 38202, Time: 0.02\n",
            "mse_b  [0.02966985]  mse_f: 0.44309523701667786   total loss: [0.4727651]\n",
            "mse_b ====== [0.0500193536]\n",
            "It: 38203, Time: 0.02\n",
            "mse_b  [0.05001935]  mse_f: 0.4225473999977112   total loss: [0.47256675]\n",
            "mse_b ====== [0.0283295885]\n",
            "It: 38204, Time: 0.03\n",
            "mse_b  [0.02832959]  mse_f: 0.4384278357028961   total loss: [0.46675742]\n",
            "mse_b ====== [0.037030872]\n",
            "It: 38205, Time: 0.02\n",
            "mse_b  [0.03703087]  mse_f: 0.42625272274017334   total loss: [0.4632836]\n",
            "mse_b ====== [0.028381642]\n",
            "It: 38206, Time: 0.02\n",
            "mse_b  [0.02838164]  mse_f: 0.432689905166626   total loss: [0.46107155]\n",
            "mse_b ====== [0.0290128551]\n",
            "It: 38207, Time: 0.02\n",
            "mse_b  [0.02901286]  mse_f: 0.43167582154273987   total loss: [0.46068868]\n",
            "mse_b ====== [0.0366280079]\n",
            "It: 38208, Time: 0.03\n",
            "mse_b  [0.03662801]  mse_f: 0.4262232184410095   total loss: [0.46285123]\n",
            "mse_b ====== [0.0277449396]\n",
            "It: 38209, Time: 0.02\n",
            "mse_b  [0.02774494]  mse_f: 0.43750280141830444   total loss: [0.46524775]\n",
            "mse_b ====== [0.0467262603]\n",
            "It: 38210, Time: 0.02\n",
            "mse_b  [0.04672626]  mse_f: 0.42295151948928833   total loss: [0.46967778]\n",
            "mse_b ====== [0.0286566988]\n",
            "It: 38211, Time: 0.02\n",
            "mse_b  [0.0286567]  mse_f: 0.44116756319999695   total loss: [0.46982425]\n",
            "mse_b ====== [0.0506157652]\n",
            "It: 38212, Time: 0.02\n",
            "mse_b  [0.05061577]  mse_f: 0.42186495661735535   total loss: [0.4724807]\n",
            "mse_b ====== [0.029004002]\n",
            "It: 38213, Time: 0.02\n",
            "mse_b  [0.029004]  mse_f: 0.4398401379585266   total loss: [0.46884415]\n",
            "mse_b ====== [0.044041872]\n",
            "It: 38214, Time: 0.02\n",
            "mse_b  [0.04404187]  mse_f: 0.42309486865997314   total loss: [0.46713674]\n",
            "mse_b ====== [0.0271183513]\n",
            "It: 38215, Time: 0.02\n",
            "mse_b  [0.02711835]  mse_f: 0.43627306818962097   total loss: [0.46339142]\n",
            "mse_b ====== [0.0337605588]\n",
            "It: 38216, Time: 0.02\n",
            "mse_b  [0.03376056]  mse_f: 0.4269845187664032   total loss: [0.46074507]\n",
            "mse_b ====== [0.030238539]\n",
            "It: 38217, Time: 0.02\n",
            "mse_b  [0.03023854]  mse_f: 0.4297335147857666   total loss: [0.45997205]\n",
            "mse_b ====== [0.0282471776]\n",
            "It: 38218, Time: 0.02\n",
            "mse_b  [0.02824718]  mse_f: 0.43241795897483826   total loss: [0.46066514]\n",
            "mse_b ====== [0.0383019112]\n",
            "It: 38219, Time: 0.03\n",
            "mse_b  [0.03830191]  mse_f: 0.42509686946868896   total loss: [0.46339878]\n",
            "mse_b ====== [0.0279745944]\n",
            "It: 38220, Time: 0.03\n",
            "mse_b  [0.02797459]  mse_f: 0.4386202096939087   total loss: [0.46659482]\n",
            "mse_b ====== [0.0531794]\n",
            "It: 38221, Time: 0.02\n",
            "mse_b  [0.0531794]  mse_f: 0.4203156530857086   total loss: [0.47349507]\n",
            "mse_b ====== [0.0316516347]\n",
            "It: 38222, Time: 0.02\n",
            "mse_b  [0.03165163]  mse_f: 0.4430707097053528   total loss: [0.47472236]\n",
            "mse_b ====== [0.0621992722]\n",
            "It: 38223, Time: 0.02\n",
            "mse_b  [0.06219927]  mse_f: 0.4190294146537781   total loss: [0.48122868]\n",
            "mse_b ====== [0.0301147029]\n",
            "It: 38224, Time: 0.02\n",
            "mse_b  [0.0301147]  mse_f: 0.44398707151412964   total loss: [0.47410178]\n",
            "mse_b ====== [0.0497865118]\n",
            "It: 38225, Time: 0.02\n",
            "mse_b  [0.04978651]  mse_f: 0.4214721918106079   total loss: [0.4712587]\n",
            "mse_b ====== [0.027722463]\n",
            "It: 38226, Time: 0.02\n",
            "mse_b  [0.02772246]  mse_f: 0.43647700548171997   total loss: [0.46419948]\n",
            "mse_b ====== [0.0334043168]\n",
            "It: 38227, Time: 0.02\n",
            "mse_b  [0.03340432]  mse_f: 0.4270733892917633   total loss: [0.4604777]\n",
            "mse_b ====== [0.0305563472]\n",
            "It: 38228, Time: 0.03\n",
            "mse_b  [0.03055635]  mse_f: 0.42904767394065857   total loss: [0.45960402]\n",
            "mse_b ====== [0.0271462053]\n",
            "It: 38229, Time: 0.02\n",
            "mse_b  [0.02714621]  mse_f: 0.43466365337371826   total loss: [0.46180987]\n",
            "mse_b ====== [0.0457738563]\n",
            "It: 38230, Time: 0.02\n",
            "mse_b  [0.04577386]  mse_f: 0.42154309153556824   total loss: [0.46731696]\n",
            "mse_b ====== [0.0302147605]\n",
            "It: 38231, Time: 0.02\n",
            "mse_b  [0.03021476]  mse_f: 0.4410790503025055   total loss: [0.4712938]\n",
            "mse_b ====== [0.0612299144]\n",
            "It: 38232, Time: 0.02\n",
            "mse_b  [0.06122991]  mse_f: 0.4185335040092468   total loss: [0.47976342]\n",
            "mse_b ====== [0.0314866155]\n",
            "It: 38233, Time: 0.02\n",
            "mse_b  [0.03148662]  mse_f: 0.44479623436927795   total loss: [0.47628284]\n",
            "mse_b ====== [0.0601815432]\n",
            "It: 38234, Time: 0.03\n",
            "mse_b  [0.06018154]  mse_f: 0.4189078211784363   total loss: [0.47908938]\n",
            "mse_b ====== [0.0304176435]\n",
            "It: 38235, Time: 0.02\n",
            "mse_b  [0.03041764]  mse_f: 0.44054195284843445   total loss: [0.4709596]\n",
            "mse_b ====== [0.04677964]\n",
            "It: 38236, Time: 0.02\n",
            "mse_b  [0.04677964]  mse_f: 0.4210817813873291   total loss: [0.4678614]\n",
            "mse_b ====== [0.0273777749]\n",
            "It: 38237, Time: 0.02\n",
            "mse_b  [0.02737777]  mse_f: 0.4349210858345032   total loss: [0.46229887]\n",
            "mse_b ====== [0.0328498445]\n",
            "It: 38238, Time: 0.03\n",
            "mse_b  [0.03284984]  mse_f: 0.42672398686408997   total loss: [0.45957384]\n",
            "mse_b ====== [0.0317267627]\n",
            "It: 38239, Time: 0.02\n",
            "mse_b  [0.03172676]  mse_f: 0.42718949913978577   total loss: [0.45891625]\n",
            "mse_b ====== [0.0280456226]\n",
            "It: 38240, Time: 0.02\n",
            "mse_b  [0.02804562]  mse_f: 0.4328846037387848   total loss: [0.46093023]\n",
            "mse_b ====== [0.0433116965]\n",
            "It: 38241, Time: 0.02\n",
            "mse_b  [0.0433117]  mse_f: 0.4218125343322754   total loss: [0.46512422]\n",
            "mse_b ====== [0.0288080759]\n",
            "It: 38242, Time: 0.02\n",
            "mse_b  [0.02880808]  mse_f: 0.4397536814212799   total loss: [0.46856177]\n",
            "mse_b ====== [0.0566803291]\n",
            "It: 38243, Time: 0.03\n",
            "mse_b  [0.05668033]  mse_f: 0.4188668727874756   total loss: [0.4755472]\n",
            "mse_b ====== [0.0319188163]\n",
            "It: 38244, Time: 0.02\n",
            "mse_b  [0.03191882]  mse_f: 0.4422127604484558   total loss: [0.47413158]\n",
            "mse_b ====== [0.0612005591]\n",
            "It: 38245, Time: 0.02\n",
            "mse_b  [0.06120056]  mse_f: 0.4178188741207123   total loss: [0.47901943]\n",
            "mse_b ====== [0.0299236942]\n",
            "It: 38246, Time: 0.02\n",
            "mse_b  [0.02992369]  mse_f: 0.4416618347167969   total loss: [0.47158554]\n",
            "mse_b ====== [0.0491759703]\n",
            "It: 38247, Time: 0.02\n",
            "mse_b  [0.04917597]  mse_f: 0.42048799991607666   total loss: [0.46966398]\n",
            "mse_b ====== [0.0276240669]\n",
            "It: 38248, Time: 0.02\n",
            "mse_b  [0.02762407]  mse_f: 0.43582209944725037   total loss: [0.46344617]\n",
            "mse_b ====== [0.0354885533]\n",
            "It: 38249, Time: 0.02\n",
            "mse_b  [0.03548855]  mse_f: 0.42485374212265015   total loss: [0.4603423]\n",
            "mse_b ====== [0.0289459359]\n",
            "It: 38250, Time: 0.04\n",
            "mse_b  [0.02894594]  mse_f: 0.42930638790130615   total loss: [0.4582523]\n",
            "mse_b ====== [0.0283033922]\n",
            "It: 38251, Time: 0.03\n",
            "mse_b  [0.02830339]  mse_f: 0.4303078055381775   total loss: [0.4586112]\n",
            "mse_b ====== [0.0375596397]\n",
            "It: 38252, Time: 0.02\n",
            "mse_b  [0.03755964]  mse_f: 0.42333588004112244   total loss: [0.4608955]\n",
            "mse_b ====== [0.0286546331]\n",
            "It: 38253, Time: 0.03\n",
            "mse_b  [0.02865463]  mse_f: 0.435852974653244   total loss: [0.4645076]\n",
            "mse_b ====== [0.0534105524]\n",
            "It: 38254, Time: 0.02\n",
            "mse_b  [0.05341055]  mse_f: 0.41831648349761963   total loss: [0.47172704]\n",
            "mse_b ====== [0.0317769274]\n",
            "It: 38255, Time: 0.02\n",
            "mse_b  [0.03177693]  mse_f: 0.4422486126422882   total loss: [0.47402555]\n",
            "mse_b ====== [0.070492208]\n",
            "It: 38256, Time: 0.02\n",
            "mse_b  [0.07049221]  mse_f: 0.4159916043281555   total loss: [0.4864838]\n",
            "mse_b ====== [0.0359779447]\n",
            "It: 38257, Time: 0.03\n",
            "mse_b  [0.03597794]  mse_f: 0.44620510935783386   total loss: [0.48218304]\n",
            "mse_b ====== [0.07619638]\n",
            "It: 38258, Time: 0.02\n",
            "mse_b  [0.07619638]  mse_f: 0.41605818271636963   total loss: [0.49225456]\n",
            "mse_b ====== [0.0333602577]\n",
            "It: 38259, Time: 0.02\n",
            "mse_b  [0.03336026]  mse_f: 0.4441100060939789   total loss: [0.47747028]\n",
            "mse_b ====== [0.0540800691]\n",
            "It: 38260, Time: 0.02\n",
            "mse_b  [0.05408007]  mse_f: 0.418647825717926   total loss: [0.4727279]\n",
            "mse_b ====== [0.0275013577]\n",
            "It: 38261, Time: 0.02\n",
            "mse_b  [0.02750136]  mse_f: 0.43460506200790405   total loss: [0.4621064]\n",
            "mse_b ====== [0.0312717967]\n",
            "It: 38262, Time: 0.02\n",
            "mse_b  [0.0312718]  mse_f: 0.4267415404319763   total loss: [0.45801333]\n",
            "mse_b ====== [0.0355842933]\n",
            "It: 38263, Time: 0.02\n",
            "mse_b  [0.03558429]  mse_f: 0.4241846203804016   total loss: [0.45976892]\n",
            "mse_b ====== [0.0283465646]\n",
            "It: 38264, Time: 0.03\n",
            "mse_b  [0.02834656]  mse_f: 0.4378504753112793   total loss: [0.46619704]\n",
            "mse_b ====== [0.0626655445]\n",
            "It: 38265, Time: 0.02\n",
            "mse_b  [0.06266554]  mse_f: 0.4169299006462097   total loss: [0.47959545]\n",
            "mse_b ====== [0.0344853923]\n",
            "It: 38266, Time: 0.02\n",
            "mse_b  [0.03448539]  mse_f: 0.4454059898853302   total loss: [0.4798914]\n",
            "mse_b ====== [0.0737138242]\n",
            "It: 38267, Time: 0.02\n",
            "mse_b  [0.07371382]  mse_f: 0.41535502672195435   total loss: [0.48906887]\n",
            "mse_b ====== [0.031263642]\n",
            "It: 38268, Time: 0.02\n",
            "mse_b  [0.03126364]  mse_f: 0.4419941306114197   total loss: [0.47325778]\n",
            "mse_b ====== [0.0448767431]\n",
            "It: 38269, Time: 0.02\n",
            "mse_b  [0.04487674]  mse_f: 0.42001211643218994   total loss: [0.46488887]\n",
            "mse_b ====== [0.028198041]\n",
            "It: 38270, Time: 0.02\n",
            "mse_b  [0.02819804]  mse_f: 0.42975395917892456   total loss: [0.457952]\n",
            "mse_b ====== [0.0287244618]\n",
            "It: 38271, Time: 0.02\n",
            "mse_b  [0.02872446]  mse_f: 0.4295957386493683   total loss: [0.4583202]\n",
            "mse_b ====== [0.0433041975]\n",
            "It: 38272, Time: 0.02\n",
            "mse_b  [0.0433042]  mse_f: 0.4215732216835022   total loss: [0.46487743]\n",
            "mse_b ====== [0.0319528133]\n",
            "It: 38273, Time: 0.02\n",
            "mse_b  [0.03195281]  mse_f: 0.4414259195327759   total loss: [0.47337872]\n",
            "mse_b ====== [0.0790711865]\n",
            "It: 38274, Time: 0.02\n",
            "mse_b  [0.07907119]  mse_f: 0.4143912196159363   total loss: [0.4934624]\n",
            "mse_b ====== [0.0364707671]\n",
            "It: 38275, Time: 0.02\n",
            "mse_b  [0.03647077]  mse_f: 0.4473268985748291   total loss: [0.48379767]\n",
            "mse_b ====== [0.0674636]\n",
            "It: 38276, Time: 0.02\n",
            "mse_b  [0.0674636]  mse_f: 0.4161711633205414   total loss: [0.48363477]\n",
            "mse_b ====== [0.0285677444]\n",
            "It: 38277, Time: 0.02\n",
            "mse_b  [0.02856774]  mse_f: 0.43664461374282837   total loss: [0.46521235]\n",
            "mse_b ====== [0.0313879624]\n",
            "It: 38278, Time: 0.02\n",
            "mse_b  [0.03138796]  mse_f: 0.42591792345046997   total loss: [0.45730588]\n",
            "mse_b ====== [0.0374217518]\n",
            "It: 38279, Time: 0.02\n",
            "mse_b  [0.03742175]  mse_f: 0.423231840133667   total loss: [0.4606536]\n",
            "mse_b ====== [0.0319620073]\n",
            "It: 38280, Time: 0.02\n",
            "mse_b  [0.03196201]  mse_f: 0.44096964597702026   total loss: [0.47293165]\n",
            "mse_b ====== [0.081084162]\n",
            "It: 38281, Time: 0.03\n",
            "mse_b  [0.08108416]  mse_f: 0.41714370250701904   total loss: [0.49822786]\n",
            "mse_b ====== [0.0406340957]\n",
            "It: 38282, Time: 0.02\n",
            "mse_b  [0.0406341]  mse_f: 0.45118090510368347   total loss: [0.491815]\n",
            "mse_b ====== [0.0870416909]\n",
            "It: 38283, Time: 0.02\n",
            "mse_b  [0.08704169]  mse_f: 0.41294679045677185   total loss: [0.4999885]\n",
            "mse_b ====== [0.0287930556]\n",
            "It: 38284, Time: 0.03\n",
            "mse_b  [0.02879306]  mse_f: 0.43931591510772705   total loss: [0.46810898]\n",
            "mse_b ====== [0.0287898798]\n",
            "It: 38285, Time: 0.02\n",
            "mse_b  [0.02878988]  mse_f: 0.42947977781295776   total loss: [0.45826966]\n",
            "mse_b ====== [0.0555204302]\n",
            "It: 38286, Time: 0.02\n",
            "mse_b  [0.05552043]  mse_f: 0.4178850054740906   total loss: [0.47340542]\n",
            "mse_b ====== [0.0416710638]\n",
            "It: 38287, Time: 0.02\n",
            "mse_b  [0.04167106]  mse_f: 0.4505311846733093   total loss: [0.49220225]\n",
            "mse_b ====== [0.111257747]\n",
            "It: 38288, Time: 0.02\n",
            "mse_b  [0.11125775]  mse_f: 0.42029446363449097   total loss: [0.5315522]\n",
            "mse_b ====== [0.0391073525]\n",
            "It: 38289, Time: 0.03\n",
            "mse_b  [0.03910735]  mse_f: 0.4481905400753021   total loss: [0.4872979]\n",
            "mse_b ====== [0.0416958034]\n",
            "It: 38290, Time: 0.02\n",
            "mse_b  [0.0416958]  mse_f: 0.42249417304992676   total loss: [0.46418998]\n",
            "mse_b ====== [0.0385765694]\n",
            "It: 38291, Time: 0.03\n",
            "mse_b  [0.03857657]  mse_f: 0.4231174886226654   total loss: [0.46169406]\n",
            "mse_b ====== [0.0340367407]\n",
            "It: 38292, Time: 0.02\n",
            "mse_b  [0.03403674]  mse_f: 0.4479333758354187   total loss: [0.48197013]\n",
            "mse_b ====== [0.0986169651]\n",
            "It: 38293, Time: 0.02\n",
            "mse_b  [0.09861697]  mse_f: 0.4185418486595154   total loss: [0.5171588]\n",
            "mse_b ====== [0.0380221307]\n",
            "It: 38294, Time: 0.02\n",
            "mse_b  [0.03802213]  mse_f: 0.4474636912345886   total loss: [0.48548582]\n",
            "mse_b ====== [0.0441638455]\n",
            "It: 38295, Time: 0.02\n",
            "mse_b  [0.04416385]  mse_f: 0.42080098390579224   total loss: [0.46496484]\n",
            "mse_b ====== [0.0312060323]\n",
            "It: 38296, Time: 0.03\n",
            "mse_b  [0.03120603]  mse_f: 0.4267444610595703   total loss: [0.4579505]\n",
            "mse_b ====== [0.031418357]\n",
            "It: 38297, Time: 0.02\n",
            "mse_b  [0.03141836]  mse_f: 0.4409698247909546   total loss: [0.47238818]\n",
            "mse_b ====== [0.0873704553]\n",
            "It: 38298, Time: 0.02\n",
            "mse_b  [0.08737046]  mse_f: 0.4151124954223633   total loss: [0.50248295]\n",
            "mse_b ====== [0.0360508785]\n",
            "It: 38299, Time: 0.02\n",
            "mse_b  [0.03605088]  mse_f: 0.4468018710613251   total loss: [0.48285276]\n",
            "mse_b ====== [0.0516868792]\n",
            "It: 38300, Time: 0.02\n",
            "mse_b  [0.05168688]  mse_f: 0.4172321856021881   total loss: [0.46891907]\n",
            "mse_b ====== [0.0293664169]\n",
            "It: 38301, Time: 0.02\n",
            "mse_b  [0.02936642]  mse_f: 0.42775681614875793   total loss: [0.45712322]\n",
            "mse_b ====== [0.0291873794]\n",
            "It: 38302, Time: 0.02\n",
            "mse_b  [0.02918738]  mse_f: 0.43636131286621094   total loss: [0.4655487]\n",
            "mse_b ====== [0.0770552307]\n",
            "It: 38303, Time: 0.03\n",
            "mse_b  [0.07705523]  mse_f: 0.41605114936828613   total loss: [0.49310637]\n",
            "mse_b ====== [0.0411093198]\n",
            "It: 38304, Time: 0.02\n",
            "mse_b  [0.04110932]  mse_f: 0.4489559531211853   total loss: [0.49006528]\n",
            "mse_b ====== [0.07934466]\n",
            "It: 38305, Time: 0.02\n",
            "mse_b  [0.07934466]  mse_f: 0.41587793827056885   total loss: [0.4952226]\n",
            "mse_b ====== [0.029299926]\n",
            "It: 38306, Time: 0.03\n",
            "mse_b  [0.02929993]  mse_f: 0.43706202507019043   total loss: [0.46636194]\n",
            "mse_b ====== [0.028568225]\n",
            "It: 38307, Time: 0.02\n",
            "mse_b  [0.02856822]  mse_f: 0.4293903708457947   total loss: [0.4579586]\n",
            "mse_b ====== [0.0582897626]\n",
            "It: 38308, Time: 0.02\n",
            "mse_b  [0.05828976]  mse_f: 0.41724368929862976   total loss: [0.47553346]\n",
            "mse_b ====== [0.0394152068]\n",
            "It: 38309, Time: 0.02\n",
            "mse_b  [0.03941521]  mse_f: 0.4515452980995178   total loss: [0.4909605]\n",
            "mse_b ====== [0.09154284]\n",
            "It: 38310, Time: 0.02\n",
            "mse_b  [0.09154284]  mse_f: 0.41938722133636475   total loss: [0.51093006]\n",
            "mse_b ====== [0.0335271955]\n",
            "It: 38311, Time: 0.02\n",
            "mse_b  [0.0335272]  mse_f: 0.4416274130344391   total loss: [0.4751546]\n",
            "mse_b ====== [0.0373192]\n",
            "It: 38312, Time: 0.02\n",
            "mse_b  [0.0373192]  mse_f: 0.42105594277381897   total loss: [0.45837516]\n",
            "mse_b ====== [0.0391471]\n",
            "It: 38313, Time: 0.02\n",
            "mse_b  [0.0391471]  mse_f: 0.4225733280181885   total loss: [0.46172044]\n",
            "mse_b ====== [0.0338184573]\n",
            "It: 38314, Time: 0.02\n",
            "mse_b  [0.03381846]  mse_f: 0.44516292214393616   total loss: [0.47898138]\n",
            "mse_b ====== [0.0950150564]\n",
            "It: 38315, Time: 0.02\n",
            "mse_b  [0.09501506]  mse_f: 0.4134368896484375   total loss: [0.50845194]\n",
            "mse_b ====== [0.0358233899]\n",
            "It: 38316, Time: 0.02\n",
            "mse_b  [0.03582339]  mse_f: 0.44445204734802246   total loss: [0.48027545]\n",
            "mse_b ====== [0.0432932936]\n",
            "It: 38317, Time: 0.02\n",
            "mse_b  [0.04329329]  mse_f: 0.4199816584587097   total loss: [0.46327496]\n",
            "mse_b ====== [0.0323235393]\n",
            "It: 38318, Time: 0.02\n",
            "mse_b  [0.03232354]  mse_f: 0.42390912771224976   total loss: [0.45623267]\n",
            "mse_b ====== [0.0300977379]\n",
            "It: 38319, Time: 0.02\n",
            "mse_b  [0.03009774]  mse_f: 0.4382896423339844   total loss: [0.46838737]\n",
            "mse_b ====== [0.0812011212]\n",
            "It: 38320, Time: 0.02\n",
            "mse_b  [0.08120112]  mse_f: 0.4144427180290222   total loss: [0.49564385]\n",
            "mse_b ====== [0.035824053]\n",
            "It: 38321, Time: 0.02\n",
            "mse_b  [0.03582405]  mse_f: 0.44441014528274536   total loss: [0.4802342]\n",
            "mse_b ====== [0.0453214385]\n",
            "It: 38322, Time: 0.02\n",
            "mse_b  [0.04532144]  mse_f: 0.42002779245376587   total loss: [0.46534923]\n",
            "mse_b ====== [0.0299478602]\n",
            "It: 38323, Time: 0.03\n",
            "mse_b  [0.02994786]  mse_f: 0.42540937662124634   total loss: [0.45535722]\n",
            "mse_b ====== [0.0291896146]\n",
            "It: 38324, Time: 0.02\n",
            "mse_b  [0.02918961]  mse_f: 0.4351896047592163   total loss: [0.46437922]\n",
            "mse_b ====== [0.0694285184]\n",
            "It: 38325, Time: 0.03\n",
            "mse_b  [0.06942852]  mse_f: 0.4186933934688568   total loss: [0.48812193]\n",
            "mse_b ====== [0.0383868814]\n",
            "It: 38326, Time: 0.03\n",
            "mse_b  [0.03838688]  mse_f: 0.44825202226638794   total loss: [0.4866389]\n",
            "mse_b ====== [0.0771845207]\n",
            "It: 38327, Time: 0.02\n",
            "mse_b  [0.07718452]  mse_f: 0.41262900829315186   total loss: [0.48981354]\n",
            "mse_b ====== [0.0286262501]\n",
            "It: 38328, Time: 0.02\n",
            "mse_b  [0.02862625]  mse_f: 0.4355829656124115   total loss: [0.46420923]\n",
            "mse_b ====== [0.0308366511]\n",
            "It: 38329, Time: 0.03\n",
            "mse_b  [0.03083665]  mse_f: 0.42538219690322876   total loss: [0.45621884]\n",
            "mse_b ====== [0.0511111394]\n",
            "It: 38330, Time: 0.02\n",
            "mse_b  [0.05111114]  mse_f: 0.41670656204223633   total loss: [0.4678177]\n",
            "mse_b ====== [0.0375592262]\n",
            "It: 38331, Time: 0.02\n",
            "mse_b  [0.03755923]  mse_f: 0.44587963819503784   total loss: [0.48343885]\n",
            "mse_b ====== [0.0959312543]\n",
            "It: 38332, Time: 0.02\n",
            "mse_b  [0.09593125]  mse_f: 0.4153856635093689   total loss: [0.5113169]\n",
            "mse_b ====== [0.0335534625]\n",
            "It: 38333, Time: 0.02\n",
            "mse_b  [0.03355346]  mse_f: 0.44259971380233765   total loss: [0.47615317]\n",
            "mse_b ====== [0.0354618318]\n",
            "It: 38334, Time: 0.02\n",
            "mse_b  [0.03546183]  mse_f: 0.4229465425014496   total loss: [0.4584084]\n",
            "mse_b ====== [0.0411398262]\n",
            "It: 38335, Time: 0.02\n",
            "mse_b  [0.04113983]  mse_f: 0.4191007614135742   total loss: [0.4602406]\n",
            "mse_b ====== [0.0322623514]\n",
            "It: 38336, Time: 0.02\n",
            "mse_b  [0.03226235]  mse_f: 0.44342514872550964   total loss: [0.4756875]\n",
            "mse_b ====== [0.0788372904]\n",
            "It: 38337, Time: 0.02\n",
            "mse_b  [0.07883729]  mse_f: 0.4181061387062073   total loss: [0.4969434]\n",
            "mse_b ====== [0.0342062712]\n",
            "It: 38338, Time: 0.02\n",
            "mse_b  [0.03420627]  mse_f: 0.442118763923645   total loss: [0.47632504]\n",
            "mse_b ====== [0.0371964835]\n",
            "It: 38339, Time: 0.02\n",
            "mse_b  [0.03719648]  mse_f: 0.4221826195716858   total loss: [0.4593791]\n",
            "mse_b ====== [0.0333579332]\n",
            "It: 38340, Time: 0.02\n",
            "mse_b  [0.03335793]  mse_f: 0.4230087995529175   total loss: [0.45636672]\n",
            "mse_b ====== [0.0300468281]\n",
            "It: 38341, Time: 0.03\n",
            "mse_b  [0.03004683]  mse_f: 0.4400356411933899   total loss: [0.47008246]\n",
            "mse_b ====== [0.0753158778]\n",
            "It: 38342, Time: 0.02\n",
            "mse_b  [0.07531588]  mse_f: 0.4161533713340759   total loss: [0.49146926]\n",
            "mse_b ====== [0.0343803391]\n",
            "It: 38343, Time: 0.03\n",
            "mse_b  [0.03438034]  mse_f: 0.4437684118747711   total loss: [0.47814876]\n",
            "mse_b ====== [0.0513243675]\n",
            "It: 38344, Time: 0.03\n",
            "mse_b  [0.05132437]  mse_f: 0.4146161377429962   total loss: [0.4659405]\n",
            "mse_b ====== [0.0286531858]\n",
            "It: 38345, Time: 0.03\n",
            "mse_b  [0.02865319]  mse_f: 0.42642977833747864   total loss: [0.45508295]\n",
            "mse_b ====== [0.0288393386]\n",
            "It: 38346, Time: 0.03\n",
            "mse_b  [0.02883934]  mse_f: 0.43374893069267273   total loss: [0.46258828]\n",
            "mse_b ====== [0.0717066899]\n",
            "It: 38347, Time: 0.02\n",
            "mse_b  [0.07170669]  mse_f: 0.41601258516311646   total loss: [0.48771927]\n",
            "mse_b ====== [0.0385717079]\n",
            "It: 38348, Time: 0.02\n",
            "mse_b  [0.03857171]  mse_f: 0.4448559880256653   total loss: [0.4834277]\n",
            "mse_b ====== [0.0709971711]\n",
            "It: 38349, Time: 0.02\n",
            "mse_b  [0.07099717]  mse_f: 0.4126284122467041   total loss: [0.4836256]\n",
            "mse_b ====== [0.0284948424]\n",
            "It: 38350, Time: 0.02\n",
            "mse_b  [0.02849484]  mse_f: 0.43250954151153564   total loss: [0.46100438]\n",
            "mse_b ====== [0.0278022978]\n",
            "It: 38351, Time: 0.03\n",
            "mse_b  [0.0278023]  mse_f: 0.43287163972854614   total loss: [0.46067393]\n",
            "mse_b ====== [0.0684272945]\n",
            "It: 38352, Time: 0.03\n",
            "mse_b  [0.06842729]  mse_f: 0.41202688217163086   total loss: [0.48045418]\n",
            "mse_b ====== [0.0358258113]\n",
            "It: 38353, Time: 0.03\n",
            "mse_b  [0.03582581]  mse_f: 0.44641250371932983   total loss: [0.48223832]\n",
            "mse_b ====== [0.060377039]\n",
            "It: 38354, Time: 0.03\n",
            "mse_b  [0.06037704]  mse_f: 0.42400699853897095   total loss: [0.48438403]\n",
            "mse_b ====== [0.0296501219]\n",
            "It: 38355, Time: 0.03\n",
            "mse_b  [0.02965012]  mse_f: 0.43478578329086304   total loss: [0.4644359]\n",
            "mse_b ====== [0.031631507]\n",
            "It: 38356, Time: 0.02\n",
            "mse_b  [0.03163151]  mse_f: 0.4221540093421936   total loss: [0.4537855]\n",
            "mse_b ====== [0.0390322283]\n",
            "It: 38357, Time: 0.02\n",
            "mse_b  [0.03903223]  mse_f: 0.4227358400821686   total loss: [0.46176806]\n",
            "mse_b ====== [0.0312628224]\n",
            "It: 38358, Time: 0.03\n",
            "mse_b  [0.03126282]  mse_f: 0.44394320249557495   total loss: [0.47520602]\n",
            "mse_b ====== [0.0720398352]\n",
            "It: 38359, Time: 0.03\n",
            "mse_b  [0.07203984]  mse_f: 0.4136570990085602   total loss: [0.48569694]\n",
            "mse_b ====== [0.03252827]\n",
            "It: 38360, Time: 0.03\n",
            "mse_b  [0.03252827]  mse_f: 0.43763118982315063   total loss: [0.47015947]\n",
            "mse_b ====== [0.0456606559]\n",
            "It: 38361, Time: 0.02\n",
            "mse_b  [0.04566066]  mse_f: 0.4163548946380615   total loss: [0.46201554]\n",
            "mse_b ====== [0.0288611427]\n",
            "It: 38362, Time: 0.02\n",
            "mse_b  [0.02886114]  mse_f: 0.42557063698768616   total loss: [0.45443177]\n",
            "mse_b ====== [0.0276109762]\n",
            "It: 38363, Time: 0.02\n",
            "mse_b  [0.02761098]  mse_f: 0.43031045794487   total loss: [0.45792145]\n",
            "mse_b ====== [0.0623668134]\n",
            "It: 38364, Time: 0.02\n",
            "mse_b  [0.06236681]  mse_f: 0.4116586446762085   total loss: [0.47402546]\n",
            "mse_b ====== [0.0331242904]\n",
            "It: 38365, Time: 0.02\n",
            "mse_b  [0.03312429]  mse_f: 0.4403235912322998   total loss: [0.4734479]\n",
            "mse_b ====== [0.0591683574]\n",
            "It: 38366, Time: 0.02\n",
            "mse_b  [0.05916836]  mse_f: 0.4141066074371338   total loss: [0.47327498]\n",
            "mse_b ====== [0.0282735]\n",
            "It: 38367, Time: 0.03\n",
            "mse_b  [0.0282735]  mse_f: 0.4326750636100769   total loss: [0.46094856]\n",
            "mse_b ====== [0.0282378029]\n",
            "It: 38368, Time: 0.02\n",
            "mse_b  [0.0282378]  mse_f: 0.427107572555542   total loss: [0.45534536]\n",
            "mse_b ====== [0.0432098284]\n",
            "It: 38369, Time: 0.02\n",
            "mse_b  [0.04320983]  mse_f: 0.4175129532814026   total loss: [0.46072277]\n",
            "mse_b ====== [0.0323080719]\n",
            "It: 38370, Time: 0.02\n",
            "mse_b  [0.03230807]  mse_f: 0.44040313363075256   total loss: [0.4727112]\n",
            "mse_b ====== [0.0685499236]\n",
            "It: 38371, Time: 0.02\n",
            "mse_b  [0.06854992]  mse_f: 0.42091459035873413   total loss: [0.48946452]\n",
            "mse_b ====== [0.0354603566]\n",
            "It: 38372, Time: 0.02\n",
            "mse_b  [0.03546036]  mse_f: 0.4434199333190918   total loss: [0.4788803]\n",
            "mse_b ====== [0.0592174828]\n",
            "It: 38373, Time: 0.02\n",
            "mse_b  [0.05921748]  mse_f: 0.41237956285476685   total loss: [0.47159705]\n",
            "mse_b ====== [0.0272067152]\n",
            "It: 38374, Time: 0.02\n",
            "mse_b  [0.02720672]  mse_f: 0.43009135127067566   total loss: [0.45729807]\n",
            "mse_b ====== [0.0282785]\n",
            "It: 38375, Time: 0.03\n",
            "mse_b  [0.0282785]  mse_f: 0.42812275886535645   total loss: [0.45640126]\n",
            "mse_b ====== [0.0556567088]\n",
            "It: 38376, Time: 0.02\n",
            "mse_b  [0.05565671]  mse_f: 0.4149242639541626   total loss: [0.47058097]\n",
            "mse_b ====== [0.0392759442]\n",
            "It: 38377, Time: 0.02\n",
            "mse_b  [0.03927594]  mse_f: 0.44209176301956177   total loss: [0.4813677]\n",
            "mse_b ====== [0.0926961601]\n",
            "It: 38378, Time: 0.02\n",
            "mse_b  [0.09269616]  mse_f: 0.41057121753692627   total loss: [0.5032674]\n",
            "mse_b ====== [0.032288447]\n",
            "It: 38379, Time: 0.02\n",
            "mse_b  [0.03228845]  mse_f: 0.4372634291648865   total loss: [0.46955186]\n",
            "mse_b ====== [0.0336309895]\n",
            "It: 38380, Time: 0.02\n",
            "mse_b  [0.03363099]  mse_f: 0.4234788417816162   total loss: [0.45710984]\n",
            "mse_b ====== [0.0449572913]\n",
            "It: 38381, Time: 0.02\n",
            "mse_b  [0.04495729]  mse_f: 0.41542667150497437   total loss: [0.46038395]\n",
            "mse_b ====== [0.0326707624]\n",
            "It: 38382, Time: 0.03\n",
            "mse_b  [0.03267076]  mse_f: 0.43707340955734253   total loss: [0.46974418]\n",
            "mse_b ====== [0.0706303641]\n",
            "It: 38383, Time: 0.03\n",
            "mse_b  [0.07063036]  mse_f: 0.417318195104599   total loss: [0.48794857]\n",
            "mse_b ====== [0.0324690789]\n",
            "It: 38384, Time: 0.02\n",
            "mse_b  [0.03246908]  mse_f: 0.44543421268463135   total loss: [0.4779033]\n",
            "mse_b ====== [0.040374022]\n",
            "It: 38385, Time: 0.02\n",
            "mse_b  [0.04037402]  mse_f: 0.42144325375556946   total loss: [0.46181726]\n",
            "mse_b ====== [0.0294612925]\n",
            "It: 38386, Time: 0.02\n",
            "mse_b  [0.02946129]  mse_f: 0.4225781559944153   total loss: [0.45203945]\n",
            "mse_b ====== [0.0313417464]\n",
            "It: 38387, Time: 0.02\n",
            "mse_b  [0.03134175]  mse_f: 0.43070703744888306   total loss: [0.46204877]\n",
            "mse_b ====== [0.0554401502]\n",
            "It: 38388, Time: 0.02\n",
            "mse_b  [0.05544015]  mse_f: 0.42143678665161133   total loss: [0.47687694]\n",
            "mse_b ====== [0.0337323733]\n",
            "It: 38389, Time: 0.02\n",
            "mse_b  [0.03373237]  mse_f: 0.44268956780433655   total loss: [0.47642195]\n",
            "mse_b ====== [0.0715756193]\n",
            "It: 38390, Time: 0.02\n",
            "mse_b  [0.07157562]  mse_f: 0.40914106369018555   total loss: [0.48071668]\n",
            "mse_b ====== [0.0290840976]\n",
            "It: 38391, Time: 0.02\n",
            "mse_b  [0.0290841]  mse_f: 0.433639258146286   total loss: [0.46272334]\n",
            "mse_b ====== [0.0325657129]\n",
            "It: 38392, Time: 0.02\n",
            "mse_b  [0.03256571]  mse_f: 0.42044195532798767   total loss: [0.45300767]\n",
            "mse_b ====== [0.0404484272]\n",
            "It: 38393, Time: 0.02\n",
            "mse_b  [0.04044843]  mse_f: 0.4157826602458954   total loss: [0.4562311]\n",
            "mse_b ====== [0.0320913047]\n",
            "It: 38394, Time: 0.03\n",
            "mse_b  [0.0320913]  mse_f: 0.43652454018592834   total loss: [0.46861583]\n",
            "mse_b ====== [0.0777465403]\n",
            "It: 38395, Time: 0.02\n",
            "mse_b  [0.07774654]  mse_f: 0.408742755651474   total loss: [0.4864893]\n",
            "mse_b ====== [0.0315711237]\n",
            "It: 38396, Time: 0.02\n",
            "mse_b  [0.03157112]  mse_f: 0.4352646470069885   total loss: [0.46683577]\n",
            "mse_b ====== [0.0341609642]\n",
            "It: 38397, Time: 0.03\n",
            "mse_b  [0.03416096]  mse_f: 0.4237062931060791   total loss: [0.45786726]\n",
            "mse_b ====== [0.0375395641]\n",
            "It: 38398, Time: 0.02\n",
            "mse_b  [0.03753956]  mse_f: 0.4191829562187195   total loss: [0.45672253]\n",
            "mse_b ====== [0.0293807089]\n",
            "It: 38399, Time: 0.02\n",
            "mse_b  [0.02938071]  mse_f: 0.4324563443660736   total loss: [0.46183705]\n",
            "mse_b ====== [0.0607090257]\n",
            "It: 38400, Time: 0.02\n",
            "mse_b  [0.06070903]  mse_f: 0.4192638099193573   total loss: [0.47997284]\n",
            "mse_b ====== [0.0356039554]\n",
            "It: 38401, Time: 0.02\n",
            "mse_b  [0.03560396]  mse_f: 0.4476286768913269   total loss: [0.48323262]\n",
            "mse_b ====== [0.0623534024]\n",
            "It: 38402, Time: 0.02\n",
            "mse_b  [0.0623534]  mse_f: 0.417580246925354   total loss: [0.47993365]\n",
            "mse_b ====== [0.0294202026]\n",
            "It: 38403, Time: 0.02\n",
            "mse_b  [0.0294202]  mse_f: 0.4324170649051666   total loss: [0.46183726]\n",
            "mse_b ====== [0.0393605679]\n",
            "It: 38404, Time: 0.02\n",
            "mse_b  [0.03936057]  mse_f: 0.416744589805603   total loss: [0.45610517]\n",
            "mse_b ====== [0.0325521864]\n",
            "It: 38405, Time: 0.02\n",
            "mse_b  [0.03255219]  mse_f: 0.42376458644866943   total loss: [0.45631677]\n",
            "mse_b ====== [0.0294626392]\n",
            "It: 38406, Time: 0.02\n",
            "mse_b  [0.02946264]  mse_f: 0.4348984658718109   total loss: [0.4643611]\n",
            "mse_b ====== [0.0768509582]\n",
            "It: 38407, Time: 0.02\n",
            "mse_b  [0.07685096]  mse_f: 0.40966618061065674   total loss: [0.48651713]\n",
            "mse_b ====== [0.0371265076]\n",
            "It: 38408, Time: 0.02\n",
            "mse_b  [0.03712651]  mse_f: 0.4392696022987366   total loss: [0.4763961]\n",
            "mse_b ====== [0.0622884743]\n",
            "It: 38409, Time: 0.02\n",
            "mse_b  [0.06228847]  mse_f: 0.4091595411300659   total loss: [0.471448]\n",
            "mse_b ====== [0.0294778515]\n",
            "It: 38410, Time: 0.02\n",
            "mse_b  [0.02947785]  mse_f: 0.4271661639213562   total loss: [0.45664403]\n",
            "mse_b ====== [0.0280731246]\n",
            "It: 38411, Time: 0.02\n",
            "mse_b  [0.02807312]  mse_f: 0.43091845512390137   total loss: [0.4589916]\n",
            "mse_b ====== [0.0618285]\n",
            "It: 38412, Time: 0.03\n",
            "mse_b  [0.0618285]  mse_f: 0.4090535044670105   total loss: [0.470882]\n",
            "mse_b ====== [0.0353075415]\n",
            "It: 38413, Time: 0.02\n",
            "mse_b  [0.03530754]  mse_f: 0.43739840388298035   total loss: [0.47270596]\n",
            "mse_b ====== [0.0596322194]\n",
            "It: 38414, Time: 0.03\n",
            "mse_b  [0.05963222]  mse_f: 0.4229236841201782   total loss: [0.4825559]\n",
            "mse_b ====== [0.0311074741]\n",
            "It: 38415, Time: 0.02\n",
            "mse_b  [0.03110747]  mse_f: 0.44076889753341675   total loss: [0.47187638]\n",
            "mse_b ====== [0.0390657261]\n",
            "It: 38416, Time: 0.02\n",
            "mse_b  [0.03906573]  mse_f: 0.41682255268096924   total loss: [0.45588827]\n",
            "mse_b ====== [0.0299435146]\n",
            "It: 38417, Time: 0.02\n",
            "mse_b  [0.02994351]  mse_f: 0.42341530323028564   total loss: [0.45335883]\n",
            "mse_b ====== [0.0325639099]\n",
            "It: 38418, Time: 0.02\n",
            "mse_b  [0.03256391]  mse_f: 0.4302724003791809   total loss: [0.46283633]\n",
            "mse_b ====== [0.051430162]\n",
            "It: 38419, Time: 0.02\n",
            "mse_b  [0.05143016]  mse_f: 0.41907936334609985   total loss: [0.47050953]\n",
            "mse_b ====== [0.0361281037]\n",
            "It: 38420, Time: 0.02\n",
            "mse_b  [0.0361281]  mse_f: 0.4383872151374817   total loss: [0.47451532]\n",
            "mse_b ====== [0.0902214497]\n",
            "It: 38421, Time: 0.03\n",
            "mse_b  [0.09022145]  mse_f: 0.40873879194259644   total loss: [0.49896026]\n",
            "mse_b ====== [0.0358267166]\n",
            "It: 38422, Time: 0.02\n",
            "mse_b  [0.03582672]  mse_f: 0.43835127353668213   total loss: [0.474178]\n",
            "mse_b ====== [0.048318848]\n",
            "It: 38423, Time: 0.02\n",
            "mse_b  [0.04831885]  mse_f: 0.41203853487968445   total loss: [0.46035737]\n",
            "mse_b ====== [0.0309381913]\n",
            "It: 38424, Time: 0.02\n",
            "mse_b  [0.03093819]  mse_f: 0.4234157204627991   total loss: [0.4543539]\n",
            "mse_b ====== [0.027815491]\n",
            "It: 38425, Time: 0.02\n",
            "mse_b  [0.02781549]  mse_f: 0.4328695237636566   total loss: [0.460685]\n",
            "mse_b ====== [0.0615838394]\n",
            "It: 38426, Time: 0.02\n",
            "mse_b  [0.06158384]  mse_f: 0.4093116521835327   total loss: [0.4708955]\n",
            "mse_b ====== [0.033147797]\n",
            "It: 38427, Time: 0.02\n",
            "mse_b  [0.0331478]  mse_f: 0.43686044216156006   total loss: [0.47000825]\n",
            "mse_b ====== [0.0526477695]\n",
            "It: 38428, Time: 0.02\n",
            "mse_b  [0.05264777]  mse_f: 0.4249260723590851   total loss: [0.47757384]\n",
            "mse_b ====== [0.0312069636]\n",
            "It: 38429, Time: 0.02\n",
            "mse_b  [0.03120696]  mse_f: 0.43746137619018555   total loss: [0.46866834]\n",
            "mse_b ====== [0.0395164452]\n",
            "It: 38430, Time: 0.03\n",
            "mse_b  [0.03951645]  mse_f: 0.41524502635002136   total loss: [0.45476148]\n",
            "mse_b ====== [0.0280479155]\n",
            "It: 38431, Time: 0.02\n",
            "mse_b  [0.02804792]  mse_f: 0.4254584014415741   total loss: [0.45350632]\n",
            "mse_b ====== [0.0323562399]\n",
            "It: 38432, Time: 0.02\n",
            "mse_b  [0.03235624]  mse_f: 0.427621066570282   total loss: [0.4599773]\n",
            "mse_b ====== [0.0422871672]\n",
            "It: 38433, Time: 0.02\n",
            "mse_b  [0.04228717]  mse_f: 0.4185009300708771   total loss: [0.4607881]\n",
            "mse_b ====== [0.0328639112]\n",
            "It: 38434, Time: 0.03\n",
            "mse_b  [0.03286391]  mse_f: 0.43280184268951416   total loss: [0.46566576]\n",
            "mse_b ====== [0.0823233873]\n",
            "It: 38435, Time: 0.02\n",
            "mse_b  [0.08232339]  mse_f: 0.4081515669822693   total loss: [0.49047494]\n",
            "mse_b ====== [0.0384293571]\n",
            "It: 38436, Time: 0.02\n",
            "mse_b  [0.03842936]  mse_f: 0.43850797414779663   total loss: [0.47693732]\n",
            "mse_b ====== [0.0657909587]\n",
            "It: 38437, Time: 0.03\n",
            "mse_b  [0.06579096]  mse_f: 0.40801000595092773   total loss: [0.47380096]\n",
            "mse_b ====== [0.0306299534]\n",
            "It: 38438, Time: 0.02\n",
            "mse_b  [0.03062995]  mse_f: 0.4317171573638916   total loss: [0.46234712]\n",
            "mse_b ====== [0.0303899348]\n",
            "It: 38439, Time: 0.02\n",
            "mse_b  [0.03038993]  mse_f: 0.4308440685272217   total loss: [0.461234]\n",
            "mse_b ====== [0.0442587659]\n",
            "It: 38440, Time: 0.02\n",
            "mse_b  [0.04425877]  mse_f: 0.41363614797592163   total loss: [0.45789492]\n",
            "mse_b ====== [0.0294162501]\n",
            "It: 38441, Time: 0.02\n",
            "mse_b  [0.02941625]  mse_f: 0.4309326410293579   total loss: [0.4603489]\n",
            "mse_b ====== [0.0492795482]\n",
            "It: 38442, Time: 0.02\n",
            "mse_b  [0.04927955]  mse_f: 0.42723414301872253   total loss: [0.47651368]\n",
            "mse_b ====== [0.034986306]\n",
            "It: 38443, Time: 0.02\n",
            "mse_b  [0.03498631]  mse_f: 0.443386435508728   total loss: [0.47837275]\n",
            "mse_b ====== [0.0500833653]\n",
            "It: 38444, Time: 0.02\n",
            "mse_b  [0.05008337]  mse_f: 0.41553372144699097   total loss: [0.4656171]\n",
            "mse_b ====== [0.0283528343]\n",
            "It: 38445, Time: 0.03\n",
            "mse_b  [0.02835283]  mse_f: 0.42983365058898926   total loss: [0.45818648]\n",
            "mse_b ====== [0.0467858873]\n",
            "It: 38446, Time: 0.02\n",
            "mse_b  [0.04678589]  mse_f: 0.4163858890533447   total loss: [0.46317178]\n",
            "mse_b ====== [0.0290709753]\n",
            "It: 38447, Time: 0.03\n",
            "mse_b  [0.02907098]  mse_f: 0.42702239751815796   total loss: [0.45609337]\n",
            "mse_b ====== [0.0281786695]\n",
            "It: 38448, Time: 0.02\n",
            "mse_b  [0.02817867]  mse_f: 0.4241071045398712   total loss: [0.45228577]\n",
            "mse_b ====== [0.0575535]\n",
            "It: 38449, Time: 0.02\n",
            "mse_b  [0.0575535]  mse_f: 0.4109511375427246   total loss: [0.46850464]\n",
            "mse_b ====== [0.0387409627]\n",
            "It: 38450, Time: 0.02\n",
            "mse_b  [0.03874096]  mse_f: 0.4395647943019867   total loss: [0.47830576]\n",
            "mse_b ====== [0.0944536105]\n",
            "It: 38451, Time: 0.02\n",
            "mse_b  [0.09445361]  mse_f: 0.4061804711818695   total loss: [0.5006341]\n",
            "mse_b ====== [0.0414190777]\n",
            "It: 38452, Time: 0.02\n",
            "mse_b  [0.04141908]  mse_f: 0.44358664751052856   total loss: [0.48500574]\n",
            "mse_b ====== [0.0625843778]\n",
            "It: 38453, Time: 0.02\n",
            "mse_b  [0.06258438]  mse_f: 0.42963382601737976   total loss: [0.4922182]\n",
            "mse_b ====== [0.0327392742]\n",
            "It: 38454, Time: 0.02\n",
            "mse_b  [0.03273927]  mse_f: 0.4421088993549347   total loss: [0.47484818]\n",
            "mse_b ====== [0.0318516493]\n",
            "It: 38455, Time: 0.03\n",
            "mse_b  [0.03185165]  mse_f: 0.42031535506248474   total loss: [0.452167]\n",
            "mse_b ====== [0.0381661467]\n",
            "It: 38456, Time: 0.02\n",
            "mse_b  [0.03816615]  mse_f: 0.42122822999954224   total loss: [0.45939437]\n",
            "mse_b ====== [0.0365568]\n",
            "It: 38457, Time: 0.03\n",
            "mse_b  [0.0365568]  mse_f: 0.4501066505908966   total loss: [0.48666346]\n",
            "mse_b ====== [0.0754651]\n",
            "It: 38458, Time: 0.02\n",
            "mse_b  [0.0754651]  mse_f: 0.42786407470703125   total loss: [0.50332916]\n",
            "mse_b ====== [0.044919543]\n",
            "It: 38459, Time: 0.02\n",
            "mse_b  [0.04491954]  mse_f: 0.44476571679115295   total loss: [0.48968527]\n",
            "mse_b ====== [0.107076868]\n",
            "It: 38460, Time: 0.02\n",
            "mse_b  [0.10707687]  mse_f: 0.4009513854980469   total loss: [0.50802827]\n",
            "mse_b ====== [0.0292385]\n",
            "It: 38461, Time: 0.02\n",
            "mse_b  [0.0292385]  mse_f: 0.4358484745025635   total loss: [0.46508697]\n",
            "mse_b ====== [0.0285453051]\n",
            "It: 38462, Time: 0.02\n",
            "mse_b  [0.02854531]  mse_f: 0.4275318384170532   total loss: [0.45607716]\n",
            "mse_b ====== [0.0880989358]\n",
            "It: 38463, Time: 0.02\n",
            "mse_b  [0.08809894]  mse_f: 0.4044976234436035   total loss: [0.49259657]\n",
            "mse_b ====== [0.0509150699]\n",
            "It: 38464, Time: 0.02\n",
            "mse_b  [0.05091507]  mse_f: 0.44432511925697327   total loss: [0.49524018]\n",
            "mse_b ====== [0.105036125]\n",
            "It: 38465, Time: 0.02\n",
            "mse_b  [0.10503612]  mse_f: 0.41096508502960205   total loss: [0.5160012]\n",
            "mse_b ====== [0.0329377688]\n",
            "It: 38466, Time: 0.02\n",
            "mse_b  [0.03293777]  mse_f: 0.4345256984233856   total loss: [0.46746346]\n",
            "mse_b ====== [0.0301293675]\n",
            "It: 38467, Time: 0.02\n",
            "mse_b  [0.03012937]  mse_f: 0.4357329308986664   total loss: [0.4658623]\n",
            "mse_b ====== [0.0828163251]\n",
            "It: 38468, Time: 0.02\n",
            "mse_b  [0.08281633]  mse_f: 0.40456444025039673   total loss: [0.48738077]\n",
            "mse_b ====== [0.035202872]\n",
            "It: 38469, Time: 0.03\n",
            "mse_b  [0.03520287]  mse_f: 0.43977510929107666   total loss: [0.47497797]\n",
            "mse_b ====== [0.0382327624]\n",
            "It: 38470, Time: 0.03\n",
            "mse_b  [0.03823276]  mse_f: 0.44421184062957764   total loss: [0.4824446]\n",
            "mse_b ====== [0.0432965457]\n",
            "It: 38471, Time: 0.02\n",
            "mse_b  [0.04329655]  mse_f: 0.43327540159225464   total loss: [0.47657195]\n",
            "mse_b ====== [0.0285585038]\n",
            "It: 38472, Time: 0.02\n",
            "mse_b  [0.0285585]  mse_f: 0.4266423285007477   total loss: [0.45520082]\n",
            "mse_b ====== [0.0414212346]\n",
            "It: 38473, Time: 0.02\n",
            "mse_b  [0.04142123]  mse_f: 0.43190860748291016   total loss: [0.47332984]\n",
            "mse_b ====== [0.0428329408]\n",
            "It: 38474, Time: 0.02\n",
            "mse_b  [0.04283294]  mse_f: 0.45921945571899414   total loss: [0.5020524]\n",
            "mse_b ====== [0.0510420352]\n",
            "It: 38475, Time: 0.02\n",
            "mse_b  [0.05104204]  mse_f: 0.43527621030807495   total loss: [0.48631823]\n",
            "mse_b ====== [0.0308661982]\n",
            "It: 38476, Time: 0.02\n",
            "mse_b  [0.0308662]  mse_f: 0.4343510568141937   total loss: [0.46521726]\n",
            "mse_b ====== [0.0690241233]\n",
            "It: 38477, Time: 0.03\n",
            "mse_b  [0.06902412]  mse_f: 0.4135113060474396   total loss: [0.48253542]\n",
            "mse_b ====== [0.0306993797]\n",
            "It: 38478, Time: 0.03\n",
            "mse_b  [0.03069938]  mse_f: 0.44446223974227905   total loss: [0.4751616]\n",
            "mse_b ====== [0.02989305]\n",
            "It: 38479, Time: 0.02\n",
            "mse_b  [0.02989305]  mse_f: 0.4245682954788208   total loss: [0.45446134]\n",
            "mse_b ====== [0.0522178933]\n",
            "It: 38480, Time: 0.02\n",
            "mse_b  [0.05221789]  mse_f: 0.411965936422348   total loss: [0.46418384]\n",
            "mse_b ====== [0.0409904048]\n",
            "It: 38481, Time: 0.02\n",
            "mse_b  [0.0409904]  mse_f: 0.4453069865703583   total loss: [0.4862974]\n",
            "mse_b ====== [0.0929717347]\n",
            "It: 38482, Time: 0.02\n",
            "mse_b  [0.09297173]  mse_f: 0.40797436237335205   total loss: [0.5009461]\n",
            "mse_b ====== [0.031333331]\n",
            "It: 38483, Time: 0.03\n",
            "mse_b  [0.03133333]  mse_f: 0.4304976165294647   total loss: [0.46183094]\n",
            "mse_b ====== [0.0314294249]\n",
            "It: 38484, Time: 0.03\n",
            "mse_b  [0.03142942]  mse_f: 0.44469571113586426   total loss: [0.47612512]\n",
            "mse_b ====== [0.0680661947]\n",
            "It: 38485, Time: 0.02\n",
            "mse_b  [0.06806619]  mse_f: 0.42530128359794617   total loss: [0.4933675]\n",
            "mse_b ====== [0.0315922499]\n",
            "It: 38486, Time: 0.02\n",
            "mse_b  [0.03159225]  mse_f: 0.4313778877258301   total loss: [0.46297014]\n",
            "mse_b ====== [0.0394508578]\n",
            "It: 38487, Time: 0.02\n",
            "mse_b  [0.03945086]  mse_f: 0.4288167357444763   total loss: [0.4682676]\n",
            "mse_b ====== [0.0375759266]\n",
            "It: 38488, Time: 0.02\n",
            "mse_b  [0.03757593]  mse_f: 0.45685237646102905   total loss: [0.4944283]\n",
            "mse_b ====== [0.0338975564]\n",
            "It: 38489, Time: 0.02\n",
            "mse_b  [0.03389756]  mse_f: 0.4418725371360779   total loss: [0.4757701]\n",
            "mse_b ====== [0.0316623673]\n",
            "It: 38490, Time: 0.02\n",
            "mse_b  [0.03166237]  mse_f: 0.4207984209060669   total loss: [0.4524608]\n",
            "mse_b ====== [0.0485994108]\n",
            "It: 38491, Time: 0.03\n",
            "mse_b  [0.04859941]  mse_f: 0.425352543592453   total loss: [0.47395197]\n",
            "mse_b ====== [0.0360816792]\n",
            "It: 38492, Time: 0.02\n",
            "mse_b  [0.03608168]  mse_f: 0.45017218589782715   total loss: [0.48625386]\n",
            "mse_b ====== [0.0320342705]\n",
            "It: 38493, Time: 0.02\n",
            "mse_b  [0.03203427]  mse_f: 0.43277838826179504   total loss: [0.46481267]\n",
            "mse_b ====== [0.0474995524]\n",
            "It: 38494, Time: 0.02\n",
            "mse_b  [0.04749955]  mse_f: 0.4106209874153137   total loss: [0.45812052]\n",
            "mse_b ====== [0.0359199718]\n",
            "It: 38495, Time: 0.03\n",
            "mse_b  [0.03591997]  mse_f: 0.44815605878829956   total loss: [0.48407602]\n",
            "mse_b ====== [0.0782503784]\n",
            "It: 38496, Time: 0.02\n",
            "mse_b  [0.07825038]  mse_f: 0.4185580611228943   total loss: [0.49680844]\n",
            "mse_b ====== [0.0301219895]\n",
            "It: 38497, Time: 0.03\n",
            "mse_b  [0.03012199]  mse_f: 0.4302458167076111   total loss: [0.4603678]\n",
            "mse_b ====== [0.0321110748]\n",
            "It: 38498, Time: 0.02\n",
            "mse_b  [0.03211107]  mse_f: 0.43746525049209595   total loss: [0.46957633]\n",
            "mse_b ====== [0.0533336252]\n",
            "It: 38499, Time: 0.02\n",
            "mse_b  [0.05333363]  mse_f: 0.4396730661392212   total loss: [0.4930067]\n",
            "mse_b ====== [0.0303866565]\n",
            "It: 38500, Time: 0.03\n",
            "mse_b  [0.03038666]  mse_f: 0.4380091428756714   total loss: [0.4683958]\n",
            "mse_b ====== [0.0359513536]\n",
            "It: 38501, Time: 0.02\n",
            "mse_b  [0.03595135]  mse_f: 0.41601449251174927   total loss: [0.45196584]\n",
            "mse_b ====== [0.0355797671]\n",
            "It: 38502, Time: 0.02\n",
            "mse_b  [0.03557977]  mse_f: 0.4433465600013733   total loss: [0.47892633]\n",
            "mse_b ====== [0.0343567729]\n",
            "It: 38503, Time: 0.02\n",
            "mse_b  [0.03435677]  mse_f: 0.45324742794036865   total loss: [0.4876042]\n",
            "mse_b ====== [0.0327550769]\n",
            "It: 38504, Time: 0.02\n",
            "mse_b  [0.03275508]  mse_f: 0.43101125955581665   total loss: [0.46376634]\n",
            "mse_b ====== [0.0555023141]\n",
            "It: 38505, Time: 0.02\n",
            "mse_b  [0.05550231]  mse_f: 0.412036657333374   total loss: [0.46753898]\n",
            "mse_b ====== [0.0396845415]\n",
            "It: 38506, Time: 0.02\n",
            "mse_b  [0.03968454]  mse_f: 0.4520058035850525   total loss: [0.49169034]\n",
            "mse_b ====== [0.0812711567]\n",
            "It: 38507, Time: 0.02\n",
            "mse_b  [0.08127116]  mse_f: 0.4180260896682739   total loss: [0.49929726]\n",
            "mse_b ====== [0.0319626145]\n",
            "It: 38508, Time: 0.02\n",
            "mse_b  [0.03196261]  mse_f: 0.4314612150192261   total loss: [0.46342382]\n",
            "mse_b ====== [0.049015604]\n",
            "It: 38509, Time: 0.02\n",
            "mse_b  [0.0490156]  mse_f: 0.4303800165653229   total loss: [0.47939563]\n",
            "mse_b ====== [0.0372375958]\n",
            "It: 38510, Time: 0.02\n",
            "mse_b  [0.0372376]  mse_f: 0.4675500988960266   total loss: [0.5047877]\n",
            "mse_b ====== [0.0470511802]\n",
            "It: 38511, Time: 0.02\n",
            "mse_b  [0.04705118]  mse_f: 0.4326011538505554   total loss: [0.47965235]\n",
            "mse_b ====== [0.032165952]\n",
            "It: 38512, Time: 0.02\n",
            "mse_b  [0.03216595]  mse_f: 0.4275209903717041   total loss: [0.45968693]\n",
            "mse_b ====== [0.0584220327]\n",
            "It: 38513, Time: 0.02\n",
            "mse_b  [0.05842203]  mse_f: 0.42018741369247437   total loss: [0.47860944]\n",
            "mse_b ====== [0.0328227282]\n",
            "It: 38514, Time: 0.02\n",
            "mse_b  [0.03282273]  mse_f: 0.44575199484825134   total loss: [0.47857472]\n",
            "mse_b ====== [0.0301651768]\n",
            "It: 38515, Time: 0.02\n",
            "mse_b  [0.03016518]  mse_f: 0.430534690618515   total loss: [0.46069986]\n",
            "mse_b ====== [0.0720782354]\n",
            "It: 38516, Time: 0.02\n",
            "mse_b  [0.07207824]  mse_f: 0.40482842922210693   total loss: [0.47690666]\n",
            "mse_b ====== [0.0423056073]\n",
            "It: 38517, Time: 0.02\n",
            "mse_b  [0.04230561]  mse_f: 0.44604599475860596   total loss: [0.4883516]\n",
            "mse_b ====== [0.0915727839]\n",
            "It: 38518, Time: 0.02\n",
            "mse_b  [0.09157278]  mse_f: 0.40366604924201965   total loss: [0.49523884]\n",
            "mse_b ====== [0.0284895264]\n",
            "It: 38519, Time: 0.02\n",
            "mse_b  [0.02848953]  mse_f: 0.42719700932502747   total loss: [0.45568654]\n",
            "mse_b ====== [0.0294040181]\n",
            "It: 38520, Time: 0.02\n",
            "mse_b  [0.02940402]  mse_f: 0.44082897901535034   total loss: [0.470233]\n",
            "mse_b ====== [0.0755693913]\n",
            "It: 38521, Time: 0.02\n",
            "mse_b  [0.07556939]  mse_f: 0.4166331887245178   total loss: [0.49220258]\n",
            "mse_b ====== [0.034767773]\n",
            "It: 38522, Time: 0.02\n",
            "mse_b  [0.03476777]  mse_f: 0.4322015047073364   total loss: [0.46696928]\n",
            "mse_b ====== [0.0551997535]\n",
            "It: 38523, Time: 0.02\n",
            "mse_b  [0.05519975]  mse_f: 0.4254460334777832   total loss: [0.48064578]\n",
            "mse_b ====== [0.0411997922]\n",
            "It: 38524, Time: 0.02\n",
            "mse_b  [0.04119979]  mse_f: 0.4637630879878998   total loss: [0.50496286]\n",
            "mse_b ====== [0.043070782]\n",
            "It: 38525, Time: 0.02\n",
            "mse_b  [0.04307078]  mse_f: 0.4393814206123352   total loss: [0.4824522]\n",
            "mse_b ====== [0.0301500242]\n",
            "It: 38526, Time: 0.02\n",
            "mse_b  [0.03015002]  mse_f: 0.4209701418876648   total loss: [0.45112017]\n",
            "mse_b ====== [0.041865129]\n",
            "It: 38527, Time: 0.02\n",
            "mse_b  [0.04186513]  mse_f: 0.42262712121009827   total loss: [0.46449226]\n",
            "mse_b ====== [0.0386322364]\n",
            "It: 38528, Time: 0.03\n",
            "mse_b  [0.03863224]  mse_f: 0.44689780473709106   total loss: [0.48553005]\n",
            "mse_b ====== [0.040971119]\n",
            "It: 38529, Time: 0.02\n",
            "mse_b  [0.04097112]  mse_f: 0.4430208206176758   total loss: [0.48399195]\n",
            "mse_b ====== [0.08866065]\n",
            "It: 38530, Time: 0.02\n",
            "mse_b  [0.08866065]  mse_f: 0.4059702157974243   total loss: [0.49463087]\n",
            "mse_b ====== [0.0493398905]\n",
            "It: 38531, Time: 0.02\n",
            "mse_b  [0.04933989]  mse_f: 0.44281071424484253   total loss: [0.4921506]\n",
            "mse_b ====== [0.105276413]\n",
            "It: 38532, Time: 0.02\n",
            "mse_b  [0.10527641]  mse_f: 0.4045361578464508   total loss: [0.5098126]\n",
            "mse_b ====== [0.0284204111]\n",
            "It: 38533, Time: 0.02\n",
            "mse_b  [0.02842041]  mse_f: 0.42594772577285767   total loss: [0.45436814]\n",
            "mse_b ====== [0.0293933339]\n",
            "It: 38534, Time: 0.02\n",
            "mse_b  [0.02939333]  mse_f: 0.43665963411331177   total loss: [0.46605298]\n",
            "mse_b ====== [0.108656853]\n",
            "It: 38535, Time: 0.02\n",
            "mse_b  [0.10865685]  mse_f: 0.40956762433052063   total loss: [0.5182245]\n",
            "mse_b ====== [0.0362681821]\n",
            "It: 38536, Time: 0.02\n",
            "mse_b  [0.03626818]  mse_f: 0.43341174721717834   total loss: [0.46967992]\n",
            "mse_b ====== [0.0380711928]\n",
            "It: 38537, Time: 0.03\n",
            "mse_b  [0.03807119]  mse_f: 0.42592060565948486   total loss: [0.4639918]\n",
            "mse_b ====== [0.0431900099]\n",
            "It: 38538, Time: 0.02\n",
            "mse_b  [0.04319001]  mse_f: 0.4402558505535126   total loss: [0.48344585]\n",
            "mse_b ====== [0.0313003846]\n",
            "It: 38539, Time: 0.02\n",
            "mse_b  [0.03130038]  mse_f: 0.4357157051563263   total loss: [0.4670161]\n",
            "mse_b ====== [0.0449925251]\n",
            "It: 38540, Time: 0.02\n",
            "mse_b  [0.04499253]  mse_f: 0.40846291184425354   total loss: [0.45345545]\n",
            "mse_b ====== [0.03961429]\n",
            "It: 38541, Time: 0.02\n",
            "mse_b  [0.03961429]  mse_f: 0.441347599029541   total loss: [0.4809619]\n",
            "mse_b ====== [0.0424938053]\n",
            "It: 38542, Time: 0.02\n",
            "mse_b  [0.04249381]  mse_f: 0.4518895149230957   total loss: [0.49438334]\n",
            "mse_b ====== [0.0338637643]\n",
            "It: 38543, Time: 0.02\n",
            "mse_b  [0.03386376]  mse_f: 0.4333915114402771   total loss: [0.46725526]\n",
            "mse_b ====== [0.0444774702]\n",
            "It: 38544, Time: 0.02\n",
            "mse_b  [0.04447747]  mse_f: 0.4077892303466797   total loss: [0.4522667]\n",
            "mse_b ====== [0.0327341668]\n",
            "It: 38545, Time: 0.02\n",
            "mse_b  [0.03273417]  mse_f: 0.4431731104850769   total loss: [0.47590727]\n",
            "mse_b ====== [0.0511384308]\n",
            "It: 38546, Time: 0.03\n",
            "mse_b  [0.05113843]  mse_f: 0.429196834564209   total loss: [0.48033527]\n",
            "mse_b ====== [0.0309083592]\n",
            "It: 38547, Time: 0.02\n",
            "mse_b  [0.03090836]  mse_f: 0.41839075088500977   total loss: [0.4492991]\n",
            "mse_b ====== [0.0302352775]\n",
            "It: 38548, Time: 0.03\n",
            "mse_b  [0.03023528]  mse_f: 0.4264954924583435   total loss: [0.45673078]\n",
            "mse_b ====== [0.0653164834]\n",
            "It: 38549, Time: 0.02\n",
            "mse_b  [0.06531648]  mse_f: 0.4233931005001068   total loss: [0.48870957]\n",
            "mse_b ====== [0.0301601458]\n",
            "It: 38550, Time: 0.02\n",
            "mse_b  [0.03016015]  mse_f: 0.438431978225708   total loss: [0.46859214]\n",
            "mse_b ====== [0.0395634584]\n",
            "It: 38551, Time: 0.02\n",
            "mse_b  [0.03956346]  mse_f: 0.4088575839996338   total loss: [0.44842103]\n",
            "mse_b ====== [0.03294174]\n",
            "It: 38552, Time: 0.02\n",
            "mse_b  [0.03294174]  mse_f: 0.43727967143058777   total loss: [0.4702214]\n",
            "mse_b ====== [0.0358450711]\n",
            "It: 38553, Time: 0.02\n",
            "mse_b  [0.03584507]  mse_f: 0.4455353319644928   total loss: [0.4813804]\n",
            "mse_b ====== [0.0333677456]\n",
            "It: 38554, Time: 0.02\n",
            "mse_b  [0.03336775]  mse_f: 0.4261154532432556   total loss: [0.4594832]\n",
            "mse_b ====== [0.0447497554]\n",
            "It: 38555, Time: 0.02\n",
            "mse_b  [0.04474976]  mse_f: 0.40801793336868286   total loss: [0.4527677]\n",
            "mse_b ====== [0.0319437385]\n",
            "It: 38556, Time: 0.02\n",
            "mse_b  [0.03194374]  mse_f: 0.442278653383255   total loss: [0.4742224]\n",
            "mse_b ====== [0.0533212312]\n",
            "It: 38557, Time: 0.02\n",
            "mse_b  [0.05332123]  mse_f: 0.42117783427238464   total loss: [0.47449908]\n",
            "mse_b ====== [0.0287319627]\n",
            "It: 38558, Time: 0.03\n",
            "mse_b  [0.02873196]  mse_f: 0.4180217981338501   total loss: [0.44675377]\n",
            "mse_b ====== [0.0280953906]\n",
            "It: 38559, Time: 0.02\n",
            "mse_b  [0.02809539]  mse_f: 0.42902812361717224   total loss: [0.45712352]\n",
            "mse_b ====== [0.0572299138]\n",
            "It: 38560, Time: 0.02\n",
            "mse_b  [0.05722991]  mse_f: 0.42737114429473877   total loss: [0.48460105]\n",
            "mse_b ====== [0.0348535739]\n",
            "It: 38561, Time: 0.03\n",
            "mse_b  [0.03485357]  mse_f: 0.43390151858329773   total loss: [0.4687551]\n",
            "mse_b ====== [0.0613289885]\n",
            "It: 38562, Time: 0.02\n",
            "mse_b  [0.06132899]  mse_f: 0.4070282578468323   total loss: [0.46835724]\n",
            "mse_b ====== [0.0464616567]\n",
            "It: 38563, Time: 0.02\n",
            "mse_b  [0.04646166]  mse_f: 0.4526330530643463   total loss: [0.49909472]\n",
            "mse_b ====== [0.0830922946]\n",
            "It: 38564, Time: 0.02\n",
            "mse_b  [0.08309229]  mse_f: 0.4413963556289673   total loss: [0.5244886]\n",
            "mse_b ====== [0.0435802378]\n",
            "It: 38565, Time: 0.02\n",
            "mse_b  [0.04358024]  mse_f: 0.45748791098594666   total loss: [0.5010682]\n",
            "mse_b ====== [0.098432906]\n",
            "It: 38566, Time: 0.02\n",
            "mse_b  [0.09843291]  mse_f: 0.40258103609085083   total loss: [0.50101393]\n",
            "mse_b ====== [0.0342942923]\n",
            "It: 38567, Time: 0.02\n",
            "mse_b  [0.03429429]  mse_f: 0.4418763518333435   total loss: [0.47617066]\n",
            "mse_b ====== [0.0374724455]\n",
            "It: 38568, Time: 0.02\n",
            "mse_b  [0.03747245]  mse_f: 0.439947247505188   total loss: [0.4774197]\n",
            "mse_b ====== [0.0620130375]\n",
            "It: 38569, Time: 0.02\n",
            "mse_b  [0.06201304]  mse_f: 0.4238845109939575   total loss: [0.48589754]\n",
            "mse_b ====== [0.0467004962]\n",
            "It: 38570, Time: 0.02\n",
            "mse_b  [0.0467005]  mse_f: 0.4542018175125122   total loss: [0.5009023]\n",
            "mse_b ====== [0.109396756]\n",
            "It: 38571, Time: 0.02\n",
            "mse_b  [0.10939676]  mse_f: 0.40743547677993774   total loss: [0.51683223]\n",
            "mse_b ====== [0.0287272949]\n",
            "It: 38572, Time: 0.02\n",
            "mse_b  [0.02872729]  mse_f: 0.42220064997673035   total loss: [0.45092794]\n",
            "mse_b ====== [0.0433202647]\n",
            "It: 38573, Time: 0.02\n",
            "mse_b  [0.04332026]  mse_f: 0.4455503225326538   total loss: [0.4888706]\n",
            "mse_b ====== [0.190052524]\n",
            "It: 38574, Time: 0.03\n",
            "mse_b  [0.19005252]  mse_f: 0.40279853343963623   total loss: [0.59285104]\n",
            "mse_b ====== [0.036240764]\n",
            "It: 38575, Time: 0.02\n",
            "mse_b  [0.03624076]  mse_f: 0.4263986349105835   total loss: [0.4626394]\n",
            "mse_b ====== [0.128982738]\n",
            "It: 38576, Time: 0.03\n",
            "mse_b  [0.12898274]  mse_f: 0.5003905892372131   total loss: [0.6293733]\n",
            "mse_b ====== [0.502981663]\n",
            "It: 38577, Time: 0.02\n",
            "mse_b  [0.50298166]  mse_f: 0.46400460600852966   total loss: [0.9669863]\n",
            "mse_b ====== [0.353623927]\n",
            "It: 38578, Time: 0.02\n",
            "mse_b  [0.35362393]  mse_f: 0.48057305812835693   total loss: [0.834197]\n",
            "mse_b ====== [0.176352739]\n",
            "It: 38579, Time: 0.02\n",
            "mse_b  [0.17635274]  mse_f: 0.6958824396133423   total loss: [0.8722352]\n",
            "mse_b ====== [0.0478105582]\n",
            "It: 38580, Time: 0.02\n",
            "mse_b  [0.04781056]  mse_f: 0.5003270506858826   total loss: [0.5481376]\n",
            "mse_b ====== [0.197658092]\n",
            "It: 38581, Time: 0.02\n",
            "mse_b  [0.19765809]  mse_f: 0.5419721603393555   total loss: [0.7396302]\n",
            "mse_b ====== [0.155656725]\n",
            "It: 38582, Time: 0.02\n",
            "mse_b  [0.15565673]  mse_f: 0.536225438117981   total loss: [0.69188213]\n",
            "mse_b ====== [0.0825046822]\n",
            "It: 38583, Time: 0.02\n",
            "mse_b  [0.08250468]  mse_f: 0.551809549331665   total loss: [0.63431424]\n",
            "mse_b ====== [0.077965863]\n",
            "It: 38584, Time: 0.02\n",
            "mse_b  [0.07796586]  mse_f: 0.5654522776603699   total loss: [0.64341813]\n",
            "mse_b ====== [0.170700118]\n",
            "It: 38585, Time: 0.02\n",
            "mse_b  [0.17070012]  mse_f: 0.5047034025192261   total loss: [0.67540354]\n",
            "mse_b ====== [0.253140301]\n",
            "It: 38586, Time: 0.02\n",
            "mse_b  [0.2531403]  mse_f: 0.5378358960151672   total loss: [0.79097617]\n",
            "mse_b ====== [0.105772391]\n",
            "It: 38587, Time: 0.02\n",
            "mse_b  [0.10577239]  mse_f: 0.496082603931427   total loss: [0.601855]\n",
            "mse_b ====== [0.0771252662]\n",
            "It: 38588, Time: 0.02\n",
            "mse_b  [0.07712527]  mse_f: 0.6305372714996338   total loss: [0.7076625]\n",
            "mse_b ====== [0.0534098297]\n",
            "It: 38589, Time: 0.02\n",
            "mse_b  [0.05340983]  mse_f: 0.5917454957962036   total loss: [0.6451553]\n",
            "mse_b ====== [0.200924799]\n",
            "It: 38590, Time: 0.02\n",
            "mse_b  [0.2009248]  mse_f: 0.4664457440376282   total loss: [0.66737056]\n",
            "mse_b ====== [0.128570065]\n",
            "It: 38591, Time: 0.02\n",
            "mse_b  [0.12857006]  mse_f: 0.500956654548645   total loss: [0.62952673]\n",
            "mse_b ====== [0.16778475]\n",
            "It: 38592, Time: 0.03\n",
            "mse_b  [0.16778475]  mse_f: 0.5744476318359375   total loss: [0.7422324]\n",
            "mse_b ====== [0.140945]\n",
            "It: 38593, Time: 0.02\n",
            "mse_b  [0.140945]  mse_f: 0.5205535888671875   total loss: [0.6614986]\n",
            "mse_b ====== [0.141360193]\n",
            "It: 38594, Time: 0.02\n",
            "mse_b  [0.1413602]  mse_f: 0.49683842062950134   total loss: [0.6381986]\n",
            "mse_b ====== [0.114062347]\n",
            "It: 38595, Time: 0.02\n",
            "mse_b  [0.11406235]  mse_f: 0.4779624938964844   total loss: [0.59202486]\n",
            "mse_b ====== [0.0774148479]\n",
            "It: 38596, Time: 0.02\n",
            "mse_b  [0.07741485]  mse_f: 0.6439259052276611   total loss: [0.7213408]\n",
            "mse_b ====== [0.0707300901]\n",
            "It: 38597, Time: 0.02\n",
            "mse_b  [0.07073009]  mse_f: 0.6066657900810242   total loss: [0.6773959]\n",
            "mse_b ====== [0.14756301]\n",
            "It: 38598, Time: 0.02\n",
            "mse_b  [0.14756301]  mse_f: 0.5019412040710449   total loss: [0.6495042]\n",
            "mse_b ====== [0.0479281731]\n",
            "It: 38599, Time: 0.02\n",
            "mse_b  [0.04792817]  mse_f: 0.5786402821540833   total loss: [0.62656844]\n",
            "mse_b ====== [0.282033503]\n",
            "It: 38600, Time: 0.03\n",
            "mse_b  [0.2820335]  mse_f: 0.5675090551376343   total loss: [0.84954256]\n",
            "mse_b ====== [0.249455482]\n",
            "It: 38601, Time: 0.02\n",
            "mse_b  [0.24945548]  mse_f: 0.47482573986053467   total loss: [0.7242812]\n",
            "mse_b ====== [0.270856678]\n",
            "It: 38602, Time: 0.02\n",
            "mse_b  [0.27085668]  mse_f: 0.46631956100463867   total loss: [0.73717624]\n",
            "mse_b ====== [0.132369787]\n",
            "It: 38603, Time: 0.02\n",
            "mse_b  [0.13236979]  mse_f: 0.6149783134460449   total loss: [0.74734807]\n",
            "mse_b ====== [0.0566350818]\n",
            "It: 38604, Time: 0.02\n",
            "mse_b  [0.05663508]  mse_f: 0.6936345100402832   total loss: [0.7502696]\n",
            "mse_b ====== [0.147895157]\n",
            "It: 38605, Time: 0.02\n",
            "mse_b  [0.14789516]  mse_f: 0.48518291115760803   total loss: [0.6330781]\n",
            "mse_b ====== [0.103732571]\n",
            "It: 38606, Time: 0.02\n",
            "mse_b  [0.10373257]  mse_f: 0.575127899646759   total loss: [0.6788605]\n",
            "mse_b ====== [0.156511545]\n",
            "It: 38607, Time: 0.02\n",
            "mse_b  [0.15651155]  mse_f: 0.5952624678611755   total loss: [0.751774]\n",
            "mse_b ====== [0.147621036]\n",
            "It: 38608, Time: 0.02\n",
            "mse_b  [0.14762104]  mse_f: 0.48014459013938904   total loss: [0.62776566]\n",
            "mse_b ====== [0.206218511]\n",
            "It: 38609, Time: 0.02\n",
            "mse_b  [0.20621851]  mse_f: 0.48259472846984863   total loss: [0.6888132]\n",
            "mse_b ====== [0.0912370086]\n",
            "It: 38610, Time: 0.02\n",
            "mse_b  [0.09123701]  mse_f: 0.5633993148803711   total loss: [0.6546363]\n",
            "mse_b ====== [0.0773246]\n",
            "It: 38611, Time: 0.03\n",
            "mse_b  [0.0773246]  mse_f: 0.5747233033180237   total loss: [0.6520479]\n",
            "mse_b ====== [0.238410458]\n",
            "It: 38612, Time: 0.02\n",
            "mse_b  [0.23841046]  mse_f: 0.45403221249580383   total loss: [0.69244266]\n",
            "mse_b ====== [0.273144811]\n",
            "It: 38613, Time: 0.02\n",
            "mse_b  [0.2731448]  mse_f: 0.49890637397766113   total loss: [0.7720512]\n",
            "mse_b ====== [0.0571890846]\n",
            "It: 38614, Time: 0.02\n",
            "mse_b  [0.05718908]  mse_f: 0.5074694156646729   total loss: [0.5646585]\n",
            "mse_b ====== [0.106690086]\n",
            "It: 38615, Time: 0.02\n",
            "mse_b  [0.10669009]  mse_f: 0.5493122935295105   total loss: [0.6560024]\n",
            "mse_b ====== [0.266857952]\n",
            "It: 38616, Time: 0.02\n",
            "mse_b  [0.26685795]  mse_f: 0.49774134159088135   total loss: [0.7645993]\n",
            "mse_b ====== [0.340267956]\n",
            "It: 38617, Time: 0.02\n",
            "mse_b  [0.34026796]  mse_f: 0.46483486890792847   total loss: [0.8051028]\n",
            "mse_b ====== [0.0505491719]\n",
            "It: 38618, Time: 0.02\n",
            "mse_b  [0.05054917]  mse_f: 0.5071322321891785   total loss: [0.5576814]\n",
            "mse_b ====== [0.201290622]\n",
            "It: 38619, Time: 0.02\n",
            "mse_b  [0.20129062]  mse_f: 0.6837373971939087   total loss: [0.885028]\n",
            "mse_b ====== [0.174914181]\n",
            "It: 38620, Time: 0.02\n",
            "mse_b  [0.17491418]  mse_f: 0.5599551200866699   total loss: [0.7348693]\n",
            "mse_b ====== [0.35922274]\n",
            "It: 38621, Time: 0.02\n",
            "mse_b  [0.35922274]  mse_f: 0.561184287071228   total loss: [0.92040706]\n",
            "mse_b ====== [0.191456601]\n",
            "It: 38622, Time: 0.02\n",
            "mse_b  [0.1914566]  mse_f: 0.6309458017349243   total loss: [0.8224024]\n",
            "mse_b ====== [0.078339383]\n",
            "It: 38623, Time: 0.03\n",
            "mse_b  [0.07833938]  mse_f: 0.6192249059677124   total loss: [0.6975643]\n",
            "mse_b ====== [0.149468407]\n",
            "It: 38624, Time: 0.02\n",
            "mse_b  [0.1494684]  mse_f: 0.704697847366333   total loss: [0.85416627]\n",
            "mse_b ====== [0.153004542]\n",
            "It: 38625, Time: 0.02\n",
            "mse_b  [0.15300454]  mse_f: 0.6004657745361328   total loss: [0.7534703]\n",
            "mse_b ====== [0.436433375]\n",
            "It: 38626, Time: 0.02\n",
            "mse_b  [0.43643337]  mse_f: 0.5812349915504456   total loss: [1.0176684]\n",
            "mse_b ====== [0.230430692]\n",
            "It: 38627, Time: 0.02\n",
            "mse_b  [0.23043069]  mse_f: 0.4695699214935303   total loss: [0.70000064]\n",
            "mse_b ====== [0.192729622]\n",
            "It: 38628, Time: 0.02\n",
            "mse_b  [0.19272962]  mse_f: 0.6228398680686951   total loss: [0.8155695]\n",
            "mse_b ====== [0.216455981]\n",
            "It: 38629, Time: 0.02\n",
            "mse_b  [0.21645598]  mse_f: 0.9556690454483032   total loss: [1.172125]\n",
            "mse_b ====== [0.0970018506]\n",
            "It: 38630, Time: 0.02\n",
            "mse_b  [0.09700185]  mse_f: 0.47203028202056885   total loss: [0.56903213]\n",
            "mse_b ====== [0.339037806]\n",
            "It: 38631, Time: 0.03\n",
            "mse_b  [0.3390378]  mse_f: 0.813456118106842   total loss: [1.152494]\n",
            "mse_b ====== [0.469296396]\n",
            "It: 38632, Time: 0.02\n",
            "mse_b  [0.4692964]  mse_f: 0.8232962489128113   total loss: [1.2925926]\n",
            "mse_b ====== [0.11950618]\n",
            "It: 38633, Time: 0.02\n",
            "mse_b  [0.11950618]  mse_f: 0.7570403218269348   total loss: [0.8765465]\n",
            "mse_b ====== [0.312584966]\n",
            "It: 38634, Time: 0.02\n",
            "mse_b  [0.31258497]  mse_f: 1.124836802482605   total loss: [1.4374218]\n",
            "mse_b ====== [0.263737]\n",
            "It: 38635, Time: 0.02\n",
            "mse_b  [0.263737]  mse_f: 0.6902213096618652   total loss: [0.9539583]\n",
            "mse_b ====== [0.258460104]\n",
            "It: 38636, Time: 0.02\n",
            "mse_b  [0.2584601]  mse_f: 0.6813265085220337   total loss: [0.9397866]\n",
            "mse_b ====== [0.631592274]\n",
            "It: 38637, Time: 0.02\n",
            "mse_b  [0.6315923]  mse_f: 0.7948282957077026   total loss: [1.4264206]\n",
            "mse_b ====== [0.330673754]\n",
            "It: 38638, Time: 0.02\n",
            "mse_b  [0.33067375]  mse_f: 0.7637051343917847   total loss: [1.094379]\n",
            "mse_b ====== [0.548938155]\n",
            "It: 38639, Time: 0.02\n",
            "mse_b  [0.54893816]  mse_f: 0.9860817790031433   total loss: [1.5350199]\n",
            "mse_b ====== [0.0911613107]\n",
            "It: 38640, Time: 0.03\n",
            "mse_b  [0.09116131]  mse_f: 0.827377200126648   total loss: [0.9185385]\n",
            "mse_b ====== [0.350189149]\n",
            "It: 38641, Time: 0.03\n",
            "mse_b  [0.35018915]  mse_f: 1.0779693126678467   total loss: [1.4281585]\n",
            "mse_b ====== [0.143097058]\n",
            "It: 38642, Time: 0.03\n",
            "mse_b  [0.14309706]  mse_f: 1.2694776058197021   total loss: [1.4125746]\n",
            "mse_b ====== [0.381015807]\n",
            "It: 38643, Time: 0.02\n",
            "mse_b  [0.3810158]  mse_f: 1.348266363143921   total loss: [1.7292821]\n",
            "mse_b ====== [0.358371973]\n",
            "It: 38644, Time: 0.02\n",
            "mse_b  [0.35837197]  mse_f: 1.4704594612121582   total loss: [1.8288314]\n",
            "mse_b ====== [0.162038878]\n",
            "It: 38645, Time: 0.02\n",
            "mse_b  [0.16203888]  mse_f: 0.6329705715179443   total loss: [0.79500943]\n",
            "mse_b ====== [0.294145]\n",
            "It: 38646, Time: 0.02\n",
            "mse_b  [0.294145]  mse_f: 1.3429118394851685   total loss: [1.6370568]\n",
            "mse_b ====== [0.0926408097]\n",
            "It: 38647, Time: 0.03\n",
            "mse_b  [0.09264081]  mse_f: 1.3830294609069824   total loss: [1.4756702]\n",
            "mse_b ====== [0.263192981]\n",
            "It: 38648, Time: 0.02\n",
            "mse_b  [0.26319298]  mse_f: 1.6111373901367188   total loss: [1.8743304]\n",
            "mse_b ====== [0.803767204]\n",
            "It: 38649, Time: 0.02\n",
            "mse_b  [0.8037672]  mse_f: 1.2037444114685059   total loss: [2.0075116]\n",
            "mse_b ====== [0.687908649]\n",
            "It: 38650, Time: 0.02\n",
            "mse_b  [0.68790865]  mse_f: 1.1423765420913696   total loss: [1.8302852]\n",
            "mse_b ====== [0.64113009]\n",
            "It: 38651, Time: 0.02\n",
            "mse_b  [0.6411301]  mse_f: 1.072833776473999   total loss: [1.7139639]\n",
            "mse_b ====== [0.36161083]\n",
            "It: 38652, Time: 0.02\n",
            "mse_b  [0.36161083]  mse_f: 1.6559994220733643   total loss: [2.0176103]\n",
            "mse_b ====== [1.28223205]\n",
            "It: 38653, Time: 0.02\n",
            "mse_b  [1.282232]  mse_f: 1.1773120164871216   total loss: [2.4595442]\n",
            "mse_b ====== [0.549095631]\n",
            "It: 38654, Time: 0.02\n",
            "mse_b  [0.54909563]  mse_f: 0.7123742699623108   total loss: [1.2614698]\n",
            "mse_b ====== [1.21039009]\n",
            "It: 38655, Time: 0.03\n",
            "mse_b  [1.2103901]  mse_f: 1.0183709859848022   total loss: [2.2287612]\n",
            "mse_b ====== [0.668496847]\n",
            "It: 38656, Time: 0.02\n",
            "mse_b  [0.66849685]  mse_f: 1.511696696281433   total loss: [2.1801934]\n",
            "mse_b ====== [1.1471417]\n",
            "It: 38657, Time: 0.02\n",
            "mse_b  [1.1471417]  mse_f: 2.068650245666504   total loss: [3.215792]\n",
            "mse_b ====== [0.958015382]\n",
            "It: 38658, Time: 0.02\n",
            "mse_b  [0.9580154]  mse_f: 1.0072426795959473   total loss: [1.9652581]\n",
            "mse_b ====== [0.782555223]\n",
            "It: 38659, Time: 0.02\n",
            "mse_b  [0.7825552]  mse_f: 2.331352710723877   total loss: [3.1139078]\n",
            "mse_b ====== [0.820026517]\n",
            "It: 38660, Time: 0.02\n",
            "mse_b  [0.8200265]  mse_f: 5.8287248611450195   total loss: [6.6487513]\n",
            "mse_b ====== [2.73281384]\n",
            "It: 38661, Time: 0.02\n",
            "mse_b  [2.7328138]  mse_f: 2.9879064559936523   total loss: [5.7207203]\n",
            "mse_b ====== [1.7165513]\n",
            "It: 38662, Time: 0.02\n",
            "mse_b  [1.7165513]  mse_f: 4.051272869110107   total loss: [5.767824]\n",
            "mse_b ====== [2.50990844]\n",
            "It: 38663, Time: 0.02\n",
            "mse_b  [2.5099084]  mse_f: 3.4936347007751465   total loss: [6.003543]\n",
            "mse_b ====== [0.738851964]\n",
            "It: 38664, Time: 0.03\n",
            "mse_b  [0.73885196]  mse_f: 3.2390599250793457   total loss: [3.977912]\n",
            "mse_b ====== [2.55604529]\n",
            "It: 38665, Time: 0.02\n",
            "mse_b  [2.5560453]  mse_f: 4.171408653259277   total loss: [6.727454]\n",
            "mse_b ====== [2.28629565]\n",
            "It: 38666, Time: 0.02\n",
            "mse_b  [2.2862957]  mse_f: 2.4097118377685547   total loss: [4.6960077]\n",
            "mse_b ====== [1.58783078]\n",
            "It: 38667, Time: 0.02\n",
            "mse_b  [1.5878308]  mse_f: 4.266452312469482   total loss: [5.8542833]\n",
            "mse_b ====== [1.81398332]\n",
            "It: 38668, Time: 0.02\n",
            "mse_b  [1.8139833]  mse_f: 3.2102227210998535   total loss: [5.024206]\n",
            "mse_b ====== [1.80637884]\n",
            "It: 38669, Time: 0.02\n",
            "mse_b  [1.8063788]  mse_f: 2.43143892288208   total loss: [4.237818]\n",
            "mse_b ====== [2.92915916]\n",
            "It: 38670, Time: 0.02\n",
            "mse_b  [2.9291592]  mse_f: 2.832871913909912   total loss: [5.762031]\n",
            "mse_b ====== [0.85047996]\n",
            "It: 38671, Time: 0.02\n",
            "mse_b  [0.85047996]  mse_f: 2.5989603996276855   total loss: [3.4494405]\n",
            "mse_b ====== [1.16027856]\n",
            "It: 38672, Time: 0.02\n",
            "mse_b  [1.1602786]  mse_f: 5.191596031188965   total loss: [6.3518744]\n",
            "mse_b ====== [0.799295664]\n",
            "It: 38673, Time: 0.02\n",
            "mse_b  [0.79929566]  mse_f: 4.249534606933594   total loss: [5.04883]\n",
            "mse_b ====== [1.53844309]\n",
            "It: 38674, Time: 0.03\n",
            "mse_b  [1.5384431]  mse_f: 2.262907028198242   total loss: [3.80135]\n",
            "mse_b ====== [2.23527884]\n",
            "It: 38675, Time: 0.02\n",
            "mse_b  [2.2352788]  mse_f: 2.5074706077575684   total loss: [4.742749]\n",
            "mse_b ====== [1.74565315]\n",
            "It: 38676, Time: 0.02\n",
            "mse_b  [1.7456532]  mse_f: 1.8697123527526855   total loss: [3.6153655]\n",
            "mse_b ====== [2.45767498]\n",
            "It: 38677, Time: 0.03\n",
            "mse_b  [2.457675]  mse_f: 3.3189589977264404   total loss: [5.776634]\n",
            "mse_b ====== [1.14318669]\n",
            "It: 38678, Time: 0.02\n",
            "mse_b  [1.1431867]  mse_f: 2.2234601974487305   total loss: [3.3666468]\n",
            "mse_b ====== [1.72168386]\n",
            "It: 38679, Time: 0.03\n",
            "mse_b  [1.7216839]  mse_f: 2.907595157623291   total loss: [4.629279]\n",
            "mse_b ====== [1.57281864]\n",
            "It: 38680, Time: 0.02\n",
            "mse_b  [1.5728186]  mse_f: 3.0341334342956543   total loss: [4.606952]\n",
            "mse_b ====== [0.805386424]\n",
            "It: 38681, Time: 0.02\n",
            "mse_b  [0.8053864]  mse_f: 3.111677885055542   total loss: [3.9170642]\n",
            "mse_b ====== [0.565767944]\n",
            "It: 38682, Time: 0.02\n",
            "mse_b  [0.56576794]  mse_f: 3.1990408897399902   total loss: [3.764809]\n",
            "mse_b ====== [0.958898604]\n",
            "It: 38683, Time: 0.03\n",
            "mse_b  [0.9588986]  mse_f: 2.130148410797119   total loss: [3.089047]\n",
            "mse_b ====== [2.39456916]\n",
            "It: 38684, Time: 0.02\n",
            "mse_b  [2.3945692]  mse_f: 2.647665023803711   total loss: [5.0422344]\n",
            "mse_b ====== [2.00284934]\n",
            "It: 38685, Time: 0.02\n",
            "mse_b  [2.0028493]  mse_f: 2.763242721557617   total loss: [4.7660923]\n",
            "mse_b ====== [0.976314783]\n",
            "It: 38686, Time: 0.02\n",
            "mse_b  [0.9763148]  mse_f: 2.9724974632263184   total loss: [3.9488122]\n",
            "mse_b ====== [1.23946714]\n",
            "It: 38687, Time: 0.03\n",
            "mse_b  [1.2394671]  mse_f: 3.3760478496551514   total loss: [4.6155148]\n",
            "mse_b ====== [1.11477494]\n",
            "It: 38688, Time: 0.02\n",
            "mse_b  [1.114775]  mse_f: 3.203878402709961   total loss: [4.318653]\n",
            "mse_b ====== [1.56836247]\n",
            "It: 38689, Time: 0.02\n",
            "mse_b  [1.5683625]  mse_f: 2.5233545303344727   total loss: [4.091717]\n",
            "mse_b ====== [1.22910953]\n",
            "It: 38690, Time: 0.02\n",
            "mse_b  [1.2291095]  mse_f: 2.6498465538024902   total loss: [3.878956]\n",
            "mse_b ====== [1.09545481]\n",
            "It: 38691, Time: 0.02\n",
            "mse_b  [1.0954548]  mse_f: 1.8241150379180908   total loss: [2.91957]\n",
            "mse_b ====== [2.36755824]\n",
            "It: 38692, Time: 0.02\n",
            "mse_b  [2.3675582]  mse_f: 2.249429702758789   total loss: [4.616988]\n",
            "mse_b ====== [1.74494565]\n",
            "It: 38693, Time: 0.02\n",
            "mse_b  [1.7449456]  mse_f: 2.0662620067596436   total loss: [3.8112078]\n",
            "mse_b ====== [1.78740084]\n",
            "It: 38694, Time: 0.02\n",
            "mse_b  [1.7874008]  mse_f: 2.9993014335632324   total loss: [4.786702]\n",
            "mse_b ====== [1.99259388]\n",
            "It: 38695, Time: 0.02\n",
            "mse_b  [1.9925939]  mse_f: 2.506885051727295   total loss: [4.499479]\n",
            "mse_b ====== [1.24070144]\n",
            "It: 38696, Time: 0.02\n",
            "mse_b  [1.2407014]  mse_f: 3.5318493843078613   total loss: [4.7725506]\n",
            "mse_b ====== [1.19027519]\n",
            "It: 38697, Time: 0.02\n",
            "mse_b  [1.1902752]  mse_f: 2.6283180713653564   total loss: [3.8185933]\n",
            "mse_b ====== [1.36738896]\n",
            "It: 38698, Time: 0.02\n",
            "mse_b  [1.367389]  mse_f: 2.4256932735443115   total loss: [3.7930822]\n",
            "mse_b ====== [2.01284146]\n",
            "It: 38699, Time: 0.02\n",
            "mse_b  [2.0128415]  mse_f: 1.8980025053024292   total loss: [3.9108438]\n",
            "mse_b ====== [1.98884439]\n",
            "It: 38700, Time: 0.02\n",
            "mse_b  [1.9888444]  mse_f: 2.0080649852752686   total loss: [3.9969094]\n",
            "mse_b ====== [1.57563341]\n",
            "It: 38701, Time: 0.03\n",
            "mse_b  [1.5756334]  mse_f: 2.1157476902008057   total loss: [3.691381]\n",
            "mse_b ====== [1.82536304]\n",
            "It: 38702, Time: 0.03\n",
            "mse_b  [1.825363]  mse_f: 1.9329471588134766   total loss: [3.7583103]\n",
            "mse_b ====== [1.45176482]\n",
            "It: 38703, Time: 0.02\n",
            "mse_b  [1.4517648]  mse_f: 2.811899423599243   total loss: [4.2636642]\n",
            "mse_b ====== [1.24041986]\n",
            "It: 38704, Time: 0.02\n",
            "mse_b  [1.2404199]  mse_f: 2.132622480392456   total loss: [3.3730423]\n",
            "mse_b ====== [1.21234548]\n",
            "It: 38705, Time: 0.02\n",
            "mse_b  [1.2123455]  mse_f: 3.1140594482421875   total loss: [4.326405]\n",
            "mse_b ====== [0.861559033]\n",
            "It: 38706, Time: 0.02\n",
            "mse_b  [0.86155903]  mse_f: 3.1576461791992188   total loss: [4.019205]\n",
            "mse_b ====== [1.05236971]\n",
            "It: 38707, Time: 0.02\n",
            "mse_b  [1.0523697]  mse_f: 3.469184160232544   total loss: [4.521554]\n",
            "mse_b ====== [1.05367374]\n",
            "It: 38708, Time: 0.02\n",
            "mse_b  [1.0536737]  mse_f: 3.187901020050049   total loss: [4.241575]\n",
            "mse_b ====== [1.19158494]\n",
            "It: 38709, Time: 0.02\n",
            "mse_b  [1.191585]  mse_f: 1.4952638149261475   total loss: [2.6868486]\n",
            "mse_b ====== [1.07405829]\n",
            "It: 38710, Time: 0.03\n",
            "mse_b  [1.0740583]  mse_f: 2.124070167541504   total loss: [3.1981285]\n",
            "mse_b ====== [0.551907182]\n",
            "It: 38711, Time: 0.02\n",
            "mse_b  [0.5519072]  mse_f: 1.692378282546997   total loss: [2.2442856]\n",
            "mse_b ====== [1.17260408]\n",
            "It: 38712, Time: 0.02\n",
            "mse_b  [1.1726041]  mse_f: 2.6138529777526855   total loss: [3.786457]\n",
            "mse_b ====== [0.918127537]\n",
            "It: 38713, Time: 0.02\n",
            "mse_b  [0.91812754]  mse_f: 4.61342716217041   total loss: [5.5315547]\n",
            "mse_b ====== [0.779947042]\n",
            "It: 38714, Time: 0.02\n",
            "mse_b  [0.77994704]  mse_f: 2.209503650665283   total loss: [2.9894507]\n",
            "mse_b ====== [0.835006714]\n",
            "It: 38715, Time: 0.02\n",
            "mse_b  [0.8350067]  mse_f: 6.4113264083862305   total loss: [7.246333]\n",
            "mse_b ====== [2.09249115]\n",
            "It: 38716, Time: 0.02\n",
            "mse_b  [2.0924911]  mse_f: 3.5257091522216797   total loss: [5.6182003]\n",
            "mse_b ====== [2.7351141]\n",
            "It: 38717, Time: 0.02\n",
            "mse_b  [2.735114]  mse_f: 5.026099681854248   total loss: [7.761214]\n",
            "mse_b ====== [1.81839085]\n",
            "It: 38718, Time: 0.02\n",
            "mse_b  [1.8183908]  mse_f: 2.277569055557251   total loss: [4.0959597]\n",
            "mse_b ====== [1.41050172]\n",
            "It: 38719, Time: 0.03\n",
            "mse_b  [1.4105017]  mse_f: 7.966553688049316   total loss: [9.377055]\n",
            "mse_b ====== [2.15449715]\n",
            "It: 38720, Time: 0.02\n",
            "mse_b  [2.1544971]  mse_f: 5.284799575805664   total loss: [7.4392967]\n",
            "mse_b ====== [2.51439095]\n",
            "It: 38721, Time: 0.02\n",
            "mse_b  [2.514391]  mse_f: 7.593454837799072   total loss: [10.107845]\n",
            "mse_b ====== [1.75171268]\n",
            "It: 38722, Time: 0.02\n",
            "mse_b  [1.7517127]  mse_f: 1.1533212661743164   total loss: [2.905034]\n",
            "mse_b ====== [1.32274842]\n",
            "It: 38723, Time: 0.02\n",
            "mse_b  [1.3227484]  mse_f: 11.836764335632324   total loss: [13.1595125]\n",
            "mse_b ====== [2.26467109]\n",
            "It: 38724, Time: 0.02\n",
            "mse_b  [2.264671]  mse_f: 3.6706628799438477   total loss: [5.935334]\n",
            "mse_b ====== [3.89165139]\n",
            "It: 38725, Time: 0.02\n",
            "mse_b  [3.8916514]  mse_f: 9.892095565795898   total loss: [13.783747]\n",
            "mse_b ====== [4.9152379]\n",
            "It: 38726, Time: 0.02\n",
            "mse_b  [4.915238]  mse_f: 6.525116920471191   total loss: [11.440355]\n",
            "mse_b ====== [5.13686943]\n",
            "It: 38727, Time: 0.02\n",
            "mse_b  [5.1368694]  mse_f: 3.0955989360809326   total loss: [8.232469]\n",
            "mse_b ====== [2.51669669]\n",
            "It: 38728, Time: 0.03\n",
            "mse_b  [2.5166967]  mse_f: 7.734395980834961   total loss: [10.251093]\n",
            "mse_b ====== [2.67153859]\n",
            "It: 38729, Time: 0.02\n",
            "mse_b  [2.6715386]  mse_f: 6.1651763916015625   total loss: [8.836715]\n",
            "mse_b ====== [3.24076176]\n",
            "It: 38730, Time: 0.02\n",
            "mse_b  [3.2407618]  mse_f: 5.982188701629639   total loss: [9.22295]\n",
            "mse_b ====== [2.02795935]\n",
            "It: 38731, Time: 0.02\n",
            "mse_b  [2.0279593]  mse_f: 7.351144790649414   total loss: [9.379105]\n",
            "mse_b ====== [5.97910404]\n",
            "It: 38732, Time: 0.02\n",
            "mse_b  [5.979104]  mse_f: 3.0006461143493652   total loss: [8.97975]\n",
            "mse_b ====== [5.14706087]\n",
            "It: 38733, Time: 0.02\n",
            "mse_b  [5.147061]  mse_f: 3.9499993324279785   total loss: [9.09706]\n",
            "mse_b ====== [1.73221135]\n",
            "It: 38734, Time: 0.02\n",
            "mse_b  [1.7322114]  mse_f: 5.563579559326172   total loss: [7.2957907]\n",
            "mse_b ====== [5.67273521]\n",
            "It: 38735, Time: 0.02\n",
            "mse_b  [5.672735]  mse_f: 4.654428005218506   total loss: [10.327164]\n",
            "mse_b ====== [4.14681959]\n",
            "It: 38736, Time: 0.02\n",
            "mse_b  [4.1468196]  mse_f: 5.1713666915893555   total loss: [9.318186]\n",
            "mse_b ====== [3.02225828]\n",
            "It: 38737, Time: 0.02\n",
            "mse_b  [3.0222583]  mse_f: 2.82568359375   total loss: [5.847942]\n",
            "mse_b ====== [7.39446163]\n",
            "It: 38738, Time: 0.02\n",
            "mse_b  [7.3944616]  mse_f: 3.461308717727661   total loss: [10.85577]\n",
            "mse_b ====== [4.01229906]\n",
            "It: 38739, Time: 0.02\n",
            "mse_b  [4.012299]  mse_f: 4.090142726898193   total loss: [8.102442]\n",
            "mse_b ====== [2.19070482]\n",
            "It: 38740, Time: 0.02\n",
            "mse_b  [2.1907048]  mse_f: 3.7439076900482178   total loss: [5.9346123]\n",
            "mse_b ====== [6.26966286]\n",
            "It: 38741, Time: 0.02\n",
            "mse_b  [6.269663]  mse_f: 5.903356552124023   total loss: [12.173019]\n",
            "mse_b ====== [3.24168634]\n",
            "It: 38742, Time: 0.02\n",
            "mse_b  [3.2416863]  mse_f: 2.668924331665039   total loss: [5.9106107]\n",
            "mse_b ====== [3.25425243]\n",
            "It: 38743, Time: 0.02\n",
            "mse_b  [3.2542524]  mse_f: 2.812955856323242   total loss: [6.0672083]\n",
            "mse_b ====== [6.86380386]\n",
            "It: 38744, Time: 0.02\n",
            "mse_b  [6.863804]  mse_f: 4.035914897918701   total loss: [10.899719]\n",
            "mse_b ====== [4.99492311]\n",
            "It: 38745, Time: 0.02\n",
            "mse_b  [4.994923]  mse_f: 2.593235969543457   total loss: [7.588159]\n",
            "mse_b ====== [3.16518831]\n",
            "It: 38746, Time: 0.03\n",
            "mse_b  [3.1651883]  mse_f: 3.0724449157714844   total loss: [6.237633]\n",
            "mse_b ====== [4.62249231]\n",
            "It: 38747, Time: 0.02\n",
            "mse_b  [4.6224923]  mse_f: 4.603916168212891   total loss: [9.226408]\n",
            "mse_b ====== [2.71466041]\n",
            "It: 38748, Time: 0.02\n",
            "mse_b  [2.7146604]  mse_f: 2.952479362487793   total loss: [5.66714]\n",
            "mse_b ====== [2.42170382]\n",
            "It: 38749, Time: 0.02\n",
            "mse_b  [2.4217038]  mse_f: 4.590300559997559   total loss: [7.0120044]\n",
            "mse_b ====== [5.35472488]\n",
            "It: 38750, Time: 0.02\n",
            "mse_b  [5.354725]  mse_f: 2.653658866882324   total loss: [8.008384]\n",
            "mse_b ====== [4.27894402]\n",
            "It: 38751, Time: 0.02\n",
            "mse_b  [4.278944]  mse_f: 3.787930965423584   total loss: [8.066875]\n",
            "mse_b ====== [1.79468405]\n",
            "It: 38752, Time: 0.02\n",
            "mse_b  [1.794684]  mse_f: 3.2239813804626465   total loss: [5.0186653]\n",
            "mse_b ====== [2.87008524]\n",
            "It: 38753, Time: 0.02\n",
            "mse_b  [2.8700852]  mse_f: 2.6497039794921875   total loss: [5.519789]\n",
            "mse_b ====== [3.6117723]\n",
            "It: 38754, Time: 0.03\n",
            "mse_b  [3.6117723]  mse_f: 6.728886604309082   total loss: [10.340659]\n",
            "mse_b ====== [1.4596746]\n",
            "It: 38755, Time: 0.03\n",
            "mse_b  [1.4596746]  mse_f: 3.101818799972534   total loss: [4.5614934]\n",
            "mse_b ====== [1.50482774]\n",
            "It: 38756, Time: 0.02\n",
            "mse_b  [1.5048277]  mse_f: 2.4473133087158203   total loss: [3.952141]\n",
            "mse_b ====== [3.46563816]\n",
            "It: 38757, Time: 0.02\n",
            "mse_b  [3.4656382]  mse_f: 4.6488037109375   total loss: [8.114442]\n",
            "mse_b ====== [3.22364855]\n",
            "It: 38758, Time: 0.02\n",
            "mse_b  [3.2236485]  mse_f: 3.991065502166748   total loss: [7.214714]\n",
            "mse_b ====== [1.48202]\n",
            "It: 38759, Time: 0.02\n",
            "mse_b  [1.48202]  mse_f: 4.5574846267700195   total loss: [6.0395045]\n",
            "mse_b ====== [1.23689902]\n",
            "It: 38760, Time: 0.02\n",
            "mse_b  [1.236899]  mse_f: 3.0694327354431152   total loss: [4.3063316]\n",
            "mse_b ====== [2.0820291]\n",
            "It: 38761, Time: 0.02\n",
            "mse_b  [2.082029]  mse_f: 3.6431100368499756   total loss: [5.725139]\n",
            "mse_b ====== [2.87705183]\n",
            "It: 38762, Time: 0.02\n",
            "mse_b  [2.8770518]  mse_f: 5.335611343383789   total loss: [8.212664]\n",
            "mse_b ====== [3.14451027]\n",
            "It: 38763, Time: 0.02\n",
            "mse_b  [3.1445103]  mse_f: 4.581024169921875   total loss: [7.7255344]\n",
            "mse_b ====== [3.53742886]\n",
            "It: 38764, Time: 0.02\n",
            "mse_b  [3.5374289]  mse_f: 3.149541139602661   total loss: [6.6869698]\n",
            "mse_b ====== [2.90387583]\n",
            "It: 38765, Time: 0.02\n",
            "mse_b  [2.9038758]  mse_f: 1.962294101715088   total loss: [4.86617]\n",
            "mse_b ====== [1.65109539]\n",
            "It: 38766, Time: 0.02\n",
            "mse_b  [1.6510954]  mse_f: 2.243588447570801   total loss: [3.8946838]\n",
            "mse_b ====== [2.16446543]\n",
            "It: 38767, Time: 0.02\n",
            "mse_b  [2.1644654]  mse_f: 5.480756759643555   total loss: [7.645222]\n",
            "mse_b ====== [1.84027684]\n",
            "It: 38768, Time: 0.02\n",
            "mse_b  [1.8402768]  mse_f: 4.762869834899902   total loss: [6.6031466]\n",
            "mse_b ====== [1.4713906]\n",
            "It: 38769, Time: 0.02\n",
            "mse_b  [1.4713906]  mse_f: 2.4417972564697266   total loss: [3.913188]\n",
            "mse_b ====== [2.67838693]\n",
            "It: 38770, Time: 0.02\n",
            "mse_b  [2.678387]  mse_f: 1.5231802463531494   total loss: [4.201567]\n",
            "mse_b ====== [2.98431826]\n",
            "It: 38771, Time: 0.02\n",
            "mse_b  [2.9843183]  mse_f: 1.9781827926635742   total loss: [4.962501]\n",
            "mse_b ====== [2.38033628]\n",
            "It: 38772, Time: 0.02\n",
            "mse_b  [2.3803363]  mse_f: 2.8760926723480225   total loss: [5.2564287]\n",
            "mse_b ====== [2.68444729]\n",
            "It: 38773, Time: 0.02\n",
            "mse_b  [2.6844473]  mse_f: 4.195842742919922   total loss: [6.88029]\n",
            "mse_b ====== [2.38823986]\n",
            "It: 38774, Time: 0.02\n",
            "mse_b  [2.3882399]  mse_f: 3.5098042488098145   total loss: [5.898044]\n",
            "mse_b ====== [2.31109214]\n",
            "It: 38775, Time: 0.03\n",
            "mse_b  [2.3110921]  mse_f: 2.0904440879821777   total loss: [4.401536]\n",
            "mse_b ====== [3.34520674]\n",
            "It: 38776, Time: 0.02\n",
            "mse_b  [3.3452067]  mse_f: 1.8732733726501465   total loss: [5.21848]\n",
            "mse_b ====== [3.17467499]\n",
            "It: 38777, Time: 0.02\n",
            "mse_b  [3.174675]  mse_f: 2.5044734477996826   total loss: [5.6791487]\n",
            "mse_b ====== [1.83842325]\n",
            "It: 38778, Time: 0.02\n",
            "mse_b  [1.8384233]  mse_f: 3.517420768737793   total loss: [5.355844]\n",
            "mse_b ====== [1.79081857]\n",
            "It: 38779, Time: 0.02\n",
            "mse_b  [1.7908186]  mse_f: 4.094832420349121   total loss: [5.885651]\n",
            "mse_b ====== [1.88390565]\n",
            "It: 38780, Time: 0.02\n",
            "mse_b  [1.8839056]  mse_f: 2.323245048522949   total loss: [4.2071505]\n",
            "mse_b ====== [2.00194383]\n",
            "It: 38781, Time: 0.02\n",
            "mse_b  [2.0019438]  mse_f: 1.7293951511383057   total loss: [3.731339]\n",
            "mse_b ====== [2.72488713]\n",
            "It: 38782, Time: 0.02\n",
            "mse_b  [2.7248871]  mse_f: 2.925860643386841   total loss: [5.650748]\n",
            "mse_b ====== [2.90062666]\n",
            "It: 38783, Time: 0.03\n",
            "mse_b  [2.9006267]  mse_f: 3.8511507511138916   total loss: [6.7517776]\n",
            "mse_b ====== [2.38329434]\n",
            "It: 38784, Time: 0.02\n",
            "mse_b  [2.3832943]  mse_f: 3.7576186656951904   total loss: [6.140913]\n",
            "mse_b ====== [2.03759646]\n",
            "It: 38785, Time: 0.02\n",
            "mse_b  [2.0375965]  mse_f: 3.5379691123962402   total loss: [5.5755653]\n",
            "mse_b ====== [1.68930757]\n",
            "It: 38786, Time: 0.02\n",
            "mse_b  [1.6893076]  mse_f: 3.4709086418151855   total loss: [5.1602163]\n",
            "mse_b ====== [1.65284753]\n",
            "It: 38787, Time: 0.02\n",
            "mse_b  [1.6528475]  mse_f: 2.4213907718658447   total loss: [4.0742383]\n",
            "mse_b ====== [2.15915251]\n",
            "It: 38788, Time: 0.02\n",
            "mse_b  [2.1591525]  mse_f: 2.3152949810028076   total loss: [4.4744473]\n",
            "mse_b ====== [2.11930943]\n",
            "It: 38789, Time: 0.02\n",
            "mse_b  [2.1193094]  mse_f: 3.0378637313842773   total loss: [5.157173]\n",
            "mse_b ====== [2.06594324]\n",
            "It: 38790, Time: 0.02\n",
            "mse_b  [2.0659432]  mse_f: 3.6783840656280518   total loss: [5.7443275]\n",
            "mse_b ====== [1.95243776]\n",
            "It: 38791, Time: 0.02\n",
            "mse_b  [1.9524378]  mse_f: 3.3505382537841797   total loss: [5.302976]\n",
            "mse_b ====== [1.66268778]\n",
            "It: 38792, Time: 0.02\n",
            "mse_b  [1.6626878]  mse_f: 3.192594528198242   total loss: [4.8552823]\n",
            "mse_b ====== [2.05559278]\n",
            "It: 38793, Time: 0.03\n",
            "mse_b  [2.0555928]  mse_f: 2.763620138168335   total loss: [4.819213]\n",
            "mse_b ====== [2.44211483]\n",
            "It: 38794, Time: 0.02\n",
            "mse_b  [2.4421148]  mse_f: 2.413364887237549   total loss: [4.8554797]\n",
            "mse_b ====== [2.19615483]\n",
            "It: 38795, Time: 0.02\n",
            "mse_b  [2.1961548]  mse_f: 3.300495147705078   total loss: [5.4966497]\n",
            "mse_b ====== [2.08149385]\n",
            "It: 38796, Time: 0.02\n",
            "mse_b  [2.0814939]  mse_f: 4.385431289672852   total loss: [6.466925]\n",
            "mse_b ====== [2.11311245]\n",
            "It: 38797, Time: 0.02\n",
            "mse_b  [2.1131124]  mse_f: 3.287785768508911   total loss: [5.400898]\n",
            "mse_b ====== [2.19366932]\n",
            "It: 38798, Time: 0.02\n",
            "mse_b  [2.1936693]  mse_f: 2.4505577087402344   total loss: [4.644227]\n",
            "mse_b ====== [2.10123205]\n",
            "It: 38799, Time: 0.02\n",
            "mse_b  [2.101232]  mse_f: 3.2266845703125   total loss: [5.3279166]\n",
            "mse_b ====== [1.8667562]\n",
            "It: 38800, Time: 0.02\n",
            "mse_b  [1.8667562]  mse_f: 2.929475784301758   total loss: [4.796232]\n",
            "mse_b ====== [1.99534786]\n",
            "It: 38801, Time: 0.02\n",
            "mse_b  [1.9953479]  mse_f: 2.823531150817871   total loss: [4.818879]\n",
            "mse_b ====== [2.00126767]\n",
            "It: 38802, Time: 0.02\n",
            "mse_b  [2.0012677]  mse_f: 3.8743786811828613   total loss: [5.8756466]\n",
            "mse_b ====== [1.66758192]\n",
            "It: 38803, Time: 0.02\n",
            "mse_b  [1.6675819]  mse_f: 3.1253345012664795   total loss: [4.7929163]\n",
            "mse_b ====== [1.65533435]\n",
            "It: 38804, Time: 0.02\n",
            "mse_b  [1.6553344]  mse_f: 2.5945682525634766   total loss: [4.2499027]\n",
            "mse_b ====== [1.49121356]\n",
            "It: 38805, Time: 0.02\n",
            "mse_b  [1.4912136]  mse_f: 2.605043888092041   total loss: [4.096257]\n",
            "mse_b ====== [1.19390488]\n",
            "It: 38806, Time: 0.02\n",
            "mse_b  [1.1939049]  mse_f: 1.7745509147644043   total loss: [2.9684558]\n",
            "mse_b ====== [1.40307188]\n",
            "It: 38807, Time: 0.02\n",
            "mse_b  [1.4030719]  mse_f: 2.508563280105591   total loss: [3.9116352]\n",
            "mse_b ====== [1.75138235]\n",
            "It: 38808, Time: 0.02\n",
            "mse_b  [1.7513824]  mse_f: 2.9368343353271484   total loss: [4.6882167]\n",
            "mse_b ====== [1.98128]\n",
            "It: 38809, Time: 0.02\n",
            "mse_b  [1.98128]  mse_f: 2.618556499481201   total loss: [4.5998363]\n",
            "mse_b ====== [1.85099423]\n",
            "It: 38810, Time: 0.02\n",
            "mse_b  [1.8509942]  mse_f: 2.6364808082580566   total loss: [4.487475]\n",
            "mse_b ====== [1.41934633]\n",
            "It: 38811, Time: 0.02\n",
            "mse_b  [1.4193463]  mse_f: 2.499661922454834   total loss: [3.9190083]\n",
            "mse_b ====== [1.36901391]\n",
            "It: 38812, Time: 0.02\n",
            "mse_b  [1.3690139]  mse_f: 1.9973375797271729   total loss: [3.3663516]\n",
            "mse_b ====== [1.59875333]\n",
            "It: 38813, Time: 0.03\n",
            "mse_b  [1.5987533]  mse_f: 2.4143073558807373   total loss: [4.0130606]\n",
            "mse_b ====== [1.77351463]\n",
            "It: 38814, Time: 0.02\n",
            "mse_b  [1.7735146]  mse_f: 3.0146236419677734   total loss: [4.7881384]\n",
            "mse_b ====== [1.51377928]\n",
            "It: 38815, Time: 0.03\n",
            "mse_b  [1.5137793]  mse_f: 2.8153042793273926   total loss: [4.3290834]\n",
            "mse_b ====== [1.10610652]\n",
            "It: 38816, Time: 0.02\n",
            "mse_b  [1.1061065]  mse_f: 3.1071157455444336   total loss: [4.2132225]\n",
            "mse_b ====== [1.145]\n",
            "It: 38817, Time: 0.02\n",
            "mse_b  [1.145]  mse_f: 2.2194018363952637   total loss: [3.3644018]\n",
            "mse_b ====== [1.60820961]\n",
            "It: 38818, Time: 0.02\n",
            "mse_b  [1.6082096]  mse_f: 2.3075733184814453   total loss: [3.915783]\n",
            "mse_b ====== [1.81239498]\n",
            "It: 38819, Time: 0.02\n",
            "mse_b  [1.812395]  mse_f: 2.735372543334961   total loss: [4.5477676]\n",
            "mse_b ====== [1.93791652]\n",
            "It: 38820, Time: 0.02\n",
            "mse_b  [1.9379165]  mse_f: 2.361398220062256   total loss: [4.2993145]\n",
            "mse_b ====== [2.02288389]\n",
            "It: 38821, Time: 0.02\n",
            "mse_b  [2.022884]  mse_f: 2.700711250305176   total loss: [4.723595]\n",
            "mse_b ====== [1.52061772]\n",
            "It: 38822, Time: 0.02\n",
            "mse_b  [1.5206177]  mse_f: 2.2902283668518066   total loss: [3.810846]\n",
            "mse_b ====== [1.19183969]\n",
            "It: 38823, Time: 0.02\n",
            "mse_b  [1.1918397]  mse_f: 1.7952356338500977   total loss: [2.9870753]\n",
            "mse_b ====== [1.33118939]\n",
            "It: 38824, Time: 0.02\n",
            "mse_b  [1.3311894]  mse_f: 2.379969596862793   total loss: [3.711159]\n",
            "mse_b ====== [1.16772246]\n",
            "It: 38825, Time: 0.02\n",
            "mse_b  [1.1677225]  mse_f: 2.3479318618774414   total loss: [3.5156543]\n",
            "mse_b ====== [1.09543037]\n",
            "It: 38826, Time: 0.02\n",
            "mse_b  [1.0954304]  mse_f: 3.1215548515319824   total loss: [4.216985]\n",
            "mse_b ====== [1.34409249]\n",
            "It: 38827, Time: 0.02\n",
            "mse_b  [1.3440925]  mse_f: 3.1830551624298096   total loss: [4.527148]\n",
            "mse_b ====== [1.41410017]\n",
            "It: 38828, Time: 0.02\n",
            "mse_b  [1.4141002]  mse_f: 2.1457104682922363   total loss: [3.5598106]\n",
            "mse_b ====== [1.31742716]\n",
            "It: 38829, Time: 0.02\n",
            "mse_b  [1.3174272]  mse_f: 2.0619254112243652   total loss: [3.3793526]\n",
            "mse_b ====== [1.25246835]\n",
            "It: 38830, Time: 0.03\n",
            "mse_b  [1.2524683]  mse_f: 2.0301361083984375   total loss: [3.2826045]\n",
            "mse_b ====== [1.25893891]\n",
            "It: 38831, Time: 0.03\n",
            "mse_b  [1.2589389]  mse_f: 2.2162158489227295   total loss: [3.4751549]\n",
            "mse_b ====== [1.54908288]\n",
            "It: 38832, Time: 0.03\n",
            "mse_b  [1.5490829]  mse_f: 2.4791276454925537   total loss: [4.0282106]\n",
            "mse_b ====== [1.67242587]\n",
            "It: 38833, Time: 0.02\n",
            "mse_b  [1.6724259]  mse_f: 2.0976080894470215   total loss: [3.7700338]\n",
            "mse_b ====== [1.28987181]\n",
            "It: 38834, Time: 0.02\n",
            "mse_b  [1.2898718]  mse_f: 2.1236441135406494   total loss: [3.413516]\n",
            "mse_b ====== [1.19545472]\n",
            "It: 38835, Time: 0.02\n",
            "mse_b  [1.1954547]  mse_f: 1.7820913791656494   total loss: [2.9775462]\n",
            "mse_b ====== [1.65931082]\n",
            "It: 38836, Time: 0.02\n",
            "mse_b  [1.6593108]  mse_f: 1.6906274557113647   total loss: [3.3499384]\n",
            "mse_b ====== [1.70042419]\n",
            "It: 38837, Time: 0.02\n",
            "mse_b  [1.7004242]  mse_f: 2.150512456893921   total loss: [3.8509367]\n",
            "mse_b ====== [1.44223464]\n",
            "It: 38838, Time: 0.02\n",
            "mse_b  [1.4422346]  mse_f: 1.9291993379592896   total loss: [3.371434]\n",
            "mse_b ====== [1.52168453]\n",
            "It: 38839, Time: 0.02\n",
            "mse_b  [1.5216845]  mse_f: 2.1479196548461914   total loss: [3.6696043]\n",
            "mse_b ====== [1.24627602]\n",
            "It: 38840, Time: 0.02\n",
            "mse_b  [1.246276]  mse_f: 1.6094915866851807   total loss: [2.8557677]\n",
            "mse_b ====== [0.864210844]\n",
            "It: 38841, Time: 0.02\n",
            "mse_b  [0.86421084]  mse_f: 1.760129690170288   total loss: [2.6243405]\n",
            "mse_b ====== [0.988304496]\n",
            "It: 38842, Time: 0.02\n",
            "mse_b  [0.9883045]  mse_f: 2.218757152557373   total loss: [3.2070618]\n",
            "mse_b ====== [1.06598628]\n",
            "It: 38843, Time: 0.02\n",
            "mse_b  [1.0659863]  mse_f: 2.0277538299560547   total loss: [3.09374]\n",
            "mse_b ====== [0.943278372]\n",
            "It: 38844, Time: 0.03\n",
            "mse_b  [0.9432784]  mse_f: 2.024751663208008   total loss: [2.96803]\n",
            "mse_b ====== [1.3248601]\n",
            "It: 38845, Time: 0.03\n",
            "mse_b  [1.3248601]  mse_f: 1.5391509532928467   total loss: [2.864011]\n",
            "mse_b ====== [1.86968434]\n",
            "It: 38846, Time: 0.02\n",
            "mse_b  [1.8696843]  mse_f: 1.6220104694366455   total loss: [3.491695]\n",
            "mse_b ====== [1.56626117]\n",
            "It: 38847, Time: 0.02\n",
            "mse_b  [1.5662612]  mse_f: 2.052685022354126   total loss: [3.618946]\n",
            "mse_b ====== [0.982741237]\n",
            "It: 38848, Time: 0.02\n",
            "mse_b  [0.98274124]  mse_f: 2.0957741737365723   total loss: [3.0785155]\n",
            "mse_b ====== [0.889086783]\n",
            "It: 38849, Time: 0.02\n",
            "mse_b  [0.8890868]  mse_f: 2.411215305328369   total loss: [3.300302]\n",
            "mse_b ====== [0.9304111]\n",
            "It: 38850, Time: 0.02\n",
            "mse_b  [0.9304111]  mse_f: 1.889678716659546   total loss: [2.8200898]\n",
            "mse_b ====== [1.00899756]\n",
            "It: 38851, Time: 0.02\n",
            "mse_b  [1.0089976]  mse_f: 2.089855194091797   total loss: [3.0988526]\n",
            "mse_b ====== [1.10408115]\n",
            "It: 38852, Time: 0.02\n",
            "mse_b  [1.1040812]  mse_f: 2.2291014194488525   total loss: [3.3331826]\n",
            "mse_b ====== [1.36898422]\n",
            "It: 38853, Time: 0.02\n",
            "mse_b  [1.3689842]  mse_f: 1.983186960220337   total loss: [3.3521712]\n",
            "mse_b ====== [1.45981777]\n",
            "It: 38854, Time: 0.02\n",
            "mse_b  [1.4598178]  mse_f: 1.9284250736236572   total loss: [3.3882427]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mse_b ====== [1.37095857]\n",
            "It: 38855, Time: 0.02\n",
            "mse_b  [1.3709586]  mse_f: 1.5436680316925049   total loss: [2.9146266]\n",
            "mse_b ====== [1.19994009]\n",
            "It: 38856, Time: 0.02\n",
            "mse_b  [1.1999401]  mse_f: 1.5767097473144531   total loss: [2.77665]\n",
            "mse_b ====== [0.996786833]\n",
            "It: 38857, Time: 0.02\n",
            "mse_b  [0.99678683]  mse_f: 1.7296292781829834   total loss: [2.726416]\n",
            "mse_b ====== [0.847109258]\n",
            "It: 38858, Time: 0.02\n",
            "mse_b  [0.84710926]  mse_f: 1.6192684173583984   total loss: [2.4663777]\n",
            "mse_b ====== [0.885418892]\n",
            "It: 38859, Time: 0.02\n",
            "mse_b  [0.8854189]  mse_f: 1.978132963180542   total loss: [2.8635519]\n",
            "mse_b ====== [0.876563489]\n",
            "It: 38860, Time: 0.02\n",
            "mse_b  [0.8765635]  mse_f: 1.6597033739089966   total loss: [2.5362668]\n",
            "mse_b ====== [0.839564502]\n",
            "It: 38861, Time: 0.02\n",
            "mse_b  [0.8395645]  mse_f: 1.811680793762207   total loss: [2.6512454]\n",
            "mse_b ====== [0.949658513]\n",
            "It: 38862, Time: 0.03\n",
            "mse_b  [0.9496585]  mse_f: 1.9837899208068848   total loss: [2.9334483]\n",
            "mse_b ====== [1.21491718]\n",
            "It: 38863, Time: 0.02\n",
            "mse_b  [1.2149172]  mse_f: 1.9910842180252075   total loss: [3.2060013]\n",
            "mse_b ====== [1.18996549]\n",
            "It: 38864, Time: 0.02\n",
            "mse_b  [1.1899655]  mse_f: 2.1499006748199463   total loss: [3.3398662]\n",
            "mse_b ====== [1.03183174]\n",
            "It: 38865, Time: 0.02\n",
            "mse_b  [1.0318317]  mse_f: 1.8037707805633545   total loss: [2.8356025]\n",
            "mse_b ====== [0.966048121]\n",
            "It: 38866, Time: 0.02\n",
            "mse_b  [0.9660481]  mse_f: 1.7187025547027588   total loss: [2.6847506]\n",
            "mse_b ====== [0.698109508]\n",
            "It: 38867, Time: 0.02\n",
            "mse_b  [0.6981095]  mse_f: 1.7025889158248901   total loss: [2.4006984]\n",
            "mse_b ====== [0.568781555]\n",
            "It: 38868, Time: 0.02\n",
            "mse_b  [0.56878155]  mse_f: 1.7881474494934082   total loss: [2.356929]\n",
            "mse_b ====== [0.814858675]\n",
            "It: 38869, Time: 0.03\n",
            "mse_b  [0.8148587]  mse_f: 2.142868995666504   total loss: [2.9577277]\n",
            "mse_b ====== [0.813335896]\n",
            "It: 38870, Time: 0.02\n",
            "mse_b  [0.8133359]  mse_f: 1.7330482006072998   total loss: [2.546384]\n",
            "mse_b ====== [0.705719113]\n",
            "It: 38871, Time: 0.02\n",
            "mse_b  [0.7057191]  mse_f: 1.9749974012374878   total loss: [2.6807165]\n",
            "mse_b ====== [0.866601706]\n",
            "It: 38872, Time: 0.02\n",
            "mse_b  [0.8666017]  mse_f: 1.9365832805633545   total loss: [2.803185]\n",
            "mse_b ====== [1.01413369]\n",
            "It: 38873, Time: 0.02\n",
            "mse_b  [1.0141337]  mse_f: 1.836137294769287   total loss: [2.850271]\n",
            "mse_b ====== [0.952933967]\n",
            "It: 38874, Time: 0.02\n",
            "mse_b  [0.95293397]  mse_f: 1.6595633029937744   total loss: [2.6124973]\n",
            "mse_b ====== [0.805072427]\n",
            "It: 38875, Time: 0.02\n",
            "mse_b  [0.8050724]  mse_f: 1.3846797943115234   total loss: [2.189752]\n",
            "mse_b ====== [0.61723876]\n",
            "It: 38876, Time: 0.02\n",
            "mse_b  [0.61723876]  mse_f: 1.9008212089538574   total loss: [2.51806]\n",
            "mse_b ====== [0.531825781]\n",
            "It: 38877, Time: 0.02\n",
            "mse_b  [0.5318258]  mse_f: 1.8981850147247314   total loss: [2.4300108]\n",
            "mse_b ====== [0.735059917]\n",
            "It: 38878, Time: 0.02\n",
            "mse_b  [0.7350599]  mse_f: 2.0386388301849365   total loss: [2.7736988]\n",
            "mse_b ====== [0.76213789]\n",
            "It: 38879, Time: 0.02\n",
            "mse_b  [0.7621379]  mse_f: 1.7398762702941895   total loss: [2.5020142]\n",
            "mse_b ====== [0.773967266]\n",
            "It: 38880, Time: 0.02\n",
            "mse_b  [0.77396727]  mse_f: 1.4102375507354736   total loss: [2.1842048]\n",
            "mse_b ====== [0.753564715]\n",
            "It: 38881, Time: 0.02\n",
            "mse_b  [0.7535647]  mse_f: 1.7216787338256836   total loss: [2.4752436]\n",
            "mse_b ====== [0.737344384]\n",
            "It: 38882, Time: 0.02\n",
            "mse_b  [0.7373444]  mse_f: 1.5271235704421997   total loss: [2.264468]\n",
            "mse_b ====== [0.678090394]\n",
            "It: 38883, Time: 0.02\n",
            "mse_b  [0.6780904]  mse_f: 1.6035244464874268   total loss: [2.2816148]\n",
            "mse_b ====== [0.54241]\n",
            "It: 38884, Time: 0.02\n",
            "mse_b  [0.54241]  mse_f: 1.5615684986114502   total loss: [2.1039786]\n",
            "mse_b ====== [0.634112895]\n",
            "It: 38885, Time: 0.02\n",
            "mse_b  [0.6341129]  mse_f: 1.6162488460540771   total loss: [2.2503617]\n",
            "mse_b ====== [0.829894423]\n",
            "It: 38886, Time: 0.02\n",
            "mse_b  [0.8298944]  mse_f: 1.8917540311813354   total loss: [2.7216485]\n",
            "mse_b ====== [0.830279708]\n",
            "It: 38887, Time: 0.02\n",
            "mse_b  [0.8302797]  mse_f: 1.5690211057662964   total loss: [2.3993008]\n",
            "mse_b ====== [0.913263559]\n",
            "It: 38888, Time: 0.02\n",
            "mse_b  [0.91326356]  mse_f: 1.4325716495513916   total loss: [2.3458352]\n",
            "mse_b ====== [1.06878555]\n",
            "It: 38889, Time: 0.02\n",
            "mse_b  [1.0687855]  mse_f: 1.417472004890442   total loss: [2.4862576]\n",
            "mse_b ====== [0.933167219]\n",
            "It: 38890, Time: 0.02\n",
            "mse_b  [0.9331672]  mse_f: 1.4479248523712158   total loss: [2.381092]\n",
            "mse_b ====== [0.741571486]\n",
            "It: 38891, Time: 0.02\n",
            "mse_b  [0.7415715]  mse_f: 1.7284044027328491   total loss: [2.469976]\n",
            "mse_b ====== [0.783343673]\n",
            "It: 38892, Time: 0.02\n",
            "mse_b  [0.7833437]  mse_f: 1.3968958854675293   total loss: [2.1802397]\n",
            "mse_b ====== [0.858485]\n",
            "It: 38893, Time: 0.02\n",
            "mse_b  [0.858485]  mse_f: 1.249955177307129   total loss: [2.1084402]\n",
            "mse_b ====== [0.800443351]\n",
            "It: 38894, Time: 0.02\n",
            "mse_b  [0.80044335]  mse_f: 1.4629838466644287   total loss: [2.2634273]\n",
            "mse_b ====== [0.733732462]\n",
            "It: 38895, Time: 0.02\n",
            "mse_b  [0.73373246]  mse_f: 1.532530665397644   total loss: [2.266263]\n",
            "mse_b ====== [0.732060194]\n",
            "It: 38896, Time: 0.02\n",
            "mse_b  [0.7320602]  mse_f: 1.8546762466430664   total loss: [2.5867364]\n",
            "mse_b ====== [0.688509405]\n",
            "It: 38897, Time: 0.02\n",
            "mse_b  [0.6885094]  mse_f: 1.7825134992599487   total loss: [2.4710228]\n",
            "mse_b ====== [0.653957069]\n",
            "It: 38898, Time: 0.02\n",
            "mse_b  [0.65395707]  mse_f: 1.3941470384597778   total loss: [2.048104]\n",
            "mse_b ====== [0.759350657]\n",
            "It: 38899, Time: 0.02\n",
            "mse_b  [0.75935066]  mse_f: 1.4855594635009766   total loss: [2.2449102]\n",
            "mse_b ====== [0.775894582]\n",
            "It: 38900, Time: 0.02\n",
            "mse_b  [0.7758946]  mse_f: 1.2893232107162476   total loss: [2.0652177]\n",
            "mse_b ====== [0.584242344]\n",
            "It: 38901, Time: 0.02\n",
            "mse_b  [0.58424234]  mse_f: 1.3395261764526367   total loss: [1.9237685]\n",
            "mse_b ====== [0.504332542]\n",
            "It: 38902, Time: 0.02\n",
            "mse_b  [0.50433254]  mse_f: 1.4765148162841797   total loss: [1.9808474]\n",
            "mse_b ====== [0.676876187]\n",
            "It: 38903, Time: 0.02\n",
            "mse_b  [0.6768762]  mse_f: 1.2218042612075806   total loss: [1.8986804]\n",
            "mse_b ====== [0.723740399]\n",
            "It: 38904, Time: 0.02\n",
            "mse_b  [0.7237404]  mse_f: 1.418920636177063   total loss: [2.142661]\n",
            "mse_b ====== [0.522252]\n",
            "It: 38905, Time: 0.03\n",
            "mse_b  [0.522252]  mse_f: 1.4128648042678833   total loss: [1.9351168]\n",
            "mse_b ====== [0.46866861]\n",
            "It: 38906, Time: 0.02\n",
            "mse_b  [0.4686686]  mse_f: 1.4406448602676392   total loss: [1.9093134]\n",
            "mse_b ====== [0.577619314]\n",
            "It: 38907, Time: 0.02\n",
            "mse_b  [0.5776193]  mse_f: 1.42539644241333   total loss: [2.0030158]\n",
            "mse_b ====== [0.668296]\n",
            "It: 38908, Time: 0.02\n",
            "mse_b  [0.668296]  mse_f: 1.212992787361145   total loss: [1.8812888]\n",
            "mse_b ====== [0.811806]\n",
            "It: 38909, Time: 0.02\n",
            "mse_b  [0.811806]  mse_f: 1.5749588012695312   total loss: [2.3867648]\n",
            "mse_b ====== [0.836170197]\n",
            "It: 38910, Time: 0.02\n",
            "mse_b  [0.8361702]  mse_f: 1.3860528469085693   total loss: [2.222223]\n",
            "mse_b ====== [0.790110826]\n",
            "It: 38911, Time: 0.02\n",
            "mse_b  [0.7901108]  mse_f: 1.2931444644927979   total loss: [2.0832553]\n",
            "mse_b ====== [0.830054939]\n",
            "It: 38912, Time: 0.02\n",
            "mse_b  [0.83005494]  mse_f: 1.2966012954711914   total loss: [2.1266563]\n",
            "mse_b ====== [0.826554954]\n",
            "It: 38913, Time: 0.02\n",
            "mse_b  [0.82655495]  mse_f: 1.381777286529541   total loss: [2.2083323]\n",
            "mse_b ====== [0.75972712]\n",
            "It: 38914, Time: 0.02\n",
            "mse_b  [0.7597271]  mse_f: 1.5860174894332886   total loss: [2.3457446]\n",
            "mse_b ====== [0.733186245]\n",
            "It: 38915, Time: 0.02\n",
            "mse_b  [0.73318624]  mse_f: 1.312424898147583   total loss: [2.0456111]\n",
            "mse_b ====== [0.687737823]\n",
            "It: 38916, Time: 0.02\n",
            "mse_b  [0.6877378]  mse_f: 1.3680925369262695   total loss: [2.0558305]\n",
            "mse_b ====== [0.595234]\n",
            "It: 38917, Time: 0.02\n",
            "mse_b  [0.595234]  mse_f: 1.3745125532150269   total loss: [1.9697466]\n",
            "mse_b ====== [0.563497305]\n",
            "It: 38918, Time: 0.02\n",
            "mse_b  [0.5634973]  mse_f: 1.1875317096710205   total loss: [1.751029]\n",
            "mse_b ====== [0.617549658]\n",
            "It: 38919, Time: 0.02\n",
            "mse_b  [0.61754966]  mse_f: 1.4138267040252686   total loss: [2.0313764]\n",
            "mse_b ====== [0.616589904]\n",
            "It: 38920, Time: 0.02\n",
            "mse_b  [0.6165899]  mse_f: 1.1881963014602661   total loss: [1.8047862]\n",
            "mse_b ====== [0.584922791]\n",
            "It: 38921, Time: 0.02\n",
            "mse_b  [0.5849228]  mse_f: 1.1739025115966797   total loss: [1.7588253]\n",
            "mse_b ====== [0.640104175]\n",
            "It: 38922, Time: 0.02\n",
            "mse_b  [0.6401042]  mse_f: 1.3769690990447998   total loss: [2.0170732]\n",
            "mse_b ====== [0.722987652]\n",
            "It: 38923, Time: 0.02\n",
            "mse_b  [0.72298765]  mse_f: 1.207305669784546   total loss: [1.9302933]\n",
            "mse_b ====== [0.736739337]\n",
            "It: 38924, Time: 0.02\n",
            "mse_b  [0.73673934]  mse_f: 1.213548183441162   total loss: [1.9502876]\n",
            "mse_b ====== [0.776760697]\n",
            "It: 38925, Time: 0.02\n",
            "mse_b  [0.7767607]  mse_f: 1.0054022073745728   total loss: [1.7821629]\n",
            "mse_b ====== [0.859770417]\n",
            "It: 38926, Time: 0.02\n",
            "mse_b  [0.8597704]  mse_f: 0.9933971762657166   total loss: [1.8531675]\n",
            "mse_b ====== [0.737144947]\n",
            "It: 38927, Time: 0.02\n",
            "mse_b  [0.73714495]  mse_f: 1.2391750812530518   total loss: [1.97632]\n",
            "mse_b ====== [0.585819483]\n",
            "It: 38928, Time: 0.02\n",
            "mse_b  [0.5858195]  mse_f: 1.2955055236816406   total loss: [1.881325]\n",
            "mse_b ====== [0.582009852]\n",
            "It: 38929, Time: 0.02\n",
            "mse_b  [0.58200985]  mse_f: 1.4068901538848877   total loss: [1.9889]\n",
            "mse_b ====== [0.501360953]\n",
            "It: 38930, Time: 0.02\n",
            "mse_b  [0.50136095]  mse_f: 1.178743600845337   total loss: [1.6801045]\n",
            "mse_b ====== [0.531819105]\n",
            "It: 38931, Time: 0.02\n",
            "mse_b  [0.5318191]  mse_f: 1.075705647468567   total loss: [1.6075248]\n",
            "mse_b ====== [0.625159681]\n",
            "It: 38932, Time: 0.02\n",
            "mse_b  [0.6251597]  mse_f: 1.2932639122009277   total loss: [1.9184237]\n",
            "mse_b ====== [0.518956065]\n",
            "It: 38933, Time: 0.02\n",
            "mse_b  [0.51895607]  mse_f: 1.2149142026901245   total loss: [1.7338703]\n",
            "mse_b ====== [0.511232257]\n",
            "It: 38934, Time: 0.02\n",
            "mse_b  [0.51123226]  mse_f: 1.2452003955841064   total loss: [1.7564327]\n",
            "mse_b ====== [0.580253899]\n",
            "It: 38935, Time: 0.02\n",
            "mse_b  [0.5802539]  mse_f: 1.3510942459106445   total loss: [1.9313481]\n",
            "mse_b ====== [0.481861115]\n",
            "It: 38936, Time: 0.02\n",
            "mse_b  [0.4818611]  mse_f: 1.2532813549041748   total loss: [1.7351425]\n",
            "mse_b ====== [0.487086624]\n",
            "It: 38937, Time: 0.03\n",
            "mse_b  [0.48708662]  mse_f: 1.3051468133926392   total loss: [1.7922335]\n",
            "mse_b ====== [0.540630221]\n",
            "It: 38938, Time: 0.02\n",
            "mse_b  [0.5406302]  mse_f: 1.1796807050704956   total loss: [1.7203109]\n",
            "mse_b ====== [0.435210228]\n",
            "It: 38939, Time: 0.02\n",
            "mse_b  [0.43521023]  mse_f: 1.263586163520813   total loss: [1.6987964]\n",
            "mse_b ====== [0.353764266]\n",
            "It: 38940, Time: 0.02\n",
            "mse_b  [0.35376427]  mse_f: 1.5278615951538086   total loss: [1.8816259]\n",
            "mse_b ====== [0.337296158]\n",
            "It: 38941, Time: 0.02\n",
            "mse_b  [0.33729616]  mse_f: 1.4675822257995605   total loss: [1.8048784]\n",
            "mse_b ====== [0.300068974]\n",
            "It: 38942, Time: 0.02\n",
            "mse_b  [0.30006897]  mse_f: 1.502328872680664   total loss: [1.8023978]\n",
            "mse_b ====== [0.306492984]\n",
            "It: 38943, Time: 0.02\n",
            "mse_b  [0.30649298]  mse_f: 1.2281314134597778   total loss: [1.5346243]\n",
            "mse_b ====== [0.396333784]\n",
            "It: 38944, Time: 0.02\n",
            "mse_b  [0.39633378]  mse_f: 1.0233858823776245   total loss: [1.4197197]\n",
            "mse_b ====== [0.472349]\n",
            "It: 38945, Time: 0.02\n",
            "mse_b  [0.472349]  mse_f: 1.1625715494155884   total loss: [1.6349206]\n",
            "mse_b ====== [0.448500246]\n",
            "It: 38946, Time: 0.03\n",
            "mse_b  [0.44850025]  mse_f: 0.9863986968994141   total loss: [1.434899]\n",
            "mse_b ====== [0.374495387]\n",
            "It: 38947, Time: 0.02\n",
            "mse_b  [0.3744954]  mse_f: 1.0566644668579102   total loss: [1.4311599]\n",
            "mse_b ====== [0.337155461]\n",
            "It: 38948, Time: 0.02\n",
            "mse_b  [0.33715546]  mse_f: 1.2263681888580322   total loss: [1.5635237]\n",
            "mse_b ====== [0.328473181]\n",
            "It: 38949, Time: 0.02\n",
            "mse_b  [0.32847318]  mse_f: 1.1727499961853027   total loss: [1.5012232]\n",
            "mse_b ====== [0.312499523]\n",
            "It: 38950, Time: 0.02\n",
            "mse_b  [0.31249952]  mse_f: 1.270015001296997   total loss: [1.5825145]\n",
            "mse_b ====== [0.312390029]\n",
            "It: 38951, Time: 0.02\n",
            "mse_b  [0.31239003]  mse_f: 1.1451244354248047   total loss: [1.4575145]\n",
            "mse_b ====== [0.333483219]\n",
            "It: 38952, Time: 0.02\n",
            "mse_b  [0.33348322]  mse_f: 1.27462637424469   total loss: [1.6081096]\n",
            "mse_b ====== [0.32323733]\n",
            "It: 38953, Time: 0.02\n",
            "mse_b  [0.32323733]  mse_f: 1.2917859554290771   total loss: [1.6150233]\n",
            "mse_b ====== [0.399550617]\n",
            "It: 38954, Time: 0.02\n",
            "mse_b  [0.39955062]  mse_f: 1.1142631769180298   total loss: [1.5138137]\n",
            "mse_b ====== [0.436588377]\n",
            "It: 38955, Time: 0.02\n",
            "mse_b  [0.43658838]  mse_f: 1.1571924686431885   total loss: [1.5937809]\n",
            "mse_b ====== [0.317558527]\n",
            "It: 38956, Time: 0.02\n",
            "mse_b  [0.31755853]  mse_f: 1.008406400680542   total loss: [1.3259649]\n",
            "mse_b ====== [0.383629233]\n",
            "It: 38957, Time: 0.02\n",
            "mse_b  [0.38362923]  mse_f: 1.1052011251449585   total loss: [1.4888303]\n",
            "mse_b ====== [0.482706696]\n",
            "It: 38958, Time: 0.02\n",
            "mse_b  [0.4827067]  mse_f: 1.142737865447998   total loss: [1.6254445]\n",
            "mse_b ====== [0.375587285]\n",
            "It: 38959, Time: 0.02\n",
            "mse_b  [0.37558728]  mse_f: 1.1313966512680054   total loss: [1.506984]\n",
            "mse_b ====== [0.338640213]\n",
            "It: 38960, Time: 0.02\n",
            "mse_b  [0.3386402]  mse_f: 1.2294403314590454   total loss: [1.5680805]\n",
            "mse_b ====== [0.377025634]\n",
            "It: 38961, Time: 0.02\n",
            "mse_b  [0.37702563]  mse_f: 1.0501010417938232   total loss: [1.4271266]\n",
            "mse_b ====== [0.352097183]\n",
            "It: 38962, Time: 0.02\n",
            "mse_b  [0.35209718]  mse_f: 1.1135014295578003   total loss: [1.4655986]\n",
            "mse_b ====== [0.339085817]\n",
            "It: 38963, Time: 0.02\n",
            "mse_b  [0.33908582]  mse_f: 1.0668760538101196   total loss: [1.4059619]\n",
            "mse_b ====== [0.368920505]\n",
            "It: 38964, Time: 0.02\n",
            "mse_b  [0.3689205]  mse_f: 0.9837188720703125   total loss: [1.3526394]\n",
            "mse_b ====== [0.392567486]\n",
            "It: 38965, Time: 0.02\n",
            "mse_b  [0.3925675]  mse_f: 1.1032042503356934   total loss: [1.4957718]\n",
            "mse_b ====== [0.382311285]\n",
            "It: 38966, Time: 0.02\n",
            "mse_b  [0.38231128]  mse_f: 0.9569240808486938   total loss: [1.3392353]\n",
            "mse_b ====== [0.37244454]\n",
            "It: 38967, Time: 0.02\n",
            "mse_b  [0.37244454]  mse_f: 1.0809050798416138   total loss: [1.4533496]\n",
            "mse_b ====== [0.314662]\n",
            "It: 38968, Time: 0.02\n",
            "mse_b  [0.314662]  mse_f: 1.0630011558532715   total loss: [1.3776631]\n",
            "mse_b ====== [0.240393579]\n",
            "It: 38969, Time: 0.02\n",
            "mse_b  [0.24039358]  mse_f: 0.9460768699645996   total loss: [1.1864705]\n",
            "mse_b ====== [0.267243207]\n",
            "It: 38970, Time: 0.02\n",
            "mse_b  [0.2672432]  mse_f: 1.1282258033752441   total loss: [1.395469]\n",
            "mse_b ====== [0.377973139]\n",
            "It: 38971, Time: 0.02\n",
            "mse_b  [0.37797314]  mse_f: 1.0182991027832031   total loss: [1.3962722]\n",
            "mse_b ====== [0.438777328]\n",
            "It: 38972, Time: 0.02\n",
            "mse_b  [0.43877733]  mse_f: 1.0368865728378296   total loss: [1.4756639]\n",
            "mse_b ====== [0.384898305]\n",
            "It: 38973, Time: 0.02\n",
            "mse_b  [0.3848983]  mse_f: 1.0841498374938965   total loss: [1.4690481]\n",
            "mse_b ====== [0.307303071]\n",
            "It: 38974, Time: 0.02\n",
            "mse_b  [0.30730307]  mse_f: 1.0590651035308838   total loss: [1.3663682]\n",
            "mse_b ====== [0.229789287]\n",
            "It: 38975, Time: 0.02\n",
            "mse_b  [0.22978929]  mse_f: 1.1464385986328125   total loss: [1.3762279]\n",
            "mse_b ====== [0.248936489]\n",
            "It: 38976, Time: 0.02\n",
            "mse_b  [0.24893649]  mse_f: 0.9663349390029907   total loss: [1.2152715]\n",
            "mse_b ====== [0.378096461]\n",
            "It: 38977, Time: 0.02\n",
            "mse_b  [0.37809646]  mse_f: 1.0133559703826904   total loss: [1.3914524]\n",
            "mse_b ====== [0.314863592]\n",
            "It: 38978, Time: 0.02\n",
            "mse_b  [0.3148636]  mse_f: 1.0081632137298584   total loss: [1.3230268]\n",
            "mse_b ====== [0.29081288]\n",
            "It: 38979, Time: 0.02\n",
            "mse_b  [0.29081288]  mse_f: 0.930730402469635   total loss: [1.2215433]\n",
            "mse_b ====== [0.461442471]\n",
            "It: 38980, Time: 0.02\n",
            "mse_b  [0.46144247]  mse_f: 0.9750851392745972   total loss: [1.4365276]\n",
            "mse_b ====== [0.425746202]\n",
            "It: 38981, Time: 0.02\n",
            "mse_b  [0.4257462]  mse_f: 0.8935366868972778   total loss: [1.3192829]\n",
            "mse_b ====== [0.304077744]\n",
            "It: 38982, Time: 0.02\n",
            "mse_b  [0.30407774]  mse_f: 1.0687587261199951   total loss: [1.3728365]\n",
            "mse_b ====== [0.276165456]\n",
            "It: 38983, Time: 0.02\n",
            "mse_b  [0.27616546]  mse_f: 1.1012513637542725   total loss: [1.3774168]\n",
            "mse_b ====== [0.227129668]\n",
            "It: 38984, Time: 0.03\n",
            "mse_b  [0.22712967]  mse_f: 1.121110439300537   total loss: [1.3482401]\n",
            "mse_b ====== [0.190903991]\n",
            "It: 38985, Time: 0.02\n",
            "mse_b  [0.19090399]  mse_f: 1.2223749160766602   total loss: [1.4132789]\n",
            "mse_b ====== [0.198224723]\n",
            "It: 38986, Time: 0.02\n",
            "mse_b  [0.19822472]  mse_f: 1.0087025165557861   total loss: [1.2069273]\n",
            "mse_b ====== [0.229825601]\n",
            "It: 38987, Time: 0.02\n",
            "mse_b  [0.2298256]  mse_f: 0.9990986585617065   total loss: [1.2289243]\n",
            "mse_b ====== [0.274285972]\n",
            "It: 38988, Time: 0.02\n",
            "mse_b  [0.27428597]  mse_f: 0.9061751961708069   total loss: [1.1804612]\n",
            "mse_b ====== [0.320392311]\n",
            "It: 38989, Time: 0.02\n",
            "mse_b  [0.3203923]  mse_f: 0.8834003210067749   total loss: [1.2037926]\n",
            "mse_b ====== [0.323354334]\n",
            "It: 38990, Time: 0.03\n",
            "mse_b  [0.32335433]  mse_f: 1.0478628873825073   total loss: [1.3712173]\n",
            "mse_b ====== [0.256429434]\n",
            "It: 38991, Time: 0.02\n",
            "mse_b  [0.25642943]  mse_f: 0.9255901575088501   total loss: [1.1820196]\n",
            "mse_b ====== [0.22328499]\n",
            "It: 38992, Time: 0.02\n",
            "mse_b  [0.22328499]  mse_f: 1.0348639488220215   total loss: [1.2581489]\n",
            "mse_b ====== [0.225302696]\n",
            "It: 38993, Time: 0.02\n",
            "mse_b  [0.2253027]  mse_f: 0.9991036057472229   total loss: [1.2244062]\n",
            "mse_b ====== [0.195849895]\n",
            "It: 38994, Time: 0.02\n",
            "mse_b  [0.1958499]  mse_f: 0.9684065580368042   total loss: [1.1642565]\n",
            "mse_b ====== [0.169119924]\n",
            "It: 38995, Time: 0.02\n",
            "mse_b  [0.16911992]  mse_f: 1.0486950874328613   total loss: [1.217815]\n",
            "mse_b ====== [0.207132638]\n",
            "It: 38996, Time: 0.02\n",
            "mse_b  [0.20713264]  mse_f: 0.9673201441764832   total loss: [1.1744528]\n",
            "mse_b ====== [0.246521533]\n",
            "It: 38997, Time: 0.02\n",
            "mse_b  [0.24652153]  mse_f: 1.0901505947113037   total loss: [1.3366721]\n",
            "mse_b ====== [0.240579754]\n",
            "It: 38998, Time: 0.02\n",
            "mse_b  [0.24057975]  mse_f: 0.9459124803543091   total loss: [1.1864922]\n",
            "mse_b ====== [0.297404051]\n",
            "It: 38999, Time: 0.02\n",
            "mse_b  [0.29740405]  mse_f: 0.9152674078941345   total loss: [1.2126715]\n",
            "mse_b ====== [0.239448383]\n",
            "It: 39000, Time: 0.02\n",
            "mse_b  [0.23944838]  mse_f: 0.9499354362487793   total loss: [1.1893839]\n",
            "mse_b ====== [0.168291748]\n",
            "It: 39001, Time: 0.02\n",
            "mse_b  [0.16829175]  mse_f: 0.9361950159072876   total loss: [1.1044867]\n",
            "mse_b ====== [0.220489621]\n",
            "It: 39002, Time: 0.02\n",
            "mse_b  [0.22048962]  mse_f: 1.0290377140045166   total loss: [1.2495273]\n",
            "mse_b ====== [0.219909519]\n",
            "It: 39003, Time: 0.02\n",
            "mse_b  [0.21990952]  mse_f: 0.9030479788780212   total loss: [1.1229575]\n",
            "mse_b ====== [0.258574456]\n",
            "It: 39004, Time: 0.03\n",
            "mse_b  [0.25857446]  mse_f: 0.9479682445526123   total loss: [1.2065427]\n",
            "mse_b ====== [0.293201834]\n",
            "It: 39005, Time: 0.02\n",
            "mse_b  [0.29320183]  mse_f: 0.8473643660545349   total loss: [1.1405662]\n",
            "mse_b ====== [0.243224517]\n",
            "It: 39006, Time: 0.02\n",
            "mse_b  [0.24322452]  mse_f: 0.8406755328178406   total loss: [1.0839001]\n",
            "mse_b ====== [0.234903559]\n",
            "It: 39007, Time: 0.02\n",
            "mse_b  [0.23490356]  mse_f: 0.9336481690406799   total loss: [1.1685517]\n",
            "mse_b ====== [0.243366078]\n",
            "It: 39008, Time: 0.02\n",
            "mse_b  [0.24336608]  mse_f: 0.8850171566009521   total loss: [1.1283833]\n",
            "mse_b ====== [0.209597617]\n",
            "It: 39009, Time: 0.02\n",
            "mse_b  [0.20959762]  mse_f: 0.999184787273407   total loss: [1.2087824]\n",
            "mse_b ====== [0.221581087]\n",
            "It: 39010, Time: 0.02\n",
            "mse_b  [0.22158109]  mse_f: 0.8593988418579102   total loss: [1.08098]\n",
            "mse_b ====== [0.263938189]\n",
            "It: 39011, Time: 0.02\n",
            "mse_b  [0.2639382]  mse_f: 0.8529835939407349   total loss: [1.1169218]\n",
            "mse_b ====== [0.253859341]\n",
            "It: 39012, Time: 0.02\n",
            "mse_b  [0.25385934]  mse_f: 0.8469041585922241   total loss: [1.1007636]\n",
            "mse_b ====== [0.239774346]\n",
            "It: 39013, Time: 0.02\n",
            "mse_b  [0.23977435]  mse_f: 0.7313475608825684   total loss: [0.9711219]\n",
            "mse_b ====== [0.277932614]\n",
            "It: 39014, Time: 0.02\n",
            "mse_b  [0.2779326]  mse_f: 0.8989715576171875   total loss: [1.1769042]\n",
            "mse_b ====== [0.278730422]\n",
            "It: 39015, Time: 0.02\n",
            "mse_b  [0.27873042]  mse_f: 0.8504781723022461   total loss: [1.1292086]\n",
            "mse_b ====== [0.264165223]\n",
            "It: 39016, Time: 0.02\n",
            "mse_b  [0.26416522]  mse_f: 0.9572688341140747   total loss: [1.2214341]\n",
            "mse_b ====== [0.260460228]\n",
            "It: 39017, Time: 0.02\n",
            "mse_b  [0.26046023]  mse_f: 0.8451815843582153   total loss: [1.1056418]\n",
            "mse_b ====== [0.278381974]\n",
            "It: 39018, Time: 0.02\n",
            "mse_b  [0.27838197]  mse_f: 0.7796781063079834   total loss: [1.05806]\n",
            "mse_b ====== [0.291960388]\n",
            "It: 39019, Time: 0.02\n",
            "mse_b  [0.2919604]  mse_f: 0.8294477462768555   total loss: [1.1214081]\n",
            "mse_b ====== [0.293913037]\n",
            "It: 39020, Time: 0.02\n",
            "mse_b  [0.29391304]  mse_f: 0.7156274318695068   total loss: [1.0095404]\n",
            "mse_b ====== [0.241105258]\n",
            "It: 39021, Time: 0.02\n",
            "mse_b  [0.24110526]  mse_f: 0.811980128288269   total loss: [1.0530853]\n",
            "mse_b ====== [0.199015528]\n",
            "It: 39022, Time: 0.03\n",
            "mse_b  [0.19901553]  mse_f: 0.8241291046142578   total loss: [1.0231446]\n",
            "mse_b ====== [0.208795816]\n",
            "It: 39023, Time: 0.03\n",
            "mse_b  [0.20879582]  mse_f: 0.8915817737579346   total loss: [1.1003776]\n",
            "mse_b ====== [0.177058294]\n",
            "It: 39024, Time: 0.02\n",
            "mse_b  [0.1770583]  mse_f: 0.8419938087463379   total loss: [1.0190521]\n",
            "mse_b ====== [0.232429653]\n",
            "It: 39025, Time: 0.02\n",
            "mse_b  [0.23242965]  mse_f: 0.7486998438835144   total loss: [0.9811295]\n",
            "mse_b ====== [0.304276854]\n",
            "It: 39026, Time: 0.02\n",
            "mse_b  [0.30427685]  mse_f: 0.7842379212379456   total loss: [1.0885148]\n",
            "mse_b ====== [0.284941167]\n",
            "It: 39027, Time: 0.02\n",
            "mse_b  [0.28494117]  mse_f: 0.7249279618263245   total loss: [1.0098691]\n",
            "mse_b ====== [0.284082264]\n",
            "It: 39028, Time: 0.02\n",
            "mse_b  [0.28408226]  mse_f: 0.7876600027084351   total loss: [1.0717423]\n",
            "mse_b ====== [0.212423831]\n",
            "It: 39029, Time: 0.02\n",
            "mse_b  [0.21242383]  mse_f: 0.7180753946304321   total loss: [0.9304992]\n",
            "mse_b ====== [0.182389393]\n",
            "It: 39030, Time: 0.02\n",
            "mse_b  [0.1823894]  mse_f: 0.7682855725288391   total loss: [0.95067495]\n",
            "mse_b ====== [0.18025957]\n",
            "It: 39031, Time: 0.02\n",
            "mse_b  [0.18025957]  mse_f: 0.8179712295532227   total loss: [0.9982308]\n",
            "mse_b ====== [0.153594524]\n",
            "It: 39032, Time: 0.02\n",
            "mse_b  [0.15359452]  mse_f: 0.7197475433349609   total loss: [0.87334204]\n",
            "mse_b ====== [0.217988282]\n",
            "It: 39033, Time: 0.02\n",
            "mse_b  [0.21798828]  mse_f: 0.7874718904495239   total loss: [1.0054601]\n",
            "mse_b ====== [0.226504192]\n",
            "It: 39034, Time: 0.02\n",
            "mse_b  [0.22650419]  mse_f: 0.7104288339614868   total loss: [0.93693304]\n",
            "mse_b ====== [0.23052603]\n",
            "It: 39035, Time: 0.02\n",
            "mse_b  [0.23052603]  mse_f: 0.7554687261581421   total loss: [0.98599476]\n",
            "mse_b ====== [0.18894732]\n",
            "It: 39036, Time: 0.02\n",
            "mse_b  [0.18894732]  mse_f: 0.7171258926391602   total loss: [0.9060732]\n",
            "mse_b ====== [0.135491356]\n",
            "It: 39037, Time: 0.02\n",
            "mse_b  [0.13549136]  mse_f: 0.7634459137916565   total loss: [0.8989373]\n",
            "mse_b ====== [0.166010946]\n",
            "It: 39038, Time: 0.02\n",
            "mse_b  [0.16601095]  mse_f: 0.8323100805282593   total loss: [0.99832106]\n",
            "mse_b ====== [0.159359545]\n",
            "It: 39039, Time: 0.02\n",
            "mse_b  [0.15935954]  mse_f: 0.814712405204773   total loss: [0.974072]\n",
            "mse_b ====== [0.146695316]\n",
            "It: 39040, Time: 0.02\n",
            "mse_b  [0.14669532]  mse_f: 0.9115360975265503   total loss: [1.0582314]\n",
            "mse_b ====== [0.169458956]\n",
            "It: 39041, Time: 0.02\n",
            "mse_b  [0.16945896]  mse_f: 0.7629959583282471   total loss: [0.93245494]\n",
            "mse_b ====== [0.204973966]\n",
            "It: 39042, Time: 0.02\n",
            "mse_b  [0.20497397]  mse_f: 0.7899479269981384   total loss: [0.9949219]\n",
            "mse_b ====== [0.217342585]\n",
            "It: 39043, Time: 0.02\n",
            "mse_b  [0.21734259]  mse_f: 0.7079735994338989   total loss: [0.9253162]\n",
            "mse_b ====== [0.231657013]\n",
            "It: 39044, Time: 0.02\n",
            "mse_b  [0.23165701]  mse_f: 0.6722618937492371   total loss: [0.9039189]\n",
            "mse_b ====== [0.209609732]\n",
            "It: 39045, Time: 0.02\n",
            "mse_b  [0.20960973]  mse_f: 0.7250826358795166   total loss: [0.9346924]\n",
            "mse_b ====== [0.160847843]\n",
            "It: 39046, Time: 0.02\n",
            "mse_b  [0.16084784]  mse_f: 0.681995153427124   total loss: [0.842843]\n",
            "mse_b ====== [0.156493306]\n",
            "It: 39047, Time: 0.02\n",
            "mse_b  [0.1564933]  mse_f: 0.7895397543907166   total loss: [0.94603306]\n",
            "mse_b ====== [0.155503258]\n",
            "It: 39048, Time: 0.02\n",
            "mse_b  [0.15550326]  mse_f: 0.710729718208313   total loss: [0.866233]\n",
            "mse_b ====== [0.162659019]\n",
            "It: 39049, Time: 0.03\n",
            "mse_b  [0.16265902]  mse_f: 0.7224836945533752   total loss: [0.8851427]\n",
            "mse_b ====== [0.177818984]\n",
            "It: 39050, Time: 0.02\n",
            "mse_b  [0.17781898]  mse_f: 0.7346053123474121   total loss: [0.9124243]\n",
            "mse_b ====== [0.22207275]\n",
            "It: 39051, Time: 0.02\n",
            "mse_b  [0.22207275]  mse_f: 0.7423633933067322   total loss: [0.9644362]\n",
            "mse_b ====== [0.206473053]\n",
            "It: 39052, Time: 0.02\n",
            "mse_b  [0.20647305]  mse_f: 0.7924070358276367   total loss: [0.9988801]\n",
            "mse_b ====== [0.148689434]\n",
            "It: 39053, Time: 0.02\n",
            "mse_b  [0.14868943]  mse_f: 0.7288612127304077   total loss: [0.87755066]\n",
            "mse_b ====== [0.145613477]\n",
            "It: 39054, Time: 0.02\n",
            "mse_b  [0.14561348]  mse_f: 0.7911437153816223   total loss: [0.9367572]\n",
            "mse_b ====== [0.110542715]\n",
            "It: 39055, Time: 0.02\n",
            "mse_b  [0.11054271]  mse_f: 0.7392734885215759   total loss: [0.8498162]\n",
            "mse_b ====== [0.158686653]\n",
            "It: 39056, Time: 0.02\n",
            "mse_b  [0.15868665]  mse_f: 0.7531447410583496   total loss: [0.9118314]\n",
            "mse_b ====== [0.208564043]\n",
            "It: 39057, Time: 0.02\n",
            "mse_b  [0.20856404]  mse_f: 0.7272261381149292   total loss: [0.9357902]\n",
            "mse_b ====== [0.23732084]\n",
            "It: 39058, Time: 0.02\n",
            "mse_b  [0.23732084]  mse_f: 0.6882987022399902   total loss: [0.92561954]\n",
            "mse_b ====== [0.254638731]\n",
            "It: 39059, Time: 0.02\n",
            "mse_b  [0.25463873]  mse_f: 0.7040314674377441   total loss: [0.9586702]\n",
            "mse_b ====== [0.166541412]\n",
            "It: 39060, Time: 0.03\n",
            "mse_b  [0.16654141]  mse_f: 0.6667864322662354   total loss: [0.83332783]\n",
            "mse_b ====== [0.122830138]\n",
            "It: 39061, Time: 0.03\n",
            "mse_b  [0.12283014]  mse_f: 0.7545207142829895   total loss: [0.87735087]\n",
            "mse_b ====== [0.123943128]\n",
            "It: 39062, Time: 0.03\n",
            "mse_b  [0.12394313]  mse_f: 0.7070045471191406   total loss: [0.8309477]\n",
            "mse_b ====== [0.113690861]\n",
            "It: 39063, Time: 0.02\n",
            "mse_b  [0.11369086]  mse_f: 0.7574763298034668   total loss: [0.8711672]\n",
            "mse_b ====== [0.125792667]\n",
            "It: 39064, Time: 0.02\n",
            "mse_b  [0.12579267]  mse_f: 0.7531338334083557   total loss: [0.8789265]\n",
            "mse_b ====== [0.153274894]\n",
            "It: 39065, Time: 0.02\n",
            "mse_b  [0.1532749]  mse_f: 0.7010812759399414   total loss: [0.85435617]\n",
            "mse_b ====== [0.137215793]\n",
            "It: 39066, Time: 0.02\n",
            "mse_b  [0.1372158]  mse_f: 0.7049824595451355   total loss: [0.84219825]\n",
            "mse_b ====== [0.102210879]\n",
            "It: 39067, Time: 0.02\n",
            "mse_b  [0.10221088]  mse_f: 0.6581493020057678   total loss: [0.7603602]\n",
            "mse_b ====== [0.127511308]\n",
            "It: 39068, Time: 0.02\n",
            "mse_b  [0.12751131]  mse_f: 0.7259334325790405   total loss: [0.85344476]\n",
            "mse_b ====== [0.120403081]\n",
            "It: 39069, Time: 0.03\n",
            "mse_b  [0.12040308]  mse_f: 0.6821787357330322   total loss: [0.8025818]\n",
            "mse_b ====== [0.1386538]\n",
            "It: 39070, Time: 0.02\n",
            "mse_b  [0.1386538]  mse_f: 0.7174402475357056   total loss: [0.85609406]\n",
            "mse_b ====== [0.181048051]\n",
            "It: 39071, Time: 0.02\n",
            "mse_b  [0.18104805]  mse_f: 0.6757323741912842   total loss: [0.8567804]\n",
            "mse_b ====== [0.192078188]\n",
            "It: 39072, Time: 0.02\n",
            "mse_b  [0.19207819]  mse_f: 0.646192193031311   total loss: [0.83827037]\n",
            "mse_b ====== [0.161193699]\n",
            "It: 39073, Time: 0.02\n",
            "mse_b  [0.1611937]  mse_f: 0.7036399841308594   total loss: [0.8648337]\n",
            "mse_b ====== [0.113890558]\n",
            "It: 39074, Time: 0.02\n",
            "mse_b  [0.11389056]  mse_f: 0.6915570497512817   total loss: [0.8054476]\n",
            "mse_b ====== [0.101515844]\n",
            "It: 39075, Time: 0.02\n",
            "mse_b  [0.10151584]  mse_f: 0.770892858505249   total loss: [0.8724087]\n",
            "mse_b ====== [0.103767335]\n",
            "It: 39076, Time: 0.02\n",
            "mse_b  [0.10376734]  mse_f: 0.7329902648925781   total loss: [0.8367576]\n",
            "mse_b ====== [0.104236066]\n",
            "It: 39077, Time: 0.02\n",
            "mse_b  [0.10423607]  mse_f: 0.7165898680686951   total loss: [0.82082593]\n",
            "mse_b ====== [0.107172564]\n",
            "It: 39078, Time: 0.03\n",
            "mse_b  [0.10717256]  mse_f: 0.6884756088256836   total loss: [0.79564816]\n",
            "mse_b ====== [0.139544919]\n",
            "It: 39079, Time: 0.04\n",
            "mse_b  [0.13954492]  mse_f: 0.6436468362808228   total loss: [0.78319174]\n",
            "mse_b ====== [0.132179707]\n",
            "It: 39080, Time: 0.02\n",
            "mse_b  [0.1321797]  mse_f: 0.6641684770584106   total loss: [0.7963482]\n",
            "mse_b ====== [0.0886299759]\n",
            "It: 39081, Time: 0.02\n",
            "mse_b  [0.08862998]  mse_f: 0.6917511224746704   total loss: [0.7803811]\n",
            "mse_b ====== [0.0909137726]\n",
            "It: 39082, Time: 0.02\n",
            "mse_b  [0.09091377]  mse_f: 0.767139196395874   total loss: [0.85805297]\n",
            "mse_b ====== [0.0731379241]\n",
            "It: 39083, Time: 0.02\n",
            "mse_b  [0.07313792]  mse_f: 0.6949977874755859   total loss: [0.7681357]\n",
            "mse_b ====== [0.127461538]\n",
            "It: 39084, Time: 0.02\n",
            "mse_b  [0.12746154]  mse_f: 0.659880518913269   total loss: [0.7873421]\n",
            "mse_b ====== [0.1696257]\n",
            "It: 39085, Time: 0.02\n",
            "mse_b  [0.1696257]  mse_f: 0.5847244262695312   total loss: [0.7543501]\n",
            "mse_b ====== [0.156237543]\n",
            "It: 39086, Time: 0.02\n",
            "mse_b  [0.15623754]  mse_f: 0.5888640284538269   total loss: [0.7451016]\n",
            "mse_b ====== [0.118798375]\n",
            "It: 39087, Time: 0.02\n",
            "mse_b  [0.11879838]  mse_f: 0.6129465103149414   total loss: [0.7317449]\n",
            "mse_b ====== [0.103620604]\n",
            "It: 39088, Time: 0.03\n",
            "mse_b  [0.1036206]  mse_f: 0.632614016532898   total loss: [0.7362346]\n",
            "mse_b ====== [0.111217961]\n",
            "It: 39089, Time: 0.02\n",
            "mse_b  [0.11121796]  mse_f: 0.6610066890716553   total loss: [0.77222466]\n",
            "mse_b ====== [0.0871691853]\n",
            "It: 39090, Time: 0.02\n",
            "mse_b  [0.08716919]  mse_f: 0.6019423007965088   total loss: [0.6891115]\n",
            "mse_b ====== [0.141036898]\n",
            "It: 39091, Time: 0.02\n",
            "mse_b  [0.1410369]  mse_f: 0.606801450252533   total loss: [0.7478384]\n",
            "mse_b ====== [0.188993961]\n",
            "It: 39092, Time: 0.02\n",
            "mse_b  [0.18899396]  mse_f: 0.5734251737594604   total loss: [0.7624191]\n",
            "mse_b ====== [0.153769284]\n",
            "It: 39093, Time: 0.02\n",
            "mse_b  [0.15376928]  mse_f: 0.6104736924171448   total loss: [0.764243]\n",
            "mse_b ====== [0.122892074]\n",
            "It: 39094, Time: 0.02\n",
            "mse_b  [0.12289207]  mse_f: 0.6287556290626526   total loss: [0.7516477]\n",
            "mse_b ====== [0.130360439]\n",
            "It: 39095, Time: 0.03\n",
            "mse_b  [0.13036044]  mse_f: 0.658542275428772   total loss: [0.7889027]\n",
            "mse_b ====== [0.111588508]\n",
            "It: 39096, Time: 0.03\n",
            "mse_b  [0.11158851]  mse_f: 0.6379998326301575   total loss: [0.7495884]\n",
            "mse_b ====== [0.137497306]\n",
            "It: 39097, Time: 0.02\n",
            "mse_b  [0.1374973]  mse_f: 0.5829398036003113   total loss: [0.7204371]\n",
            "mse_b ====== [0.172414765]\n",
            "It: 39098, Time: 0.02\n",
            "mse_b  [0.17241476]  mse_f: 0.6167119741439819   total loss: [0.78912675]\n",
            "mse_b ====== [0.147527322]\n",
            "It: 39099, Time: 0.02\n",
            "mse_b  [0.14752732]  mse_f: 0.5970081090927124   total loss: [0.74453545]\n",
            "mse_b ====== [0.120008066]\n",
            "It: 39100, Time: 0.02\n",
            "mse_b  [0.12000807]  mse_f: 0.6337825059890747   total loss: [0.75379056]\n",
            "mse_b ====== [0.115335844]\n",
            "It: 39101, Time: 0.02\n",
            "mse_b  [0.11533584]  mse_f: 0.6233022809028625   total loss: [0.7386381]\n",
            "mse_b ====== [0.0911506638]\n",
            "It: 39102, Time: 0.02\n",
            "mse_b  [0.09115066]  mse_f: 0.6309773921966553   total loss: [0.72212803]\n",
            "mse_b ====== [0.0831017792]\n",
            "It: 39103, Time: 0.02\n",
            "mse_b  [0.08310178]  mse_f: 0.6080312132835388   total loss: [0.691133]\n",
            "mse_b ====== [0.110934041]\n",
            "It: 39104, Time: 0.02\n",
            "mse_b  [0.11093404]  mse_f: 0.5973768830299377   total loss: [0.7083109]\n",
            "mse_b ====== [0.12386395]\n",
            "It: 39105, Time: 0.02\n",
            "mse_b  [0.12386395]  mse_f: 0.6240612268447876   total loss: [0.74792516]\n",
            "mse_b ====== [0.100287683]\n",
            "It: 39106, Time: 0.02\n",
            "mse_b  [0.10028768]  mse_f: 0.6133866310119629   total loss: [0.7136743]\n",
            "mse_b ====== [0.0745373294]\n",
            "It: 39107, Time: 0.03\n",
            "mse_b  [0.07453733]  mse_f: 0.6548430919647217   total loss: [0.7293804]\n",
            "mse_b ====== [0.0954131782]\n",
            "It: 39108, Time: 0.02\n",
            "mse_b  [0.09541318]  mse_f: 0.6155925989151001   total loss: [0.7110058]\n",
            "mse_b ====== [0.103609525]\n",
            "It: 39109, Time: 0.02\n",
            "mse_b  [0.10360952]  mse_f: 0.5937423706054688   total loss: [0.6973519]\n",
            "mse_b ====== [0.100956395]\n",
            "It: 39110, Time: 0.02\n",
            "mse_b  [0.1009564]  mse_f: 0.5593217611312866   total loss: [0.66027814]\n",
            "mse_b ====== [0.144273922]\n",
            "It: 39111, Time: 0.02\n",
            "mse_b  [0.14427392]  mse_f: 0.5560200214385986   total loss: [0.70029396]\n",
            "mse_b ====== [0.13052991]\n",
            "It: 39112, Time: 0.02\n",
            "mse_b  [0.13052991]  mse_f: 0.5532037019729614   total loss: [0.6837336]\n",
            "mse_b ====== [0.0913447961]\n",
            "It: 39113, Time: 0.02\n",
            "mse_b  [0.0913448]  mse_f: 0.6075724363327026   total loss: [0.6989172]\n",
            "mse_b ====== [0.0887768641]\n",
            "It: 39114, Time: 0.02\n",
            "mse_b  [0.08877686]  mse_f: 0.6394253969192505   total loss: [0.7282023]\n",
            "mse_b ====== [0.0739895701]\n",
            "It: 39115, Time: 0.02\n",
            "mse_b  [0.07398957]  mse_f: 0.6476866006851196   total loss: [0.7216762]\n",
            "mse_b ====== [0.0899936259]\n",
            "It: 39116, Time: 0.03\n",
            "mse_b  [0.08999363]  mse_f: 0.6603214740753174   total loss: [0.75031507]\n",
            "mse_b ====== [0.126663372]\n",
            "It: 39117, Time: 0.02\n",
            "mse_b  [0.12666337]  mse_f: 0.5907703638076782   total loss: [0.71743375]\n",
            "mse_b ====== [0.137678817]\n",
            "It: 39118, Time: 0.02\n",
            "mse_b  [0.13767882]  mse_f: 0.5891095399856567   total loss: [0.72678834]\n",
            "mse_b ====== [0.101788484]\n",
            "It: 39119, Time: 0.02\n",
            "mse_b  [0.10178848]  mse_f: 0.5663228034973145   total loss: [0.66811126]\n",
            "mse_b ====== [0.10640952]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3ad53ea0e3ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m MSE_b1, MSE_f1, weightu, weightf, loss_saman, list_model_u = fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star1,xtt,ytt,xbb,ybb,xrr,yrr,xll1,yll1,xll2,yll2, \n\u001b[0m\u001b[1;32m    534\u001b[0m                                                                  \u001b[0mtf_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m                                                                  tf_iter2=1000, newton_iter1=5000,newton_iter2=15000)\n",
            "\u001b[0;32m<ipython-input-11-3ad53ea0e3ae>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star, xtt, ytt, xbb, ybb, xrr, yrr, xll1, yll1, xll2, yll2, tf_iter, tf_iter2, newton_iter1, newton_iter2)\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mt_f_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_sz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_sz\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_sz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             loss_value, mse_b, mse_f, grads, grads_ub, grads_fu = grad(u_model, x_f_batch, y_f_batch,\n\u001b[0m\u001b[1;32m    447\u001b[0m                                                                        \u001b[0mxb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                                                                        \u001b[0mub_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_ub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "loss2=[]\n",
        "for i in range(len(loss_saman)):\n",
        "  loss2.append(loss_saman[i][0].numpy())\n",
        "print(\"Min: \", pd.Series(loss2).idxmin())\n",
        "print(\"Max: \", pd.Series(loss2).idxmax())\n",
        "print('loss_min',loss_saman[pd.Series(loss2).idxmin()])\n",
        "\n",
        "model_u=list_model_u[pd.Series(loss2).idxmin()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiDS7Tq5aLD5",
        "outputId": "55138356-2950-4708-baf5-0717a53f27bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min:  38558\n",
            "Max:  0\n",
            "loss_min tf.Tensor([0.44675377], shape=(1,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of Temeprature"
      ],
      "metadata": {
        "id": "TmmoxqvXjN29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N=100\n",
        "x1 = (np.linspace(0, xmax, N+1)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, N+1)).flatten()[:, None]\n",
        "X1, Y1 = np.meshgrid(x1, y1)\n",
        "\n",
        "X_star1 = np.hstack((X1.flatten()[:, None], Y1.flatten()[:, None]))\n",
        "\n",
        "X_star = tf.convert_to_tensor(X_star1, dtype=tf.float32)\n",
        "#up, vp, pp = u_x_model(X_star[:, 0:1], X_star[:, 1:2])\n",
        "\n",
        "UU=model_u(tf.concat([X_star[:, 0:1], X_star[:, 1:2]],1))\n",
        "\n",
        "uuu2=tf.reshape(UU,shape=[tf.shape(UU).numpy()[0]])\n",
        "U = uuu2.numpy().reshape(N+1,N+1)\n",
        "\n",
        "x1 = (np.linspace(0, xmax, N+1)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, N+1)).flatten()[:, None]\n",
        "X1, Y1 = np.meshgrid(x1, y1)\n",
        "fig, ax = plt.subplots(figsize=(4, 1))\n",
        "ax.contourf(X1,Y1,U,80,cmap='rainbow')\n",
        "\n",
        "x1 = (np.linspace(0, xmax, N+1)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, N+1)).flatten()[:, None]\n",
        "X1, Y1 = np.meshgrid(x1, y1)\n",
        "fig, ax = plt.subplots(figsize=(4, 1))\n",
        "ax.contourf(X1,Y1,U,60,vmax=1.0,vmin=0,cmap='rainbow')\n",
        "plt.axis('scaled')\n",
        "plt.title('CFD')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "MzxQh9U6aRAn",
        "outputId": "7e1a4dd6-ade2-41d3-a224-5d36988cc710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'CFD')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x72 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQcAAABZCAYAAAAzWOGUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXiUlEQVR4nO2dbYwkx1nH/8/OTk/P7t7sOrd7vksuF9tysITMi53DkQVYIQjkQGQjgWQHEZyAFD7EEOADL/kS4BMvEiJRkJF1NtiQOEE2kRwUBZAcKSAI2GcM8Us4nZzEvmCz58vtze7O9vRMz8OH7p6prqnqru7pnp49119a7Ux3TfXTNfX86qmne7qImWFlZWUla6luA6ysrBZTFg5WVlZKWThYWVkpZeFgZWWllIWDlZWVUhYOVlZWSmXCgYgeJqJtInpes5+I6FNEdJ6I/puIbi3fTCsrq3nLJHL4KwB3pux/H4B3Rn8fAfDA7GZZWVnVrUw4MPNXAXw3pcjdAB7lUF8DsEFEJ8oy0MrKqh4tl1DH2wC8Kry/EG17TS5IRB9BGF1gaaX9rluOt4FvXwaWKPxrLIX/R9Fdm8GomEUjzV2fuu2ilmi24+T5fJ56i6oMe/KqTPtjVXEeVdh5CHQ24DeYeSurXBlwMBYzPwjgQQBoXncrnz73SfzY4Byu3e9i60oX63s9dHZ76FzeAy73wr99H/AG4f99P1nhwSD7oHt+dhmV1hx1HfFrk2O3m8WObVp/Uc1il4mqtH1WFe0PZddRo+jcpW+blCsDDt8B8Hbh/cloW6qOXFrC2d++DZ//ge8H33CAt53o4R3X7eLUahfHGns4GexgfdDDtftddA4O0PIHaA0GcPsDuH0fjj8EALgHky/K8TRfWn84w+lF6vmTenrRcWJYeZIzuArnW3Wmt8mS4SdLPk4RqWxbBJmcW1b7lPm5qgC3CGA5d8moWBlweBLA/UT0OQDvBnCFmaemFLKafeD0Fx2cen4ZO8dXsP2ODfzbjX08dcLHsWMett7i4dpOD8c6PXSu6WNj6QAd9uDyAJ3AQysYoBWEztoKwi+yPZh8oc1hNhDE8gfNpnZfyw9fi3ACAMcfJuCUJa+dDYis+rQAzCHfNQBVJBObVcrTLkBOsPdS6s47EKTVJasonLJUBvTz6IkXjYplwoGIHgPwHgCbRHQBwCcANAGAmf8CwJcA/BSA8wB6AD5scmAaAZ1tgtNr4Ng3l3Dsmw2cemEZ29e56B5dxcsnB3j26ACbWz42Oj46qwOst324zSE6jg+3MYTbHMKhAC0aoE1hp3AQjI/h8nSjq7bp5HBYlzuK4BABqRUMxvBoDocJkFSlGFBVqO/UE03ozqkltafbl98nnTSOInUyBZUxeMuIRFXKAyqdSrQtEw7M/IGM/Qzgo0UNcPcAgHDsZYK7R+hcXILfXsbO8Sa6myN0jwbY2Qpw4ZohvHaA9fUhOkcGaLUCOM0RXCeA2xyitRwmL91mNN1oRLBoJJOa8XatPUvJ/Q4FQAMhgJoTALk8gMsDJUCqUhwplaV+Iztw7DdmB0dam6jOSS6fFhHKYDYFjkoyhNLLlh9FZEFOp7xRmqnmmpAUtTQZ4CNAAAChs03wVxidi0vobo3grTXgrTG6myN4qwzfHWFnK8BuawRvZQSvHaDlMNx2WGHTGaHdCl+3WpODOE0JEk4AWTFYYqmAE8PGbQzhLk0iFywhjF4iXxIjmFmUJ9LJK4/MHN9Ho/AxHATjNpGVFdnF4AUm8I0lAkQEjA5EWdGdyTTUpJ4iKhIVmsBuVtUGBwBwdwHviPBegITTA9y9JXhrgLfG2Hh9Cd4aw1tjHHtlGd4qw1sNL0X57gje6gjeSvh+txU68MWV8L8XgaPlJC9dxUAR1XQmEJEhEwNGjFZ0kYouQpEjk6JyyBw+Pps5tzeqvjuozl8+lxZFHZ8wni6iIU0ZlyfOoQPKeP9oALSnbTGJ8sqO1maNLPPAyRR4OtUGh1FKf52GRPg6BgUA+G0ev/bWGH47er2a/O+7ESBWJ04fQySW35rsGwLwIqjsYRosbjsYA6TdChLgiKMRMQKJo4+pc2zO9sVlTY+8wPyr9Qb5ukF/OH3vnO48VZLPXT4XcSoo7hPBEgOlRQMgugViDJK4TAwToa8lohXFaRfJSVUlOVoCoIQcUM10ttbIwTsSRg/xa1kiJAAIoAC8NULnYgiGzkVM4LA2cfzwdWO8D5hAQ/c+hkm4b4Q4Jo6B4rdG2Ivg8UY0pQGQmNYA01GHKHmKo5Nq6lNUnp8dPfiD6n6Hpztn+Rx1YBW3q/JJOogA6ihrHJ3I0oCmDOWeamq+MiXAZgSdSrXCwVQTSAAiKPwVhrsXvvfWwv8xMOLXIhjCcunv/fbEQURwqCORZgIa3soIfUyiDSA5lcmaxhRRuxXgoF88JzDwzYDgHWQfQ3V+slTn25YAqssViSCJYZEFkHE9DQ2cciaoq1ae6SKQAjlgDLqiqh0OcfQg5x90UoECCGEBJIEBTKAxLneQhEEaQDoXJ+87b8TlQmdKn740E1MXcdoST1liyTCZt1wDpwfClnZ7epB4KyP0r2R3pyuKc03LBalyQMB0HghQw2NcXjHtcZtDdKG/j8NtpO+vUjqYqZQFuKKqHQ5AfkDESoIC8NYApyfBIIowJp+hRPnpbQznIFmHuydPVSYRiggNb5XRuRQ6WxIaEwcUcx9AOF3pXE7/GkS4FJHTN4sO3F7xoaZzWQ8ZEZTiuarOS4SMHIHtRlM8tx0Au+HrLHgA6imN6wS4cqB2/BgsV+YIhjw5G1mz5q90oroeTX98+TTfd+SZxLa0/ENRxQDQyV9Rn7/qc+IUJJHHmMpzKMpk5DqAZL5jXnK8+T3vR3d+KmAmPqeJvFRXoeSpjTyNkacwsVS5IZVM80XzUN6cVAyRfz3+vWeZ+XRW+YWIHGKVCYVYcnQBJB1fjjQmmnZed4+mog0x0vDb8pRmUoc8nXH3VVBaUkKjSrn78/nVprfKcPfV0YUMKHdfgsWqMH0UoiBxmhNDQxd1xEpEH6J2JzdjzJoHKlM6mAGTc5GlA11Xd8OJRgsFh3lJBQwgGxphPkOuSw0MAKnQEN+L2+UpTR1S2ZZXcqJXdV5xZCUCSgURER5JcDTGUUfnciMRccj5kalcz5XlzFyPnAupQzoApCZ/d9WfyQu9NyUcdCoSZaiAIaeJddAAklMPHTCqUhkQMK1bdR6qKEvVNjE8ZHB4qzwGh++O4O4Lx1uVck+KvEtagnVcz8riRBGiVBFS2bJwyFCxaYnOoafLyuCQlTbi5lHdEYkYRamkaoe0SAwwgwYwDY6wzAhuLzuJaprMrUN+a2QEOFk7huUsHApIdZVElHmEIcrcebOAoit/WBVO3YpBI3yfBAegTsSKCVMZJiaSE6tZcvcNIpeUOtPgNlWPJvGeJguHEpQVXQD6xGd8tSQdHOp6izq96bHKUhk2q6SCRjIJPB2tyPmNRH2KhGmeBHEVV37y1qm7IiTCbtuwrqsODvHl0CxVcWUkYYcBMIC0qyXqy6zzduwylGVzmfCQAaHbBmRPdYDsqznzvrqUJd0VoSJ2XlVwMAVDVtmqwGEKjFhp4Dhs0t1PAphdPZr16o4OEGLdRZK/VedyiuSXVCpyyfqqgkNZ0oFjXvdhANk3bx025Y2QgGyY6q6I6JK4hz33Mm9ZOOSQDI0qpyZFchBlHKcOKOkSuCqlAUPn/FkRxjiRWTE8ikQmZduUxwYjOBDRnQA+ifBHAmeY+Q+l/R8C8CeYPHX608x8xtiKQ6p5wkJrQ8k5iFnqKxMsMjDyTEtkO7LuHykyNSgS7s8jcsly/jw2mDxgtgHgzwH8BMIFa54moieZWX6E7eeZ+X7jI1+FEmFRByjqVpVRyCywUNmS5SRZTjbv+0ZMYVQmgEwih9sAnGfmlwEgegT93QDMnm/9JtUiRBV1qurpSR5YAPmTwUWdrKo7WrNgVFbiUpQJHFTL3b1bUe5niegOAOcA/AYzv6ook6miv8zMc6WiDl3tUUXdCVQRFlmgiFVFMjgvVMqCiQyPMmBRVkLyiwAeY+Y+Ef0KgEcAvFcuJK6V2aFTU5WIDpTn2Q6LDgZZVfw0fZ6qGwRZikFhCglZ87yCZPIblLwqK4owgUPmcnfMLK6vdQbAH6sqEtfKPL58OnEGKgfPAsRhg4KowwKGRQfBPFVlTmVWKNQ1rXgawDuJ6HqEULgXwM+LBYjohLAE3l0AXirLQNUoWwcUDoszz6qyYVB09F50ldFORYEwCwhKvZTJzEMiuh/APyC8lPkwM79ARH8A4BlmfhLArxHRXQif7P5dAB8qYniaqgDCm8Xh01S0k1+tTp+mWYCQFwRFAFB2MnQhHhNXdSRgITBR3g5eFAJVTUeq/m3JvCMCUwjkdfy08k894hy+x8SVJQuDifJ0dhMQVOU8pln+RcqBmDqsCQBM66oCPDrVDoeyogYLhHJBYFJXmWHsrHWVefNPVbmArHqz9udx9jJ+LVr7Wpmz6M0IhCIjZ1EQlNlZq5B4bb/Kx+kB5YT/afvS6jd19LJ/Pl575FBEVwsUqgiRywRBaoc1cMaiDruov56c5Zx1balz6CxHN13GIO/TqUTVBoelANq1ANM0zzUtFlWmSUJTEKg6rq6T5+38RSTWlXbbcGzLIvygKY/z6xzfdF2Pqf0VXTk6NJHDLFAoAwKyQ5bxIJYyLgemnZtJNKAqYwSPlJFtlvBWfijJ9KPc0h+4W+YPoowSiYbOr3J8ldPrHD1r1bMqnpJ9KOCQBwxFV7jKq6qu8+cFWZ6RPG0R4SlozLhCV9GHrfru9D7xOYpZsNCBSQWVvBBLKy+3hYnjyw6vc/C61tdYaDiYQkHlUHmWuVsUlXGZKisSSIOA+LpIZx9/tsDanmmhsdtTrHVpCJNx/YYgMJnLl+H4ssOrHDxeuKalscNkkRrVilmmv4hcWDhkgUF0chkEWetciipzrmwS0hY9Xt4suOk6naIziJ1et0p4uK/YCJcl3Yrffkvh7LonRfVopiRcWt2hLerFfxPvUxxfdnjTtTwBs/U8y1zLcyHhoAODDgji9tRRsuBlplizPiBk1s+aJr3k91kAyFqoNlba6AZMj3BZI9vAl5x+fbJatCeA4mA9/N/3pZXEFDDpXhPty7nYS9qcXQc9uT1Ex09zetnJdauA62S6qrZu5W7TB7EsHBxUYIidPwZC/D52KNWK17Mm1VTqbob/y1581sQO0wy3yegfO4JusVnTTj7+rOEK1Sbq90OH7xwZAAAO+hMAxF1jDBUNTIAJUIpIXofSdJQX20F2eNHZZedWObEJANyGGSQAwGnkjygWCg4yGFRQEIEggkAVRqeNoFPHVoSiuhWJukdTTqIEpYXFqpBXN/IDSF2mfk3o7HFHzxrVVCOa6UiWJW+wDKwOktv8pNP7g+R3IsNElAgWlfKE8Fmje5rDi/tkh9Y5rYnju0v5290hc5AvDBxUYPBXWAmEGAYiBGLHjx0rK0EUlsmmaZG1CGdR3vBWF9oC4Ui3BnVIK3b2uJOLnVgezVQAyDNymcoLkl3SGyTf94dLqfsT+3w1HPKG7Gltkebs4j7ZkXVO2qJpyMlqU752d1AssqsdDllQiIHQ3RrBW+UxCLzVUWLObBoqi9JlgUWpgnk5hC0i3RLqok1yGBunVtLCWWDi+KqRTTWiiZ1Y17nH2zJGqzwjEwD4rFiCbiR0y/Y0MPxAkaAMinflrNFc3p/m6LJzy44sO6rL2TCYKmMwG3Z49qle7XAQFYNh53gIhZ3jI3Q3Qyh0jw7hrfAYBF47QMthuO1gaoQ0CQkB/QiiG3FiyaGtqUwzybOMbLpRLe7w8Taxg+s6t26EKjoSmchHI3Hn7AEvJ3ppn5vJ8gq45JUMtDQHz3Ju+b0zksqP1DBoBdmQ0KkVlB/BATXDIY4aZChsXx9g59gI3aNDdK8JsHN0gJbDOLI+wJFWgM1WAKc5mgqHYyepIgSeZWSSZWpLnvloHmcXO7jYmV0ejEeltJFH18FnlbfUnNrmU9L5PZLKkGJbDimdW9EG8jmrnFnlpKpy7UF6+zWH5Th71nGyVBscRtF37q0B3WPhtGH7+gDbp4bY2Qqw/dY+1teH6BwZ4K1rA2ys9tFaHsFtDuE2hnAao6lRMGsEUEkcFQ44uznkkStNJsfX2aKTatROG73EkUvs4HGnlTt0kc5ctg6ayTbuN6bbvN+Y/q5U5WRlObW8Xzx3ldPq2qbla7bnbEu3P9+2F1Vv5BCB4ZXvG+LCTQO88j19tI73sbXp4caNA6y3fXQcHxtNDw4FaNEAbRrCQTB2gPi/w8HUXMydIVRT2qsY2XTKO7rmCSt1YaRpxxa3y504b+eNNWsn9lrqtu3LoHDU5WSgpEl2aLENxPOXz8nt+9o6Hd/g6sKB/vMmcrzZPp9X9UUOy8D2DSOc/6EBnr+9h5M3X8Htm/s41elia3kP69THxqgHhwO4owFawQCtYIhWMBh/uboOr5KO5DrpOmER5T22so6M81M5p9yZ5Q4sd9bSO1/fIDxupXdB33Wmtnnt6W2JzzjJOnWOm3r+su29Am1jcv4qFTlWBSprrcwWgEcBvAvAJQD3MPO30urstxlPfXAfnR+9hHtueA03L7+OY8NdbB100Tk4wMZ+D63BAG5/MO7k8ZcsfqnaDl30i1EpowNXdtxYeTuLygZVHfsVd0IvBxRdNYzHGFh1prfFWkmHRUJZ7aCyuap2qrr9Z1RZa2X+MoDLzHwjEd0L4I8A3JNW7+5bRrjvl87inotP46bn/heb/7cDXO6Ff0DYcOIXJTekScMelDStaBeIIso6dhHtGbSNSZlF0Zqh82eV052zbnud3+ECqKy1Mu8G8HvR68cBfJqIiFMebX2k4+NTn3kA+PiXww0rORywV/BLK+IQph1zETULAMo870WxI0t57CxqV9G2qKEflrVW5rhMtM7FFQBHAbwhFhKXwwPQp1988fkiRleoTUg2Y8erx5KJpm2ah7LPez525Wv/+bWVuV3l2FRuP7zJpNBcE5LicnhE9IzJs/PnKWuTuRbRLmuTmYjoGZNyJrf6Za6VKZYhomUA6wgTk1ZWVodUJnAYr5VJRA7CtTKflMo8CeC+6PXPAXgqLd9gZWW1+CprrcyHAPw1EZ1HuFbmvQbHfnAGu6uStclci2iXtclMRjbVtlamlZXVYmu+DyuwsrI6NLJwsLKyUqoWOBDRnUT0P0R0noh+pw4bJHseJqJtIlqY+y6I6O1E9BUiepGIXiCijy2ATS4R/QcR/Vdk0+/XbVMsImoQ0X8S0d/XbUssIvoWEX2diJ4zvXxYtYhog4geJ6JvENFLRHS7tuy8cw7R7djnINyODeAD0u3Y87bpDgB7AB5l5pvrskMUEZ0AcIKZnyWiIwDOAviZmtuJAKwy8x4RNQH8C4CPMfPX6rIpFhH9JoDTADrM/P667QFCOAA4zczzv4lNIyJ6BMA/M/OZ6OrjCjPvqMrWETmMb8dmZh9AfDt2bWLmryK8yrIwYubXmPnZ6PUugJcQ3olap03MzHvR22b0V3tGm4hOAvhpAGfqtmWRRUTrAO5AeHURzOzrwADUAwfV7di1dvpFFxFdB+AWAP9eryXj8P05ANsA/omZa7cJwJ8B+C0A5S8YOZsYwD8S0dnopwN163oAFwH8ZTQFO0NEq7rCNiG54CKiNQBPAPh1Zu7WbQ8zB8z8gwjvlL2NiGqdhhHR+wFsM/PZOu3Q6EeY+VYA7wPw0Wj6WqeWAdwK4AFmvgXAPgBtzq8OOJjcjm0FIJrXPwHgM8z8d3XbIyoKR78C4M6aTflhAHdF8/vPAXgvEf1NvSaFYubvRP+3AXwB4ZS6Tl0AcEGI9h5HCAul6oCDye3Yb3pFyb+HALzEzH9atz0AQERbRLQRvW4jTCp/o06bmPl3mfkkM1+HsC89xcy/UKdNAEBEq1EiGVHo/pMAar0axsyvA3iViOJfZf44UlbHm/tj4nS3Y8/bDlFE9BiA9wDYJKILAD7BzA/VaRPCEfGDAL4ezfEB4OPM/KUabToB4JHoitMSgL9l5oW5dLhguhbAF0LGYxnAZ5n5y/WaBAD4VQCfiQbmlwF8WFfQ3j5tZWWllE1IWllZKWXhYGVlpZSFg5WVlVIWDlZWVkpZOFhZWSll4WBlZaWUhYOVlZVS/w+KlMc746JCrgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x72 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = (np.linspace(0, xmax, N+1)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, N+1)).flatten()[:, None]\n",
        "X1, Y1 = np.meshgrid(x1, y1)\n",
        "fig, ax = plt.subplots(figsize=(4, 1))\n",
        "ax.contourf(X1,Y1,U,80,cmap='rainbow')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "qb8UTqTOomds",
        "outputId": "00c86be5-ee39-4cad-96d9-7b2348267046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.contour.QuadContourSet at 0x7f3e88567310>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x72 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Single PINNs"
      ],
      "metadata": {
        "id": "GLKKUAH5jhEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#all in one equation\n",
        "#SAI & FI\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import scipy.io\n",
        "import math\n",
        "import matplotlib.gridspec as gridspec\n",
        "#from plotting import newfig\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras import layers, activations\n",
        "from scipy.interpolate import griddata\n",
        "#from eager_lbfgs import lbfgs, Struct\n",
        "from pyDOE import lhs\n",
        "\n",
        "weight_ub = tf.Variable([10000.0], dtype=tf.float32)\n",
        "weight_fu = tf.Variable([1.0], dtype=tf.float32)\n",
        "layer_sizes = [2, 20, 20, 20, 20, 20, 20, 20, 7]\n",
        "sizes_w = []\n",
        "sizes_b = []\n",
        "loss_saman=[]\n",
        "for i, width in enumerate(layer_sizes):\n",
        "    if i != 1:\n",
        "        sizes_w.append(int(width * layer_sizes[1]))\n",
        "        sizes_b.append(int(width if i != 0 else layer_sizes[1]))\n",
        "\n",
        "\n",
        "# L-BFGS weight getting and setting from https://github.com/pierremtb/PINNs-TF2.0\n",
        "\n",
        "def set_weights(model, w, sizes_w, sizes_b):  # 重新设置参数\n",
        "\n",
        "    for i, layer in enumerate(model.layers[1:len(sizes_w) + 1]):\n",
        "        start_weights = sum(sizes_w[:i]) + sum(sizes_b[:i])\n",
        "        end_weights = sum(sizes_w[:i + 1]) + sum(sizes_b[:i])\n",
        "        weights = w[start_weights:end_weights]\n",
        "        w_div = int(sizes_w[i] / sizes_b[i])\n",
        "        weights = tf.reshape(weights, [w_div, sizes_b[i]])\n",
        "        biases = w[end_weights:end_weights + sizes_b[i]]\n",
        "        weights_biases = [weights, biases]\n",
        "        layer.set_weights(weights_biases)\n",
        "\n",
        "\n",
        "def get_weights(model):\n",
        "    w = []\n",
        "    for layer in model.layers[1:len(sizes_w) + 1]:\n",
        "        weights_biases = layer.get_weights()\n",
        "        weights = weights_biases[0].flatten()\n",
        "        biases = weights_biases[1]\n",
        "        w.extend(weights)\n",
        "        w.extend(biases)\n",
        "    w = tf.convert_to_tensor(w)\n",
        "    return w\n",
        "\n",
        "def xavier_init(layer_sizes):\n",
        "    in_dim = layer_sizes[0]\n",
        "    out_dim = layer_sizes[1]\n",
        "    xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "    return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "\n",
        "def neural_net(layer_sizes):\n",
        "\n",
        "    input_tensor = keras.Input(shape=(layer_sizes[0],))\n",
        "\n",
        "    hide_layer_list = []\n",
        "    flag = True\n",
        "    for width in layer_sizes[1:-1]:\n",
        "        if flag:\n",
        "            x = layers.Dense(\n",
        "                width, activation=tf.nn.tanh,\n",
        "                kernel_initializer=\"glorot_normal\")(input_tensor)\n",
        "            flag = False\n",
        "        else:\n",
        "            x = layers.Dense(\n",
        "                width, activation=tf.nn.tanh,\n",
        "                kernel_initializer=\"glorot_normal\")(x)\n",
        "    output_tensor = layers.Dense(layer_sizes[-1], activation=None,kernel_initializer=\"glorot_normal\")(x)\n",
        "    print(\"xxxxxxxxxxxxxx\")\n",
        "    output0 = output_tensor[:, 0:1]\n",
        "    output1 = output_tensor[:, 1:2]\n",
        "    output2 = output_tensor[:, 2:3]\n",
        "    output3 = output_tensor[:, 3:4]\n",
        "    output4 = output_tensor[:, 4:5]\n",
        "    output5 = output_tensor[:, 5:6]\n",
        "    output6 = output_tensor[:, 6:7]\n",
        "\n",
        "    model_output = keras.models.Model(input_tensor, [output0, output1,output2,output3,output4,output5,output6])\n",
        "\n",
        "    return model_output\n",
        "\n",
        "# initialize the NN\n",
        "u_model = neural_net(layer_sizes)\n",
        "# view the NN\n",
        "u_model.summary()\n",
        "\n",
        "\n",
        "# define the loss\n",
        "def loss(x_f_batch, y_f_batch, xb, yb, ub, vb, weight_ub,weight_fu,xtop1,xtop2,xtop3,xtop4,ytop1,ytop2,ytop3,ytop4,xbottom1,xbottom2,xbottom3,xbottom4,ybottom1,ybottom2,ybottom3,ybottom4,xright,yright,xleft1,yleft1,xleft2,yleft2):\n",
        "\n",
        "    f_sai_pred, f_fi_pred,f_u_pred, f_v_pred, div_pred,f_c_pred,f_T_pred= f_model(x_f_batch, y_f_batch)\n",
        "\n",
        "\n",
        "    sai_pred,fi_pred,u_pred,v_pred,p_pred,c_pred,T_pred = u_model(tf.concat([xb, yb], 1))\n",
        "\n",
        "    #mse_b = 100*weight_ub*(tf.reduce_sum(tf.square(u_pred - ub)) + tf.reduce_sum(tf.square(v_pred - vb)))\n",
        "    #mse_b = 1*weight_ub*(tf.reduce_sum(tf.square(u_pred - ub)) + tf.reduce_sum(tf.square(v_pred - vb)))\n",
        "    loss_2 = loss_bd(xtop1,xtop2,xtop3,xtop4,ytop1,ytop2,ytop3,ytop4,xbottom1,xbottom2,xbottom3,xbottom4,ybottom1,ybottom2,ybottom3,ybottom4,xright,yright,xleft1,yleft1,xleft2,yleft2)\n",
        "    mse_b = loss_2*weight_ub#+ mse_b\n",
        "\n",
        "    mse_f = weight_fu*(tf.reduce_sum(tf.square(f_sai_pred))+tf.reduce_sum(tf.square(f_fi_pred))+tf.reduce_sum(tf.square(f_u_pred)) + tf.reduce_sum(tf.square(f_v_pred)) + tf.reduce_sum(tf.square(div_pred))+tf.reduce_sum(tf.square(f_c_pred))+tf.reduce_sum(tf.square(f_T_pred)))\n",
        "    #tf.print('reduce_max',tf.reduce_max(f_u_pred))\n",
        "    #tf.print('min or max',tf.math.minimum(f_u_pred))\n",
        "    return mse_b + mse_f, mse_b, mse_f\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def loss_bd(xtop1,xtop2,xtop3,xtop4,ytop1,ytop2,ytop3,ytop4,xbottom1,xbottom2,xbottom3,xbottom4,ybottom1,ybottom2,ybottom3,ybottom4,xright,yright,xleft1,yleft1,xleft2,yleft2):\n",
        "  saitop1,fitop1,utop1, vtop1,ptop1,ctop1,Ttop1 = u_model(tf.concat([xtop1, ytop1],1))\n",
        "  saitop2,fitop2,utop2, vtop2,ptop2,ctop2,Ttop2 = u_model(tf.concat([xtop2, ytop2],1))\n",
        "  saitop3,fitop3,utop3, vtop3,ptop3,ctop3,Ttop3 = u_model(tf.concat([xtop3, ytop3],1))\n",
        "  saitop4,fitop4,utop4, vtop4,ptop4,ctop4,Ttop4 = u_model(tf.concat([xtop4, ytop4],1))\n",
        "  saibottom1,fibottom1, ubottom1, vbottom1,pbottom1,cbottom1,Tbottom1 =u_model(tf.concat([xbottom1, ybottom1],1))\n",
        "  saibottom2,fibottom2, ubottom2, vbottom2,pbottom2,cbottom2,Tbottom2=u_model(tf.concat([xbottom2, ybottom2],1))\n",
        "  saibottom3,fibottom3, ubottom3, vbottom3,pbottom3,cbottom3,Tbottom3=u_model(tf.concat([xbottom3, ybottom3],1))\n",
        "  saibottom4,fibottom4, ubottom4, vbottom4,pbottom4,cbottom4,Tbottom4=u_model(tf.concat([xbottom4, ybottom4],1))\n",
        "  sairight,firight,uright, vright,pright,cright,Tright= u_model(tf.concat([xright, yright],1))\n",
        "  fleft1= u_model(tf.concat([xleft1, yleft1],1))\n",
        "  fleft2= u_model(tf.concat([xleft2, yleft2],1))\n",
        "\n",
        "  #tf.print('ybb3*******',ybottom3)\n",
        "  #tf.print('=======================pbottom',tf.shape(pbottom),'ybottom',tf.shape(ybottom)) reduce_sum\n",
        "\n",
        "  loss_bd = tf.reduce_sum(tf.square(saitop1-1))+tf.reduce_sum(tf.square(saitop2-0))\\\n",
        "  +tf.reduce_sum(tf.square(saitop3-1))+tf.reduce_sum(tf.square(saitop4-0))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(fitop1,ytop1)[0]))+tf.reduce_sum(tf.square(tf.gradients(fitop2,ytop2)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(fitop3,ytop3)[0]))+tf.reduce_sum(tf.square(tf.gradients(fitop4,ytop4)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(saibottom1-0))+tf.reduce_sum(tf.square(saibottom2-1))\\\n",
        "  +tf.reduce_sum(tf.square(saibottom3-0))+tf.reduce_sum(tf.square(saibottom4-1))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(fibottom1,ybottom1)[0]))+tf.reduce_sum(tf.square(tf.gradients(fibottom2,ybottom2)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(fibottom3,ybottom3)[0]))+tf.reduce_sum(tf.square(tf.gradients(fibottom4,ybottom4)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(saileft1,xleft1)[0]))+tf.reduce_sum(tf.square(tf.gradients(saileft2,xleft2)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(fileft1-1))+tf.reduce_sum(tf.square(fileft2-1))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(sairight,xright)[0]))+tf.reduce_sum(tf.square(firight-0.0))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(ptop1, ytop1)[0]))+tf.reduce_sum(tf.square(tf.gradients(ptop2, ytop2)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(ptop3, ytop3)[0]))+tf.reduce_sum(tf.square(tf.gradients(ptop4, ytop4)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(pbottom1, ybottom1)[0]))+tf.reduce_sum(tf.square(tf.gradients(pbottom2, ybottom2)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(pbottom3, ybottom3)[0]))+tf.reduce_sum(tf.square(tf.gradients(pbottom4, ybottom4)[0]))\\\n",
        "  + tf.reduce_sum(tf.square(pright))+tf.reduce_sum(tf.square(tf.gradients(pleft1, xleft1)[0]))+tf.reduce_sum(tf.square(tf.gradients(pleft2, xleft2)[0]))\\\n",
        "  + tf.reduce_sum(tf.square(uleft1-1))+tf.reduce_sum(tf.square(uleft2-1))\\\n",
        "  +tf.reduce_sum(tf.square(utop1))+tf.reduce_sum(tf.square(utop2))+tf.reduce_sum(tf.square(utop3))+tf.reduce_sum(tf.square(utop4))\\\n",
        "  +tf.reduce_sum(tf.square(ubottom1))+tf.reduce_sum(tf.square(ubottom2))+tf.reduce_sum(tf.square(ubottom3))+tf.reduce_sum(tf.square(ubottom4))\\\n",
        "  + tf.reduce_sum(tf.square(tf.gradients(uright, xright)[0]))\\\n",
        "  + tf.reduce_sum(tf.square(vleft1))+tf.reduce_sum(tf.square(vleft2))\\\n",
        "  +tf.reduce_sum(tf.square(vtop1))+tf.reduce_sum(tf.square(vtop2))+tf.reduce_sum(tf.square(vtop3))+tf.reduce_sum(tf.square(vtop4))\\\n",
        "  +tf.reduce_sum(tf.square(vbottom1))+tf.reduce_sum(tf.square(vbottom2))+tf.reduce_sum(tf.square(vbottom3))+tf.reduce_sum(tf.square(vbottom4))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(vright, xright)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(ctop1, ytop1)[0]))+tf.reduce_sum(tf.square(tf.gradients(ctop2, ytop2)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(ctop4, ytop4)[0]))+tf.reduce_sum(tf.square(tf.gradients(ctop3, ytop3)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(cbottom1, ybottom1)[0]))+tf.reduce_sum(tf.square(tf.gradients(cbottom2, ybottom2)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(cbottom3, ybottom3)[0]))+tf.reduce_sum(tf.square(tf.gradients(cbottom4, ybottom4)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(cright, xright)[0]))+tf.reduce_sum(tf.square(cleft1-1))+tf.reduce_sum(tf.square(cleft2-0.0))\\\n",
        "  +tf.reduce_sum(tf.square(Ttop1-1))+tf.reduce_sum(tf.square(Ttop2-1))\\\n",
        "  +tf.reduce_sum(tf.square(Ttop3-1))+tf.reduce_sum(tf.square(Ttop4-1))\\\n",
        "  +tf.reduce_sum(tf.square(Tbottom1-1))+tf.reduce_sum(tf.square(Tbottom2-1))\\\n",
        "  +tf.reduce_sum(tf.square(Tbottom3-1))+tf.reduce_sum(tf.square(Tbottom4-1))\\\n",
        "  +tf.reduce_sum(tf.square(tf.gradients(Tright,xright)[0]))\\\n",
        "  +tf.reduce_sum(tf.square(Tleft1))+tf.reduce_sum(tf.square(Tleft2))\n",
        "\n",
        "  #loss_bd = 0\n",
        "\n",
        "  return loss_bd\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def f_model(x, y):\n",
        "\n",
        "    sai,fi,u, v,p,c,T= u_model(tf.concat([x, y],1))\n",
        "\n",
        "    #sai_t = tf.gradients(sai, t)[0]\n",
        "    sai_x= tf.gradients(sai, x)[0]\n",
        "    sai_y= tf.gradients(sai, y)[0]\n",
        "    sai_xx= tf.gradients(sai_x, x)[0]\n",
        "    sai_yy= tf.gradients(sai_y, y)[0]\n",
        "\n",
        "    #fi_t = tf.gradients(fi, t)[0]\n",
        "    fi_x= tf.gradients(sai, x)[0]\n",
        "    fi_y= tf.gradients(sai, y)[0]\n",
        "    fi_xx= tf.gradients(sai_x, x)[0]\n",
        "    fi_yy= tf.gradients(sai_y, y)[0]\n",
        "\n",
        "\n",
        "    f_sai=(sai_xx+sai_yy)-6*sai\n",
        "    f_fi=((fi_xx)+(fi_yy))\n",
        "\n",
        "    u_x = tf.gradients(u, x)[0]\n",
        "    u_y = tf.gradients(u, y)[0]\n",
        "    u_xx = tf.gradients(u_x, x)[0]\n",
        "    u_yy = tf.gradients(u_y, y)[0]\n",
        "\n",
        "\n",
        "    v_x = tf.gradients(v, x)[0]\n",
        "    v_y = tf.gradients(v, y)[0]\n",
        "    v_xx = tf.gradients(v_x, x)[0]\n",
        "    v_yy = tf.gradients(v_y, y)[0]\n",
        "\n",
        "    p_x = tf.gradients(p, x)[0]\n",
        "    p_y = tf.gradients(p, y)[0]\n",
        "\n",
        "    c_x=tf.gradients(c,x)[0]\n",
        "    c_y=tf.gradients(c,y)[0]\n",
        "    c_xx=tf.gradients(c_x,x)[0]\n",
        "    c_yy=tf.gradients(c_y,y)[0]\n",
        "    #bu, bv, bp = u_model(tf.concat([bx, by],1))\n",
        "\n",
        "    T_x=tf.gradients(T,x)[0]\n",
        "    T_y=tf.gradients(T,y)[0]\n",
        "    T_xx=tf.gradients(T_x,x)[0]\n",
        "    T_yy=tf.gradients(T_y,y)[0]\n",
        "\n",
        "\n",
        "\n",
        "    #wh^2=6\n",
        "\n",
        "    div = u_x + v_y\n",
        "    c1 = tf.constant(0.01, dtype=tf.float32)\n",
        "    #Re=10, Sc=10,Pr=9.4\n",
        "    f_u = u*u_x + v*u_y + p_x -(1/10)*(u_xx + u_yy)+6*8*sai*fi_x/10# - ((np.pi)*tf.cos(np.pi*x)*tf.cos(np.pi*y)*tf.sin(t) - tf.cos(np.pi*y)*(tf.sin(np.pi*x))**2*tf.sin(np.pi*y)*tf.cos(t) + \\\n",
        "\n",
        "    f_v = u*v_x + v*v_y + p_y-(1/10)*(v_xx + v_yy)+6*8*sai*fi_y/10 #- (tf.cos(np.pi*x)*tf.sin(np.pi*x)*(tf.sin(np.pi*y))**2*tf.cos(t) - np.pi*tf.sin(np.pi*x)*tf.sin(np.pi*y)*tf.sin(t) - \\\n",
        "\n",
        "    f_c=u*c_x+v*c_y-((1/(10*10)))*(c_xx+c_yy)\n",
        "\n",
        "    f_T= ((u*T_x) +(v*T_y))-((1/(10*9.4))*(T_xx+T_yy)) -((1/(10*10))*(((4200/3335)*(T*c_xx))+((4200/3335)*(T*c_yy))+((2470/3335)*(T*-c_xx))+((2470/3335)*(T*-c_yy))))-((1/(10*10))*(((4200/3335)*(T_x*c_x))+((4200/3335)*(T_y*c_y))+((2470/3335)*(T_x*-c_x))+((2470/3335)*(T_y*-c_y))))\n",
        "\n",
        "\n",
        "    return f_sai, f_fi,f_u, f_v, div,f_c,f_T\n",
        "\n",
        "@tf.function\n",
        "def u_x_model(x, y):\n",
        "    sai,fi,u, v, p,c,T= u_model(tf.concat([x, y], 1))\n",
        "    return sai,fi,u,v,p,c,T\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def grad(u_model, x_f_batch, y_f_batch, xb_batch, yb_batch, ub_batch, vb_batch, weight_ub,\n",
        "         weight_fu,x_top1,x_top2,x_top3,x_top4,y_top1,y_top2,y_top3,y_top4,\n",
        "         x_bottom1,x_bottom2,x_bottom3,x_bottom4,y_bottom1,y_bottom2,y_bottom3,y_bottom4,x_right,y_right,x_left1,y_left1,x_left2,y_left2):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "        loss_value, mse_b, mse_f = loss(x_f_batch, y_f_batch, xb_batch, yb_batch, ub_batch,vb_batch, weight_ub, weight_fu,x_top1,x_top2,x_top3,x_top4,y_top1,y_top2,y_top3,y_top4,\n",
        "                                        x_bottom1,x_bottom2,x_bottom3,x_bottom4,y_bottom1,y_bottom2,y_bottom3,y_bottom4,\n",
        "                                        x_right,y_right,x_left1,y_left1,x_left2,y_left2)\n",
        "        grads = tape.gradient(loss_value, u_model.trainable_variables)\n",
        "\n",
        "        grads_ub = tape.gradient(loss_value, weight_ub)\n",
        "\n",
        "        grads_fu = tape.gradient(loss_value, weight_fu)\n",
        "\n",
        "    return loss_value, mse_b, mse_f, grads, grads_ub, grads_fu\n",
        "\n",
        "\n",
        "def fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, sai_exact1, fi_exact1,u_exact1,v_exact1,p_exact1,c_exact1,T_exact1, X_star,xtt1,xtt2,xtt3,xtt4,ytt1,ytt2,ytt3,ytt4,xbb1,xbb2,xbb3,xbb4,ybb1,ybb2,ybb3,ybb4,xrr,yrr,xll1,yll1,xll2,yll2, tf_iter, tf_iter2,newton_iter1, newton_iter2):\n",
        "\n",
        "    batch_sz = N_f\n",
        "    n_batches = N_f // batch_sz\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    tf_optimizer = tf.keras.optimizers.Adam(lr=0.005, beta_1=.99)\n",
        "    tf_optimizer_weights = tf.keras.optimizers.Adam(lr=0.003, beta_1=.99)\n",
        "    tf_optimizer_u = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
        "\n",
        "    tf.print(f\"weight_ub: {weight_ub}  weight_fu: {weight_fu}\")\n",
        "    print(\"starting Adam training\")\n",
        "\n",
        "    a = np.random.rand(1000)\n",
        "    loss_history = list(a)\n",
        "    MSE_b0 = list(a)\n",
        "    MSE_f0 = list(a)\n",
        "\n",
        "\n",
        "    MSE_b1 = []\n",
        "    MSE_f1 = []\n",
        "\n",
        "    weightu = []\n",
        "    weightf = []\n",
        "    # For mini-batch (if used)\n",
        "    for epoch in range(tf_iter):\n",
        "        for i in range(n_batches):\n",
        "            xb_batch = xb\n",
        "            yb_batch = yb\n",
        "\n",
        "            ub_batch = ub\n",
        "            vb_batch = vb\n",
        "\n",
        "            x_top1=xtt1\n",
        "            y_top1=ytt1\n",
        "\n",
        "            x_top2=xtt2\n",
        "            y_top2=ytt2\n",
        "\n",
        "            x_top3=xtt3\n",
        "            y_top3=ytt3\n",
        "\n",
        "            x_top4=xtt4\n",
        "            y_top4=ytt4\n",
        "\n",
        "            x_bottom1=xbb1\n",
        "            y_bottom1=ybb1\n",
        "\n",
        "            x_bottom2=xbb2\n",
        "            y_bottom2=ybb2\n",
        "\n",
        "            x_bottom3=xbb3\n",
        "            y_bottom3=ybb3\n",
        "\n",
        "            x_bottom4=xbb4\n",
        "            y_bottom4=ybb4\n",
        "\n",
        "            x_right=xrr\n",
        "            y_right=yrr\n",
        "\n",
        "            x_left1=xll1\n",
        "            y_left1=yll1\n",
        "\n",
        "            x_left2=xll2\n",
        "            y_left2=yll2\n",
        "\n",
        "            x_f_batch = x_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "            y_f_batch = y_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "            t_f_batch = t_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
        "\n",
        "\n",
        "            loss_value, mse_b, mse_f, grads, grads_ub, grads_fu = grad(u_model, x_f_batch, y_f_batch,\n",
        "                                                                       xb_batch, yb_batch,\n",
        "                                                                       ub_batch, vb_batch, weight_ub,\n",
        "                                                                       weight_fu,x_top1,x_top2,x_top3,x_top4,y_top1,y_top2,y_top3,y_top4,\n",
        "                                                                       x_bottom1,x_bottom2,x_bottom3,x_bottom4,y_bottom1,\n",
        "                                                                       y_bottom2,y_bottom3,y_bottom4,x_right,y_right,x_left1,y_left1,x_left2,y_left2)\n",
        "\n",
        "            tf_optimizer.apply_gradients(zip(grads, u_model.trainable_variables))\n",
        "            MSE_b0.append(mse_b)\n",
        "            MSE_f0.append(mse_f)\n",
        "\n",
        "            loss_history.append(loss_value)\n",
        "            loss_saman.append(loss_value)\n",
        "            #if loss_history[-1] < loss_history[-2] and loss_history[-2] < loss_history[-3] and loss_history[-1] < \\\n",
        "            #        loss_history[-10]:\n",
        "            #    tf_optimizer_weights.apply_gradients(zip([-grads_fu], [weight_fu]))\n",
        "            #    tf_optimizer_u.apply_gradients(zip([-grads_ub], [weight_ub]))\n",
        "\n",
        "        if epoch % 40 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('It: %d, Time: %.2f' % (epoch, elapsed))\n",
        "            tf.print(f\"mse_b  {mse_b}  mse_f: {mse_f}   total loss: {loss_value}\")\n",
        "\n",
        "            wu = weight_ub.numpy()\n",
        "            wf = weight_fu.numpy()\n",
        "\n",
        "            MSE_b1.append(mse_b)\n",
        "            MSE_f1.append(mse_f)\n",
        "\n",
        "            weightu.append(wu)\n",
        "            weightf.append(wf)\n",
        "\n",
        "            start_time = time.time()\n",
        "    tf.print(f\"weight_ub: {weight_ub}  weight_fu: {weight_fu}\")\n",
        "    sai_pred,fi_pred,u_pred,v_pred,p_pred,c_pred,T_pred = predict(X_star)\n",
        "    tf.print('epoch',epoch,'loss',loss_value)\n",
        "    #error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
        "    #print('Error u: %e' % (error_u))\n",
        "    #error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
        "    #print('Error v: %e' % (error_v))\n",
        "    #print(\"Starting L-BFGS training\")\n",
        "\n",
        "    '''\n",
        "    loss_and_flat_grad = get_loss_and_flat_grad(x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch,\n",
        "                                                vb_batch, weight_ub, weight_fu)\n",
        "\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "          get_weights(u_model),\n",
        "          Struct(), maxIter=newton_iter1, learningRate=0.8)\n",
        "\n",
        "    u_pred, v_pred, p_pred = predict(X_star)\n",
        "    error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
        "    print('Error u: %e' % (error_u))\n",
        "    error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
        "    print('Error v: %e' % (error_v))\n",
        "\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "          get_weights(u_model),\n",
        "          Struct(), maxIter=newton_iter2, learningRate=0.8)\n",
        "    '''\n",
        "    return MSE_b1, MSE_f1,  weightu, weightf,loss_saman\n",
        "\n",
        "# L-BFGS implementation from https://github.com/pierremtb/PINNs-TF2.0\n",
        "def get_loss_and_flat_grad(x_f_batch, y_f_batch , xb_batch, yb_batch,ub_batch, vb_batch,weight_ub, weight_fu):\n",
        "    def loss_and_flat_grad(w):\n",
        "        with tf.GradientTape() as tape:\n",
        "            set_weights(u_model, w, sizes_w, sizes_b)\n",
        "            loss_value, _, _ = loss(x_f_batch, y_f_batch, xb_batch, yb_batch, ub_batch, vb_batch, weight_ub, weight_fu)\n",
        "        grad = tape.gradient(loss_value, u_model.trainable_variables)\n",
        "        grad_flat = []\n",
        "        for g in grad:\n",
        "            grad_flat.append(tf.reshape(g, [-1]))\n",
        "        grad_flat = tf.concat(grad_flat, 0)\n",
        "        # print(loss_value, grad_flat)\n",
        "        return loss_value, grad_flat\n",
        "\n",
        "    return loss_and_flat_grad\n",
        "\n",
        "\n",
        "def predict(X_star):\n",
        "    X_star = tf.convert_to_tensor(X_star, dtype=tf.float32)\n",
        "    sai_star,fi_star,u_star, v_star, p_star,c_star,T_star= u_x_model(X_star[:, 0:1], X_star[:, 1:2])\n",
        "    return sai_star.numpy(), fi_star.numpy(),u_star.numpy(),v_star.numpy(),p_star.numpy(),c_star.numpy(),T_star.numpy()\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "MSE_b1, MSE_f1, weightu, weightf, loss_saman = fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu,sai_exact1,\n",
        "                                                   fi_exact1,u_exact1,v_exact1,p_exact1,c_exact1,T_exact1,X_star1,xtt1,\n",
        "                                                   xtt2,xtt3,xtt4,ytt1,ytt2,ytt3,ytt4,xbb1,xbb2,xbb3,xbb4,ybb1,ybb2,ybb3,ybb4,\n",
        "                                                   xrr,yrr,xll1,yll1,xll2,yll2,tf_iter=140000, tf_iter2=1000, newton_iter1=5000,newton_iter2=15000)\n",
        "\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "\n",
        "'''\n",
        "N_f = 10000\n",
        "Nu1 = 200\n",
        "\n",
        "weight_ub = tf.Variable([1.0], dtype=tf.float32)\n",
        "weight_fu = tf.Variable([1.0], dtype=tf.float32)\n",
        "\n",
        "x1 = (np.linspace(0, 1, 32)).flatten()[:, None]\n",
        "y1 = (np.linspace(0, 1, 32)).flatten()[:, None]\n",
        "t1 = (np.linspace(0, 1, 20)).flatten()[:, None]\n",
        "\n",
        "ttt1, ttt0 = np.meshgrid(x1, y1)\n",
        "\n",
        "tt1 = np.concatenate(([ttt1.flatten()[:, None], ttt0.flatten()[:, None], np.zeros((x1.shape[0] * y1.shape[0], 1))]), axis=1)\n",
        "x_1t = np.array([tt1[:, 0]]).T\n",
        "y_1t = np.array([tt1[:, 1]]).T\n",
        "t_1t = np.array([tt1[:, 2]]).T\n",
        "ut1 = -np.sin(t_1t) * np.sin(np.pi * x_1t) * np.sin(np.pi * x_1t) * np.sin(np.pi * y_1t) * np.cos(np.pi * y_1t)\n",
        "vt1 = np.sin(t_1t) * np.sin(np.pi * x_1t) * np.cos(np.pi * x_1t) * np.sin(np.pi * y_1t) * np.sin(np.pi * y_1t)\n",
        "\n",
        "yyy1, yyy0 = np.meshgrid(x1, t1)\n",
        "\n",
        "yy1 = np.concatenate(\n",
        "    ([yyy1.flatten()[:, None], np.min(y1) * np.ones((x1.shape[0] * t1.shape[0], 1)), yyy0.flatten()[:, None]]), axis=1)\n",
        "x_1y = np.array([yy1[:, 0]]).T\n",
        "y_1y = np.array([yy1[:, 1]]).T\n",
        "t_1y = np.array([yy1[:, 2]]).T\n",
        "uy1 = -np.sin(t_1y) * np.sin(np.pi * x_1y) * np.sin(np.pi * x_1y) * np.sin(np.pi * y_1y) * np.cos(np.pi * y_1y)\n",
        "vy1 = np.sin(t_1y) * np.sin(np.pi * x_1y) * np.cos(np.pi * x_1y) * np.sin(np.pi * y_1y) * np.sin(np.pi * y_1y)\n",
        "\n",
        "yy2 = np.concatenate(\n",
        "    ([yyy1.flatten()[:, None], np.max(y1) * np.ones((x1.shape[0] * t1.shape[0], 1)), yyy0.flatten()[:, None]]), axis=1)\n",
        "x_2y = np.array([yy2[:, 0]]).T\n",
        "y_2y = np.array([yy2[:, 1]]).T\n",
        "t_2y = np.array([yy2[:, 2]]).T\n",
        "uy2 = -np.sin(t_2y) * np.sin(np.pi * x_2y) * np.sin(np.pi * x_2y) * np.sin(np.pi * y_2y) * np.cos(np.pi * y_2y)\n",
        "vy2 = np.sin(t_2y) * np.sin(np.pi * x_2y) * np.cos(np.pi * x_2y) * np.sin(np.pi * y_2y) * np.sin(np.pi * y_2y)\n",
        "\n",
        "\n",
        "xxx1, xxx0 = np.meshgrid(y1, t1)\n",
        "\n",
        "xx1 = np.concatenate(\n",
        "    ([np.min(x1) * np.ones((y1.shape[0] * t1.shape[0], 1)), xxx1.flatten()[:, None], xxx0.flatten()[:, None]]), axis=1)\n",
        "x_1x = np.array([xx1[:, 0]]).T\n",
        "y_1x = np.array([xx1[:, 1]]).T\n",
        "t_1x = np.array([xx1[:, 2]]).T\n",
        "ux1 = -np.sin(t_1x) * np.sin(np.pi * x_1x) * np.sin(np.pi * x_1x) * np.sin(np.pi * y_1x) * np.cos(np.pi * y_1x)\n",
        "vx1 = np.sin(t_1x) * np.sin(np.pi * x_1x) * np.cos(np.pi * x_1x) * np.sin(np.pi * y_1x) * np.sin(np.pi * y_1x)\n",
        "\n",
        "xx2 = np.concatenate(\n",
        "    ([np.max(x1) * np.ones((y1.shape[0] * t1.shape[0], 1)), xxx1.flatten()[:, None], xxx0.flatten()[:, None]]), axis=1)\n",
        "x_2x = np.array([xx2[:, 0]]).T\n",
        "y_2x = np.array([xx2[:, 1]]).T\n",
        "t_2x = np.array([xx2[:, 2]]).T\n",
        "ux2 = -np.sin(t_2x) * np.sin(np.pi * x_2x) * np.sin(np.pi * x_2x) * np.sin(np.pi * y_2x) * np.cos(np.pi * y_2x)\n",
        "vx2 = np.sin(t_2x) * np.sin(np.pi * x_2x) * np.cos(np.pi * x_2x) * np.sin(np.pi * y_2x) * np.sin(np.pi * y_2x)\n",
        "\n",
        "X_u1 = np.vstack([tt1, yy1, yy2, xx1, xx2])\n",
        "u1 = np.vstack([ut1, uy1, uy2, ux1, ux2])\n",
        "v1 = np.vstack([vt1, vy1, vy2, vx1, vx2])\n",
        "\n",
        "idx_1 = np.random.choice(X_u1.shape[0], Nu1, replace=False)\n",
        "X_u_train = X_u1[idx_1, :]\n",
        "u_train = u1[idx_1, :]\n",
        "v_train = v1[idx_1, :]\n",
        "\n",
        "X1, Y1, T1 = np.meshgrid(x1, y1, t1)\n",
        "#    Exact = np.sin(np.pi*X)*np.sin(np.pi*T)*np.sin(np.pi*Z)  #100*100*100\n",
        "U_exact1 = -np.sin(T1) * np.sin(np.pi * X1) * np.sin(np.pi * X1) * np.sin(np.pi * Y1) * np.cos(np.pi * Y1)\n",
        "V_exact1 = np.sin(T1) * np.sin(np.pi * X1) * np.cos(np.pi * X1) * np.sin(np.pi * Y1) * np.sin(np.pi * Y1)\n",
        "P_exact1 = np.sin(T1) * np.sin(np.pi * X1) * np.cos(np.pi * Y1)\n",
        "\n",
        "X_star1 = np.hstack((X1.flatten()[:, None], Y1.flatten()[:, None], T1.flatten()[:, None]))\n",
        "x_star1 = np.array([X_star1[:, 0]]).T\n",
        "y_star1 = np.array([X_star1[:, 1]]).T\n",
        "t_star1 = np.array([X_star1[:, 2]]).T\n",
        "\n",
        "u_exact1 = -np.sin(t_star1) * np.sin(np.pi * x_star1) * np.sin(np.pi * x_star1) * np.sin(np.pi * y_star1) * np.cos(np.pi * y_star1)\n",
        "v_exact1 = np.sin(t_star1) * np.sin(np.pi * x_star1) * np.cos(np.pi * x_star1) * np.sin(np.pi * y_star1) * np.sin(np.pi * y_star1)\n",
        "p_exact1 = np.sin(t_star1) * np.sin(np.pi * x_star1) * np.cos(np.pi * y_star1)\n",
        "\n",
        "lb1 = X_star1.min(0)\n",
        "ub1 = X_star1.max(0)\n",
        "\n",
        "X_f_train11 = lb1 + (ub1 - lb1) * lhs(3, N_f)\n",
        "X_f = np.vstack((X_f_train11, X_u_train))\n",
        "\n",
        "xb = tf.cast(X_u_train[:, 0:1], dtype=tf.float32)\n",
        "yb = tf.cast(X_u_train[:, 1:2], dtype=tf.float32)\n",
        "tb = tf.cast(X_u_train[:, 2:3], dtype=tf.float32)\n",
        "ub = tf.cast(u_train[:, 0:1], dtype=tf.float32)\n",
        "vb = tf.cast(v_train[:, 0:1], dtype=tf.float32)\n",
        "\n",
        "\n",
        "lb = X_star1.min(0)\n",
        "rb = X_star1.max(0)\n",
        "\n",
        "x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=tf.float32)\n",
        "y_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=tf.float32)\n",
        "t_f = tf.convert_to_tensor(X_f[:, 2:3], dtype=tf.float32)\n",
        "\n",
        "start_time = time.time()\n",
        "MSE_b1, MSE_f1, weightu, weightf = fit(x_f, y_f, xb, yb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star1, tf_iter=10000, tf_iter2=1000, newton_iter1=5000,newton_iter2=15000)\n",
        "\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "u_pred, v_pred, p_pred = predict(X_star1)\n",
        "\n",
        "U_pred = u_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
        "V_pred = v_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
        "P_pred = p_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
        "\n",
        "error_uu = np.abs(u_exact1 - u_pred)\n",
        "error_vv = np.abs(v_exact1 - v_pred)\n",
        "error_pp = np.abs(p_exact1 - p_pred)\n",
        "\n",
        "error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
        "print('Error u: %e' % (error_u))\n",
        "\n",
        "error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
        "print('Error v: %e' % (error_v))\n",
        "\n",
        "error_p = np.linalg.norm(p_exact1 - p_pred, 2) / np.linalg.norm(p_exact1, 2)\n",
        "print('Error p: %e' % (error_p))\n",
        "\n",
        "dataNewNS = 'D://NS_hisyory.mat'\n",
        "scipy.io.savemat(dataNewNS, {'w_MSE_b': MSE_b1, 'w_MSE_f': MSE_f1, 'weight_u': weightu,\n",
        "                  'weight_f': weightf, 'U_pred': U_pred, 'V_pred': V_pred, 'P_pred': P_pred})\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "tQbb61JaeBAZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}